{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script.evaluate import create_qna_dataset, qna_critique, eval_ret, rag, eval_gen, filter_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import LLM, JudgeLLM\n",
    "from utils.embeddings import Embedder\n",
    "from utils.vecdb import lancedb_table, lancedb_setup\n",
    "\n",
    "\n",
    "from script.parse import parse_dir\n",
    "from script.ingest import ingest\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from rerankers import Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between Together or Bedrock for the QnA generation and RAG.\n",
    "For evaluation, prometheus-eval has been used as an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_llm = LLM(\"llama31-8\", \"together\")\n",
    "rag_llm = LLM(\"llama31-8\", \"bedrock\")\n",
    "qna_llm = LLM(\"mixtral\", \"bedrock\") # Using mixtral, we can't use system prompt\n",
    "# qna_llm = LLM(\"mixtral\", \"together\") # Using mixtral, we can't use system prompt\n",
    "judge_llm = JudgeLLM()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just generate QnA for a few files, not all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath('./data_qna/')\n",
    "docs = parse_dir(dir_path, 1000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry/doc will look something like this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/00.png\" alt=\"dataset\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a synthetic q and a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate for all the dataset. We could also choose a subset if we wanted to\n",
    "N_GENERATIONS = 395\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs =  create_qna_dataset(docs, N_GENERATIONS, qna_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One element (e.g. `outputs[0]`) would look like as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/01.png\" alt=\"qna-without-eval\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving if needed for later\n",
    "answer_path = './results/01_qna.json'\n",
    "with open(answer_path, \"w\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load if opening later\n",
    "answer_path = './results/01_qna.json'\n",
    "with open(answer_path, \"r\") as f:\n",
    "    outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will evaluate this Q and A dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "qna_critique(outputs,judge_llm) # appends to the existing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in outputs now has additional pair of data for the feedback and the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/02.png\" alt=\"qna-with-eval\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving if needed for later\n",
    "answer_path = './results/02_qna_with_critique.json'\n",
    "with open(answer_path, \"w\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load if opening later\n",
    "answer_path = './results/02_qna_with_critique.json'\n",
    "with open(answer_path, \"r\") as f:\n",
    "    outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can filter out low-quality questions based on feedback from our llm-as-a-judge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_with_critique = pd.DataFrame.from_dict(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_with_critique_filtered = filter_crit(qna_with_critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving if needed for later\n",
    "qna_with_critique_filtered.to_csv('./results/03_qna_with_critique_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load if opening later\n",
    "qna_with_critique_filtered = pd.read_csv('./results/03_qna_with_critique_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreival Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our retrieval using metrics such as hit-rate and mrr. This step won't generate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this time we will embed our entire dataset.\n",
    "\n",
    "Our entire dataset contains additional 100 files (LLM papers) from [Kaggle]. (https://www.kaggle.com/datasets/ruchi798/100-llm-papers-to-explore). We place all files in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.abspath('./data/')\n",
    "docs = parse_dir(dir_path, 1000, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BAAI/bge-small-en-v1.5'\n",
    "emb_model = Embedder(model=model_name, provider='fastembed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and add it (i.e. perform ingestion) to our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = lancedb_setup(384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = lancedb_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.count_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest(table, docs, emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.count_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a index so that we can perform text-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.create_fts_index(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our reranker models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_cross = Reranker(\n",
    "    \"mixedbread-ai/mxbai-rerank-xsmall-v1\", model_type=\"cross-encoder\"\n",
    ")\n",
    "\n",
    "reranker_colbert = Reranker(\"colbert\")\n",
    "\n",
    "reranker = {\"None\": None, \"crossencoder\": reranker_cross, \"colbert\": reranker_colbert}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate for\n",
    "- both text and vector search\n",
    "- with no reranker, cross-encode reranker, and colbert reranker\n",
    "\n",
    "for a total of six configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = []\n",
    "for search_type in [\"vector\", \"text\"]:\n",
    "    for reranker_name, reranker_function in reranker.items():\n",
    "        print(f\"Evaluating RAG - Search Type:{search_type}, ReRanker: {reranker_name}\")\n",
    "        output, mrr, hit_rate = eval_ret(\n",
    "            qna_with_critique_filtered,\n",
    "            table,\n",
    "            emb_model,\n",
    "            reranker=reranker_function,\n",
    "            reranker_name=reranker_name,\n",
    "            method=search_type,\n",
    "        )\n",
    "        answer_path = (\n",
    "            f\"./04_rag_{search_type}_{reranker_name}_mrr_{mrr}_hitrate_{hit_rate}.json\"\n",
    "        )\n",
    "        metrics_dict.append(\n",
    "            {\n",
    "                \"search_type\": search_type,\n",
    "                \"reranker\": reranker_name,\n",
    "                \"mrr\": mrr,\n",
    "                \"hit_rate\": hit_rate,\n",
    "            }\n",
    "        )\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(output, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = \"./results/04_rag_metrics.json\"\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Hit-Rate\n",
    "\n",
    "<img src=\"./img/hitrate.png\" alt=\"hitrate\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MRR\n",
    "\n",
    "<img src=\"./img/mrr.png\" alt=\"mrr\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using text search with the cross-encoder reranker has the highest MRR.\n",
    "We choose this configuration to proceed further for RAG. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take each question from the QnA dataset, get relevant docs and generate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated previously, we use text search with cross-encoder as the reranker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = rag(\n",
    "    qna_with_critique_filtered,\n",
    "    table,\n",
    "    emb_model,\n",
    "    rag_llm,\n",
    "    reranker=reranker_cross,\n",
    "    reranker_name=\"crossencoder\",\n",
    "    method=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_path = \"./results/04_rag.csv\"\n",
    "rag_df = pd.DataFrame.from_dict(outputs)\n",
    "rag_df.to_csv(rag_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_path_j = \"./results/04_rag.json\"\n",
    "\n",
    "with open(rag_path_j, \"w\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Prometheus as the llm-as-judge to evaluate our generated answer against our ground truth answers. We can then average all the scores. We obtain an average score of 4.32 on this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score = eval_gen(outputs, judge_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Score is: {avg_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_path = './results/05_rag_with_eval.json'\n",
    "with open(answer_path, \"w\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result would look something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/05.png\" alt=\"RAG-with-eval\" style=\"width: 500px;\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
