[{"question": "What is a significant advantage of State Space Models over attention mechanisms?\n", "true_answer": "State Space Models offer near-linear computational complexity, providing an advantage over attention mechanisms.", "source_doc": "multimodal.pdf", "source_id": "85b5cac71b", "retrieved_docs": ["SSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)\nmodel [153], which proposes fully diagonal parameterization of state spaces for greater efficiency.\nH3 stacks two SSMs to interact with their output and input projection, bridging the gap between\nSSMs and attention while adapting to modern hardware. Mamba [77], a selective state space model,\nhas been introduced as a strong competitor to the Transformer architecture in large language models.\nMamba incorporates a selection mechanism to eliminate irrelevant data and develops a hardware-\naware parallel algorithm for recurrent operation. This results in competitive performance compared\nto LLMs of the same capacity, with faster inference speeds that scale linearly with time and con-\nstant memory usage. In conclusion, State Space Models offer significant potential as an alternative\nto attention mechanisms by providing near-linear computational complexity and effectively captur-", "further engineering challenges and adjustments to the model that are not discussed in this paper.\n6 Conclusion\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture,\nMamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance\nof strong Transformer models. We are excited about the broad applications of selective state space models to\nbuild foundation models for di\ufb00erent domains, especially in emerging modalities requiring long context such as\ngenomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\nbackbone.\nAcknowledgments\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\nReferences", "Figure 13: The elements(left) block(middle) and architecture(right) in RWKV [151].\nThis approach parallelizes computations during training and maintains constant computational and\nmemory complexity during inference.\nState Space Models (SSMs) [152] can be formulated as a type of RNN for efficient autoregressive\ninference and have emerged as a promising alternative to attention mechanisms, offering near-linear\ncomputational complexity compared to the quadratic complexity of attention. SSMs are formulated\nas x\u2019(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), mapping a single-dimension input signal u(t) to an N-\ndimension latent state x(t) before projecting it to a single-dimension output signal y(t), with A, B, C,\nand D being parameters learned by gradient descent [152]. Several techniques have been proposed\nto enhance SSMs, such as the Structured State Space sequence model (S4) [152], which refines\nSSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)", "\u2022RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention\napproximation (attention-free Transformer (S. Zhai et al. 2021)). Its main \u201cWKV\u201d mechanism involves LTI\nrecurrences and can be viewed as the ratio of two SSMs.\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We\nhighlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei\net al. 2017), which we view as the most closely related methods to our core selective SSM.\n3 Selective State Space Models\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to\nincorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot\nuse convolutions, presenting a technical challenge of how to compute them e\ufb03ciently. We overcome this with", "Mamba: Linear-Time Sequence Modeling with Selective State Spaces\nAlbert Gu *1and Tri Dao *2\n1Machine Learning Department, Carnegie Mellon University\n2Department of Computer Science, Princeton University\nagu@cs.cmu.edu ,tri@tridao.me\nAbstract\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally\nbased on the Transformer architecture and its core attention module. Many subquadratic-time architectures\nsuch as linear attention, gated convolution and recurrent models, and structured state space models (SSMs)\nhave been developed to address Transformers\u2019 computational ine\ufb03ciency on long sequences, but they have not\nperformed as well as attention on important modalities such as language. We identify that a key weakness of\nsuch models is their inability to perform content-based reasoning, and make several improvements. First, simply"], "retrieved_docs_id": ["85b5cac71b", "53f73ec6b6", "bb2e9ee3f0", "19af82f6ad", "cb011a8185"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does VCoder enhance the object identification ability of MLLMs?\n", "true_answer": "VCoder enhances the object identification ability of MLLMs by utilizing additional perception formats, such as segmentation masks and depth maps.", "source_doc": "hallucination.pdf", "source_id": "c461600dc0", "retrieved_docs": ["task encoders are dedicated to integrating various types of latent visual information extracted by\nmultiple visual encoders. Additionally, the structural knowledge enhancement module is designed\nto utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from\nvisual inputs.\nFollowing the approach of the structural knowledge enhancement module in [ 38], another line\nof research investigates the utilization of vision tool models to enhance the perception of MLLMs.\nVCoder [ 49] utilizes additional perception formats, such as segmentation masks and depth maps,\nto enhance the object identification ability of the MLLM. Another work [ 54] ensembles additional\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "robust evaluation of object hallucination, POPE [844] pro-\nposes a polling-based object probing approach for convert-\ning object recognition into a series of binary questions, and\nthe results indicate that current MLLMs often struggle with\nobject hallucination. Cognition tasks, on the other hand, re-\nquire MLLMs to perform reasoning based on image percep-\ntion. A common reasoning task is visual question answering\n(VQA), where models answer questions about images that\ndemand reasoning about spatial relationships [845], general\nknowledge [846], or scene text [847]. To fully explore the\ncapabilities of MLLMs, HallusionBench [848] collects 200\nsophisticated visual dependent or supplement questions, on\nwhich even the most advanced MLLMs like LLaVA-1.5 [831]\nand GPT-4V [133] fail to achieve good performance.\n\u2022Evaluation paradigms. The responses of MLLMs can\nbe evaluated either in a closed-ended or an open-ended\nmanner. Traditional multimodal tasks often rely on a closed-", "ideas and foster the field\u2019s development.\nIn the realm of computer vision, object recognition is the core task, including sub-tasks such as\nobject classification [ 60], detection [ 27], and segmentation [ 37], etc. Similarly, studies on halluci-\nnation in MLLMs primarily focus on object hallucination. In pre-MLLM era, there is a pioneering\nwork on object hallucination in image captioning [ 90], evaluating object existence by comparing\ncaptions and image content. In MLLMs, object hallucination has been empirically categorized into\nthree categories: 1) category , which identifies nonexistent or incorrect object categories in the given\nimage; 2) attribute , which emphasizes descriptions of the objects\u2019 attributes, such as color, shape,\nmaterial, etc; and 3) relation , which assesses the relationships among objects, such as human-object\ninteractions or relative positions. Note that some literature may consider objects counting, objects", "image captions and object bounding boxes as visual inputs\nfor assessment. Such open-ended evaluation methods can\nimprove assessment accuracy while incurring higher costs\ndue to the involvement of humans or LLMs.\n\u2022Evaluation benchmarks. To facilitate a more thorough\nevaluation of MLLMs, various benchmarks have been devel-\noped. Part of them collect existing vision-language tasks for\ncomprehensive evaluation. For instance, LVLM-eHub [852]\naggregates 47 existing text-related visual tasks to assess\nsix distinct capabilities of MLLMs, and Reform-Eval [853]\ntakes this a step further by standardizing questions from\nexisting benchmarks into a uniform format and discusses\nhow the backbone models influence MLLMs\u2019 performance.\nIn addition to incorporating existing tasks, several work\nalso derives new questions annotated by humans or with\nthe help of LLMs. MME [839] creates a dataset by pair-\ning images from public sources with manually-collected", "truth captions. However, it may risk expressing details that it cannot discern from the image, and\ntherefore exhibit hallucinations. Thus, the authors explored approaches to enhance the model\u2019s\nend-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the\nperception limit. Regarding data, this work proposes a data filtering strategy to eliminate harmful\ntraining data that could impair the model\u2019s ability to end sequences.\n5.2 Model\n5.2.1 Scale-up Resolution. Enhancing the perception ability of MLLMs has been shown to improve\ntheir overall performance and reduce hallucination [ 14,74,75,123]. One important update when\nupgrading from LLaVA [ 75] to LLaVA-1.5 [ 74] is to scale up the CLIP ViT vision encoder from\nCLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement. Qwen-\nVL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to"], "retrieved_docs_id": ["c461600dc0", "736e8a6bfb", "595dbaf855", "c3936a45a4", "7d6e083404"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the standard self-attention mechanism's time complexity affect large language models?\n", "true_answer": "The standard self-attention mechanism has a time complexity of O(n^2), where n is the sequence length. This quadratic complexity can lead to scalability issues, particularly when dealing with long sequences in large language models (LLMs).", "source_doc": "multimodal.pdf", "source_id": "323641b323", "retrieved_docs": ["Figure 11: Organization of efficient large language models advancements.\nOccupying a significant majority of the parameter volume in MLLMs, LLM serves as a crucial entry\npoint for enhancing the efficiency of MLLMs. In this section, similar to the survey paper [160], we\nprovide a brief overview of the research progress in efficient LLMs, offering inspiration for the\ndevelopment of Efficient MLLMs.\n4.1 Attention\nIn the standard self-attention mechanism, the time complexity is O(n2), where nis the sequence\nlength. This quadratic complexity arises due to the pairwise interactions between all input tokens,\nwhich can lead to scalability issues, especially when dealing with long sequences in LLMs. To\ntackle this, researchers have developed techniques to expedite attention mechanisms and reduce\ntime complexity, such as sharing-based attention, feature information reduction, kernelization or\nlow-rank, fixed and learnable pattern strategies, and hardware-assisted attention.", "ory. Observing a serial-position-like effect in lan-\nguage models is perhaps surprising, since the self-\nattention mechanisms underlying Transformer lan-\nguage models is technically equally capable of re-\ntrieving any token from their contexts.\n7 Conclusion\nWe empirically study how language models use\nlong input contexts via a series of controlled ex-\nperiments. We show that language model perfor-\nmance degrades significantly when changing the\nposition of relevant information, indicating that\nmodels struggle to robustly access and use infor-\nmation in long input contexts. In particular, per-\nformance is often lowest when models must use\ninformation in the middle of long input contexts.\nWe conduct a preliminary investigation of the role\nof (i) model architecture, (ii) query-aware contextu-\nalization, and (iii) instruction fine-tuning to better\nunderstand how they affect how language models\nuse context. Finally, we conclude with a practi-\ncal case study of open-domain question answering,", "and reinforcement learning [ JLL21 ,CLR+21,WWX+22]. Remarkable success of the self-attention mechanism and\ntransformers has paved the way for the development of sophisticated language models such as GPT4 [ Ope23 ], Bard\n[Goo23], LLaMA [TLI+23], and ChatGPT [Ope22].\nQ:Can we characterize the optimization landscape and implicit bias of transformers?\nHow does the attention layer select and compose tokens when trained with gradient descent?\nWe address these questions by rigorously connecting the optimization geometry of the attention layer and a hard\nmax-margin SVM problem, namely (Att-SVM) , that separates and selects the optimal tokens from each input sequence.\nThis formalism, which builds on the recent work [ TLZO23 ], is practically meaningful as demonstrated through\nexperiments, and sheds light on the intricacies of self-attention. Throughout, given input sequences X,Z\u2208RT\u00d7dwith\nlength Tand embedding dimension d, we study the core cross-attention and self-attention models:", "Model Time Space\nTransformer O(T2d) O(T2+T d)\nReformer O(TlogT d)O(TlogT+T d)\nPerformer O(T d2logd)O(T dlogd+d2logd)\nLinear Transformers O(T d2) O(T d+d2)\nAFT-full O(T2d) O(T d)\nAFT-local O(T sd) O(T d)\nMEGA O(cT d) O(cd)\nRWKV (ours) O(Td) O(d)\nTable 1: Inference complexity comparison with different\nTransformers. Here Tdenotes the sequence length,\ndthe feature dimension, cis MEGA\u2019s chunk size of\nquadratic attention, and sis the size of a local window\nfor AFT.\nLLaMA (Touvron et al., 2023), and Chinchilla\n(Hoffmann et al., 2022) showcase the potential of\nTransformers in NLP. However, the self-attention\nmechanism\u2019s quadratic complexity makes it compu-\ntationally and memory intensive for tasks involving\nlong sequences and constrained resources. This\nhas stimulated research to enhance Transformers\u2019\nscalability, sometimes sacrificing some of their ef-\nfectiveness (Wang et al., 2020; Zaheer et al., 2020;\nDao et al., 2022a).\nTo tackle these challenges, we introduce the Re-", "2.1 Transformer-Based Large Language Models\nThe task of language modeling is to model the probability\nof a list of tokens(\ud835\udc651,...,\ud835\udc65\ud835\udc5b).Since language has a natural\nsequential ordering, it is common to factorize the joint prob-\nability over the whole sequence as the product of conditional\nprobabilities (a.k.a. autoregressive decomposition [3]):\n\ud835\udc43(\ud835\udc65)=\ud835\udc43(\ud835\udc651)\u00b7\ud835\udc43(\ud835\udc652|\ud835\udc651)\u00b7\u00b7\u00b7\ud835\udc43(\ud835\udc65\ud835\udc5b|\ud835\udc651,...,\ud835\udc65\ud835\udc5b\u22121).(1)\nTransformers [ 53] have become the de facto standard ar-\nchitecture for modeling the probability above at a large scale.\nThe most important component of a Transformer-based lan-\nguage model is its self-attention layers. For an input hidden\nstate sequence(\ud835\udc651,...,\ud835\udc65\ud835\udc5b) \u2208R\ud835\udc5b\u00d7\ud835\udc51, a self-attention layer\nfirst applies linear transformations on each position \ud835\udc56to get\nthe query, key, and value vectors:\n\ud835\udc5e\ud835\udc56=\ud835\udc4a\ud835\udc5e\ud835\udc65\ud835\udc56, \ud835\udc58\ud835\udc56=\ud835\udc4a\ud835\udc58\ud835\udc65\ud835\udc56, \ud835\udc63\ud835\udc56=\ud835\udc4a\ud835\udc63\ud835\udc65\ud835\udc56. (2)\nThen, the self-attention layer computes the attention score\n\ud835\udc4e\ud835\udc56\ud835\udc57by multiplying the query vector at one position with all\nthe key vectors before it and compute the output \ud835\udc5c\ud835\udc56as the"], "retrieved_docs_id": ["323641b323", "e8c8d8c36a", "9b0e4abb0c", "3f7442053f", "b07a265486"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the source of ground truth objects used in the CCEval metric?\n", "true_answer": "The source of ground truth objects used in the CCEval metric is Visual Genome.", "source_doc": "hallucination.pdf", "source_id": "6e78496733", "retrieved_docs": ["randomly samples 100 images from Visual Genome to form a benchmark. In evaluation, GPT-4\nis utilized to parse the captions generated by MLLMs and extract objects. Additionally, this work\nintroduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\nThis metric computes the ratio of objects in the caption that match the ground truth to the total\nnumber of ground truth objects. It additionally records the average number of objects as well as\nthe average length of captions as auxiliary metric. Compared with CHAIR, CCEval employs more\ndiverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption\nparsing (GPT-4 vs. rule-based tool).\nMERLIM [ 100]MERLIM ( Multi-modal Evaluation benchma Rk for Large Image-language\nModels) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks,\nincluding object recognition, instance counting, and identifying object-to-object relationships.", "MHaluBench [13] arXiv\u201924 Feb. MSCOCO [70] 1,860 Gen Acc/P/R/F \u2713 \u2713 \u2717 T2I\nVHTest [46] arXiv\u201924 Feb. MSCOCO [70] 1,200 Dis & Gen Acc \u2713 \u2713 \u2717 \u2713\nHal-Eavl [53] arXiv\u201924 Feb.MSCOCO [70] &\nLAION [92]10,000 Dis & GenAcc/P/R/F &\nLLM Assessment\u2713 \u2713 \u2713 Obj. Event\n(denoted as CHAIR \ud835\udc60):\nCHAIR \ud835\udc56=|{hallucinated objects }|\n|{all objects mentioned }|,\nCHAIR \ud835\udc60=|{sentences with hallucinated object }|\n|{all sentences}|.\nIn the paper of CHAIR [ 90], the range of objects is restricted to the 80 MSCOCO objects. Sentence\ntokenization and synonyms mapping are applied to determine whether a generated sentence\ncontains hallucinated objects. Ground-truth caption and object segmentations both serve as ground-\ntruth objects in the computation. In the MLLM era, this metric is still widely used for assessing the\nresponse of MLLMs.\nPOPE [ 69]. When used in MLLMs, the work of [ 69] argues that the CHAIR metric can be\naffected by the instruction designs and the length of generated captions. Therefore, it proposes a", "incomplete entity triple consisting of (subject, predicate, ?), the goal is to predict the missing object entity.\nOur evaluation metric is 5-shot Accuracy@ K(K=1,5), where Accuracy@ Kindicates whether one of the\nsystem\u2019s top Kpredictions matches the ground-truth label.\nDatasets. Our fact completion setup described above was inspired by the LAMA probe dataset (Petroni\net al., 2019). LAMA was an early work that established the method to probe LMs\u2019 knowledge through a fact\ncompleton prompt like \u201cThe capital of France is __\u201d. However, the original LAMA dataset only covered\n\u223c40 types of generic relational knowledge (e.g., place of birth). Here we curate a more diverse dataset to\nevaluate LMs.\nSpecifically, we identified 12 domains spanning the humanities (e.g., law), social sciences (e.g., economics),\nSTEM (e.g., biomedicine), and other general facts. For each domain, we manually identified 2-7 Wikidata", "our evaluation samples a prefix of some copyrighted content which likely appears in the pretraining corpus,\nsamples completions from target models, and measures the overlap between the completion and reference.\nData. We consider three sources for running these \u201cextraction attacks\u201d:\n(1) Books from the books corpus106,\n(2) popular books from a bestseller list,107and\n(3) source code of the linux kernel (licensed under GPL).108\nPre-processing. We sampled 1k instances for (1), the prompts of which begin from randomly selected\nsentences. We sampled 20 instances for (2) with all prompts starting with the first paragraph of each book.\nWe sampled 1k instances of (3), the prompt of which begin from randomly selected lines.\nData Resources. We list the sources of the our preprocessed data splits (including prompts and ground\ntruth completion):\n\u2022books\n\u2022popular books\n\u2022source code\n106https://github.com/soskek/bookcorpus", "OpenCHAIR [ 5]The traditional CHAIR metric relies on the closed list of 80 objects in the\nMS-COCO dataset, limiting its application. To measure object hallucination in the open-vocabulary\nsettings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\nThe \u2019open-vocabulary\u2019 manifests in two ways. Firstly, when building the benchmark, it organizes a\ndataset consisting of synthetic images with corresponding captions, which include diverse, open-\nvocabulary objects using a text-to-image diffusion model. Secondly, during computing the metric,\nCHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in ground-\ntruth annotations. In contrast, OpenCHAIR extracts concrete objects from a predicted caption and\nidentifies hallucinated objects from this list by querying an LLM. Similar to CHAIR, the final metric\ncomputation is based on the hallucination rate."], "retrieved_docs_id": ["6e78496733", "84a3c00c17", "12ecf33cbb", "0bbb8bf844", "c7d602443c"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a challenge in the integration process of retrieved passages in the generation task?\n", "true_answer": "If not handled properly, the output might appear incoherent or disjointed.", "source_doc": "RAG.pdf", "source_id": "e75af48a5e", "retrieved_docs": ["equally diverse. Hallucination is a prominent issue where the\nmodel fabricates an answer that doesn\u2019t exist in the context.\nIrrelevance is another concern where the model generates an\nanswer that fails to address the query. Further, toxicity or\nbias, where the model generates a harmful or offensive re-\nsponse, is another problem.\nFinally, the augmentation process also faces several chal-\nlenges. Crucially, the effective integration of the context from\nretrieved passages with the current generation task is of ut-\nmost importance. If mishandled, the output might appear in-\ncoherent or disjointed. Redundancy and repetition are another\nissue, particularly when multiple retrieved passages contain\nsimilar information, leading to content repetition in the gen-\neration step. Moreover, determining the importance or rele-\nvance of multiple retrieved passages to the generation task is\nchallenging, and the augmentation process needs to balance", "challenging, and the augmentation process needs to balance\nthe value of each passage appropriately. The retrieved con-\ntent may also come from different writing styles or tones, and\nthe augmentation process needs to reconcile these differences\nto ensure output consistency. Lastly, generation models may\noverly rely on augmented information, resulting in output thatmerely repeats the retrieved content, without providing new\nvalue or synthesized information.\n3.2 Advanced RAG\nAdvanced RAG has made targeted improvements to over-\ncome the deficiencies of Naive RAG. In terms of the quality\nof retrieval generation, Advanced RAG has incorporated pre-\nretrieval and post-retrieval methods. To address the indexing\nissues encountered by Naive RAG, Advanced RAG has op-\ntimized indexing through methods such as sliding window,\nfine-grained segmentation, and metadata. Concurrently, it has\nput forward various methods to optimize the retrieval process.", "depending on the needs of different tasks. If there is historical\ndialogue information, it can also be merged into the prompt\nfor multi-round dialogues.\nDrawbacks in Naive RAG\nThe Naive RAG confronts principal challenges in three ar-\neas: retrieval quality, response generation quality, and the\naugmentation process.\nRegarding retrieval quality, the issues are multifaceted.\nThe primary concern is low precision, where not all blocks\nwithin the retrieval set correlate with the query, leading to\npotential hallucination and mid-air drop issues. A secondary\nissue is low recall, which arises when not all relevant blocks\nare retrieved, thereby preventing the LLM from obtaining suf-\nficient context to synthesize an answer. Additionally, out-\ndated information presents another challenge, where data re-\ndundancy or out-of-date data can result in inaccurate retrieval\noutcomes.\nIn terms of response generation quality, the issues are\nequally diverse. Hallucination is a prominent issue where the", "Figure 12: Example of information retrieval (passage ranking). An example instance for information\nretrieval from MS MARCO.\nWe focus here on the passage ranking task: given a query qand a large corpus Cof passages, systems\nmust output a list of the top- kpassages from Cin decreasing \u201crelevance\u201d to q. We specifically study this\nin the context of re-ranking : since Cis typically extremely large (e.g. |C|>10Mpassages), we consider\nonly ranking the top- kpassages among a set retrieved for q(i.e.M(q)where|M(q)|\u226a|C|) by an efficient\nexternal retrieval mechanism (e.g. BM25; Robertson & Zaragoza, 2009).\nIR differs fundamentally from the other tasks we consider in this work, as each test example (i.e. a query)\nentails processing a large set of passages and will likely invoke the LM numerous times to do so.37Because\nof this, IR tasks have received very little attention in the few-shot in-context learning with language models,", "information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-"], "retrieved_docs_id": ["e75af48a5e", "873e6df003", "b66fd4b3d0", "fb0c34583a", "7fabdba415"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the ChipNeMo project adapt large language models for chip design?\n", "true_answer": "The ChipNeMo project adapts large language models for chip design using domain-adaptive tokenization, domain-adaptive continued pretraining, and model alignment techniques.", "source_doc": "ChipNemo.pdf", "source_id": "36c5c0c7f1", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\nMingjie Liu* 1Teodor-Dumitru Ene* 1Robert Kirby* 1Chris Cheng* 1Nathaniel Pinckney* 1\nRongjian Liang* 1Jonah Alben1Himyanshu Anand1Sanmitra Banerjee1Ismet Bayraktaroglu1\nBonita Bhaskaran1Bryan Catanzaro1Arjun Chaudhuri1Sharon Clay1Bill Dally1Laura Dang1\nParikshit Deshpande1Siddhanth Dhodhi1Sameer Halepete1Eric Hill1Jiashang Hu1Sumit Jain1\nAnkit Jindal1Brucek Khailany1George Kokai1Kishor Kunal1Xiaowei Li1Charley Lind1Hao Liu1\nStuart Oberman1Sujeet Omar1Ghasem Pasandi1Sreedhar Pratty1Jonathan Raiman1Ambar Sarkar1\nZhengjiang Shao1Hanfei Sun1Pratik P Suthar1Varun Tej1Walker Turner1Kaizhe Xu1Haoxing Ren1\nAbstract\nChipNeMo aims to explore the applications of\nlarge language models (LLMs) for industrial chip\ndesign. Instead of directly deploying off-the-\nshelf commercial or open-source LLMs, we in-\nstead adopt the following domain adaptation tech-\nniques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 4: Domain-Adapted ChipNeMo Tokenizer Improvements.\n3.1. Domain-Adaptive Tokenization\nWe adapt the LLaMA2 tokenizer (containing 32K tokens) to\nchip design datasets using the previously outlined four-step\nprocess. Approximately 9K new tokens are added to the\nLLaMA2 tokenizer. The adapted tokenizers can improve\ntokenization efficiency by 1.6% to 3.3% across various chip\ndesign datasets as shown in Figure 4. We observe no obvious\nchanges to tokenizer efficiency on public data. Importantly,\nwe have not observed significant decline in the LLM\u2019s accu-\nracy on public benchmarks when using the domain-adapted\ntokenizers even prior to DAPT.\n3.2. Domain Adaptive Pretraining\nFigure 5: Chip Domain Benchmark Result for ChipNeMo.\nFigure 5 presents the outcomes for ChipNeMo models on\nthe AutoEval benchmark for chip design domain (detailed\nin Appendix A.5). Results on open domain academic bench-\nmark results are presented in Appendix A.6. Our research", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ntask assignnment. Participants are tasked with rating the\nmodel\u2019s performance on a 7-point Likert scale for each of\nthese three assignments. The results can be found in Fig-\nure 10. Although the GPT-4 model excels in all three tasks,\noutperforming both our ChipNeMo-70B-Steer model and\nthe LLaMA2-70B-Chat model, ChipNeMo-70B-Steer does\nexhibit enhancements compared to the off-the-shelf LLaMA\nmodel of equivalent size. We attribute the comparatively\nlower improvements in summarization tasks resulting from\nour domain-adaptation to the limited necessity for domain-\nspecific knowledge in summarization compared to other\nuse-cases.\n4. Related Works\nMany domains have a significant amount of proprietary data\nwhich can be used to train a domain-specific LLM. One ap-\nproach is to train a domain specific foundation model from\nscratch, e.g., BloombergGPT(Wu et al., 2023) for finance,\nBioMedLLM(Venigalla et al., 2022) for biomed, and Galac-"], "retrieved_docs_id": ["36c5c0c7f1", "a6c3d05123", "df0b9868f2", "ac7c0c980b", "74fe22ec46"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is one of the benchmarks used for evaluating video comprehension in multimodal large language models?\n", "true_answer": "Video-LLaV A", "source_doc": "multimodal.pdf", "source_id": "d85947fa4f", "retrieved_docs": ["121\n\u201cReform-eval: Evaluating large vision language mod-\nels via unified re-formulation of task-oriented bench-\nmarks,\u201d CoRR , vol. abs/2310.02569, 2023.\n[854] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and\nY. Shan, \u201cSeed-bench: Benchmarking multimodal\nllms with generative comprehension,\u201d CoRR , vol.\nabs/2307.16125, 2023.\n[855] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu,\nX. Wang, and L. Wang, \u201cMm-vet: Evaluating large\nmultimodal models for integrated capabilities,\u201d CoRR ,\nvol. abs/2308.02490, 2023.\n[856] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y. Jiang,\n\u201cTo see is to believe: Prompting GPT-4V for better\nvisual instruction tuning,\u201d CoRR , vol. abs/2311.07574,\n2023.\n[857] Y. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang,\nand T. Sun, \u201cLlavar: Enhanced visual instruction tun-\ning for text-rich image understanding,\u201d arXiv preprint\narXiv:2306.17107 , 2023.\n[858] X. Qi, K. Huang, A. Panda, M. Wang, and P . Mittal,\n\u201cVisual adversarial examples jailbreak aligned large", "FastV[46], VTW[47]\nTraining (\u00a75)Pre-Training (\u00a75.1) Idefics2[48], TinyLLaV A[23], VILA[49]\nInstruction-Tuning (\u00a75.2) LaVIN[50], HyperLLaV A[51]\nDiverse Training Steps (\u00a75.3) SPHINX-X[14], Cobra[13], TinyGPT-V[28]\nParameter Efficient\nTransfer Learning (\u00a75.4)EAS [52], MemVP [53]\nData and Benchmarks (\u00a76)Pre-Training Data (\u00a76.1)CC595k[7], LLava-1.5-PT[54],\nShareGPT4V-PT[55],\nBunny-pretrain-LAION-2M[24],\nALLaV A-Caption-4V[29], etc.\nInstrcution-Tuning Data (\u00a76.2)LLaV A\u2019s IT[7], LLaV A-1.5\u2019s IT[54],\nShareGPT4V\u2019s IT[55], Bunny-695K[24],\nLVIS-INSTRUCT-4V[56], etc.\nBenchmarks (\u00a76.3)VQAv2[57], TextVQA[58], GQA[59],\nMME[60], MMBench[61], POPE[62]\nApplication (\u00a77)Biomedical Analysis (\u00a77.1) LLaV A-Rad [63], MoE-TinyMed [64]\nDocument Understanding (\u00a77.2)TextHawk [36], TinyChart [37],\nMonkey [65], HRVDA [66]\nVideo Comprehension (\u00a77.3)mPLUG-video [67], Video-LLaV A [44],\nMA-LMM [68], LLaMA-VID [69]\nFigure 2: Organization of efficient multimodal large language models advancements.", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "dataset. Visual features such as captions and\nbounding boxes were used to encode images.\nLLaV A yields a 85.1% relative score compared\nwith GPT-4 on a synthetic multimodal instruction\nfollowing dataset. When fine-tuned on Science QA,\nthe synergy of LLaV A and GPT-4 achieves a new\nstate-of-the-art accuracy of 92.53%.\nVideo-LLaMA (Zhang et al., 2023b) is\na multimodal framework that enhances large\nlanguage models with the ability to understand\nboth visual and auditory content in videos. The\narchitecture of Video-LLaMA consists of two\nbranche encoders: the Vision-Language (VL)\nBranch and the Audio-Language (AL) Branch, and\na language decoder (Vicuna (7B/13B) (Chiang\net al., 2023), LLaMA (7B) (Touvron et al.,\n2023a), etc.). The VL Branch includes a frozen\npre-trained image encoder (pre-trained vision\ncomponent of BLIP-2 (Li et al., 2023d), which\nincludes a ViT-G/14 and a pre-trained Q-former),\na position embedding layer, a video Q-former and\na linear layer. The AL Branch includes a pre-", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the"], "retrieved_docs_id": ["2678016e21", "d85947fa4f", "542e5c49da", "559c71259f", "114f3dada8"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "How does LURE correct hallucinations in generated text?\n", "true_answer": "LURE corrects hallucinations in generated text using a hallucination revisor, which transforms potentially hallucinatory descriptions into accurate ones. This is achieved by training the revisor model on a dataset, with the goal of reconstructing clean data from corrupted input.", "source_doc": "hallucination.pdf", "source_id": "ceeab98980", "retrieved_docs": ["reduce hallucination. Visual context refers to the visual tokens that can be grounded from the\ngenerated text response. An oracle study showed that decoding from the provided optimal visual\ncontexts eliminates over 84.5% of hallucinations. Based on the insight and observation, the authors\ndesigned mechanisms to locate the fine-grained visual information to correct each generated\ntoken that might be hallucinating. This is essentially a visual content-guided decoding strategy.\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that\nutilizes a visual matching score to steer the generation of the final outputs, balancing both object\nhallucination mitigation and text generation quality.\nOthers. The work of OPEAR [ 45] makes an interesting observation that most hallucinations\nare closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,", "Hallucination of Multimodal Large Language Models: A Survey 21\n5.4.2 Post-hoc Correction. Post-hoc correction refers to first allowing the MLLM to generate a text\nresponse and then identifying and eliminating hallucinating content, resulting in less hallucinated\noutput. This is usually achieved by grounding on visual content [ 114], pre-trained revisior [ 137],\nand self-revision [63].\nWoodpecker [ 114] is an early attempt on hallucination detection and correction. Similar to how\na woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated\ntext. The key idea of Woodpecker is to extract key concepts from the generated text and validate\nthem using visual content. Subsequently, the hallucinated concepts can be detected and corrected\naccordingly. Specifically, it consists of five stages: 1) Key concept extraction identifies the main objects\nmentioned in the generated sentences; 2) Question formulation asks questions around the extracted", "mentioned in the generated sentences; 2) Question formulation asks questions around the extracted\nobjects; 3) Visual knowledge validation answers the formulated questions via expert models; 4)\nVisual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge\nbase; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence\nunder the guidance of the visual knowledge base. Woodpecker is a training-free method, where\neach component can be implemented using either hand-crafted rules or off-the-shelf pre-trained\nmodels.\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\nSpecifically, inspired by denoising autoencoders [ 101], which are designed to reconstruct clean data\nfrom corrupted input, LURE [ 137] employs a hallucination revisor that aims to transform potentially\nhallucinatory descriptions into accurate ones. To train such a revisor model, a dataset has been", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "2.3 Hallucinations in Multimodal Large Language Models\nHallucination of MLLM generally refers to the phenomenon where the generated text response\ndoes not align with the corresponding visual content. State-of-the-art studies in this field primarily\nfocus on object hallucination, given that objects are central to research in computer vision and\nmultimodal contexts. Regarding inconsistency, two typical failure modes are: 1) missing objects,\nand 2) describing objects that are not present in the image or with incorrect statements. Empirically,\nthe second mode has been shown to be less preferable to humans. For example, the LSMDC\nchallenge [ 91] shows that correctness is more important to human judges than specificity. In\ncontrast, the coverage of objects is less perceptible to humans. Thus, object coverage is not a\nprimary focus in studies of object hallucination. Empirically, object hallucination can be categorized"], "retrieved_docs_id": ["31eefbd9eb", "b4dda01e19", "ceeab98980", "92e73c053a", "f58cf51d02"], "reranker_type": "colbert", "search_type": "text", "rr": 0.3333333333333333, "hit": 1}, {"question": "How does the Knowledge Updates feature affect the retrieval knowledge base?\n", "true_answer": "The Knowledge Updates feature directly updates the retrieval knowledge base, ensuring information remains current without the need for frequent retraining. This is suitable for dynamic data environments.", "source_doc": "RAG.pdf", "source_id": "9c38efbac6", "retrieved_docs": ["Feature Comparison RAG Fine-tuning\nKnowledge UpdatesDirectly updates the retrieval knowledge\nbase, ensuring information remains current\nwithout the need for frequent retraining, suit-\nable for dynamic data environments.Stores static data, requiring retraining for\nknowledge and data updates.\nExternal KnowledgeProficient in utilizing external resources,\nparticularly suitable for documents or other\nstructured/unstructured databases.Can be applied to align the externally learned\nknowledge from pretraining with large lan-\nguage models, but may be less practical for\nfrequently changing data sources.\nData ProcessingRequires minimal data processing and han-\ndling.Relies on constructing high-quality datasets,\nand limited datasets may not yield significant\nperformance improvements.\nModel CustomizationFocuses on information retrieval and inte-\ngrating external knowledge but may not fully\ncustomize model behavior or writing style.Allows adjustments of LLM behavior, writ-", "employs a graph encoding method that reflects the graph\nstructure into PTMs\u2019 representation space and utilizes a\nmulti-modal contrastive learning objective between graph-\ntext modes to ensure consistency between retrieved facts\nand generated text. KnowledgeGPT [Wang et al. , 2023c ]\ngenerates search queries for Knowledge Bases (KB) in code\nformat and includes predefined KB operation functions.\nApart from retrieval, KnowledgeGPT also offers the ca-\npability to store knowledge in a personalized knowledge\nbase to meet individual user needs. These structured data\nsources provide RAG with richer knowledge and context,\ncontributing to improved model performance.\nLLM Generated Content RAG\nObserving that the auxiliary information recalled by RAG\nis not always effective and may even have negative effects,\nsome studies have expanded the paradigm of RAG by delving\ndeeper into the internal knowledge of LLM. This approach\nutilizes the content generated by LLM itself for retrieval, aim-", "outperform 10\u00d7larger ones [653, 657]. Further, open-book\nQA tasks can be also employed to evaluate the recency\nof knowledge information. Pre-training or retrieving from\noutdated knowledge resources may cause LLMs to generate\nincorrect answers for time-sensitive questions [653].\nKnowledge Completion. In knowledge completion tasks,\nLLMs might be (to some extent) considered as a knowledge\nbase [576], which can be leveraged to complete or predict the\nmissing parts of knowledge units ( e.g., knowledge triples).\nSuch tasks can probe and evaluate how much and what kind\nofknowledge LLMs have learned from the pre-training\ndata. Existing knowledge completion tasks can be roughly\ndivided into knowledge graph completion tasks ( e.g.,FB15k-\n237 [572] and WN18RR [574]) and fact completion tasks ( e.g.,\nWikiFact [571]), which aim to complete the triples from a\nknowledge graph and incomplete sentences about specific\nfacts, respectively. Empirical studies have revealed that it", "swers given a retrieval-enhanced directive. It updates the gen-\nerator and retriever to minimize the semantic similarity be-\ntween documents and queries, effectively leveraging relevant\nbackground knowledge.\nAdditionally, SUGRE [Kang et al. , 2023 ]introduces the\nconcept of contrastive learning. It conducts end-to-end fine-\ntuning of both retriever and generator, ensuring highly de-\ntailed text generation and retrieved subgraphs. Using a\ncontext-aware subgraph retriever based on Graph Neural Net-\nworks (GNN), SURGE extracts relevant knowledge from a\nknowledge graph corresponding to an ongoing conversation.\nThis ensures the generated responses faithfully reflect the re-\ntrieved knowledge. SURGE employs an invariant yet efficient\ngraph encoder and a graph-text contrastive learning objective\nfor this purpose.\nIn summary, the enhancement methods during the fine-\ntuning phase exhibit several characteristics. Firstly, fine-\ntuning both LLM and retriever allows better adaptation", "Retrieval-Augmented LLM. Due to the huge amount of\nfact records in a KG, existing work typically adopts a\nretrieval model to first obtain a relatively small subgraph\nfrom KG, and then leverages it to enhance LLMs by en-\nriching the relevant knowledge. Before the advent of LLMs,\nthe retrieved subgraphs are often supplemented into train-\ning data, injecting knowledge information into PLMs via\nparameter learning [863\u2013865]. In contrast, to leverage the\nretrieved knowledge, LLMs mainly incorporate it as part of\nthe prompt, without parameter update. To implement this\napproach, there are two main technical problems, i.e.,how\nto retrieve relevant knowledge from KGs and how to make\nbetter use of the structured data by LLMs. For the first issue\n(i.e.,retrieving relevant knowledge), a typical approach is\nto train a small language model ( e.g., RoBERTa) to iden-\ntify question-related fact triples [866]. To further improve\nthe retrieval performance, several studies also propose an"], "retrieved_docs_id": ["9c38efbac6", "4c3ac6cb2e", "e798d8fddc", "977e0e1405", "d0140a8a43"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How is the \"Faithfulness Score\" calculated in the given context?\n", "true_answer": "The \"Faithfulness Score\" is calculated by comparing the number of statements supported by the context to the total number of statements, using a large language model (LLM) to break down the context into individual statements and verify their consistency with the original context.", "source_doc": "RAG.pdf", "source_id": "716582522f", "retrieved_docs": ["into individual statements using an LLM and verify\nwhether each statement is consistent with the context.\nUltimately, a \u201dFaithfulness Score\u201d is calculated by com-\nparing the number of supported statements to the total\nnumber of statements.\n2. Assessing Answer Relevance: Generate potential ques-\ntions using an LLM and calculate the similarity between\nthese questions and the original question. The Answer\nRelevance Score is derived by calculating the average\nsimilarity of all generated questions to the original ques-\ntion.\n3. Assessing Context Relevance: Extract sentences directly\nrelevant to the question using an LLM, and use the ratio\nof these sentences to the total number of sentences in the\ncontext as the Context Relevance Score.", "in pre-trained LLMs. Then we discuss how it can potentially unlock the possibility of reducing\nKV cache size without an accuracy drop. Given the normalized attention score Softmax( QK\u22a4)\nmatrix that is calculated by the query matrix Qand the key matrix K, we set the threshold as one\npercent of the maximum value in each row and calculates the corresponding sparsity.\nObservation. We conduct zero-shot inference with the pre-trained OPT model on the validation\nset of Wiki-Text-103. We plot the layer-wise sparsity within attention blocks and visualize the\nnormalized attention score matrix. The results are presented in Figure 2 (a). We observe that although\nthe LLMs are densely trained, the resulting attention score matrices are highly sparse, with a sparsity\nover95% in almost all layers.\n4", "thereliability of a measure (i.e. the measure should not be overly sensitive to undesired sources of variation\nsuch as properties of the specific annotators who labelled data). Akin to validity, we lack uniform evidence\nfor the reliability of our measurement. In particular, we highlight the importance of significance testing for\nmaking meaningful comparisons given we only run 3 random seeds (due to cost) for selecting in-context\nexamples and we evaluate on 1000 instances instead of the full validation/test set for a given dataset. That\nis, both of these settings for our evaluation may render some of our claims statistically insignificant; we\nencourage future work to consider how to better address significance given the scale of this evaluation.85\n11.3 Limitations of HELM design\nGiven the nature of our benchmark design, we highlight the important question of aggregation (see Etha-", "The calculated arithmetic intensity is 1.97. How-\never, the A100 80G GPU can perform 312 TFLOPs\nand transfer 2TB of data in one second, i.e.,its ideal\narithmetic intensity is 156. This indicates that the\nbottleneck in attention calculation lies in the process\nof data transfer ( i.e.,excessive I/O loading).\nDecoding Efficiency Issues. In this part, we briefly ana-\nlyze the decoding efficiency issues of LLMs. Overall, the\ndecoding process of LLMs can be divided into two stages\nfor overhead analysis: (1) the prefill stage, which computes\nthe hidden states of the input sequence, and (2) the incre-\nmental decoding stage, which generates a token and updates\nhidden states in an auto-regressive manner [321]. As shown\nin the above memory wall box, the arithmetic intensity of\nthe incremental decoding stage is only 1.97, which is far\nfrom the expected value of 156 (calculated according to\nthe standard configuration of A100 80GB GPU). In contrast,", "model prediction is evaluated with exact match over the\nset of possible answers. The term frequencies of a single\nquestion-answer pair (\u201cQA pair\u201d) are calculated based on\nthe number of times all salient entities of that QA pair appear\nin a sampled pretraining data sequence seen by a given\ncheckpoint. We follow the original experiment using 4 shots\nand evaluate both the training and the validation split of the\ndataset. Performance is averaged over a group of log-spaced\nbins.\nWe observe that for both arithmetic and QA experiments,\nmodel sizes affect the correlation between average perfor-\nmance and the term frequencies, indicating that this corre-\nlation is an emergent property in larger models. Smaller\n8"], "retrieved_docs_id": ["716582522f", "883f5e2638", "24ed7d9b4c", "6790d5b05c", "4948c8d49f"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does RAG's generator improve the accuracy and relevance of the generated text?\n", "true_answer": "RAG's generator enhances accuracy and relevance by leveraging the retrieved information, in contrast to conventional generative models.", "source_doc": "RAG.pdf", "source_id": "cd69a480bb", "retrieved_docs": ["retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-", "cross-attention scores, selecting the highest scoring input to-\nkens to effectively filter tokens. RECOMP [Xuet al. , 2023a ]\nproposes extractive and generative compressors, which gen-\nerate summaries by selecting relevant sentences or syn-\nthesizing document information to achieve multi-document\nquery focus summaries.In addition to that, a novel approach,\nPKG [Luoet al. , 2023 ], infuses knowledge into a white-box\nmodel through directive fine-tuning, and directly replaces the\nretriever module, used to directly output relevant documents\nbased on the query.\n5 Generator\nAnother core component in RAG is the generator, responsible\nfor transforming retrieved information into natural and fluent\ntext. Its design is inspired by traditional language models,\nbut in comparison to conventional generative models, RAG\u2019s\ngenerator enhances accuracy and relevance by leveraging the\nretrieved information. In RAG, the generator\u2019s input includes", "Figure 4: Taxonomy of RAG\u2019s Core Components\nextension of RETRO, increased the model\u2019s parameter scale.\nStudies have found consistent improvements in text genera-\ntion quality, factual accuracy, low toxicity, and downstream\ntask accuracy, particularly in knowledge-intensive tasks such\nas open-domain question answering. These research findings\nhighlight the promising direction of pretraining autoregres-\nsive language models in conjunction with retrieval for future\nfoundational models.\nIn summary, the advantages and limitations of augmented\npre-training are evident. On the positive side, this approach\noffers a more powerful foundational model, outperforming\nstandard GPT models in perplexity, text generation quality,\nand downstream task performance. Moreover, it achieves\nhigher efficiency by utilizing fewer parameters compared to\npurely pre-trained models. It particularly excels in handling\nknowledge-intensive tasks, allowing the creation of domain-", "ples include Hit Rate, MRR, NDCG, Precision, etc.\n2.Generation Module\nThe generation module here refers to the enhanced or\nsynthesized input formed by supplementing the retrieved\ndocuments into the query, distinct from the final an-\nswer/response generation, which is typically evaluated\nend-to-end. The evaluation metrics for the generation\nmodule mainly focus on context relevance, measuring\nthe relatedness of retrieved documents to the query ques-\ntion.\nEnd-to-End Evaluation\nEnd-to-end evaluation assesses the final response gener-\nated by the RAG model for a given input, involving the\nrelevance and alignment of the model-generated answers\nwith the input query. From the perspective of content\ngeneration goals, evaluation can be divided into unlabeled\nand labeled content. Unlabeled content evaluation met-\nrics include answer fidelity, answer relevance, harmless-\nness, etc., while labeled content evaluation metrics in-\nclude Accuracy and EM. Additionally, from the perspec-"], "retrieved_docs_id": ["fefa202c19", "7fabdba415", "cd69a480bb", "7411eec79c", "b023f9e1c7"], "reranker_type": "colbert", "search_type": "text", "rr": 0.3333333333333333, "hit": 1}, {"question": "What is the purpose of the source with the given context?\n", "true_answer": "The purpose of the source is to serve as a continually updated source of information, promoting ongoing growth in a certain field by providing brief overviews of key contributions.", "source_doc": "multimodal.pdf", "source_id": "6e2ea7a4ef", "retrieved_docs": ["the compression capabilities of large (foundation) models. We show that large language models are\npowerful general-purpose predictors and that the compression viewpoint provides novel insights into\nscaling laws, tokenization, and in-context learning. For example, Chinchilla 70B, while trained primarily\non text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size,\nbeating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. Finally, we show\nthat the prediction-compression equivalence allows us to use any compressor (like gzip) to build a\nconditional generative model.\n1. Introduction\nInformation theory and machine learning are inextricably linked and have even been referred to as\n\u201ctwo sides of the same coin\u201d (MacKay, 2003). One particularly elegant connection is the essential\nequivalence between probabilistic models of data and lossless compression. The source coding", "designed for in-context learning nor to follow instructions in prompts. As one concrete recommendation as\nthe language modeling space matures, we recommend model developers explicitly declare how their models\nshould be evaluated and what the scope is of their generality/when models should be preferred. We believe\nthis disclosure will help to strike a productive balance between the general-purpose possibilities for language\nmodels in the abstract and what is concretely sensible for a given model.\n7 Adaptation via prompting\nAdaptation transforms a raw language model into a system that makes predictions on test instances. In\nshort, we use prompting as our adaptation method with 5 in-context examples (when in-context examples\nare included) as depicted in Figure 23. However, many lower-level details must be specified to fully define the\nadaptation procedure: we discuss important conceptual details here and defer the remainder to Appendix J.", "indicate that extended-context models are not nec-\nessarily better than their non-extended counterparts\nat using their input context.\n3 How Well Can Language Models\nRetrieve From Input Contexts?\nGiven that language models struggle to retrieve\nand use information from the middle of their input\ncontexts in the multi-document question answering\ntask, to what extent can they simply retrieve from\ninput contexts? We study this question with a syn-\nthetic key-value retrieval task, which is designed to\nprovide a minimal testbed for the basic ability to\nretrieve matching tokens from an input context.\n3.1 Experimental Setup\nIn our synthetic key-value retrieval task, the inputs\nare (i) a string-serialized JSON object with kkey-\nvalue pairs, where each of the keys and values are\nunique, randomly-generated UUIDs and (ii) a key\nwithin the aforementioned JSON object. The goal\nis to return the value associated with the specified\nkey. Thus, each JSON object contains one relevant", "competing proposals (Chen et al., 2021; Leike et al., 2018; Gabriel, 2020). Following Leike et al.\n(2018), our aim is to train models that act in accordance with user intentions. More practically, for\nthe purpose of our language tasks, we use a framework similar to Askell et al. (2021), who de\ufb01ne\nmodels to be aligned if they are helpful, honest, and harmless.\nTo be helpful, the model should follow instructions, but also infer intention from a few-shot prompt\nor another interpretable pattern such as \u201c Q: {question}\\nA: \u201d. Since a given prompt\u2019s intention\ncan be unclear or ambiguous, we rely on judgment from our labelers, and our main metric is labeler\npreference ratings. However, since our labelers are not the users who generated the prompts, there\ncould be a divergence between what a user actually intended and what the labeler thought was\nintended from only reading the prompt.\nIt is unclear how to measure honesty in purely generative models; this requires comparing the model\u2019s", "generate for a given prompt is quite large, so we rely on samples we hope are representative. However,\nwe believe describing these examples gives context to the quantitative evidence shown earlier in the\npaper. Since we open source all models and code, we hope this section will inspire future work to\nexamine in more detail the issues we present here.\nFactual Recall For questions such as \u201cWhat is the capital of Zambia?\u201d all models consistently\ngenerate correct answers, e.g.,\nGuanaco :The capital of Zambia is Lusaka.\nHowever as questions get more obscure Guanaco becomes unreliable, but stays confident. For\ninstance in response to this prompt from HotPotQA [68]:\nUser : In what year was the singer who popularized \u2018I\u2019ll Keep the Lovelight\nBurning\u2019 born?\nGuanaco :Al Jolson is credited with popularizing the song\n\u2018I\u2019ll Keep the Lovelight Burning,\u2019 and he was born in the year\n1886.\nGuanaco generates the wrong popularizer and the wrong birthday (though the birthday given is"], "retrieved_docs_id": ["3cbeb668a3", "e1cd78ab51", "affcd43d89", "2d8456e4b2", "b8048261ec"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "What is the task of the vision-language projector in this context?\n", "true_answer": "The task of the vision-language projector is to map the visual patch embeddings into the text feature space.", "source_doc": "multimodal.pdf", "source_id": "6ac775b4ef", "retrieved_docs": ["vision and language models. It commences with a convolutional stem, succeeded by Mobile Con-\nvolution Blocks in the first and second stages, and Transformer Blocks in the third stage. Remark-\nably, ViTamin-XL, with a modest count of 436M parameters, attains an 82.9% ImageNet zero-shot\naccuracy. This outperforms the 82.0% accuracy achieved by EV A-E [80], which operates with a pa-\nrameter count ten times larger, at 4.4B. Simply replacing LLaV A\u2019s image encoder with ViTamin-L\ncan establish new standards in various MLLM performance metrics.\n2.2 Vision-Language Projector\nThe task of the vision-language projector is to map the visual patch embeddings Zvinto the text\nfeature space:\nHv=P(Zv), (2)\nwhere Hvdenotes the projected visual embeddings. The aligned visual features are used as prompts\nand inputted into the language model along with the text embeddings. Vision-language projector\n5", "Figure 5: MobileVLM v2 [17] and Honeybee [19] efficient vision-language projector.\n2.3 Small Language Model\nThe pre-trained small language model(SLM) serves as the core component of MLLMs, endowing\nit with many outstanding capabilities, such as zero-shot generalization, instruction following, and\nin-context learning. The SLM accepts input sequences containing multiple modalities and outputs\ncorresponding text sequences. A text tokenizer is typically bundled with the SLM, mapping text\nprompts Xqto the text tokens Hq. The text tokens Hqand the visual tokens Hvare concatenated as\nthe input of the language model, which outputs the final response sequence Yain an autoregressive\nmanner:\np(Ya|Hv, Hq) =LY\ni=1p(yi|Hv, Hq, y<i), (3)\nwhere Ldenotes the length of Ya. As the SLM contributes the vast majority of MLLM parameters,\nits selection is closely related to the lightweight nature of MLLM. In comparison to conventional", "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "Figure 4: BRA VE [12] concatenates features from K different Vision Encoders in a sequence-wise\nmanner. These concatenated features are then reduced by the MEQ-Former.\navoids the high cost of training an end-to-end multimodal model from scratch and effectively lever-\nages the capabilities of pre-trained language and vision models.\nMLP-based As outlined in [7, 54], the vision-language projector is typically realized using a\nstraightforward, learnable Linear Projector or a Multi-Layer Perceptron (MLP), i.e., several linear\nprojectors interleaved with non-linear activation functions, as illustrated in Table.1.\nAttention-based BLIP2 [15] introduces Q-Former, a lightweight transformer, which employs a\nset of learnable query vectors to extract visual features from a frozen vision model. Perceiver\nResampler, proposed by Flamingo[16], contemplates the use of learnable latent queries as Q in\ncross-attention, while image features are unfolded and concatenated with Q to serve as K and V in", "solving abilities of MLLMs. Generally, the input of vi-\nsual instruction tuning consists of an image and a task\ndescription, and the task is to generate a corresponding\n49. In existing work, large vision language models (LVLMs) [662] are\nalso used to term such bimodal models that are developed based on\nLLMs. We use the naming of MLLMs in this part due to its wide use in\nexisting literature."], "retrieved_docs_id": ["6ac775b4ef", "00e8c4ea32", "3238be52f9", "1fea51e26c", "7bf62771bf"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How many GPU hours are needed to train MiniGPT-v2?\n", "true_answer": "Over 800 GPU hours are needed to train MiniGPT-v2, based on NVIDIA A100 GPUs.", "source_doc": "multimodal.pdf", "source_id": "2f6f7fb082", "retrieved_docs": ["However, scalability comes at the cost of high resource demands, which hinders the development\nand deployment of large models. For example, the training of MiniGPT-v2 necessitates a total of\nover 800 GPU hours, as calculated based on NVIDIA A100 GPUs [9]. This imposes a substantial\nexpense that is difficult for researchers outside of major enterprises to bear. Aside from training,\n1* Equal contribution.\n2Yizhang Jin is an intern in Tencent, and Jian Li is the project leader.\nPreprint. Under review.arXiv:2405.10739v1  [cs.CV]  17 May 2024", "Pythia: A Suite for Analyzing Large Language Models\nD. Training Hardware and GPU hours\nWe additionally report the number of accelerators used to train each Pythia model size, alongside counts of total GPU-hours\nrequired for training our models at the throughputs that we achieve.\nModel Size GPU Count Total GPU hours required\n70 M 32 510\n160 M 32 1,030\n410 M 32 2,540\n1.0 B 64 4,830\n1.4 B 64 7,120\n2.8 B 64 14,240\n6.9 B 128 33,500\n12 B 256 72,300\nTotal 136,070\nTable 5. Model sizes in the Pythia suite, number of GPUs used during training, and the total number of GPU hours, calculated via\n(iteration time (s) \u00d7number of iterations \u00d7number of GPUs \u00f73600 s/hour). All GPUs are A100s with 40GB of memory.\nHere \u201ctotal\u201d refers to training one model of each size in our suite. For this paper, we trained two models of each size (one on\nthe Pile and one on the Pile deduplicated) and had to retrain both model suites an additional time as discussed in Appendix B.", "largest publicly available models to date finetunable\non a single GPU. Using QLORA, we train the Gua-\nnaco family of models, with the second best model\nreaching 97.8% of the performance level of ChatGPT\non the Vicuna [ 10] benchmark, while being trainable\nin less than 12 hours on a single consumer GPU;\nusing a single professional GPU over 24 hours we\nachieve 99.3% with our largest model, essentially\nclosing the gap to ChatGPT on the Vicuna bench-\nmark. When deployed, our smallest Guanaco model\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\n20 percentage points on the Vicuna benchmark (Table 6).\nQLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-\nformance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.", "including the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],\nMiniGPT-v2 [9], and MiniGPT-4[10]. These models circumvent the computational cost of train-\ning from scratch by effectively leveraging the pre-training knowledge of each modality. MLLMs\ninherit the cognitive capabilities of LLMs, showcasing numerous remarkable features such as robust\nlanguage generation and transfer learning abilities. Moreover, by establishing strong representa-\ntional connections and alignments with other modality-based models, MLLMs can process inputs\nfrom multiple modalities, significantly broadening their application scope.\nThe success of MLLMs is largely attributed to the scaling law: the performance of an AI model\nimproves as more resources, such as data, computational power, or model size, are invested into it.\nHowever, scalability comes at the cost of high resource demands, which hinders the development", "ChipNeMo: Domain-Adapted LLMs for Chip Design\n2Domain -Adaptive\nPretraining\n24B tokens of chip \ndesign docs/code\nThousands GPU hrs\nModel\nAlignmen t\n56K/128K \n(SteerLM /SFT)  insts\n+ 1.4K task insts\n100+ GPU hrsFoundation Models\nLLaMA2 \n(7B, 13B, 70B) \nChipNeMo \nChat Models\n(7B, 13B, 70B)ChipNeMo \nFoundation Models\n(7B, 13B, 70B)Pretraining\nTrillions tokens of \ninternet data\n105 \u2013 106 GPU hrs\nFigure 1: ChipNeMo Training Flow\n2023)) fine-tuned on additional Verilog data can outperform\nstate-of-art OpenAI GPT-3.5 models. Customizing LLMs\nin this manner also avoids security risks associated with\nsending proprietary chip design data to third party LLMs\nvia APIs. However, it would be prohibitively expensive to\ntrain domain-specific models for every domain from scratch,\nsince this often requires millions of GPU training hours. To\ncost-effectively train domain-specific models, we instead\npropose to combine the following techniques: Domain-"], "retrieved_docs_id": ["2f6f7fb082", "0a2bdce1fe", "da7a5fe5f7", "7a547e4fbb", "2079d05356"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the primary optimization for enhancing the efficiency of efficient MLLMs?\n", "true_answer": "The primary optimization for enhancing the efficiency of efficient MLLMs includes handling high-resolution images, compressing vision tokens, implementing efficient structures, and utilizing compact language models.", "source_doc": "multimodal.pdf", "source_id": "de74717e46", "retrieved_docs": ["and preserving user privacy.\nIn light of these challenges, there has been growing attention on the study of efficient MLLMs.\nThe primary objective of these endeavors is to decrease the resource consumption of MLLMs\nand broaden their applicability while minimizing performance degradation. Research on efficient\nMLLMs began with replacing large language models with lightweight counterparts and performing\ntypical visual instruction tuning. Subsequent studies further enhanced capabilities and expanded\nuse cases in the following ways: (1) lighter architectures were introduced with an emphasis on ef-\nficiency, aiming to reduce the number of parameters or computational complexity[25, 13, 18]; (2)\nmore specialized components were developed, focusing on efficiency optimizations tailored to ad-\nvanced architectures or imbuing specific properties, such as locality[19, 17, 12]; and (3) support\nfor resource-sensitive tasks was provided, with some works employing visual token compression", "the domain of efficient MLLMs. In addition to the survey, we have established a GitHub repository\nwhere we compile the papers featured in the survey, organizing them with the same taxonomy at\nhttps://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey. We will actively maintain it and\nincorporate new research as it emerges.\n2 Architecture\nFollowing the standard MLLM framework, efficient MLLMs can be divided into three main mod-\nules: a visual encoder gtasked with receiving and processing visual inputs, a pre-trained language\nmodel that manages the received multimodal signals and performs reasoning, and a visual-language\nprojector Pwhich functions as a bridge to align the two modalities. To enhance the efficiency of the\ngeneral MLLMs, the primary optimization lies in handling high-resolution images, compressing vi-\nsion tokens, implementing efficient structures, and utilizing compact language models, among other", "Figure 11: Organization of efficient large language models advancements.\nOccupying a significant majority of the parameter volume in MLLMs, LLM serves as a crucial entry\npoint for enhancing the efficiency of MLLMs. In this section, similar to the survey paper [160], we\nprovide a brief overview of the research progress in efficient LLMs, offering inspiration for the\ndevelopment of Efficient MLLMs.\n4.1 Attention\nIn the standard self-attention mechanism, the time complexity is O(n2), where nis the sequence\nlength. This quadratic complexity arises due to the pairwise interactions between all input tokens,\nwhich can lead to scalability issues, especially when dealing with long sequences in LLMs. To\ntackle this, researchers have developed techniques to expedite attention mechanisms and reduce\ntime complexity, such as sharing-based attention, feature information reduction, kernelization or\nlow-rank, fixed and learnable pattern strategies, and hardware-assisted attention.", "for resource-sensitive tasks was provided, with some works employing visual token compression\nto boost efficiency, enabling the transfer of MLLM capabilities to resource-intensive tasks such as\nhigh-resolution image and video understanding[35, 39, 14, 40].\nIn this survey, we aim to present an exhaustive organization of the recent advancements in the rapidly\nevolving field of efficient MLLMs, as depicted in Figure.2. We organize the literature in a taxonomy\nconsisting of six primary categories, encompassing various aspects of efficient MLLMs, including\narchitecture ,efficient vision ,efficient LLMs ,training ,data and benchmarks , and applications .\n\u2022 Architecture focuses on the MLLM framework developed by efficient techniques to reduce\nthe computational cost. The architecture is composed of multiple modality-based funda-\nmental models, exhibits characteristics distinct from single-modal models, thus promoting\nthe development of novel technologies.", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,"], "retrieved_docs_id": ["04b6ebc53f", "de74717e46", "323641b323", "cd55ca1477", "4ee780b19c"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "How is a foundation model adapted to a specific domain using DAPT?\n", "true_answer": "DAPT, or Domain-Adaptive Pre-Training, adapts a foundation model to a specific domain by continued pretraining with in-domain data. In this case, the domain-specific pre-training dataset is constructed from a collection of proprietary hardware-related code and natural language datasets.", "source_doc": "ChipNemo.pdf", "source_id": "926168a67f", "retrieved_docs": ["propose to combine the following techniques: Domain-\nAdaptive Pre-Training (DAPT) (Gururangan et al., 2020) of\nfoundation models with domain-adapted tokenizers, model\nalignment using general and domain-specific instructions,\nand retrieval-augmented generation (RAG) (Lewis et al.,\n2021b) with a trained domain-adapted retrieval model.\nAs shown in Figure 1, our approach is to start with a base\nfoundational model and apply DAPT followed by model\nalignment. DAPT, also known as continued pretraining with\nin-domain data, has been shown to be effective in areas such\nas biomedical and computer science publications, news, and\nreviews. In our case, we construct our domain-specific pre-\ntraining dataset from a collection of proprietary hardware-\nrelated code (e.g. software, RTL, verification testbenches,\netc.) and natural language datasets (e.g. hardware specifi-\ncations, documentation, etc.). We clean up and preprocess\nthe raw dataset, then continued-pretrain a foundation model", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat.", "models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "BioMedLLM(Venigalla et al., 2022) for biomed, and Galac-\ntica(Taylor et al., 2022) for science. These models were\nusually trained on more than 100B tokens of raw domain\ndata. The second approach is domain-adaptive pretraining\n(DAPT) (Gururangan et al., 2020) which continues to train\na pretrained foundation model on additional raw domain\ndata. It shows slight performance boost on domain-specific\ntasks in domains such as biomedical, computer science pub-\nlications, news, and reviews. In one example, (Lewkowycz\net al., 2022) continued-pretrained a foundation model on\ntechnical content datasets and achieved state-of-the-art per-\nformance on many quantitative reasoning tasks.\nRetrieval Augmented Generation (RAG) helps ground the\nLLM to generate accurate information and to extract up-to-\ndate information to improve knowledge-intensive NLP tasks\n(Lewis et al., 2021a). It is observed that smaller models with\nRAG can outperform larger models without RAG (Borgeaud", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus"], "retrieved_docs_id": ["926168a67f", "273b593026", "7eb44773ae", "a92ee29506", "df0b9868f2"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the DSP framework enhance the generation quality in the RAG method?\n", "true_answer": "The DSP framework enhances the generation quality in the RAG method by passing natural language text between a frozen Language Model (LM) and a Retrieval Model (RM), providing the model with more informative context.", "source_doc": "RAG.pdf", "source_id": "0264588829", "retrieved_docs": ["bility to tailor models according to specific requirements and\ndata formats, reducing the resource consumption compared\nto the pre-training phase while retaining the ability to adjust\nthe model\u2019s output style.\nInference Stage\nThe integration of RAG methods with LLM has become a\nprevalent research direction in the inference phase. Notably,\nthe research paradigm of Naive RAG relies on incorporating\nretrieval content during the inference stage.\nTo overcome the limitations of Naive RAG, researchers\nhave introduced richer context in the RAG during the in-\nference phase. The DSP [Khattab et al. , 2022 ]framework re-\nlies on a complex pipeline that involves passing natural lan-\nguage text between a frozen Language Model (LM) and a Re-\ntrieval Model (RM), providing the model with more informa-\ntive context to enhance generation quality. PKG equips LLMs\nwith a knowledge-guided module that allows access to rele-\nvant knowledge without altering the parameters of LLMs, en-", "quently, it utilizes this retrieved information to generate re-\nsponses or text, thereby enhancing the quality of predictions.\nThe RAG method allows developers to avoid the need for\nretraining the entire large model for each specific task. In-\nstead, they can attach a knowledge base, providing additional\ninformation input to the model and improving the accuracy\nof its responses. RAG methods are particularly well-suited\nfor knowledge-intensive tasks. In summary, the RAG system\nconsists of two key stages:1. Utilizing encoding models to retrieve relevant docu-\nments based on questions, such as BM25, DPR, Col-\nBERT, and similar approaches [Robertson et al. , 2009,\nKarpukhin et al. , 2020, Khattab and Zaharia, 2020 ].\n2. Generation Phase: Using the retrieved context as a con-\ndition, the system generates text.\n2.2 RAG vs Fine-tuning\nIn the optimization of Large Language Models (LLMs), in\naddition to RAG, another important optimization technique\nis fine-tuning.", "ments to assess the relevance between the retrieved doc-\numents and the query. This enhances the robustness of\nRAG [Yuet al. , 2023a ].\nNew Pattern\nThe organizational approach of Modular RAG is flexible,\nallowing for the substitution or reconfiguration of modules\nwithin the RAG process based on specific problem con-\ntexts. For Naive RAG, which consists of the two modules\nof retrieval and generation ( referred as read or sythesis in\nsome literature), this framework offers adaptability and abun-\ndance. Present research primarily explores two organizational\nparadigms, involving the addition or replacement of modules,\nas well as the adjustment of the organizational flow between\nmodules.\n\u2022Adding or Replacing Modules\nThe strategy of adding or replacing modules entails\nmaintaining the structure of Retrieval-Read while intro-\nducing additional modules to enhance specific function-\nalities. RRR [Maet al. , 2023a ]proposes the Rewrite-\nRetrieve-Read process, utilizing LLM performance as a", "the prompt together with the question, which grounds the\nLLM to produce more accurate answers. We find that using\na domain adapted language model for RAG significantly\nimproves answer quality on our domain specific questions.\nAlso, we find that fine-tuning an off-the-shelf unsupervised\npre-trained dense retrieval model with a modest amount\nof domain specific training data significantly improves re-\ntrieval accuracy. Our domain-adapted RAG implementation\ndiagram is illustrated on Figure 3.\nFigure 3: RAG Implementation Variations\nWe created our domain adapted retrieval model by fine-\ntuning the e5small unsupervised model (Wang et al., 2022)\nwith 3000 domain specific auto-generated samples using the\nTevatron framework (Gao et al., 2022). We refer readers to\nthe details on the sample generation and training process in\nAppendix A.8.\nEven with the significant gains that come with fine-tuning a\nretrieval model, the fact remains that retrieval still struggles", "most of the research on reinforcement during the inference\nstage emerged during the era of LLMs. This is primarily due\nto the high training costs associated with high-performance\nlarge models. Researchers have attempted to enhance model\ngeneration by incorporating external knowledge in a cost-\neffective manner through the inclusion of RAG modules dur-\ning the inference stage. Regarding the use of augmented\ndata, early RAG primarily focused on the application of un-\nstructured data, particularly in the context of open-domain\nquestion answering. Subsequently, the range of knowledge\nsources for retrieval expanded, with the use of high-quality\ndata as knowledge sources effectively addressing issues such\nas internalization of incorrect knowledge and hallucinations\nin large models. This includes structured knowledge, with\nknowledge graphs being a representative example. Recently,\nthere has been increased attention on self-retrieval, which in-"], "retrieved_docs_id": ["0264588829", "80558327ad", "a016e8d322", "ad55562468", "326cdd7c26"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the Funnel-Transformer model address the issue of computational efficiency in attention mechanisms?\n", "true_answer": "The Funnel-Transformer model addresses the issue of computational efficiency in attention mechanisms by progressively reducing the sequence size of hidden representations in self-attention models.", "source_doc": "multimodal.pdf", "source_id": "3045b9cbb1", "retrieved_docs": ["query heads into several groups, with each group\u2019s query heads sharing a common key-value head,\nthereby establishing a rigorous equilibrium between effectiveness and computational cost.\nFeature Information Reduction Feature Information Reduction, as evidenced by models such as\nFunnel-Transformer[145] and Set Transformer[146], addresses the crucial need for computational\nefficiency in attention mechanisms, specifically by reducing the dimensionality or quantity of input\nfeatures while preserving the essential information embedded within the data. A key motivation\nbehind this strategy stems from the potential redundancy in maintaining full-length hidden repre-\nsentations across all layers in Transformer models. Funnel-Transformer [145] tackles this issue by\nprogressively reducing the sequence size of hidden representations in self-attention models, such as\n14", "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in", "UniNet [102] introduced context-aware down-sampling modules improving information accommo-\ndation by transformer and MLP operators.\nOptimization of Attention Mechanisms Methods focus on reducing computational complexity\nby introducing adaptive attention, learning sparse attention patterns, and dynamically adjusting at-\ntention mechanisms. Fayyaz et al. [135] implemented adaptive attention by scoring and adaptively\nsampling significant tokens. PatchMerger [103] extracted global information among regional to-\nkens and exchanged local self-attention with information among regional tokens via self-attention.\nDynamicViT [104] proposed an attention masking strategy to differentiably prune tokens by block-\ning interactions with other tokens. Additionally, Sepvit [105] conducted local-global information\ninteraction within and across windows using depthwise separable self-attention. These methods\ncollectively optimize attention mechanisms, enhancing computational efficiency and performance.", "collectively optimize attention mechanisms, enhancing computational efficiency and performance.\n3.2 Pruning\nPruning involves removing less essential weights from vision transformer models, typically catego-\nrized as unstructured pruning, structured pruning, and hybrid pruning techniques.\nUnstructured Pruning focuses on eliminating individual weights without considering their struc-\ntural arrangement within the model. Rao et al. [104] introduced a dynamic token sparsification\nframework for progressive and adaptive pruning of redundant tokens based on input, integrating\na lightweight prediction module to estimate token importance scores and employing an attention\nmasking strategy to differentiate token interactions and optimize the prediction module in an end-\nto-end fashion. Cap [106] proposed a novel theoretically-grounded pruner capable of accurately\nand efficiently handling intricate weight correlations during pruning, alongside an effective fine-", "Figure 12: In GQA[59], a single set of key and value heads is allocated for each group of query\nheads, providing a balance between multi-head and multi-query attention mechanisms.\nsequence length. This reduction not only decreases computational complexity and memory usage\nbut also frees up resources that can be allocated toward building deeper or wider models.\nApproximate Attention Approximate Attention facilitates models to efficiently focus on task-\nrelevant information when processing long texts. Two pivotal concepts within Approximate Atten-\ntion are Kernelization and Low-Rank. Kernelization, e.g. [148], involves transforming a problem\ninto a kernel-based framework with the goal of converting the original issue into a more manage-\nable one in a higher-dimensional space. Kernelization is primarily employed to map text sequences\ninto a high-dimensional space, where task-related information can be more readily captured. In"], "retrieved_docs_id": ["3045b9cbb1", "c998dcd0be", "e7939ae097", "5f9ab022ea", "31d276f3e7"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the architecture of the C-Abstractor in the Hybrid Structure Honeybee project?\n", "true_answer": "The C-Abstractor, or Conventional Abstractor, in the Hybrid Structure Honeybee project employs a convolutional architecture consisting of LResNet blocks, followed by adaptive average pooling and additional LResNet blocks for abstracting visual features.", "source_doc": "multimodal.pdf", "source_id": "3238be52f9", "retrieved_docs": ["original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "Figure 5: MobileVLM v2 [17] and Honeybee [19] efficient vision-language projector.\n2.3 Small Language Model\nThe pre-trained small language model(SLM) serves as the core component of MLLMs, endowing\nit with many outstanding capabilities, such as zero-shot generalization, instruction following, and\nin-context learning. The SLM accepts input sequences containing multiple modalities and outputs\ncorresponding text sequences. A text tokenizer is typically bundled with the SLM, mapping text\nprompts Xqto the text tokens Hq. The text tokens Hqand the visual tokens Hvare concatenated as\nthe input of the language model, which outputs the final response sequence Yain an autoregressive\nmanner:\np(Ya|Hv, Hq) =LY\ni=1p(yi|Hv, Hq, y<i), (3)\nwhere Ldenotes the length of Ya. As the SLM contributes the vast majority of MLLM parameters,\nits selection is closely related to the lightweight nature of MLLM. In comparison to conventional", "that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B \u2013 chat model on human evaluation\nbenchmarks. Mixtral \u2013 Instruct also demonstrates reduced biases, and a more balanced sentiment\nprofile in benchmarks such as BBQ, and BOLD.\nWe release both Mixtral 8x7B and Mixtral 8x7B \u2013 Instruct under the Apache 2.0 license1, free for\nacademic and commercial usage, ensuring broad accessibility and potential for diverse applications.\nTo enable the community to run Mixtral with a fully open-source stack, we submitted changes to\nthe vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also\nallows the deployment of vLLM endpoints on any instance in the cloud.\n2 Architectural details\nParameter Value\ndim 4096\nn_layers 32\nhead_dim 128\nhidden_dim 14336\nn_heads 32\nn_kv_heads 8\ncontext_len 32768\nvocab_size 32000\nnum_experts 8\ntop_k_experts 2\nTable 1: Model architecture.Mixtral is based on a transformer architecture [ 31] and uses the same", "4.6.1 Architecture\nTable 6investigates the e\ufb00ects of the architecture (block) and its inner SSM layer (Figure 3). We \ufb01nd that\n\u2022Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very\nsimilar.\n\u2022Replacing the complex-valued S4 variant from previous work with a real-valued one does not a\ufb00ect performance\nmuch, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware\ne\ufb03ciency.\n\u2022Replacing any of these with a selective SSM (S6) signi\ufb01cantly improves performance, validating the motivation\nof Section 3.\n\u2022The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a\nselective layer).\nWe also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA\n(a hybrid attention architecture) in Appendix E.2.2 .\n4.6.2 Selective SSM", "\u2022GPT-1 . In 2017, the Transformer model [22] was intro-\nduced by Google, and the OpenAI team quickly adapted\ntheir language modeling work to this new neural network\narchitecture. They released the first GPT model in 2018,\ni.e., GPT-1 [122], and coined the abbreviation term GPT\nas the model name, standing for Generative Pre-Training .\nGPT-1 was developed based on a generative, decoder-only\nTransformer architecture, and adopted a hybrid approach of\nunsupervised pretraining and supervised fine-tuning. GPT-\n1 has set up the core architecture for the GPT-series models\nand established the underlying principle to model natural\nlanguage text, i.e.,predicting the next word.\n\u2022GPT-2 . Following a similar architecture of GPT-1,\nGPT-2 [26] increased the parameter scale to 1.5B, which\nwas trained with a large webpage dataset WebText. As\nclaimed in the paper of GPT-2, it sought to perform\ntasks via unsupervised language modeling, without explicit"], "retrieved_docs_id": ["3238be52f9", "00e8c4ea32", "4c2a4c92b0", "09f3db1093", "01a62fac44"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does OpenAI's embeddings-ada-02 handle context compared to full-size language models like GPT-4?\n", "true_answer": "OpenAI's embeddings-ada-02 is more sophisticated than static embedding models and can capture a certain level of context, but it may not be as sensitive to context as the latest full-size language models like GPT-4.", "source_doc": "RAG.pdf", "source_id": "5b18d3e068", "retrieved_docs": ["as much context as possible to ensure \u201chealthy\u201d out-\ncomes.Built upon the principles of large language mod-\nels like GPT, OpenAI\u2019s embeddings-ada-02 is more so-\nphisticated than static embedding models, capturing a\ncertain level of context. While it excels in contextual\nunderstanding, it may not exhibit the same sensitivity to\ncontext as the latest full-size language models like GPT-\n4.\nPost-Retrieval Process\nAfter retrieving valuable context from the database, merg-\ning it with the query for input into LLM poses challenges.\nPresenting all relevant documents to the LLM at once may\nexceed the context window limit. Concatenating numerous\ndocuments to form a lengthy retrieval prompt is ineffective,\nintroducing noise and hindering the LLM\u2019s focus on crucial\ninformation. Additional processing of the retrieved content is\nnecessary to address these issues.\n\u2022ReRank: Re-ranking to relocate the most relevant in-\nformation to the edges of the prompt is a straightfor-", "tomized embedding methods can improve retrieval rel-\nevance. The BGE [BAAI, 2023 ]embedding model is a\nfine-tunning and high-performance embedding model,such as BGE-large-EN developed by the BAAI3. To cre-\nate training data for fine-tuning the BGE model, start\nby using LLMs like gpt-3.5-turbo to formulate ques-\ntions based on document chunks, where questions and\nanswers (document chunks) form fine-tuning pairs for\nthe fine-tuning process.\n\u2022Dynamic Embedding: Dynamic embedding adjust\nbased on the context in which words appear, differing\nfrom static embedding that use a single vector for each\nword. For instance, in transformer models like BERT,\nthe same word can have varied embeddings depend-\ning on surrounding words. Evidence indicates unex-\npected high cosine similarity results, especially with text\nlengths less than 5 tokens, in OpenAI\u2019s text-embedding-\nada-002 model4. Ideally, embedding should encompass\nas much context as possible to ensure \u201chealthy\u201d out-", "2023; Rubin and Berant, 2023, inter alia ) have\nresulted in language models with larger context\nwindows (e.g., 4096, 32K, and even 100K tokens),\nbut it remains unclear how these extended-context\nlanguage models make use of their input contexts\nwhen performing downstream tasks.\nWe empirically investigate this question via\ncontrolled experiments with a variety of state-of-\nthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in set-\ntings that require accessing and using information\nwithin an input context. In particular, our experi-\nments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input con-\ntexts, then their performance should be minimally\naffected by the position of the relevant information\nin the input context.", "arXiv:2303.04671 , 2023.\n[46] OpenAI, \u201cGpt-4 technical report,\u201d OpenAI , 2023.\n[47] Y. Fu, H. Peng, and T. Khot, \u201cHow does gpt obtain its\nability? tracing emergent abilities of language models\nto their sources,\u201d Yao Fu\u2019s Notion , Dec 2022.\n[48] J. Li, T. Tang, W. X. Zhao, and J. Wen, \u201cPretrained\nlanguage model for text generation: A survey,\u201d in\nProceedings of the Thirtieth International Joint Conference\non Artificial Intelligence, IJCAI 2021, Virtual Event /\nMontreal, Canada, 19-27 August 2021 , Z. Zhou, Ed.\nijcai.org, 2021, pp. 4492\u20134499.\n[49] P . Lu, L. Qiu, W. Yu, S. Welleck, and K. Chang, \u201cA\nsurvey of deep learning for mathematical reasoning,\u201d\nCoRR , vol. abs/2212.10535, 2022.\n[50] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang,\nX. Sun, J. Xu, L. Li, and Z. Sui, \u201cA survey for in-context\nlearning,\u201d CoRR , vol. abs/2301.00234, 2023.\n[51] J. Huang and K. C. Chang, \u201cTowards reasoning\nin large language models: A survey,\u201d CoRR , vol.\nabs/2212.10403, 2022.", "i\ufb01cation to the perplexity calculation. Unlike RNN-based\nlanguage models, transformers operate on a \ufb01xed window in-\nput size. Therefore they cannot fully calculate P(t|0 :t\u22121)\nand can only calculate P(t|t\u2212w:t\u22121)wherewis the\nsize of our context: 1024 tokens. However, calculating this\nvalue for every token in our dataset is prohibitively expen-\nsive since we must compute approximately Tevaluations\nof awsized context. To evaluate our models ef\ufb01ciently we\ntake a middle ground approach termed overlapping evalu-\nation where we advance the sliding window by some over-\nlapoeach time and only compute the cross entropy losses\ncorresponding to the last otokens of the window. In our\nexperiments we utilize an overlap oof 32, and compute\nlosses over all sliding windows in such a fashion.\nE.2. LAMBADA Cloze Accuracy\nThe capability to handle long term contexts is crucial for\nstate of the art language models and is a necessary prerequi-\nsite for problems like long-form generation and document-"], "retrieved_docs_id": ["5b18d3e068", "331fc75c9e", "af5472ed0b", "8038cd5289", "608bbf7ba8"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Retrieval-Augmented Generation (RAG) improve the performance of large language models?\n", "true_answer": "RAG improves the performance of large language models by retrieving relevant information from external knowledge bases before answering questions, which enhances answer accuracy, reduces model hallucination, and is particularly beneficial for knowledge-intensive tasks.", "source_doc": "RAG.pdf", "source_id": "af911eac69", "retrieved_docs": ["Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "complement each other, enhancing the model\u2019s capabilities at\ndifferent levels. In certain situations, combining these two\ntechniques can achieve optimal model performance. The en-\ntire process of optimizing with RAG and fine-tuning may re-\nquire multiple iterations to achieve satisfactory results.\nExisting research has demonstrated significant ad-\nvantages of Retrieval-Augmented Generation (RAG)\ncompared to other methods for optimizing large lan-\nguage models [Shuster et al. , 2021, Yasunaga et al. , 2022,\nWang et al. , 2023c, Borgeaud et al. , 2022 ]:\n\u2022 RAG improves accuracy by associating answers with ex-\nternal knowledge, reducing hallucination issues in lan-\nguage models and making generated responses more ac-\ncurate and reliable.\n\u2022 The use of retrieval techniques allows the identifica-\ntion of the latest information. Compared to traditionallanguage models relying solely on training data, RAG\nmaintains the timeliness and accuracy of responses.", "els. In comparison with them, this paper aims to systemati-\ncally outline the entire process of Retrieval-Augmented Gen-\neration (RAG) and focuses specifically on research related to\naugmenting the generation of large language models through\nknowledge retrieval.\nThe development of RAG algorithms and models is il-\nlustrated in Fig 1. On a timeline, most of the research re-\nlated to RAG emerged after 2020, with a significant turn-\ning point in December 2022 when ChatGPT was released.\nSince the release of ChatGPT, research in the field of natu-\nral language processing has entered the era of large models.\nNaive RAG techniques quickly gained prominence, leading\nto a rapid increase in the number of related studies.In terms\nof enhancement strategies, research on reinforcement during\nthe pre-training and supervised fine-tuning stages has been\nongoing since the concept of RAG was introduced. However,\nmost of the research on reinforcement during the inference", "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "putational expenses for both training and inference. To ad-\ndress the limitations of purely parameterized models, lan-\nguage models can adopt a semi-parameterized approach by\nintegrating a non-parameterized corpus database with pa-\nrameterized models. This approach is known as Retrieval-\nAugmented Generation (RAG).\nThe term Retrieval-Augmented Generation (RAG) was\nfirst introduced by [Lewis et al. , 2020 ]. It combines a pre-\ntrained retriever with a pre-trained seq2seq model (generator)\nand undergoes end-to-end fine-tuning to capture knowledge\nin a more interpretable and modular way. Before the advent\nof large models, RAG primarily focused on direct optimiza-\ntion of end-to-end models. Dense retrievals on the retrieval\nside, such as the use of vector-based Dense Passage Retrieval\n(DPR) [Karpukhin et al. , 2020 ], and training smaller models\non the generation side are common practices. Due to the\noverall smaller parameter size, both the retriever and gener-"], "retrieved_docs_id": ["af911eac69", "72cb2b4f23", "483a7b216e", "6291d3f5de", "33aae5a21d"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does MoE-LLaV increase model capacity without significantly compromising inference speed?\n", "true_answer": "MoE-LLaV increases model capacity by modulating the total count of model parameters while keeping the activated parameters unchanged, which does not significantly affect the inference speed.", "source_doc": "multimodal.pdf", "source_id": "ffe176eb03", "retrieved_docs": ["mising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework\nthat effectively increases the number of parameters without compromising computational efficiency.\nFurthermore, it introduces MoE-Tuning, a three-stage training strategy designed to adapt MoE [89]\nto MLLMs and prevent model degradation caused by sparsity. MM1[30] designs two variants of\nMoE models. The first is a 3B-MoE model that employs 64 experts and substitutes a dense layer\nwith a sparse one every two layers. The second is a 7B-MoE model that utilizes 32 experts and\nsubstitutes a dense layer with a sparse one every four layers.\nMamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modal-\nity and explores different modal fusion schemes to develop an effective multi-modal Mamba. Exper-\niments show that it not only achieves competitive performance with state-of-the-art efficient meth-", "which can pose a significant computational challenge within the context window of LLMs. Ely-\nsium [92] provides a trade-off between performance and visual token consumption, where T-Selector\nis introduced as a visual token compression network to enable LLMs to distinguish individual frames\nwhile reducing visual token use. VideoLLaV A [44], building upon LanguageBind [93], unifies vi-\nsual representation into the language feature space to advance foundational LLMs towards a unified\nlanguage-vision LLM without incurring a large computational burden.\n2.5 Efficient Structures\nEfficient structures primarily explore three directions: Mixture-of-Experts, Mamba and Inference\nAcceleration.\nMixture of Experts MoE enhances model capacity by modulating the total count of model pa-\nrameters while maintaining the activated parameters unchanged, hence, not significantly compro-\nmising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework", "thus its FLOPS are larger. All models were trained for the same number of steps on identical\nhardware. Note that the MoE model going from capacity factor 2.0 to 1.25 actually slows\ndown (840 to 790) in the above experiment setup, which is unexpected.5\nWe highlight three key \ufb01ndings from Table 1: (1)Switch Transformers outperform\nboth carefully tuned dense models and MoE Transformers on a speed-quality basis. For\na \ufb01xed amount of computation and wall-clock time, Switch Transformers achieve the best\nresult. (2)The Switch Transformer has a smaller computational footprint than the MoE\ncounterpart. If we increase its size to match the training speed of the MoE Transformer,\nwe \ufb01nd this outperforms all MoE and Dense models on a per step basis as well. (3)Switch\nTransformers perform better at lower capacity factors (1.0, 1.25). Smaller expert capacities\nare indicative of the scenario in the large model regime where model memory is very scarce", "(from 6.7B to 175B) and evaluation benchmarks (HELM and lm-eval-harness). More importantly,\ncan enhance the performance of existing KV cache sparsification techniques.\n\u2022In Section 5.2, we demonstrate that H2Ocan increase the inference throughput by up to 3\u00d7,29\u00d7,\n29\u00d7compared to the state-of-the-art inference engine FlexGen, DeepSpeed and the widely used\nHugging Face Accelerate without compromising model quality.\n\u2022In Section 5.3, we present extensive ablation studies to show the effectiveness of H2Ounder\ndifferent sequence lengths, especially the input with infinite sequence length and its compatibility\nwith quantization.\nAll details (hyperparameters, data splits, etc.), along with additional experiments, are in Appendix A.\n5.1 End-to-End Results\nWe demonstrate that H2Ocan reduce KV cache memory footprint by 5-10\u00d7while achieving compa-\nrable accuracy on a majority of tasks.\nSetup. Our experiments are based on three representative model families of LLMs, including the", "expressiveness and generalization capabilities. Adapter-based tuning introduces lightweight adapter\nmodules into the pre-trained model\u2019s architecture. These adapter modules, typically composed of\nfeed-forward neural networks with a small number of parameters, are inserted between the layers\nof the original model. During fine-tuning, only the adapter parameters are updated, while the pre-\ntrained model\u2019s parameters remain fixed. This method significantly reduces the number of trainable\nparameters, leading to faster training and inference times without compromising the model\u2019s per-\nformance. LLM-Adapters [154] presents a framework for integrating various adapters into large\nlanguage models, enabling parameter-efficient fine-tuning for diverse tasks. This framework en-\n16"], "retrieved_docs_id": ["5510d4cc4e", "ffe176eb03", "52f9375839", "a265d1549e", "004ffc5dd9"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "How does the Memory Module in RAG find relevant memories?\n", "true_answer": "The Memory Module in RAG finds relevant memories by leveraging the memory capabilities of the LLM itself and finding memories most similar to the current input. It iteratively employs a retrieval-enhanced generator to create an unbounded memory pool.", "source_doc": "RAG.pdf", "source_id": "9067222c76", "retrieved_docs": ["RAG, the search module, tailored to specific sce-\nnarios, incorporates direct searches on (additional)\ncorpora in the process using LLM-generated code,\nquery languages (e.g., SQL, Cypher), or other cus-\ntom tools. The data sources for searching can include\nsearch engines, text data, tabular data, or knowledge\ngraphs [Wang et al. , 2023c ].\n\u2022Memory Module: Leveraging the memory capabili-\nties of LLM itself to guide retrieval, the principle in-\nvolves finding memories most similar to the current in-\nput. Self-mem [Cheng et al. , 2023b ]iteratively employs\na retrieval-enhanced generator to create an unbounded\nmemory pool, combining the \u201coriginal question\u201d and\n\u201cdual question.\u201d A retrieval-enhanced generative model\ncan use its own outputs to enhance itself, making the\ntext closer to the data distribution in the reasoning pro-\ncess, with the model\u2019s own outputs rather than training\ndata[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-", "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "Modular RAG\nThe modular RAG structure breaks away from the traditional\nNaive RAG framework of indexing, retrieval, and genera-\ntion, offering greater diversity and flexibility in the over-\nall process. On one hand, it integrates various methods to\nexpand functional modules, such as incorporating a search\nmodule in similarity retrieval and applying a fine-tuning ap-\nproach in the retriever [Linet al. , 2023 ]. Additionally, spe-\ncific problems have led to the emergence of restructured\nRAG modules [Yuet al. , 2022 ]and iterative approaches like\n[Shao et al. , 2023 ]. The modular RAG paradigm is becom-\ning the mainstream in the RAG domain, allowing for ei-\nther a serialized pipeline or an end-to-end training approach\nacross multiple modules.The comparison between three RAG\nparadigms is illustrated in Fig 3.\nNew Modules\n\u2022Search Module: Diverging from the similarity re-\ntrieval between queries and corpora in Naive/Advanced\nRAG, the search module, tailored to specific sce-", "lows the decide-retrieve-reflect-read process, introduc-\ning a module for active judgment. This adaptive and\ndiverse approach allows for the dynamic organization of\nmodules within the Modular RAG framework.\n4 Retriever\nIn the context of RAG, the \u201dR\u201d stands for retrieval, serving\nthe role in the RAG pipeline of retrieving the top-k relevant\ndocuments from a vast knowledge base. However, crafting\na high-quality retriever is a non-trivial task. In this chapter,\nwe organize our discussions around three key questions: 1)\nHow to acquire accurate semantic representations? 2) How\nto match the semantic spaces of queries and documents? 3)\nHow to align the output of the retriever with the preferences\nof the Large Language Model ?\n4.1 How to acquire accurate semantic\nrepresentations?\nIn RAG, semantic space is the multidimensional space where\nquery and Document are mapped. When we perform re-\ntrieval, it is measured within the semantic space. If the se-", "cross-attention scores, selecting the highest scoring input to-\nkens to effectively filter tokens. RECOMP [Xuet al. , 2023a ]\nproposes extractive and generative compressors, which gen-\nerate summaries by selecting relevant sentences or syn-\nthesizing document information to achieve multi-document\nquery focus summaries.In addition to that, a novel approach,\nPKG [Luoet al. , 2023 ], infuses knowledge into a white-box\nmodel through directive fine-tuning, and directly replaces the\nretriever module, used to directly output relevant documents\nbased on the query.\n5 Generator\nAnother core component in RAG is the generator, responsible\nfor transforming retrieved information into natural and fluent\ntext. Its design is inspired by traditional language models,\nbut in comparison to conventional generative models, RAG\u2019s\ngenerator enhances accuracy and relevance by leveraging the\nretrieved information. In RAG, the generator\u2019s input includes"], "retrieved_docs_id": ["9067222c76", "6291d3f5de", "1d479682a6", "8fe8499442", "cd69a480bb"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does REPLUG improve the initial retrieval model?\n", "true_answer": "REPLUG improves the initial retrieval model by obtaining feedback from the language model through supervised signals.", "source_doc": "RAG.pdf", "source_id": "662eb558d5", "retrieved_docs": ["domain question-answering tasks. Concerning retriever fine-\ntuning, REPlUG [Shiet al. , 2023 ]treats the language model\n(LM) as a black box and enhances it through an adjustable re-\ntrieval model. By obtaining feedback from the black-box lan-\nguage model through supervised signals, REPLUG improves\nthe initial retrieval model. UPRISE [Cheng et al. , 2023a ], on\nthe other hand, fine-tunes retrievers by creating a lightweight\nand versatile retriever through fine-tuning on diverse task\nsets. This retriever can automatically provide retrieval\nprompts for zero-shot tasks, showcasing its universality and\nimproved performance across tasks and models.\nSimultaneously, methods for fine-tuning generators in-\nclude Self-Mem [Cheng et al. , 2023b ], which fine-tunes the\ngenerator through a memory pool of examples, and\nSelf-RAG [Asai et al. , 2023b ], which satisfies active re-\ntrieval needs by generating reflection tokens. The RA-\nDIT[Linet al. , 2023 ]method fine-tunes both the generator", "ITER-RETGEN [Shao et al. , 2023 ]collaboratively utilizes\n\u201dretrieval-enhanced generation\u201d and \u201dgeneration-enhanced\nretrieval\u201d for tasks requiring reproduction of information.\nThat is, the model uses the content needed to complete the\ntask to respond to the input task, and these target contents\nserve as the information context for retrieving more relevant\nknowledge. This helps to generate better responses in another\niteration.\nIRCoT [Trivedi et al. , 2022 ]also explores retrieving docu-\nments for each generated sentence, introducing retrieval at\nevery step of the thought chain. It uses CoT to guide the re-\ntrieval and uses the retrieval results to improve CoT, ensuring\nsemantic completeness.\nAdaptive Retrieval\nIndeed, the RAG methods described in the previous two\nsections follow a passive approach where retrieval is prior-", "Retrieval-Augmented LLM. Due to the huge amount of\nfact records in a KG, existing work typically adopts a\nretrieval model to first obtain a relatively small subgraph\nfrom KG, and then leverages it to enhance LLMs by en-\nriching the relevant knowledge. Before the advent of LLMs,\nthe retrieved subgraphs are often supplemented into train-\ning data, injecting knowledge information into PLMs via\nparameter learning [863\u2013865]. In contrast, to leverage the\nretrieved knowledge, LLMs mainly incorporate it as part of\nthe prompt, without parameter update. To implement this\napproach, there are two main technical problems, i.e.,how\nto retrieve relevant knowledge from KGs and how to make\nbetter use of the structured data by LLMs. For the first issue\n(i.e.,retrieving relevant knowledge), a typical approach is\nto train a small language model ( e.g., RoBERTa) to iden-\ntify question-related fact triples [866]. To further improve\nthe retrieval performance, several studies also propose an", "as:\n\u03b6=X\nqX\nd+\u2208Da+X\nd\u2212\u2208D\u2212l\u0000\nf\u0000\nq, d+\u0001\n, f\u0000\nq, d\u2212\u0001\u0001\n(1)\nwhere Da+is the documents preferred by the LLM in the\nretrieved set and Da\u2212is not preferred. lis the standard cross\nentropy loss. In the end,it is suggested that LLMs may have a\npreference for focusing on readable rather than information-\nrich documents\nREPLUG [Shiet al. , 2023 ]uses a retriever and an LLM to\ncalculate the probability distributions of the retrieved docu-\nments, and then performs supervised training by calculating\nthe KL divergence. This simple and effective training method\nenhances the performance of the retrieval model by using an\nLM as the supervisory signal, eliminating the need for any", "RAG can outperform larger models without RAG (Borgeaud\net al., 2022). Retrieval methods include sparse retrieval\nmethods such as TF-IDF or BM25(Robertson & Zaragoza,\n2009), which analyze word statistic information and find\nmatching documents with a high dimensional sparse vec-\ntor. Dense retrieval methods such as (Karpukhin et al.,\n2020; Izacard et al., 2022a) find matching documents on\nan embedding space generated by a retrieval model pre-\ntrained on a large corpus with or without fine-tuning on a\nretrieval dataset. The retrieval model can be trained stan-\ndalone (Karpukhin et al., 2020; Izacard et al., 2022a; Shi\net al., 2023) or jointly with language models (Izacard et al.,\n2022b; Borgeaud et al., 2022). In addition, it has been shown\nthat off-the-shelf general purpose retrievers can improve a\nbaseline language model significantly without further fine-\ntuning (Ram et al., 2023). RAG is also proposed to perform\ncode generation tasks (Zhou et al., 2023) by retrieving from"], "retrieved_docs_id": ["662eb558d5", "89c7fd1852", "d0140a8a43", "9d81e48a55", "392133bc25"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the main factor contributing to the success of multimodal large language models (MLLMs)?\n", "true_answer": "The success of MLLMs is largely attributed to the scaling law, which states that the performance of an AI model improves as more resources, such as data, computational power, or model size, are invested into it.", "source_doc": "multimodal.pdf", "source_id": "7a547e4fbb", "retrieved_docs": ["Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "cient MLLMs, research state of efficient structures and strategies, and the appli-\ncations. Finally, we discuss the limitations of current efficient MLLM research\nand promising future directions. Please refer to our GitHub repository for more\ndetails: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.\n1 Introduction\nLarge-scale pretraining, a leading approach in Artificial Intelligence(AI), has seen general-purpose\nmodels like large language and multimodal models outperform specialized deep learning models\nacross many tasks. The remarkable abilities of Large Language Models (LLM) have inspired efforts\nto merge them with other modality-based models to enhance multimodal competencies. This con-\ncept is further supported by the remarkable success of proprietary models like OpenAI\u2019s GPT-4V [1]\nand Google\u2019s Gemini[2]. As a result, Multimodal Large Language Models (MLLMs) have emerged,\nincluding the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\""], "retrieved_docs_id": ["114f3dada8", "da0a465b6c", "ac70fcc9f2", "e021f7788d", "f49f3b54ce"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "What is the number of MLLMs (Multimodal Large Language Models) evaluated in Table 4?\n", "true_answer": "22 MLLMs were evaluated in Table 4.", "source_doc": "multimodal.pdf", "source_id": "de63235613", "retrieved_docs": ["Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "general-purpose interface, i.e., language models.\nNew capabilities of MLLMs. As shown in Table 1, apart from the capabilities found in previous\nLLMs [ BMR+20,CND+22], MLLMs enable new usages and possibilities. First, we can conduct\nzero- and few-shot multimodal learning by using natural language instructions and demonstration\nexamples. Second, we observe promising signals of nonverbal reasoning by evaluating the Raven\nIQ test, which measures the \ufb02uid reasoning ability of humans. Third, MLLMs naturally support\nmulti-turn interactions for general modalities, such as multimodal dialogue.\n2 K OSMOS -1: A Multimodal Large Language Model\nAs shown in Figure 1, KOSMOS -1is a multimodal language model that can perceive general\nmodalities, follow instructions, learn in context, and generate outputs. Given the previous context, the\nmodel learns to generate texts in an auto-regressive manner. Speci\ufb01cally, the backbone of KOSMOS -1"], "retrieved_docs_id": ["ac70fcc9f2", "f49f3b54ce", "da0a465b6c", "114f3dada8", "5494ed4540"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "How does the Extra Generation Module generate required context according to Wang et al. (2022a)?\n", "true_answer": "The Extra Generation Module generates required context by using a large language model (LLM) to produce the content. This approach is more likely to contain relevant information compared to direct retrieval.", "source_doc": "RAG.pdf", "source_id": "3ed835a82b", "retrieved_docs": ["data[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-\ndundancy and noise are common issues. Instead of di-\nrectly retrieving from a data source, the Extra Gener-\nation Module leverages LLM to generate the required\ncontext [Yuet al. , 2022 ]. Content generated by LLM is\nmore likely to contain relevant information compared to\ndirect retrieval.", "RAG, the search module, tailored to specific sce-\nnarios, incorporates direct searches on (additional)\ncorpora in the process using LLM-generated code,\nquery languages (e.g., SQL, Cypher), or other cus-\ntom tools. The data sources for searching can include\nsearch engines, text data, tabular data, or knowledge\ngraphs [Wang et al. , 2023c ].\n\u2022Memory Module: Leveraging the memory capabili-\nties of LLM itself to guide retrieval, the principle in-\nvolves finding memories most similar to the current in-\nput. Self-mem [Cheng et al. , 2023b ]iteratively employs\na retrieval-enhanced generator to create an unbounded\nmemory pool, combining the \u201coriginal question\u201d and\n\u201cdual question.\u201d A retrieval-enhanced generative model\ncan use its own outputs to enhance itself, making the\ntext closer to the data distribution in the reasoning pro-\ncess, with the model\u2019s own outputs rather than training\ndata[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-", "of LLMs primarily drive this approach, but also\nthe desire to have a human-in-the-loop for some\nco-writing use cases [368].\nLimited Context Window [368, 637]\nThe inability of current LLMs to keep the\nentire generated work within the context\nwindow currently constrains their long-form\napplications and generates the need for mod-\nular prompting (14).\nFor short form generation, Chakrabarty et al.\n[69] propose CoPoet (fine-tuned T5 and T0 models)\nfor collaborative poetry generation, Razumovskaia\net al. [452] use PaLM and prompting with plansfor cross-lingual short story generation, Wang et al.\n[584] use GPT-4 as part of the ReelFramer tool to\nhelp co-create news reels for social media, Ippolito\net al. [232] use LaMDA as part of the Wordcraft cre-\native writing assistant, and Calderwood et al. [63]\napply a fine-tuned GPT-3 model as part of their\nSpindle tool for helping generate choice-based in-\nteractive fiction.\nFor more general creative tasks, Haase and", "find( object )) that the LLM can then use to gen-\nerate solution code. This approach significantly\nreduces the tokens needed to provide repository/-\ncode context by only providing the API definition.\nThis API definition approach, illustrated in 13 has\nbeen used in robotics by Vemprala et al. [564] , and\nby Wang et al. [579] as part of a Minecraft agent.\nPreviously, Gupta and Kembhavi [185] used a pre-\ndefined function approach within VISPROG, which\nuses GPT-3, external python modules , and few-shot\nprompting with example programs to solve visual\ntasks.\n3.3.2 Code Infilling and Generation\nCode infilling refers to modifying or completing\nexisting code snippets based on the code context\nand instructions provided as a prompt.\nFried et al. [154] train the InCoder LLM (up\nto 6.7B parameters) to both generate Python code\nand infill existing code using a masked language\nmodeling approach. Incoder is trained using 159\nGB of text split roughly equally between Python", "ples include Hit Rate, MRR, NDCG, Precision, etc.\n2.Generation Module\nThe generation module here refers to the enhanced or\nsynthesized input formed by supplementing the retrieved\ndocuments into the query, distinct from the final an-\nswer/response generation, which is typically evaluated\nend-to-end. The evaluation metrics for the generation\nmodule mainly focus on context relevance, measuring\nthe relatedness of retrieved documents to the query ques-\ntion.\nEnd-to-End Evaluation\nEnd-to-end evaluation assesses the final response gener-\nated by the RAG model for a given input, involving the\nrelevance and alignment of the model-generated answers\nwith the input query. From the perspective of content\ngeneration goals, evaluation can be divided into unlabeled\nand labeled content. Unlabeled content evaluation met-\nrics include answer fidelity, answer relevance, harmless-\nness, etc., while labeled content evaluation metrics in-\nclude Accuracy and EM. Additionally, from the perspec-"], "retrieved_docs_id": ["3ed835a82b", "9067222c76", "35c5e878ad", "aa3f18a1ed", "b023f9e1c7"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the approach of augmented pre-training perform in knowledge-intensive tasks compared to standard GPT models?\n", "true_answer": "The approach of augmented pre-training outperforms standard GPT models in handling knowledge-intensive tasks, particularly in open-domain question answering.", "source_doc": "RAG.pdf", "source_id": "7411eec79c", "retrieved_docs": ["Figure 4: Taxonomy of RAG\u2019s Core Components\nextension of RETRO, increased the model\u2019s parameter scale.\nStudies have found consistent improvements in text genera-\ntion quality, factual accuracy, low toxicity, and downstream\ntask accuracy, particularly in knowledge-intensive tasks such\nas open-domain question answering. These research findings\nhighlight the promising direction of pretraining autoregres-\nsive language models in conjunction with retrieval for future\nfoundational models.\nIn summary, the advantages and limitations of augmented\npre-training are evident. On the positive side, this approach\noffers a more powerful foundational model, outperforming\nstandard GPT models in perplexity, text generation quality,\nand downstream task performance. Moreover, it achieves\nhigher efficiency by utilizing fewer parameters compared to\npurely pre-trained models. It particularly excels in handling\nknowledge-intensive tasks, allowing the creation of domain-", "knowledge-intensive tasks, allowing the creation of domain-\nspecific models through training on domain-specific corpora.\nHowever, there are drawbacks, including the requirement for\na substantial amount of pre-training data and larger training\nresources, as well as the issue of slower update speeds. Espe-\ncially as model size increases, the cost of retrieval-enhanced\ntraining becomes relatively higher. Despite these limitations,\nthis method demonstrates notable characteristics in terms of\nmodel robustness. Once trained, retrieval-enhanced models\nbased on pure pre-training eliminate the need for external li-brary dependencies, enhancing both generation speed and op-\nerational efficiency.\nFine-tuning Stage\nDuring the downstream fine-tuning phase, researchers have\nemployed various methods to fine-tune retrievers and gener-\nators for improved information retrieval, primarily in open-\ndomain question-answering tasks. Concerning retriever fine-", "provide enough information about whether the produced answer would contradict.\nRetrieval-augmented language model pre-training\n(REALM) [ 186] inserts retrieved documents\ninto the pre-training examples. While Guu et al.\n[186] designed REALM for extractive tasks\nsuch as question-answering, Lewis et al. [304]\npropose retrieval-augmented generation (RAG), a\nlanguage generation framework using retrievers\nfor knowledge-intensive tasks that humans could\nnot solve without access to an external knowledge\nsource. Yogatama et al. [646] propose the adaptive\nSemiparametric Language Models architecture,\nwhich incorporates the current local context, a\nshort-term memory that caches earlier-computed\nhidden states, and a long-term memory based on a\nkey-value store of (hidden-state, output) tuples. To\nequip a retrieval-augmented LLM with few-shot\nabilities that were before only emergent in LLMs\nwith many more parameters, Izacard et al. [236]\npropose a KL-divergence loss term for retrieval", "As a knowledge-intensive task, RAG employs different tech-\nnical approaches during the language model training\u2019s pre-\ntraining, fine-tuning, and inference stages.\nPre-training Stage\nSince the emergence of pre-trained models, researchers have\ndelved into enhancing the performance of Pre-trained Lan-\nguage Models (PTMs) in open-domain Question Answering\n(QA) through retrieval methods at the pre-training stage. Rec-\nognizing and expanding implicit knowledge in pre-trained\nmodels can be challenging. REALM [Arora et al. , 2023 ]in-\ntroduces a more modular and interpretable knowledge em-\nbedding approach. Following the Masked Language Model\n(MLM) paradigm, REALM models both pre-training and\nfine-tuning as a retrieve-then-predict process, where the lan-\nguage model pre-trains by predicting masked tokens ybased\non masked sentences x, modeling P(x|y).\nRETRO [Borgeaud et al. , 2022 ]leverages retrieval aug-\nmentation for pre-training a self-regressive language model,", "guistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703 .\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Kuttler, M. Lewis, Wen tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. Retrieval-\naugmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing\nSystems (NeurIPS) , 2020c.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Association\nfor Computational Linguistics (ACL) , 2021.\nYuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan. Deep entity matching with\npre-trained language models. arXiv preprint arXiv:2004.00584 , 2020.\nPercy Liang and Rob Reich. Condemning the deployment of gpt-4chan, 2022. URL https://docs.google.\ncom/forms/d/e/1FAIpQLSdh3Pgh0sGrYtRihBu-GPN7FSQoODBLvF7dVAFLZk2iuMgoLw/viewform .\n100"], "retrieved_docs_id": ["7411eec79c", "011ee221ab", "f7770d2394", "6240233238", "cce33360f9"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the SPHINX-X project create image captions with fine-grained correspondence to texts?\n", "true_answer": "The SPHINX-X project creates image captions with fine-grained correspondence to texts by using GPT-4V to generate captions from marked images and domain-specific guidelines.", "source_doc": "multimodal.pdf", "source_id": "7d5705c52b", "retrieved_docs": ["regional details and object relationships insight. During the training process, SPHINX-X utilizes the\nunaltered images rather than the marked ones. ALLaV A[29] propose to distill a caption and a QA\npair for an image within a single session. Specifically, it prompts GPT-4V with an image, and ask it\nto first generate a fine-grained caption then a VQA pair.\nAdditionally, excluding multimodal instructional data, conversations solely based on language be-\ntween users and assistants can significantly contribute to enhancing a model\u2019s conversational exper-\ntise and responsiveness to directives.For example, VILA\u2019s[49] research demonstrates that integrating\ntext-only instructional data with image-text data during the fine-tuning process not only mitigates the\ndecline in performance for text-only tasks but also enhances the accuracy of MLLM-related tasks.\nDataset Name Type I \u2192O Source Method #.Instance Representative Publications", "simultaneously saving all text annotations along with their respective bounding boxes. Ultimately,\nthese elements are converted into a unified question-answering format.\nWhile multi-task datasets provide an abundant source of data, they may not always be suitable\nfor complex real-world situations, such as engaging in multi-turn conversations. To address this\nchallenge, some research has explored the use of self-instruction by leveraging LLMs to gener-\nate text-based or multimodal instruction-following data from a limited number of hand-annotated\nsamples. SPHINX-X[14] assembles a rich multi-domain dataset with fine-grained correspondence\nbetween images and texts.It gathers images from diverse sources and then employs annotations to\napply various markers onto the original images. By prompting GPT-4V with these marked images\nand tailored domain-specific guidelines, the system generates captions that offer an image overview,", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "data, training strategy, and safety and alignment.\n\u2022Visual instruction data . Extensive work [831, 856] has\nempirically found that both quantity and quality of visual\ninstructions have an important impact on model perfor-\nmance of MLLMs. One basic way to construct visual in-\nstructions is to leverage the exceptional capability of LLMs\nto synthesize instructions based on text descriptions of\nimages [851]. To further enhance the quality of instructions,\none can construct fine-grained visual instructions with the\nhelp of human annotation [833, 857] or synthesize more\ncomplex data through carefully-designed prompts [835].\nDespite the effectiveness of the above LLM-based ap-\nproaches, one primary question emerges as to whether a\nLLM ( i.e.,text generation model without training on any\nimages) possesses the ability to generate sufficiently good\nvisual instructions solely based on verbalized visual infor-\nmation ( e.g., captions and coordinates). Specially, existing", "A summary of frequently used pre-training datasets can be found in Table.3. High-quality IT data\ncan be derived from task-specific datasets. For instance, consider a sample from VQA datasets where\nthe input includes an image and a natural language question, and the output is the text-based answer\nto the question based on the image. This could easily form the multimodal input and response\nof the instruction sample. The instructions, or task descriptions, can be obtained either through\nmanual creation or semi-automatic generation with the help of GPT. In addition to utilizing publicly\navailable task-specific datasets, SPHINX-X[14] assembles a dataset focused on OCR from a wide\nrange of PDF data sourced from the internet. Specifically, it begins by gathering a large-scale PDF\ndataset from the web. It then obtains the rendering results of each page in the PDF file, while\nsimultaneously saving all text annotations along with their respective bounding boxes. Ultimately,"], "retrieved_docs_id": ["54fa378ef2", "7d5705c52b", "92e73c053a", "ab2e727941", "db45826cee"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "What is one ability of the RGB model in augmented generation that deals with erroneous information?\n", "true_answer": "Counterfactual Robustness is the ability of the RGB model in augmented generation that deals with erroneous information by identifying and handling it when receiving instructions about potential risks in retrieved information.", "source_doc": "RAG.pdf", "source_id": "070aa6c4f4", "retrieved_docs": ["augmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness\nThis capability measures the model\u2019s efficiency in han-\ndling noisy documents, which are those related to the\nquestion but do not contain useful information.\n2.Negative Rejection\nWhen documents retrieved by the model lack the knowl-\nedge required to answer a question, the model should\ncorrectly refuse to respond. In the test setting for neg-\native rejection, external documents contain only noise.\nIdeally, the LLM should issue a \u201dlack of information\u201d or\nsimilar refusal signal.\n3.Information Integration\nThis ability assesses whether the model can integrate\ninformation from multiple documents to answer more\ncomplex questions.4.Counterfactual Robustness\nThis test aims to evaluate whether the model can iden-\ntify and deal with known erroneous information in doc-\numents when receiving instructions about potential risks\nin retrieved information. Counterfactual robustness tests", "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "provide enough information about whether the produced answer would contradict.\nRetrieval-augmented language model pre-training\n(REALM) [ 186] inserts retrieved documents\ninto the pre-training examples. While Guu et al.\n[186] designed REALM for extractive tasks\nsuch as question-answering, Lewis et al. [304]\npropose retrieval-augmented generation (RAG), a\nlanguage generation framework using retrievers\nfor knowledge-intensive tasks that humans could\nnot solve without access to an external knowledge\nsource. Yogatama et al. [646] propose the adaptive\nSemiparametric Language Models architecture,\nwhich incorporates the current local context, a\nshort-term memory that caches earlier-computed\nhidden states, and a long-term memory based on a\nkey-value store of (hidden-state, output) tuples. To\nequip a retrieval-augmented LLM with few-shot\nabilities that were before only emergent in LLMs\nwith many more parameters, Izacard et al. [236]\npropose a KL-divergence loss term for retrieval", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions."], "retrieved_docs_id": ["070aa6c4f4", "6291d3f5de", "af911eac69", "f7770d2394", "92e73c053a"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the perplexity score of the Mini-Gemini [26] Gemma-2B model?\n", "true_answer": "The perplexity score of the Mini-Gemini [26] Gemma-2B model is 56.2.", "source_doc": "multimodal.pdf", "source_id": "0ad4077d27", "retrieved_docs": ["Cobra [13] Mamba-2.8B 75.9 58.5 - 46.0 52.0 - - - - - - 88.0 - -\nMini-Gemini [26] Gemma-2B - - - 56.2 - 31.7/29.1 29.4 1341.0 312.0 59.8 - - - 31.1\nVary-toy [27] Qwen-1.8B - - - - - - - - - - - - - 29.0\nTinyGPT-V [28] Phi-2 (2.7B) - 33.6 - - 24.8 - - - - - - - - -\nSPHINX-Tiny [14] TinyLlama-1.1B - - - 57.8 - - 26.4 1261.2 242.1 56.6 17.1/- 82.2 52.3 23.8\nALLaV A-Longer [29] Phi-2 (2.7B) - 50.0 - 50.3 - 33.2/- - 1564.6\u202064.6 - - 71.7 35.5\nMM1-3B [30] MM1-3B 82.5 - 76.1 72.9 - 38.6/35.7 32.6 1469.4 303.1 70.8 63.9/69.4 87.6 76.8 42.2\nLLaV A-Gemma [31] Gemma-2b-it 71.4 58.7 - - - - - 1133.0 307.0 - - 85.3 - 19.1\nMipha-3B [32] Phi-2 (2.7B) 81.3\u221763.9\u221770.9 56.6 47.5 - - 1488.9 295.0 69.7 - 86.7 - 32.1\nVL-Mamba [18] Mamba-2.8B 76.6 56.2 65.4 48.9 - - - 1369.6 - 57.0 - 84.4 - 32.6\nMiniCPM-V 2.0[33] MiniCPM-2.4B - - - 74.1 - 38.2/- 38.7 1808.6\u202069.6 - - - -\nDeepSeek-VL [34] DeepSeek-LLM-1B - - - - - 32.2/- 31.1 - - 64.6 -/66.7 87.6 - 36.8", "corpora are often brief and contain noise, which can be refined and filtered using automated meth-\nods, such as employing the CLIP [13] model to eliminate image-text pairs with low similarity scores.\nA summary of frequently used pre-training datasets can be found in Figure2.\nDataset Name X Modality #.X #.T #.X-T Representative Publications\nCC3M [162] Image 3.3M 3.3M 3.3M TinyGPT-V[28],MM1[30]\nCC12M [163] Image 12.4M 12.4M 12.4M MM1[30]\nSBU [164] Image 1M 1M 1M TinyGPT-V[28]\nLAION-5B [165] Image 5.9B 5.9B 5.9B TinyGPT-V[28]\nLAION-COCO[166] Image 600M 600M 600M Vary-toy [27]\nCOYO [167] Image 747M 747M 747M MM1[30]\nCOCO Caption[168] Image 164K 1M 1M Vary-toy [27]\nCC595k [7] Image 595K 595K 595KMobileVLM [20],LLaV A-Phi [21],\nLLaV A-Gemma [31],Mini-Gemini [26]\nRefCOCO[169] Image 20K 142K 142K Vary-toy [27]\nDocVQA[170] Image 12K 50K 50K Vary-toy [27]\nLLava-1.5-PT[54] Image 558K 558K 558KImp-v1 [22],MoE-LLaV A [25],\nVary-toy [27],Mipha [32],\nVL-Mamba [18],Tiny-LLaV A [23]", "MM1-3B-MoE-Chat [30] CLIP DFN-ViT-H [83] 378 - - 3B\u2217C-Abstractor [19]\nLLaV A-Gemma [31] DinoV2 [76] - - Gemma-2b-it[78] 2B -\nMipha-3B [32] SigLIP [75] 384 - Phi-2[74] 2.7B -\nVL-Mamba [18] SigLIP-SO [75] 384 - Mamba-2.8B-Slimpj[77] 2.8B VSS-L2[18]\nMiniCPM-V 2.0[33] SigLIP [75] - 0.4B MiniCPM[70] 2.4B Perceiver Resampler [16]\nDeepSeek-VL [34] SigLIP-L [75] 384 0.4B DeepSeek-LLM[84] 1.3B MLP\nKarmaVLM[71] SigLIP-SO [75] 384 0.4B Qwen1.5[79] 0.5B -\nmoondream2[72] SigLIP[75] - - Phi-1.5[85] 1.3B -\nBunny-v1.1-4B[24] SigLIP[75] 1152\u2020- Phi-3-Mini-4K[86] 3.8B -\nTable 1: The summary of 17 mainstream efficient MMLMs.\u2217indicates activated parameters.\u2020:High\nresolution support is achieved with S2-Wrapper[40].\nIn line with mainstream MLLM practices, efficient MLLMs select pre-trained models that are se-\nmantically aligned with the text, represented by CLIP [73]. This approach facilitates better align-\nment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a", "Efficient MLLMArchitecture (\u00a72)Vision Encoder (\u00a72.1)ViTamin [11], BRA VE[12],\nCobra[13], SPHINX-X[14]\nVision-Language Projector (\u00a72.2)QFormer [15], Perceiver Resampler[16],\nLDPv2[17], VSS[18], C/D-Abstractor[19],\nMEQ-Former[12]\nSmall Language Models (\u00a72.3)MobileVLM [20], LLaV A-Phi [21],\nImp-v1 [22], TinyLLaV A [23],\nBunny [24], Gemini Nano-2 [2],\nMobileVLM-v2 [17], MoE-LLaV A [25],\nCobra [13], Mini-Gemini [26],\nVary-toy [27], TinyGPT-V [28],\nSPHINX-Tiny [14], ALLaV A [29],\nMM1 [30], LLaV A-Gemma [31],\nMipha [32], VL-Mamba [18]\nMiniCPM-V 2.0 [33], DeepSeek-VL [34]\nVision Token Compression (\u00a72.4)Mini-Gemini [26], LLaV A-UHD [35],\nTextHawk [36], TinyChart [37], P2G [38],\nIXC2-4KHD [39], SPHINX-X[14], S2[40]\nLLaV A-PruMerge[41], MADTP[42],\nMoV A[43], Video-LLaV A[44]\nEfficient Structures (\u00a72.5)SPD [45], MoE-LLaV A [25],\nMM1 [30], Cobra [13], VL-Mamba [18],\nFastV[46], VTW[47]\nTraining (\u00a75)Pre-Training (\u00a75.1) Idefics2[48], TinyLLaV A[23], VILA[49]", "its selection is closely related to the lightweight nature of MLLM. In comparison to conventional\nMLLMs with parameter sizes ranging from 7 billion to tens of billions[87, 88], efficient MLLMs\ntypically employ language models with less than 3 billion parameters, such as phi2-2.7B[74] by\nMicrosoft and Gemma-2B[78] by Google. Phi-2 trained on special data recipes can match the per-\nformance of models 25 times larger trained on regular data. Phi-3-mini [86] can be easily deployed\nlocally on a modern phone and achieves a quality that seems on-par with models such as Mixtral\n8x7B [89] and GPT-3.5. In addition to utilizing pre-trained models, MobileVLM[20] downscales\nLLaMA[87] and trains from scratch using open-source datasets. The specific model scaling is illus-\ntrated in the Table.1 and Table.4.\n2.4 Vision Token Compression\nInitial research has underscored the potential of MLLMs across various tasks, including visual ques-"], "retrieved_docs_id": ["0ad4077d27", "a75a61f59f", "e8fc8ad809", "93d03b64f9", "26327c579e"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does ITRG enhance adaptability for tasks requiring multiple-step reasoning?\n", "true_answer": "ITRG enhances adaptability for tasks requiring multiple-step reasoning by iteratively retrieving and searching for the correct reasoning path.", "source_doc": "RAG.pdf", "source_id": "1f6c13012c", "retrieved_docs": ["vant knowledge without altering the parameters of LLMs, en-\nabling the model to perform more sophisticated tasks. Addi-\ntionally, CREA-ICL [Liet al. , 2023b ]leverages synchronous\nretrieval of cross-lingual knowledge to assist in acquiring ad-\nditional information, while RECITE forms context by sam-\npling one or more paragraphs from LLMs.\nDuring the inference phase, optimizing the process of RAG\ncan benefit adaptation to more challenging tasks. For ex-ample, ITRG [Feng et al. , 2023a ]enhances adaptability for\ntasks requiring multiple-step reasoning by iteratively retriev-\ning and searching for the correct reasoning path. ITER-\nRETGEN [Shao et al. , 2023 ]employs an iterative approach\nto coalesce retrieval and generation, achieving an alternating\nprocess of \u201dretrieval-enhanced generation\u201d and \u201dgeneration-\nenhanced retrieval.\u201d\nOn the other hand, IRCOT [Trivedi et al. , 2022 ]merges the\nconcepts of RAG and CoT [Weiet al. , 2022 ], employing al-", "refine the reasoning processes, Self-Refine [685] elicits feed-\nback from LLMs on self-generated solutions, enabling the\niterative refinement of solutions based on the feedback.\nMoreover, several studies improve the consistency in the\nreasoning chain of LLMs through the integration of process-\nbased supervision during training [688, 689]. As a promis-\ning solution, recent approaches reformulate the complex\nreasoning tasks into code generation tasks, where the strict\nexecution of the generated code ensures the consistency\nbetween the reasoning process and the outcome. Also,\nit has been revealed that there might exist inconsistency\nbetween tasks with similar inputs, where small changesin the task description may cause the model to produce\ndifferent results [49, 592]. To mitigate this problem, self-\nconsistency [436] adopts the ensemble of multiple reasoning\npaths to enhance the decoding process of LLMs.\nReasoning Inconsistency\nLLMs may generate the correct answer following", "adapters into LLMs, empowering researchers to im-\nplement adapter-based PEFT methods for a wide\nrange of tasks. To evaluate different PEFT meth-\nods on downstream tasks, we construct two high-\nquality fine-tuning datasets to enhance PEFT per-\nformance on math reasoning and commonsense rea-\nsoning tasks. By utilizing the LLM-Adapter toolkit\nand the constructed fine-tuning datasets, we con-\nduct a comprehensive empirical study and find the\nanswer of research questions on the optimal place-\nment and configuration of different PEFT methods,\nthe impact of adapter architectures, and the influ-\nence of ID and OOD scenarios. We hope this work", "phase to capture key semantic meanings. In the later\nstages of this process, larger blocks with more contex-\ntual information are provided to the language model\n(LM). This two-step retrieval method helps strike a bal-\nance between efficiency and contextually rich responses.\n\u2022StepBack-prompt: Integrated into the RAG process,\nthe StepBack-prompt approach [Zheng et al. , 2023 ]en-\ncourages LLM to step back from specific instances and\nengage in reasoning about the underlying general con-\ncepts or principles. Experimental findings indicate a sig-\nnificant performance improvement in various challeng-\ning, inference-intensive tasks with the incorporation of\nbackward prompts, showcasing its natural adaptability\nto RAG. The retrieval-enhancing steps can be applied in\nboth the generation of answers to backward prompts and\nthe final question-answering process.\n\u2022Subqueries: Various query strategies can be employed in\ndifferent scenarios, including using query engines pro-", "Figure 1.2: Larger models make increasingly ef\ufb01cient use of in-context information. We show in-context learning\nperformance on a simple task requiring the model to remove random symbols from a word, both with and without a\nnatural language task description (see Sec. 3.9.2). The steeper \u201cin-context learning curves\u201d for large models demonstrate\nimproved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\nof tasks.\nsuf\ufb01cient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\nto a conceptual limitation in our current NLP techniques, this adaptability has practical advantages \u2013 it allows humans\nto seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\ndialogue. To be broadly useful, we would someday like our NLP systems to have this same \ufb02uidity and generality."], "retrieved_docs_id": ["1f6c13012c", "c938be55de", "3d9464fa95", "ad03b3dcc5", "74ba43f588"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Llama-2 optimize memory bandwidth during autoregressive decoding?\n", "true_answer": "Llama-2 optimizes memory bandwidth during autoregressive decoding by using a technique called grouped-query attention (GQA), which is a Sharing-based Attention mechanism that partitions all query heads into several groups, with each group\u2019s query heads sharing a common key-value head.", "source_doc": "multimodal.pdf", "source_id": "7d67b0debb", "retrieved_docs": ["low-rank, fixed and learnable pattern strategies, and hardware-assisted attention.\nSharing-based Attention Sharing-based Attention aims to expedite attention computation dur-\ning inference by by sharing computation resources across multiple Key-Value heads. For exam-\nple, Llama-2 [91] incorporates a technique called grouped-query attention (GQA) [143] to opti-\nmize memory bandwidth during the autoregressive decoding. GQA is a Sharing-based Attention\ntechnique that seeks to achieve a balance between performance and efficiency, positioned between\nmulti-head attention and multi-query attention [144] mechanisms. In multi-head attention, each head\nutilizes a distinct set of linear transformation parameters for queries, keys, and values. Conversely,\nmulti-query attention shares a single set of key-value heads across all queries. GQA partitions all\nquery heads into several groups, with each group\u2019s query heads sharing a common key-value head,", "be accommodated. Moreover, inefficient memory manage-\nment can further decrease the batch size, as shown in Fig. 2.\nAdditionally, given the current trends, the GPU\u2019s computa-\ntion speed grows faster than the memory capacity [ 17]. For\nexample, from NVIDIA A100 to H100, The FLOPS increases\nby more than 2x, but the GPU memory stays at 80GB max-\nimum. Therefore, we believe the memory will become an\nincreasingly significant bottleneck.\nComplex decoding algorithms. LLM services offer a range\nof decoding algorithms for users to select from, each with\nvarying implications for memory management complexity.\nFor example, when users request multiple random samples\nfrom a single input prompt, a typical use case in program\nsuggestion [ 18], the KV cache of the prompt part, which\naccounts for 12% of the total KV cache memory in our ex-\nperiment (\u00a76.3), can be shared to minimize memory usage.\nOn the other hand, the KV cache during the autoregressive", "On the other hand, the KV cache during the autoregressive\ngeneration phase should remain unshared due to the dif-\nferent sample results and their dependence on context and\nposition. The extent of KV cache sharing depends on the\nspecific decoding algorithm employed. In more sophisticated\nalgorithms like beam search [ 49], different request beams\ncan share larger portions (up to 55% memory saving, see\u00a76.3) of their KV cache, and the sharing pattern evolves as\nthe decoding process advances.\nScheduling for unknown input & output lengths. The\nrequests to an LLM service exhibit variability in their input\nand output lengths. This requires the memory management\nsystem to accommodate a wide range of prompt lengths. In\naddition, as the output length of a request grows at decoding,\nthe memory required for its KV cache also expands and may\nexhaust available memory for incoming requests or ongoing\ngeneration for existing prompts. The system needs to make", "3\u00d7faster compared to the PyTorch implementation for\nOPT-30B. Remarkably, for bigger models that have to be\ndistributed across multiple GPUs, SmoothQuant achieves\nsimilar or even better latency using only half the number of\nGPUs (1 GPU instead of 2 for OPT-66B, 4 GPUs instead\nof 8 for OPT-175B). This could greatly lower the cost of\nserving LLMs. The amount of memory needed when us-\ning SmoothQuant-O3 in FasterTransformer is reduced by a\nfactor of almost 2 \u00d7, as shown on Figure 9 (bottom).\nDecoding-stage. In Table 7, we show SmoothQuant can\nsignificantly accelerate the autoregressive decoding stage\n7", "inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\ngeneration. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\nwithout sacrificing performance on non-code related benchmarks.\nMistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SW A) [ 6,3].\nGQA significantly accelerates the inference speed, and also reduces the memory requirement during\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\napplications. In addition, SW A is designed to handle longer sequences more effectively at a reduced\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023"], "retrieved_docs_id": ["7d67b0debb", "75b52830d5", "99dba88b12", "0b7b873fcf", "cd08da6a81"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What percentage of the overall cost does DAPT account for in ChipNeMo training?\n", "true_answer": "DAPT accounts for less than 1.5% of the overall cost in ChipNeMo training.", "source_doc": "ChipNemo.pdf", "source_id": "d9ae12f819", "retrieved_docs": ["large adapter exhibiting a slight improvement.\n3.4. Training Cost\nAll models have undergone training using 128 A100 GPUs.\nWe estimate the costs associated with domain adaptive pre-\ntraining for ChipNeMo as illustrated in Table 1. It is worth\nnoting that DAPT accounts for less than 1.5% of the overall\n5", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat.", "models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting"], "retrieved_docs_id": ["d9ae12f819", "273b593026", "7eb44773ae", "df0b9868f2", "a5a7c4ceb0"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does a purely parameterized language model acquire and store world knowledge?\n", "true_answer": "A purely parameterized language model acquires world knowledge from vast corpora and stores it in the parameters of the model.", "source_doc": "RAG.pdf", "source_id": "fc82ce8e28", "retrieved_docs": ["play distinct roles. Parametric knowledge is acquired through\ntraining LLMs and stored in the neural network weights, rep-\nresenting the model\u2019s understanding and generalization of\nthe training data, forming the foundation for generated re-\nsponses. Non-parametric knowledge, on the other hand, re-\nsides in external knowledge sources such as vector databases,\nnot encoded directly into the model but treated as updatable\nsupplementary information. Non-parametric knowledge em-\npowers LLMs to access and leverage the latest or domain-\nspecific information, enhancing the accuracy and relevance\nof responses.\nPurely parameterized language models (LLMs) store their\nworld knowledge, which is acquired from vast corpora, in\nthe parameters of the model. Nevertheless, such models have\ntheir limitations. Firstly, it is difficult to retain all the knowl-\nedge from the training corpus, especially for less common\nand more specific knowledge. Secondly, since the model", "the retrieval performance, several studies also propose an\niterative reading-then-reasoning framework, enabling the\nLLM to interact with the KG multiple times and acquire the\nrequired knowledge in a more accurate way [458]. For the\nsecond issue ( i.e.,utilizing retrieved knowledge), a straight-\nforward approach is to serialize the retrieved subgraph\nand craft specific prompts to include it as the input of\nLLMs [471, 651]. However, due to the loss of structured\ninformation in knowledge serialization, LLMs cannot fully\ncapture the structural semantics conveyed by original KGs.\nTo address this issue, several model-based approaches train\na specialized language model ( e.g., T5) to transform the\nsubgraph into the natural language text [867]. To guarantee\nthe transformation accuracy, it relies on sufficient training\npairs (often unsupervised constructed) [868] and excellent\nmodel capability [869].\nSynergy-Augmented LLM. To solve complex tasks ( e.g.,", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "Figure 4: Taxonomy of RAG\u2019s Core Components\nextension of RETRO, increased the model\u2019s parameter scale.\nStudies have found consistent improvements in text genera-\ntion quality, factual accuracy, low toxicity, and downstream\ntask accuracy, particularly in knowledge-intensive tasks such\nas open-domain question answering. These research findings\nhighlight the promising direction of pretraining autoregres-\nsive language models in conjunction with retrieval for future\nfoundational models.\nIn summary, the advantages and limitations of augmented\npre-training are evident. On the positive side, this approach\noffers a more powerful foundational model, outperforming\nstandard GPT models in perplexity, text generation quality,\nand downstream task performance. Moreover, it achieves\nhigher efficiency by utilizing fewer parameters compared to\npurely pre-trained models. It particularly excels in handling\nknowledge-intensive tasks, allowing the creation of domain-", "putational expenses for both training and inference. To ad-\ndress the limitations of purely parameterized models, lan-\nguage models can adopt a semi-parameterized approach by\nintegrating a non-parameterized corpus database with pa-\nrameterized models. This approach is known as Retrieval-\nAugmented Generation (RAG).\nThe term Retrieval-Augmented Generation (RAG) was\nfirst introduced by [Lewis et al. , 2020 ]. It combines a pre-\ntrained retriever with a pre-trained seq2seq model (generator)\nand undergoes end-to-end fine-tuning to capture knowledge\nin a more interpretable and modular way. Before the advent\nof large models, RAG primarily focused on direct optimiza-\ntion of end-to-end models. Dense retrievals on the retrieval\nside, such as the use of vector-based Dense Passage Retrieval\n(DPR) [Karpukhin et al. , 2020 ], and training smaller models\non the generation side are common practices. Due to the\noverall smaller parameter size, both the retriever and gener-"], "retrieved_docs_id": ["fc82ce8e28", "fb5c13b0e4", "4fffd3dc2b", "7411eec79c", "33aae5a21d"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How do pre-trained language models primarily function?\n", "true_answer": "Pre-trained language models primarily function as completion machines.", "source_doc": "hallucination.pdf", "source_id": "f524021191", "retrieved_docs": ["During this stage, language models engage in autoregressive prediction, wherein they predict the\nsubsequent token in a sequence. By undergoing self-supervised training on vast textual datasets,\nthese models develop an understanding of language syntax, gain access to world knowledge, and\nenhance their reasoning capabilities. This pre-training process establishes a solid groundwork for\nthe models to undertake subsequent fine-tuning tasks effectively.\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge\nand skills, it\u2019s important to acknowledge that its primary focus is on optimizing for completion.\nConsequently, pre-trained LLMs essentially function as completion machines, which may create\na misalignment between the objective of predicting the next word within LLMs and the user\u2019s\nobjective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-", "holistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and\ufb01ne-tuning . The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning allpre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the", "This can be attributed to the pre-training phase already capturing significant information, leaving the fine-tuning\nstage primarily to focus on task-specific adjustments. In essence, LoRA offers a compelling approach to parameter\nreduction by leveraging the notion of intrinsic dimension in weight matrices. By adopting a mathematically rigorous\nframework, LoRA enables more efficient adaptation of pre-trained language models to new tasks during the fine-tuning", "to the largest one \u2013 SNLI ( \u2248550k training examples).\n5 Analysis\nImpact of number of layers transferred We observed the impact of transferring a variable number\nof layers from unsupervised pre-training to the supervised target task. Figure 2(left) illustrates the\nperformance of our approach on MultiNLI and RACE as a function of the number of layers transferred.\nWe observe the standard result that transferring embeddings improves performance and that each\ntransformer layer provides further bene\ufb01ts up to 9% for full transfer on MultiNLI. This indicates that\neach layer in the pre-trained model contains useful functionality for solving target tasks.\nFigure 2: ( left) Effect of transferring increasing number of layers from the pre-trained language\nmodel on RACE and MultiNLI. ( right ) Plot showing the evolution of zero-shot performance on\ndifferent tasks as a function of LM pre-training updates. Performance per task is normalized between", "since the loss surface of LLMs should not have a large curvature. Note that this holds only when we\nteach the LLMs natural language-based tasks (or code-based if pre-trained with code). A synthetic\nloss function unrelated to pre-training tasks will indeed face the large curvature problem.\nLocal optimum is good enough The goal of fine-tuning is to adapt LLMs to new tasks and do-\nmains without significantly changing the model itself. Therefore, a local optimum is often a good\nenough solution, and the limited training data (compared to pre-training corpus) makes it difficult to\npush the model to a faraway global optimum.\n3"], "retrieved_docs_id": ["f524021191", "8b03dbb287", "bf757f65af", "d256376dcc", "59bfe0a036"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How did using a larger learning rate affect the model's performance in the ablation studies?\n", "true_answer": "Using a larger learning rate led to substantial degradations across all domain-specific and academic benchmarks, except on coding.", "source_doc": "ChipNemo.pdf", "source_id": "49056b4ebb", "retrieved_docs": ["prohibitive costs of training enough models.\nAn easy yet expensive fix is to run ablations\nby varying one factor at a time, e.g., keeping\nmost hyper-parameters fixed except the model\nsize [ 44] or context lengths [ 557]. A cheaper po-\ntential remedy can be zero-shot hyper-parameter\ntransfer from smaller models to larger ones [ 608,\n633]. Yang et al. [633] find that when using the \u00b5P\nnetwork parameterization scheme, one can transfer\nthe effect of changing hyper-parameters such as the\nlearning rate across varying model depths, batch\nsizes, sequence lengths, and training times, which\nthey verify empirically up to a 6.7B model. How-\never, it has yet to be verified if such transferability\nstill holds for other varying factors; and if so, re-\nsearchers could afford to conduct more ablation\nexperiments via smaller models.\nIf additional experiments are prohibitively ex-\npensive, another recommendation is to report eval-\nuation results beyond aggregated performance mea-", "ablation studies to say this conclusively. However, Du et al. (2021) did perform ablation studies on the same\ntraining corpus, and show that the improvement in few-shot learning from careful data \ufb01ltering is extremely\nsigni\ufb01cant.\nSimilarly, we did not perform ablation studies to tease out the e\ufb00ects of (1) vs (2), due to the high training\ncost of performing such a study at full scale. In other words, a critical open scaling question is: \u201cHow would\na 62B parameter model trained for 7T tokens compare to our 540B parameter model trained for 780B tokens?\nWhat about a 120B model for 3.6T tokens? 240B for 1.8T tokens?\u201d It is clear that such a model would have\nroughly the same total training cost as PaLM 540B. However, if downstream task performance were to be\ncomparable, the smaller model would certainly be preferable, as the inference cost is proportional to its size.\nVery recently, Ho\ufb00mann et al. (2022) was published to explore this exact question. There, the authors train", "token log-probability when conditioned on the document and question. For DPRD [ 46] (winograd\nschemas), we replace the de\ufb01nite pronoun with the two possible referrents and predict the resolution\nthat the generative model assigns higher average token log-probability to the rest of the sequence\nafter the substitution.\nAblation studies We perform three different ablation studies (Table 5). First, we examine the\nperformance of our method without the auxiliary LM objective during \ufb01ne-tuning. We observe that\nthe auxiliary objective helps on the NLI tasks and QQP. Overall, the trend suggests that larger datasets\nbene\ufb01t from the auxiliary objective but smaller datasets do not. Second, we analyze the effect of the\nTransformer by comparing it with a single layer 2048 unit LSTM using the same framework. We\nobserve a 5.6 average score drop when using the LSTM instead of the Transformer. The LSTM only\noutperforms the Transformer on one dataset \u2013 MRPC. Finally, we also compare with our transformer", "3.3. Training Ablation Studies\nFor our ablation studies, we conducted multiple rounds of\ndomain adaptive pre-training. We provide brief summaries\nand refer to the Appendix A.6 for details.\nThe differences between training with the augmented tok-\nenizer and the original tokenizer appeared to be negligible.\nWe thus primarily attribute the accuracy degradation on\nopen-domain academic benchmarks to domain data. More-\nover, the removal of the public dataset only slightly re-\ngressed on most tasks including academic benchmarks.\nIn our exploration, we experimented with employing a larger\nlearning rate, as in CodeLLaMA (Rozi `ere et al., 2023). We\nobserved large spikes in training loss at the initial training\nsteps. Although this approach eventually led to improved\ntraining and validation loss, we noted substantial degrada-\ntions across all domain-specific and academic benchmarks,\nexcept on coding. We hypothesize that a smaller learning", "sense inference (Zellers et al., 2018). Given a sen-\ntence, the task is to choose the most plausible con-\ntinuation among four choices.\nWhen \ufb01ne-tuning on the SW AG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-speci\ufb01c parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation Cdenotes a score for each choice\nwhich is normalized with a softmax layer.\nWe \ufb01ne-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16. Re-\nsults are presented in Table 4. BERT LARGE out-\nperforms the authors\u2019 baseline ESIM+ELMo sys-\ntem by +27.1% and OpenAI GPT by 8.3%.\n5 Ablation Studies\nIn this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance. Additional"], "retrieved_docs_id": ["758ce381e6", "23ab72b1b7", "7f205dcffa", "49056b4ebb", "115954ef30"], "reranker_type": "colbert", "search_type": "text", "rr": 0.25, "hit": 1}, {"question": "Which Microsoft language model, with less than 3 billion parameters, can match the performance of models 25 times larger?\n", "true_answer": "Phi-2, as trained on special data recipes, can match the performance of models 25 times larger trained on regular data.", "source_doc": "multimodal.pdf", "source_id": "26327c579e", "retrieved_docs": ["its selection is closely related to the lightweight nature of MLLM. In comparison to conventional\nMLLMs with parameter sizes ranging from 7 billion to tens of billions[87, 88], efficient MLLMs\ntypically employ language models with less than 3 billion parameters, such as phi2-2.7B[74] by\nMicrosoft and Gemma-2B[78] by Google. Phi-2 trained on special data recipes can match the per-\nformance of models 25 times larger trained on regular data. Phi-3-mini [86] can be easily deployed\nlocally on a modern phone and achieves a quality that seems on-par with models such as Mixtral\n8x7B [89] and GPT-3.5. In addition to utilizing pre-trained models, MobileVLM[20] downscales\nLLaMA[87] and trains from scratch using open-source datasets. The specific model scaling is illus-\ntrated in the Table.1 and Table.4.\n2.4 Vision Token Compression\nInitial research has underscored the potential of MLLMs across various tasks, including visual ques-", "samples generated from the 8.3 billion parameters model\nin the Appendix C. Recently researchers from Microsoft in\ncollaboration with NVIDIA trained a 17 billion parameter\nGPT-2 model called Turing-NLG (Microsoft, 2020) using\nMegatron and showed that the accuracies further improve\nas they scale the model, highlighting the value of larger\nmodels.\nTo ensure we do not train on any data found in our test sets,\nwe calculate the percentage of test set 8-grams that also\nappear in our training set as done in previous work (Rad-\nford et al., 2019). The WikiText103 test set has at most\nFigure 6. Validation set perplexity. All language models are trained\nfor 300k iterations. Larger language models converge notice-\nably faster and converge to lower validation perplexities than their\nsmaller counterparts.\nTable 4. Model con\ufb01gurations used for BERT.\nParameter Layers Hidden Attention Total\nCount Size Heads GPUs\n336M 24 1024 16 128\n1.3B 24 2048 32 256\n3.9B 48 2560 40 512", "formance on several classification and semantic textual similarity splits. However, with 7.1\nbillion parameters it is an order of magnitude larger than models like the displayed mul-\ntilingual MiniLM37and MPNet38. SGPT-BLOOM-1.7B-nli39performs significantly worse,\nlikely due to less parameters and its finetuning being shorter (NLI is a much smaller dataset\nthan MS-MARCO). Apart from the BLOOM models, ST5-XL40is the largest model with\n1.2 billion parameters. However, as an English-only model its performance on non-English\n36.hf.co/bigscience/sgpt-bloom-7b1-msmarco\n37.hf.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n38.hf.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n39.hf.co/bigscience/sgpt-bloom-1b7-nli\n40.hf.co/sentence-transformers/sentence-t5-xl\n35", "Larger models require fewer samples to reach the same performance10864The optimal model size grows smoothly with the loss target and compute budgetLine color indicates\nnumber of parameters\n1071091011Tokens ProcessedCompute (PF-days)10-910-610-3100Test LossCompute-e\ufb03cient training stops far short of convergence\n103109106103 Params109 Params\n10864Figure 2 We show a series of language model training runs, with models ranging in size from 103to109\nparameters (excluding embeddings).\n100x Batch Size<10x Serial Steps>1,000,000x Model SizeData requirements\ngrow relatively slowlyOptimal model size\nincreases very quicklyMinimum serial steps increases negligibly\nFigure 3 As more compute becomes available, we can choose how much to allocate towards training larger\nmodels, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in\ncompute. For optimally compute-ef\ufb01cient training, most of the increase should go towards increased model", "et al., 2019) as appropriate to optimize performance. We use\nFlash Attention (Dao et al., 2022) for improved hardware\nthroughput.\nThe most notable divergence from standard training proce-\ndures is that we use a much larger batch size than what is\nstandard for training small language models. It is widely\nheld (McCandlish et al., 2018; Zhang et al., 2019; Kaplan\net al., 2020; Brown et al., 2020; Hoffmann et al., 2022)that using larger batch sizes is desirable, but that smaller\nLLMs require smaller batch sizes to avoid convergence is-\nsues. Contrary to this literature, we find no convergence\nissues with using batch sizes 4\u00d7to8\u00d7what is considered\nstandard for models with less than 1 billion parameters.\nConsequently, we use a batch size of 1024 samples with a\nsequence length of 2048 (2,097,152 tokens) for all models,\nin order to maintain consistency across all Pythia model\ntraining runs.\nModel Size GPU Count GPT-3 GPUs Speed-Up\n70 M 32 4 8 \u00d7\n160 M 32 8 4 \u00d7\n410 M 32 8 4 \u00d7\n1.0 B 64 16 4 \u00d7"], "retrieved_docs_id": ["26327c579e", "873274797e", "43f9bfc42e", "a6bca59686", "840c435c37"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Hardware-Aware Quantization optimize the performance of neural network models on specific hardware platforms?\n", "true_answer": "Hardware-Aware Quantization optimizes the performance of neural network models on specific hardware platforms by adjusting precision levels and quantization strategies to maximize performance and energy efficiency during inference.", "source_doc": "multimodal.pdf", "source_id": "31efe3044d", "retrieved_docs": ["Bit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain\naccuracy throughout quantization. PackQViT [129] mitigates outlier effects during quantization.\nBiViT [128] introduces Softmax-aware Binarization to adjust the binarization process, minimizing\nerrors in binarizing softmax attention values. Xiao et al. [142] integrated a gradient regularization\nscheme to curb weight oscillation during binarization training and introduced an activation shift\nmodule to reduce information distortion in activations. Additionally, BinaryViT [130] integrates\nessential architectural elements from CNNs into a pure ViT framework, enhancing its capabilities.\nHardware-Aware Quantization optimizes the quantization process of neural network models for\nspecific hardware platforms ( e.g., GPUs [131], FPGA [132]). It adjusts precision levels and quan-\ntization strategies to maximize performance and energy efficiency during inference. For example,", "[67] AE Gamal, L Hemachandra, Itzhak Shperling, and\nV Wei. Using simulated annealing to design good\ncodes. IEEE Transactions on Information Theory ,\n33(1):116\u2013123, 1987.\n[68] Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell\nNahmias. Confounding tradeoffs for neu-\nral network quantization. arXiv preprint\narXiv:2102.06366 , 2021.\n[69] Sahaj Garg, Joe Lou, Anirudh Jain, and Mitchell\nNahmias. Dynamic precision analog computing for\nneural networks. arXiv preprint arXiv:2102.06365 ,\n2021.\n[70] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng\nTai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and\nKurt Keutzer. SqueezeNext: Hardware-aware\nneural network design. Workshop paper in CVPR ,\n2018.\n[71] Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. An integrated approach to neural network\ndesign, training, and inference. Univ. California,\nBerkeley, Berkeley, CA, USA, Tech. Rep , 2020.\n[72] Boris Ginsburg, Sergei Nikolaev, Ahmad Kiswani,\nHao Wu, Amir Gholaminejad, Slawomir Kierat,", "NN are grouped into sensitive/insensitive to quantization,\nand higher/lower bits are used for each layer. As such,\none can minimize accuracy degradation and still bene\ufb01t\nfrom reduced memory footprint and faster speed up with\nlow precision quantization. Recent work [ 267] has also\nshown that this approach is hardware-ef\ufb01cient as mixed-\nprecision is only used across operations/layers.\nC. Hardware Aware Quantization\nOne of the goals of quantization is to improve the\ninference latency. However, not all hardware provide\nthe same speed up after a certain layer/operation is\nquantized. In fact, the bene\ufb01ts from quantization is\nhardware-dependant, with many factors such as on-chip\nmemory, bandwidth, and cache hierarchy affecting the\nquantization speed up.\nIt is important to consider this fact for achieving\noptimal bene\ufb01ts through hardware-aware quantization [ 87,\n91,246,250,254,256,265,267]. In particular, the\nwork [ 246] uses a reinforcement learning agent to", "*Equal contribution1Qualcomm AI Research, an initia-\ntive of Qualcomm Technologies, Inc.. Correspondence to:\nMarkus Nagel <markusn@qti.qualcomm.com >, Rana Ali Am-\njad<ramjad@qti.qualcomm.com >, Tijmen Blankevoort <tij-\nmen@qti.qualcomm.com >.\nProceedings of the 37thInternational Conference on Machine\nLearning , Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).cations, and even dedicated low-power hardware.\nOne effective way to optimize neural networks for infer-\nence is neural network quantization (Krishnamoorthi, 2018;\nGuo, 2018). In quantization, neural network weights and\nactivations are kept in a low-bit representation for both\nmemory transfer and calculations in order to reduce power\nconsumption and inference time. The process of quantizing\na network generally introduces noise, which results in a loss\nof performance. Various prior works adapt the quantization\nprocedure to minimize the loss in performance while going\nas low as possible in the number of bits used.", "[40] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured\npruning in the Kronecker-factored eigenbasis. In International Conference on Machine Learning\n(ICML) , 2019.\n[41] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-aware automated\nquantization with mixed precision. In Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2019.\n[42] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training\nnetwork quantization via bit-split and stitching. In International Conference on Machine\nLearning (ICML) , 2020.\n[43] Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. Automatic neural network compression by\nsparsity-quantization joint learning: A constrained optimization-based approach. In Conference\non Computer Vision and Pattern Recognition (CVPR) , 2020.\n[44] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan"], "retrieved_docs_id": ["31efe3044d", "a1f1a4c591", "3e3cb80a9a", "7542ef2a73", "1aac93e2d3"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "Which large language models are mentioned as being explored for application in EDA algorithms and the chip design process?\n", "true_answer": "The large language models mentioned are ChatGPT, Bard, Vicuna, and those discussed in the papers by Khailany et al. (2020), Ren & Fojtik (2021), and Roy et al. (2021).", "source_doc": "ChipNemo.pdf", "source_id": "0e1c7b711e", "retrieved_docs": ["customization for enhancing the effectiveness of\nlarge language models in specialized applications.\n1. Introduction\nOver the last few decades, Electronic Design Automation\n(EDA) algorithms and tools have provided huge gains in\nchip design productivity. Coupled with the exponential\nincreases in transistor densities provided by Moore\u2019s law,\nEDA has enabled the development of feature-rich complex\nSoC designs with billions of transistors. More recently, re-\n*Equal contribution1NVIDIA.searchers have been exploring ways to apply AI to EDA al-\ngorithms and the chip design process to further improve chip\ndesign productivity (Khailany et al., 2020; Ren & Fojtik,\n2021; Roy et al., 2021). However, many time-consuming\nchip design tasks that involve interfacing with natural lan-\nguages or programming languages still have not been auto-\nmated. The latest advancements in commercial (ChatGPT,\nBard, etc.) and open-source (Vicuna (Chiang et al., 2023),", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "Bard, etc.) and open-source (Vicuna (Chiang et al., 2023),\nLLaMA2 (Touvron et al., 2023), etc.) large language mod-\nels (LLM) provide an unprecedented opportunity to help\nautomate these language-related chip design tasks. Indeed,\nearly academic research (Thakur et al., 2023; Blocklove\net al., 2023; He et al., 2023) has explored applications of\nLLMs for generating Register Transfer Level (RTL) code\nthat can perform simple tasks in small design modules as\nwell as generating scripts for EDA tools.\nWe believe that LLMs have the potential to help chip de-\nsign productivity by using generative AI to automate many\nlanguage-related chip design tasks such as code generation,\nresponses to engineering questions via a natural language\ninterface, analysis and report generation, and bug triage. In\nthis study, we focus on three specific LLM applications: an\nengineering assistant chatbot for GPU ASIC and Architec-\nture design engineers, which understands internal hardware", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nMingjie Liu* 1Teodor-Dumitru Ene* 1Robert Kirby* 1Chris Cheng* 1Nathaniel Pinckney* 1\nRongjian Liang* 1Jonah Alben1Himyanshu Anand1Sanmitra Banerjee1Ismet Bayraktaroglu1\nBonita Bhaskaran1Bryan Catanzaro1Arjun Chaudhuri1Sharon Clay1Bill Dally1Laura Dang1\nParikshit Deshpande1Siddhanth Dhodhi1Sameer Halepete1Eric Hill1Jiashang Hu1Sumit Jain1\nAnkit Jindal1Brucek Khailany1George Kokai1Kishor Kunal1Xiaowei Li1Charley Lind1Hao Liu1\nStuart Oberman1Sujeet Omar1Ghasem Pasandi1Sreedhar Pratty1Jonathan Raiman1Ambar Sarkar1\nZhengjiang Shao1Hanfei Sun1Pratik P Suthar1Varun Tej1Walker Turner1Kaizhe Xu1Haoxing Ren1\nAbstract\nChipNeMo aims to explore the applications of\nlarge language models (LLMs) for industrial chip\ndesign. Instead of directly deploying off-the-\nshelf commercial or open-source LLMs, we in-\nstead adopt the following domain adaptation tech-\nniques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment", "et al., 2023) and SteerLM (Dong et al., 2023).\nResearchers have started to apply LLM to chip design prob-\nlems. Early works such as Dave (Pearce et al., 2020) first\nexplored the possibility of generating Verilog from En-\nglish with a language model (GPT-2). Following that work,\n(Thakur et al., 2023) showed that fine-tuned open-source\nLLMs (CodeGen) on Verilog datasets collected from GitHub\nand Verilog textbooks outperformed state-of-the-art OpenAI\nmodels such as code-davinci-002 on 17 Verilog questions.\n(Liu et al., 2023) proposed a benchmark with more than\n150 problems and demonstrated that the Verilog code gen-\neration capability of pretrained language models could be\nimproved with supervised fine-tuning by bootstrapping with\nLLM generated synthetic problem-code pairs. Chip-Chat\n(Blocklove et al., 2023) experimented with conversational\nflows to design and verify a 8-bit accumulator-based micro-\nprocessor with GPT-4 and GPT-3.5. Their findings showed"], "retrieved_docs_id": ["0e1c7b711e", "a6c3d05123", "f23b3625e0", "36c5c0c7f1", "cdf1ac39e3"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How did UniNet [102] improve information accommodation by transformer and MLP operators?\n", "true_answer": "UniNet [102] introduced context-aware down-sampling modules to improve information accommodation by transformer and MLP operators.", "source_doc": "multimodal.pdf", "source_id": "e7939ae097", "retrieved_docs": ["UniNet [102] introduced context-aware down-sampling modules improving information accommo-\ndation by transformer and MLP operators.\nOptimization of Attention Mechanisms Methods focus on reducing computational complexity\nby introducing adaptive attention, learning sparse attention patterns, and dynamically adjusting at-\ntention mechanisms. Fayyaz et al. [135] implemented adaptive attention by scoring and adaptively\nsampling significant tokens. PatchMerger [103] extracted global information among regional to-\nkens and exchanged local self-attention with information among regional tokens via self-attention.\nDynamicViT [104] proposed an attention masking strategy to differentiably prune tokens by block-\ning interactions with other tokens. Additionally, Sepvit [105] conducted local-global information\ninteraction within and across windows using depthwise separable self-attention. These methods\ncollectively optimize attention mechanisms, enhancing computational efficiency and performance.", "Former [97] analyzed ViT-based model architectures and operators, introducing a dimension-\nconsistent pure transformer paradigm and employing latency-driven slimming to produce optimized\nmodels. Additionally, EfficientFormerV2 [98] proposed a supernet with low latency and high pa-\nrameter efficiency.\nArchitecture Search Methods involve employing neural architecture search algorithms [113]\nto explore and discover compact architectures tailored to specific tasks or constraints. For in-\nstance, Autoformer [99] intertwined weights within layers, enabling thorough training of thousands\nof subnets. NASViT [100] introduced a gradient projection algorithm, switchable layer scaling,\nand streamlined data augmentation, enhancing convergence and performance. Additionally, TF-\nTAS [101] investigated training-free architecture search methods, proposing an efficient scheme.\nUniNet [102] introduced context-aware down-sampling modules improving information accommo-", "anddata-controlled gating . In recall and reasoning tasks on sequences of thousands to hundreds of\nthousands of tokens, Hyena improves accuracy by more than 50points over operators relying on state-\nspaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-\nthe-art for dense-attention-free architectures on language modeling in standard datasets ( WikiText103\nandThe Pile ), reaching Transformer quality with a 20%reduction in training compute required at\nsequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length\n8K, and 100\u00d7faster at sequence length 64K.\n1 Introduction\nLarge Transformers have enabled a number of breakthrough advances in modeling language, vision, audio,\nbiology and numerous other domains (Vaswani et al., 2017), (Dosovitskiy et al., 2020), (Radford et al., 2022),\n(Cramer, 2021). Much of the success of Transformers, powered by the attention operator (Vaswani et al.,", "guarantees that we do not introduce any additional latency during inference compared to a \ufb01ne-tuned\nmodel by construction.\n4.2 A PPLYING LORA TOTRANSFORMER\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\nthe self-attention module ( Wq,Wk,Wv,Wo) and two in the MLP module. We treat Wq(orWk,Wv)\nas a single matrix of dimension dmodel\u00d7dmodel , even though the output dimension is usually sliced\ninto attention heads. We limit our study to only adapting the attention weights for downstream\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\nand parameter-ef\ufb01ciency.We further study the effect on adapting different types of attention weight\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\nlayers, LayerNorm layers, and biases to a future work.", "FC1 Weights Self-Attention Block FC1 Layer ReLU FC2 Layer \u2026\n\u00d7 =ReLU FC2 Weights = \u00d7\n\u2026MLP Block \nFC1 Input \nFC1 Output FC2 input FC2 Output Figure 2: The architecture of a Transformer layer and how neurons\nare sparsely activated in FC1 and FC2 layers due to the ReLU func-\ntion. The neurons that are activated are represented as green rows or\ncolumns encircled by red lines. The output vector from FC1 is then\nsupplied to FC2 as its input vector.\nblock generates embedding vectors by capturing the relation-\nships among input tokens. In this process, different heads\nfocus on extracting distinct feature information. The compu-\ntation results from these different heads are aggregated and\nthen utilized as the input for the MLP block. The MLP block\napplies non-linear transformations via fully connected layers\nand activation functions to re\ufb01ne the input sequence repre-\nsentation. The output either advances to subsequent layers or\nforms the LLM\u2019s \ufb01nal output."], "retrieved_docs_id": ["e7939ae097", "6ed104ce6b", "3000b5c09f", "8acb08bc5f", "a90364b4f6"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does TinyViT facilitate knowledge distillation during pretraining?\n", "true_answer": "TinyViT facilitates knowledge distillation during pretraining by pre-storing logits from large teacher models in the hardware, enabling memory and computational efficiency when transferring knowledge to scaled-down student transformers.", "source_doc": "multimodal.pdf", "source_id": "534dcc9fda", "retrieved_docs": ["Homomorphic KDs can further classified into logit-level [114, 115], patch-level [117], module-\nlevel [116], and feature-level KDs [118]. For logit-level methods, in DeiT [114], a distillation token\nis incorporated into the self-attention module to emulate the class label inferred by the teacher model,\nfacilitating interaction between the student attention and layers, thus enabling the learning of hard\nlabels during back-propagation. TinyViT [115] applies distillation during pretraining, where logits\nfrom large teacher models are pre-stored in the hardware, enabling memory and computational ef-\nficiency when transferring knowledge to scaled-down student transformers. Patch-level techniques\nlike DeiT-Tiny [117] train a small student model to match a pre-trained teacher model on patch-level\nstructures, then optimize with a decomposed manifold matching loss for reduced computational\ncosts. Module-level methods involve segregating teacher modules from a pre-trained unified model,", "4.5 Combination with Knowledge Distillation\nKnowledge distillation is a popular technique to improve the performance of small models (Romero\net al., 2014; Hinton et al., 2015). In knowledge distillation, the small model (student) is trained to\nmimic the output of a larger fine-tuned model (teacher) such that the performance of the small\nmodel can be improved.\nWe remark that compression methods are complementary to knowledge distillation. We show\nit by integrating knowledge distillation into LoSparse and other pruning methods. Specifically,\nwe choose a DeBERTaV3-base model that is fine-tuned on specific tasks as the teacher model\nand a compressed DeBERTaV3-base model as the student model. Then we conduct layer-wise\ndistillation for them. Please see Appendix E for more training details. Table 5 shows the results.\nWe find that distillation can further improve the performance of LoSparse and other compression", "narios. Most existing works focus on knowledge distillation. For instance, BERT-PKD (Sun et al., 2019)\nis a patient knowledge distillation approach that compresses the original BERT model into a lightweight\nshallow network. Different from traditional knowledge distillation methods, BERT-PKD enables an\nexploitation of rich information in the teacher\u2019s hidden layers by utilizing a layer-wise distillation con-\nstraint. DistillBERT (Sanh et al., 2019) pre-trains a smaller general-purpose language model on the\nsame corpus as vanilla BERT. Distilled BiLSTM (Tang et al., 2019) adopts a single-layer BiLSTM as\nthe student model and achieves comparable results with ELMo (Peters et al., 2018) through much fewer\nparameters and less inference time. TinyBERT (Jiao et al., 2019) exploits a novel attention-based distil-\nlation schema that encourages the linguistic knowledge in teacher to be well transferred into the student", "VTP[110], PS-ViT[111]\nHybrid Pruning SPViT [112], ViT-Slim [113]\nKnowledge Distillation (\u00a73.3)Homomorphic KDDeiT [114], TinyViT [115], m2mKD [116],\nDeiT-Tiny [117], MiniViT [118]\nHeteromorphic KD DearKD [119], CiT [120]\nQuantization (\u00a73.4)Post-Training QuantizationPTQ4ViT [121], APQ-ViT [122],\nNoisyQuant [123]\nQuantization-Aware TrainingQuantformer [124] Bit-shrinking [125],\nQ-ViT [126], TerViT [127], BiViT [128],\nPackQViT [129], BinaryViT [130]\nHardware-Aware Quantization GPUSQ-ViT[131], Auto-ViT-Acc [132]\nFigure 9: Organization of efficient vision advancements.\n10", "impact while the two distillation losses account for a large portion of the performance.\n5 Related work\nTask-speci\ufb01c distillation Most of the prior works focus on building task-speci\ufb01c distillation se-\ntups. Tang et al. [2019] transfer \ufb01ne-tune classi\ufb01cation model BERT to an LSTM-based classi\ufb01er.\nChatterjee [2019] distill BERT model \ufb01ne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it bene\ufb01cial to use a general-purpose\npre-training distillation rather than a task-speci\ufb01c distillation. Turc et al. [2019] use the original\npretraining objective to train smaller student, then \ufb01ne-tuned via distillation. As shown in the abla-\ntion study, we found it bene\ufb01cial to leverage the teacher\u2019s knowledge to pre-train with additional\ndistillation signal.\nMulti-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using"], "retrieved_docs_id": ["534dcc9fda", "17e9bf4ddc", "70e68288ff", "8a087225e4", "edc68e9b9b"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a component of the modular RAG structure that diverges from the traditional similarity retrieval method?\n", "true_answer": "The Search Module is a component of the modular RAG structure that diverges from the traditional similarity retrieval method, as it is tailored to specific scenes.", "source_doc": "RAG.pdf", "source_id": "1d479682a6", "retrieved_docs": ["Modular RAG\nThe modular RAG structure breaks away from the traditional\nNaive RAG framework of indexing, retrieval, and genera-\ntion, offering greater diversity and flexibility in the over-\nall process. On one hand, it integrates various methods to\nexpand functional modules, such as incorporating a search\nmodule in similarity retrieval and applying a fine-tuning ap-\nproach in the retriever [Linet al. , 2023 ]. Additionally, spe-\ncific problems have led to the emergence of restructured\nRAG modules [Yuet al. , 2022 ]and iterative approaches like\n[Shao et al. , 2023 ]. The modular RAG paradigm is becom-\ning the mainstream in the RAG domain, allowing for ei-\nther a serialized pipeline or an end-to-end training approach\nacross multiple modules.The comparison between three RAG\nparadigms is illustrated in Fig 3.\nNew Modules\n\u2022Search Module: Diverging from the similarity re-\ntrieval between queries and corpora in Naive/Advanced\nRAG, the search module, tailored to specific sce-", "different scenarios, including using query engines pro-\nvided by frameworks like LlamaIndex, employing tree\nqueries, utilizing vector queries, or employing the most\nbasic sequential querying of chunks.\u2022HyDE: This approach is grounded on the assumption\nthat the generated answers may be closer in the embed-\nding space than a direct query. Utilizing LLM, HyDE\ngenerates a hypothetical document (answer) in response\nto a query, embeds the document, and employs this em-\nbedding to retrieve real documents similar to the hypo-\nthetical one. In contrast to seeking embedding similarity\nbased on the query, this method emphasizes the embed-\nding similarity from answer to answer. However, it may\nnot consistently yield favorable results, particularly in\ninstances where the language model is unfamiliar with\nthe discussed topic, potentially leading to an increased\ngeneration of error-prone instances.\nModular RAG\nThe modular RAG structure breaks away from the traditional", "ments to assess the relevance between the retrieved doc-\numents and the query. This enhances the robustness of\nRAG [Yuet al. , 2023a ].\nNew Pattern\nThe organizational approach of Modular RAG is flexible,\nallowing for the substitution or reconfiguration of modules\nwithin the RAG process based on specific problem con-\ntexts. For Naive RAG, which consists of the two modules\nof retrieval and generation ( referred as read or sythesis in\nsome literature), this framework offers adaptability and abun-\ndance. Present research primarily explores two organizational\nparadigms, involving the addition or replacement of modules,\nas well as the adjustment of the organizational flow between\nmodules.\n\u2022Adding or Replacing Modules\nThe strategy of adding or replacing modules entails\nmaintaining the structure of Retrieval-Read while intro-\nducing additional modules to enhance specific function-\nalities. RRR [Maet al. , 2023a ]proposes the Rewrite-\nRetrieve-Read process, utilizing LLM performance as a", "lows the decide-retrieve-reflect-read process, introduc-\ning a module for active judgment. This adaptive and\ndiverse approach allows for the dynamic organization of\nmodules within the Modular RAG framework.\n4 Retriever\nIn the context of RAG, the \u201dR\u201d stands for retrieval, serving\nthe role in the RAG pipeline of retrieving the top-k relevant\ndocuments from a vast knowledge base. However, crafting\na high-quality retriever is a non-trivial task. In this chapter,\nwe organize our discussions around three key questions: 1)\nHow to acquire accurate semantic representations? 2) How\nto match the semantic spaces of queries and documents? 3)\nHow to align the output of the retriever with the preferences\nof the Large Language Model ?\n4.1 How to acquire accurate semantic\nrepresentations?\nIn RAG, semantic space is the multidimensional space where\nquery and Document are mapped. When we perform re-\ntrieval, it is measured within the semantic space. If the se-", "providing an effective solution to the incomplete and insuf-\nficient knowledge problem inherent in purely parameterized\nmodels.\nThe paper systematically reviews and analyzes the current\nresearch approaches and future development paths of RAG,\nsummarizing them into three main paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG. Subsequently, the paper\nprovides a consolidated summary of the three core compo-\nnents: Retrieval, Augmented, and Generation, highlighting\nthe improvement directions and current technological char-\nacteristics of RAG. In the section on augmentation methods,the current work is organized into three aspects: the augmen-\ntation stages of RAG, augmentation data sources, and aug-\nmentation process. Furthermore, the paper summarizes the\nevaluation system, applicable scenarios, and other relevant\ncontent related to RAG. Through this article, readers gain a\nmore comprehensive and systematic understanding of large\nmodels and retrieval-Augmented generation. They become"], "retrieved_docs_id": ["1d479682a6", "d96393bb4b", "a016e8d322", "8fe8499442", "1bd400d39e"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the purpose of the StepBack-prompt approach in the RAG process?\n", "true_answer": "The StepBack-prompt approach encourages the language model to step back from specific instances and engage in reasoning about the underlying general concepts or principles.", "source_doc": "RAG.pdf", "source_id": "ad03b3dcc5", "retrieved_docs": ["phase to capture key semantic meanings. In the later\nstages of this process, larger blocks with more contex-\ntual information are provided to the language model\n(LM). This two-step retrieval method helps strike a bal-\nance between efficiency and contextually rich responses.\n\u2022StepBack-prompt: Integrated into the RAG process,\nthe StepBack-prompt approach [Zheng et al. , 2023 ]en-\ncourages LLM to step back from specific instances and\nengage in reasoning about the underlying general con-\ncepts or principles. Experimental findings indicate a sig-\nnificant performance improvement in various challeng-\ning, inference-intensive tasks with the incorporation of\nbackward prompts, showcasing its natural adaptability\nto RAG. The retrieval-enhancing steps can be applied in\nboth the generation of answers to backward prompts and\nthe final question-answering process.\n\u2022Subqueries: Various query strategies can be employed in\ndifferent scenarios, including using query engines pro-", "put forward various methods to optimize the retrieval process.\nIn terms of specific implementation, Advanced RAG can be\nadjusted either through a pipeline or in an end-to-end manner.\nPre-Retrieval Process\n\u2022Optimizing Data Indexing\nThe purpose of optimizing data indexing is to enhance\nthe quality of indexed content. Currently, there are five\nmain strategies employed for this purpose: increasing\nthe granularity of indexed data, optimizing index struc-\ntures, adding metadata, alignment optimization, and\nmixed retrieval.\n1.Enhancing Data Granularity: The objective of\npre-index optimization is to improve text standard-\nization, consistency, and ensure factual accuracy\nand contextual richness to guarantee the perfor-\nmance of the RAG system. Text standardization in-\nvolves removing irrelevant information and special\ncharacters to enhance the efficiency of the retriever.\nIn terms of consistency, the primary task is to elim-\ninate ambiguity in entities and terms, along with", "ments to assess the relevance between the retrieved doc-\numents and the query. This enhances the robustness of\nRAG [Yuet al. , 2023a ].\nNew Pattern\nThe organizational approach of Modular RAG is flexible,\nallowing for the substitution or reconfiguration of modules\nwithin the RAG process based on specific problem con-\ntexts. For Naive RAG, which consists of the two modules\nof retrieval and generation ( referred as read or sythesis in\nsome literature), this framework offers adaptability and abun-\ndance. Present research primarily explores two organizational\nparadigms, involving the addition or replacement of modules,\nas well as the adjustment of the organizational flow between\nmodules.\n\u2022Adding or Replacing Modules\nThe strategy of adding or replacing modules entails\nmaintaining the structure of Retrieval-Read while intro-\nducing additional modules to enhance specific function-\nalities. RRR [Maet al. , 2023a ]proposes the Rewrite-\nRetrieve-Read process, utilizing LLM performance as a", "providing an effective solution to the incomplete and insuf-\nficient knowledge problem inherent in purely parameterized\nmodels.\nThe paper systematically reviews and analyzes the current\nresearch approaches and future development paths of RAG,\nsummarizing them into three main paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG. Subsequently, the paper\nprovides a consolidated summary of the three core compo-\nnents: Retrieval, Augmented, and Generation, highlighting\nthe improvement directions and current technological char-\nacteristics of RAG. In the section on augmentation methods,the current work is organized into three aspects: the augmen-\ntation stages of RAG, augmentation data sources, and aug-\nmentation process. Furthermore, the paper summarizes the\nevaluation system, applicable scenarios, and other relevant\ncontent related to RAG. Through this article, readers gain a\nmore comprehensive and systematic understanding of large\nmodels and retrieval-Augmented generation. They become", "probabilities. This approach is designed to handle situations\nwhere LLMs might need additional knowledge.\nSelf-RAG [Asai et al. , 2023b ]introduces an important in-\nnovation called Reflection tokens. These special tokens are\ngenerated to review the output and come in two types: Re-\ntrieve and Critic. The model can autonomously decide when\nto retrieve paragraphs or use a set threshold to trigger re-\ntrieval. When retrieval is needed, the generator processes\nmultiple paragraphs simultaneously, performing fragment-\nlevel beam search to obtain the best sequence. The scores for\neach subdivision are updated using Critic scores, and these\nweights can be adjusted during the inference process to cus-\ntomize the model\u2019s behavior. The Self-RAG framework also\nallows the LLM to autonomously determine whether recall\nis necessary, avoiding training additional classifiers or rely-\ning on NLI models. This enhances the model\u2019s ability to au-\ntonomously judge inputs and generate accurate answers."], "retrieved_docs_id": ["ad03b3dcc5", "8a71abd00a", "a016e8d322", "1bd400d39e", "2449b179e1"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does a recent work address the hallucination problem in Multi-Modal Language Learning Models (MLLMs)?\n", "true_answer": "A recent work addresses the hallucination problem in MLLMs by utilizing the Efficient Fine-grained Unlearning Framework (EFUF) and the CLIP model to construct a dataset.", "source_doc": "hallucination.pdf", "source_id": "2dd3a385f4", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest", "independent hallucination categories. In this work, we classify them under the attribute category.\nThe definition of hallucination types aligns well with the domain of compositional generalization [ 79,\n121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.\n3 HALLUCINATION CAUSES\nHallucinations have multifaceted origins, spanning the entire spectrum of MLLMs\u2019 capability\nacquisition process. In this section, we delve into the root causes of hallucinations in MLLMs,\nprimarily categorized into four aspects: Data ,Model ,Training , and Inference .\n3.1 Data\nData stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and\ninstruction-following capabilities. However, it can inadvertently become the source of MLLM\nhallucinations. This mainly manifests in three aspects: quantity, quality, and statistical bias.\n3.1.1 Quantity. Deep learning models are data-hungry, especially large models like MLLMs. The", "overall framework, may be promising research directions.\n6.3 Advancements in Model Architecture\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective\narchitectures specifically tailored to hallucination remains a challenge. Developing advanced model\narchitectures capable of capturing complex linguistic structures and generating coherent and con-\ntextually relevant output based on input visual content is essential for improving the performance of\nMLLMs. Future research can explore innovative architectural designs based on identified causes of\nhallucination. This includes developing stronger visual perception models, innovative cross-modal\ninteraction modules capable of transferring cross-modal information seamlessly, and novel large\nlanguage model architectures faithful to input visual content and text instructions, etc.\n6.4 Establishing Standardized Benchmarks", "to provide fine-grained feedback at the sentence level. The collected human preference data is\nused to train a reward model. Additionally, it leverages advanced vision perception models to\nautomatically score the grounding and fidelity of the text generated by an MLLM. Both sources are\ncombined into a single reward score during the reinforcement learning procedure.\n5.3.3 Unlearning. Unlearning refers to a technique designed to induce a model to \u2019forget\u2019 specific\nbehaviors or data, primarily through the application of gradient ascent methods [ 9]. Recently,\nunlearning for LLMs has been receiving increasing attention [ 50], effectively eliminating privacy\nvulnerabilities in LLMs. In the context of MLLMs, a recent work [ 109] introduces the Efficient\nFine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the\nhallucination problem. Specifically, it utilizes the CLIP model to construct a dataset comprised of", "generated content remains consistent and contextually relevant to the input modality requires\nsophisticated techniques for capturing and modeling cross-modal relationships. The direction of\ncross-modal alignment encompasses both MLLMs training and hallucination evaluation. Regarding\ntraining, future research should explore methods for aligning representations between different\nmodalities. Achieving this goal may involve designing more advanced architectures, introducing\nadditional learning objectives [ 52], or incorporating diverse supervision signals [ 16]. Regarding\nevaluation, cross-modal consistency checking has been a long-standing topic, ranging from multi-\nmodal understanding [ 66,88] to text-to-image generation [ 13,17]. Drawing on proven experiences\nfrom these domains to improve the assessment of MLLM hallucination, or unifying them into an\noverall framework, may be promising research directions.\n6.3 Advancements in Model Architecture"], "retrieved_docs_id": ["33d47ad8cc", "c67d2cac99", "c8e35c3848", "2dd3a385f4", "83c3718d9d"], "reranker_type": "colbert", "search_type": "text", "rr": 0.25, "hit": 1}, {"question": "Who wrote the 2024 survey on hallucination of multimodal large language models?\n", "true_answer": "Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, and Zheng Zhang", "source_doc": "hallucination.pdf", "source_id": "9bfe24c206", "retrieved_docs": ["alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.\nCCS Concepts: \u2022Computing methodologies \u2192Computer vision ;Natural language processing ;Machine\nlearning .\nAdditional Key Words and Phrases: Hallucination, Multimodal, Large Language Models, Vision-Language\nModels.\nACM Reference Format:\nZechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou.\n2024. Hallucination of Multimodal Large Language Models: A Survey. Preprint 1, 1 (April 2024), 30 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n\u2217Corresponding Author\nAuthors\u2019 addresses: Zechen Bai, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore,\nzechenbai@u.nus.edu; Pichao Wang, Amazon Prime Video, Washington, USA, pichaowang@gmail.com; Tianjun Xiao,\nAWS Shanghai AI Lab, Shanghai, China, tianjux@amazon.com; Tong He, AWS Shanghai AI Lab, Shanghai, China, htong@", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest"], "retrieved_docs_id": ["9bfe24c206", "72dc971633", "114f3dada8", "f49f3b54ce", "33d47ad8cc"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What are some methods for optimizing attention mechanisms in vision transformers?\n", "true_answer": "Some methods for optimizing attention mechanisms in vision transformers include PatchMerger, DynamicViT, and Sepvit.", "source_doc": "multimodal.pdf", "source_id": "20b3b3179f", "retrieved_docs": ["Optimizing Attention Mechanism Many transformer variants (\u201cx-formers\u201d) have been introduced to\nreduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020;\nKitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al.,\n2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other\nefficient methods (Katharopoulos et al., 2020; Jaegle et al., 2021).\nSome recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022; Jang et al.,\n2019) share similarities with RWKV\u2019s chunked computation scheme. Despite being memory-efficient,\ntheir time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV\nachieves better space and time complexity during inference by formulating a linear attention as an RNN.\nAttention Free Models Another line of research replaces the attention mechanism with other modules to", "anddata-controlled gating . In recall and reasoning tasks on sequences of thousands to hundreds of\nthousands of tokens, Hyena improves accuracy by more than 50points over operators relying on state-\nspaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-\nthe-art for dense-attention-free architectures on language modeling in standard datasets ( WikiText103\nandThe Pile ), reaching Transformer quality with a 20%reduction in training compute required at\nsequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length\n8K, and 100\u00d7faster at sequence length 64K.\n1 Introduction\nLarge Transformers have enabled a number of breakthrough advances in modeling language, vision, audio,\nbiology and numerous other domains (Vaswani et al., 2017), (Dosovitskiy et al., 2020), (Radford et al., 2022),\n(Cramer, 2021). Much of the success of Transformers, powered by the attention operator (Vaswani et al.,", "of the remainder. To avoid materializing the full attention matrix, one can use LSH to \ufb01nd potential locations\nof large entries, and use matrix completion [ 52] to \ufb01nd a low-rank decomposition. Gradient descent can \ufb01nd\nglobal optimum for this matrix completion problem [ 23]. However, it still requires too many iterations to be\nused in each training step.\nA.2 E\ufb03cient Transformers\nSparse, Low-rank Approx.: Transformer-based model such as BERT [ 38] has achieved unprecedented\nperformance in natural language processing. Recently, Vision Transformers [ 28,69] has also achieved\ncomparable performance to the traditional convolutional neural network in computer vision tasks [ 66].\nHowever, the quadratic computation of the attention layers constrains the scalability of Transformers. There\nare many existing directions to overcome this bottleneck, including attention matrix approximation such\nas Reformer [ 36], Performer [ 17], leveraging a side memory module that can access multiple tokens at", "UniNet [102] introduced context-aware down-sampling modules improving information accommo-\ndation by transformer and MLP operators.\nOptimization of Attention Mechanisms Methods focus on reducing computational complexity\nby introducing adaptive attention, learning sparse attention patterns, and dynamically adjusting at-\ntention mechanisms. Fayyaz et al. [135] implemented adaptive attention by scoring and adaptively\nsampling significant tokens. PatchMerger [103] extracted global information among regional to-\nkens and exchanged local self-attention with information among regional tokens via self-attention.\nDynamicViT [104] proposed an attention masking strategy to differentiably prune tokens by block-\ning interactions with other tokens. Additionally, Sepvit [105] conducted local-global information\ninteraction within and across windows using depthwise separable self-attention. These methods\ncollectively optimize attention mechanisms, enhancing computational efficiency and performance.", "Transformers as Support Vector Machines\nDavoud Ataee Tarzanagh1\u22c6Yingcong Li2\u22c6Christos Thrampoulidis3Samet Oymak4\u2020\nAbstract\nSince its inception in \u201cAttention Is All You Need\u201d, the transformer architecture has led to revolutionary advance-\nments in natural language processing. The attention layer within the transformer admits a sequence of input tokens\nXand makes them interact through pairwise similarities computed as softmax (XQK\u22a4X\u22a4), where (K,Q)are the\ntrainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of\nself-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear\nconstraints on the outer-products of token pairs. This formalism builds on [TLZO23] and allows us to characterize the\nimplicit bias of 1-layer transformers optimized with gradient descent, as follows. (1)Optimizing the attention layer, pa-"], "retrieved_docs_id": ["5cf4b56414", "3000b5c09f", "c4bed422ac", "e7939ae097", "06a008cc8b"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "What is the objective of the pre-training phase in cross-modal feature alignment?\n", "true_answer": "The objective of the pre-training phase is to achieve cross-modal feature alignment between models from each modality.", "source_doc": "hallucination.pdf", "source_id": "0be0058571", "retrieved_docs": ["Category HallucinationAttribute HallucinationRelation Hallucination\nFig. 3. Three types of typical hallucination.\nPre-training. Given that models from each modality are pre-trained on their respective data, the\nobjective of this pre-training phase is to achieve cross-modal feature alignment. During training,\nboth the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface\nbeing trained. Similar to traditional VLMs training, as exemplified by CLIP [ 88], web-scale image-\ntext pairs [ 92] are utilized for training. Given that the final output is at the LLM side, the most\nwidely used loss function in this stage is the text generation loss, typically cross-entropy loss, which\naligns with the pre-training of LLMs. Certain studies (e.g., [ 22,66]) explore the incorporation of\ncontrastive loss and image-text matching loss to further enhance alignment. After training, the\ninterface module maps the visual features into the input embedding space of the LLM.", "generated content remains consistent and contextually relevant to the input modality requires\nsophisticated techniques for capturing and modeling cross-modal relationships. The direction of\ncross-modal alignment encompasses both MLLMs training and hallucination evaluation. Regarding\ntraining, future research should explore methods for aligning representations between different\nmodalities. Achieving this goal may involve designing more advanced architectures, introducing\nadditional learning objectives [ 52], or incorporating diverse supervision signals [ 16]. Regarding\nevaluation, cross-modal consistency checking has been a long-standing topic, ranging from multi-\nmodal understanding [ 66,88] to text-to-image generation [ 13,17]. Drawing on proven experiences\nfrom these domains to improve the assessment of MLLM hallucination, or unifying them into an\noverall framework, may be promising research directions.\n6.3 Advancements in Model Architecture", "Data quality relevant to hallucinations can be further categorized into the following three facets.\n\u2022Noisy data. As mentioned in the definition section, training MLLMs involves two stages. The\npre-training stage employs image-text pairs crawled from the web, which contain inaccurate,\nmisaligned, or corrupted data samples. The noisy data would limit the cross-modal feature\nalignment [ 117,120], which serves as the foundation of MLLMs. As for the instruction tuning\ndata, prevalent methods, such as LLaVA [ 75], utilize the advanced GPT-4 [ 82] model to\ngenerate instructions. However, ChatGPT is a language model that cannot interpret visual\ncontent, leading to the risk of noisy data. Moreover, language models themselves suffer\nfrom the issue of hallucination [ 44], further increasing the risk. LLaVA-1.5 [ 74] adds human\nannotated QA data into instruction following and shows improved results, revealing the\neffect of noisy data.", "sequences. Moreover, XPOSoptimizes attention resolution so that the position information can\nbe captured more precisely. The method XPOSis ef\ufb01cient and effective in both interpolation and\nextrapolation settings.\n2.3 Training Objective\nTheKOSMOS -1training is conducted on web-scale multimodal corpora, including monomodal data\n(e.g., text corpus), cross-modal paired data (e.g., image-caption pairs), and interleaved multimodal\ndata (e.g., documents of arbitrarily interleaved images and texts). To be speci\ufb01c, we use monomodal\ndata for representation learning. For example, language modeling with text data pretrains instruction\nfollowing, in-context learning, and various language tasks. Moreover, cross-modal pairs and inter-\nleaved data learn to align the perception of general modalities with language models. Interleaved data\nalso naturally \ufb01t in the multimodal language modeling task. We present more details of training data\ncollection in Section 3.1.", "interface preserves most of the information, but lacks supervision on the projected feature.\nVisualization in [ 52] reveals that the features after the projection layer remain distinct from\nthe language embeddings. The distribution gap causes trouble in cross-modal interaction,\nleading to hallucination. On the other hand, Q-former-like [ 66] architecture has diverse\nsupervision on the extracted visual feature, aligning it to the language embedding space.\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual\ninformation.\n3.3 Training\nThe training objective of MLLMs is basically the same as LLMs, i.e,auto-regressive next token\nprediction loss. This loss is straightforward yet effective and easy to scale up, showing promising\nperformance in language modeling. However, some studies in the field of MLLMs have suggested\nthat the next-token prediction loss might not be suitable for learning visual content due to its"], "retrieved_docs_id": ["0be0058571", "83c3718d9d", "dcdb797076", "79427d1903", "0342fa09e6"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is one example of a task-specific dataset used to derive high-quality IT data?\n", "true_answer": "A sample from VQA (Visual Question Answering) datasets, where the input includes an image and a natural language question, and the output is the text-based answer to the question based on the image, is one example of a task-specific dataset used to derive high-quality IT data.", "source_doc": "multimodal.pdf", "source_id": "db45826cee", "retrieved_docs": ["A summary of frequently used pre-training datasets can be found in Table.3. High-quality IT data\ncan be derived from task-specific datasets. For instance, consider a sample from VQA datasets where\nthe input includes an image and a natural language question, and the output is the text-based answer\nto the question based on the image. This could easily form the multimodal input and response\nof the instruction sample. The instructions, or task descriptions, can be obtained either through\nmanual creation or semi-automatic generation with the help of GPT. In addition to utilizing publicly\navailable task-specific datasets, SPHINX-X[14] assembles a dataset focused on OCR from a wide\nrange of PDF data sourced from the internet. Specifically, it begins by gathering a large-scale PDF\ndataset from the web. It then obtains the rendering results of each page in the PDF file, while\nsimultaneously saving all text annotations along with their respective bounding boxes. Ultimately,", "to the lack of high-quality evaluation datasets. The recently\nintroduced Natural Questions dataset (Kwiatkowski et al.,\n2https://github.com/CLD2Owners/cld22019) is a promising resource to test this more quantita-\ntively. Similar to translation, the context of the language\nmodel is seeded with example question answer pairs which\nhelps the model infer the short answer style of the dataset.\nGPT-2 answers 4.1% of questions correctly when evalu-\nated by the exact match metric commonly used on reading\ncomprehension datasets like SQUAD.3As a comparison\npoint, the smallest model does not exceed the 1.0% accu-\nracy of an incredibly simple baseline which returns the most\ncommon answer for each question type (who, what, where,\netc...). GPT-2 answers 5.3 times more questions correctly,\nsuggesting that model capacity has been a major factor in\nthe poor performance of neural systems on this kind of task\nas of yet. The probability GPT-2 assigns to its generated", "Published in Transactions on Machine Learning Research (08/2023)\nE Targeted evaluations\nE.1 Language\nE.1.1 The Pile\nScenario Description. The Pile (Gao et al., 2021a) is a language modeling scenario that includes large-\nscale English texts from a diverse list of domains. As an example, an input for the OpenSubtitle (Tiedemann,\n2016) subset of the scenario looks like:\n\u201cIt came from down here.\u201d \u201cWhat were you thinking bringing a stranger here?\u201d \u201c... look\nout for herself.\u201d \u201cI wouldn\u2019t be alive if it wasn\u2019t for her.\u201d \u201cYeah, well, I\u2019m protecting you\nnow.\u201d\nThe textual output of a language model should be the same with the input. The main metric for the scenario\nis bits per byte (BPB).\nData. Gao et al. (2021a) created the dataset by incorporating or extending high-quality existing datasets\nand by filtering or extracting from a few raw data sources. The total size of the dataset is 825GB, and each", "Data Curation With the increasing data needed to train language models (and other models for other\nmodalities), it remains challenging to curate a high-quality dataset. Besides the technical challenges\nof composing a large-scale dataset and the decisions that go into making it, these decisions and their\ninfluence on the final models are costly to assess due to the high computational resources required\nto train such models. With WIMBD , we hope to ease the decisions that go into crafting large-scale\ndatasets by surfacing patterns and trends about what goes into them and what is left out from different\naspects, such as data quality, community and society measurements, etc.\nData Documentation Adding to previous works that call for more data documentation, such as\nDatasheets (Gebru et al., 2021) and Data Statements (McMillan-Major et al., 2023), we argue for\nthe importance of documenting such information. While previous works often focused and tailored", "rated responses from existing human feedback data [373].\nFor the second issue, non-RL alignment approaches mainly\nfine-tune LLMs in a supervised learning way (the same\nas the original instruction tuning loss) on a high-quality\nalignment dataset, meanwhile auxiliary learning objectives\ncan be used to enhance the alignment performance, e.g.,\nranking responses or contrasting instruction-response pairs.\nAlignment Data Collection. The construction of alignment\ndata is important to effectively align the behaviors of LLMs\nwith human preferences. To collect high-quality alignment\ndata, some work tries to reuse existing reward models to\nselect high-rated responses, and others explore to leverage\npowerful LLMs ( e.g., ChatGPT) or build a simulated envi-\nronment to generate synthetic alignment examples. Next,\nwe will discuss these three lines of research.\n\u2022Reward model based approaches. The reward model in\nRLHF has been trained to measure the alignment degree"], "retrieved_docs_id": ["db45826cee", "112d761dce", "33524db997", "c5299aa292", "88e2324ed2"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How many image-question pairs does MERLIM contain and what is its focus?\n", "true_answer": "MERLIM contains over 279K image-question pairs and has a strong focus on detecting cross-modal hallucinations.", "source_doc": "hallucination.pdf", "source_id": "198c99577c", "retrieved_docs": ["including object recognition, instance counting, and identifying object-to-object relationships.\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal\nhallucinations. Interestingly, when organizing the data, a set of edited images is intentionally added.\nBased on the original image, an inpainting strategy is employed to remove one object instance in\nthe image. With this original-edited image pair, one can compare the output of the target MLLM\nand identify the hallucinated objects that lack visual grounding.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "Hallucination of Multimodal Large Language Models: A Survey 11\nthere are four popular object related subtasks in its perception evaluation, including object existence,\ncount, position, color. Similar to POPE, these tasks are formulated as Yes-or-Notasks.\nCIEM [ 42]CIEM is a benchmark to evaluate hallucination of MLLMs. Unlike previous works\nutilize human annotated objects, CIEM is generated using an automatic pipeline. The pipeline takes\nthe text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows\nthat the generated data has low error rate, around 5%.\nMMHal-Bench [ 96]Comprising 96 image-question pairs, ranging in 8 question categories \u00d712\nobject topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs. The\n8 question categories cover various types of hallucination, including object attributes, counting,", "54\nAnswer the following mathematical reasoning questions:\nQ:    S am has 12 marbles. He gives 1/4 of them to his sister. \nHow many marbles does Sam have left?N x If a rectangle has a length of 6 cm and a width of 3 cm, \nwhat is the perimeter of the rectangle?\nFor a rectangle, add up the length and width and double it. So, the perimeter of this rectangle is (6 + 3) x 2 = 18 cm.\nThe answer is 18 cm.Q:\nA:\nLLM A:The answer is 9.A: He gives (1 / 4) x 12 = 3 marbles. \nSoSam is left with 12 \u2013 3 = 9 marbles. \nThe answer is 9.\n:Chain -of-Thought :Task description :Demonstration :QueryIn-Context Learning Chain -of-Thought Prompting\nQ:\nA:\nQ:\nA:Answer the following mathematical reasoning questions:\nQ:     Sam has 12 marbles. He gives 1/4 of them to his sister. \nHow many marbles does Sam have left?NxThe answer is 8.\nIf a rectangle has a length of 6 cm and a width of 3 cm, what is the perimeter of the rectangle?", "down automata. For our intents and purposes, we formulated this task as a conditional-generation task,\nwherein we predict the sequence of closing parentheses of a D3word without its last few closing parenthesis.\nData. We generated the data in the style of Suzgun et al. (2019b). The input-output pairs are similar to\nthosefoundinBIG-Bench(Srivastavaetal.,2022)for D4. Noadditionalpre-processingordata-augmentation\nwas done.\nE.3.5 GSM8K\nScenario Description. GSM8K (Cobbe et al., 2021) is a mathematical reasoning scenario that features\ngrade-school-level math word problems. For example, one input-output pair in the train set is: \u201cNatalia sold\nclips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia\nsell altogether in April and May?\u201d \u21d2\u201cNatalia sold 48/2 = \u00ab48/2=24\u00bb24 clips in May. Natalia sold 48+24\n= \u00ab48+24=72\u00bb72 clips altogether in April and May. The answer is 72\u201d.", "A: The answer is 62.Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each, how many bananas do they have?Embedding 1Embedding \u2026Embedding N\nAsk-Me-AnythingFormulate a question for the given context. Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each.Q: What is the total number of bananas they possess?Answer the question using arithmetic.Q: Lisa has 5 easy peelers. She buys 2 more nets with 6 each. How many easy peelers does she have?A: The answer is 17.Q: The cafeteria has 37 bananas. They bought 5 more bunches with 5 each.Q: What is the total number of bananas they possess?A: The answer is 62.Prompt Chain 3Prompt Chain 2Prompt Chain 1"], "retrieved_docs_id": ["198c99577c", "004e988006", "43fab9283b", "b73e90f6d5", "d96ec3c40a"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How much does domain-specific data improve the retriever hit rate in the ChipNeMo system?\n", "true_answer": "The retriever hit rate is improved by 30% using domain-specific data in the ChipNeMo system.", "source_doc": "ChipNemo.pdf", "source_id": "df0b9868f2", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "vant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation", "uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "and required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the differencein retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022ChipNeMo-70B-Steer outperforms similar sized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and", "LLM\u2019s Preference\nIn the RAG pipeline, even if we employ the above techniques\nto enhance the retrieval hit rate, it may still not improve the\nfinal effect of RAG, because the retrieved documents may not\nbe what LLM needs. Thus, this section introduces two meth-\nods to align the outputs of the retriever and the preferences of\nthe LLM.\nLLM supervised training Many works leverage various\nfeedback signals from large language models to fine-tune em-\nbedding models. AAR [Yuet al. , 2023b ]provides supervi-\nsory signals for a pre-trained retriever through an encoder-\ndecoder architecture LM. By determining the LM\u2019s preferred\ndocuments through FiD cross-attention scores, the retriever\nis then fine-tuned with hard negative sampling and standard\ncross-entropy loss. Ultimately, the fine-tuned retriever can\ndirectly be used to enhance unseen target LMs, thereby per-\nforming better in the target task. The training loss of retriever\nas:\n\u03b6=X\nqX\nd+\u2208Da+X\nd\u2212\u2208D\u2212l\u0000\nf\u0000\nq, d+\u0001\n, f\u0000\nq, d\u2212\u0001\u0001\n(1)"], "retrieved_docs_id": ["df0b9868f2", "28f0897bcb", "c7d05c4b43", "1ed1c2ae54", "6b0d47093c"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does fine-tuning retrievers and generators in the downstream phase improve information retrieval?\n", "true_answer": "Fine-tuning retrievers and generators in the downstream phase primarily enhances information retrieval in open-domain question-answering tasks.", "source_doc": "RAG.pdf", "source_id": "011ee221ab", "retrieved_docs": ["tiple downstream tasks, fine-tuning the retriever with two dif-\nferent supervised signals via hard labeling of the dataset and\nthe soft reward derived from LLM.\nThis somewhat improves the semantic representation\nthrough both domain knowledge injection and downstream\ntask fine-tuning. However, the retrievers trained by this ap-\nproach are not intuitively helpful for large language models,\nso some work has been done to supervise the fine-tuning of\nEmbedding models directly through feedback signals from\nthe LLM. (This section will be presented in 4.4)\n4.2 How to Match the Semantic Space of Queries\nand Documents\nIn the RAG application, some retrievers use the same embed-\nding model to encode the query and doc, while others use two\nmodels to separately encode the query and doc. Moreover, the\noriginal query of the user may have problems of poor expres-\nsion and lack of semantic information. Therefore, aligning\nthe semantic space of the user\u2019s query and documents is very", "domain question-answering tasks. Concerning retriever fine-\ntuning, REPlUG [Shiet al. , 2023 ]treats the language model\n(LM) as a black box and enhances it through an adjustable re-\ntrieval model. By obtaining feedback from the black-box lan-\nguage model through supervised signals, REPLUG improves\nthe initial retrieval model. UPRISE [Cheng et al. , 2023a ], on\nthe other hand, fine-tunes retrievers by creating a lightweight\nand versatile retriever through fine-tuning on diverse task\nsets. This retriever can automatically provide retrieval\nprompts for zero-shot tasks, showcasing its universality and\nimproved performance across tasks and models.\nSimultaneously, methods for fine-tuning generators in-\nclude Self-Mem [Cheng et al. , 2023b ], which fine-tunes the\ngenerator through a memory pool of examples, and\nSelf-RAG [Asai et al. , 2023b ], which satisfies active re-\ntrieval needs by generating reflection tokens. The RA-\nDIT[Linet al. , 2023 ]method fine-tunes both the generator", "tuning both LLM and retriever allows better adaptation\nto specific tasks, offering the flexibility to fine-tune ei-\nther one or both simultaneously, as seen in methods like\nRePlug [Shiet al. , 2023 ]and RA-DIT [Linet al. , 2023 ]. Sec-\nondly, the benefits of this fine-tuning extend to adapt-\ning to diverse downstream tasks, as demonstrated by\nUPRISE [Cheng et al. , 2023a ], making the model more ver-\nsatile. Additionally, fine-tuning enables models to better ac-\ncommodate different data structures in various corpora, par-\nticularly advantageous for graph-structured corpora, as high-\nlighted by the SUGRE method.\nHowever, fine-tuning during this phase comes with limita-\ntions, such as the need for datasets specifically prepared for\nRAG fine-tuning and the requirement for substantial compu-\ntational resources compared to the RAG during the inference\nphase. Overall, during fine-tuning, researchers have the flexi-\nbility to tailor models according to specific requirements and", "knowledge-intensive tasks, allowing the creation of domain-\nspecific models through training on domain-specific corpora.\nHowever, there are drawbacks, including the requirement for\na substantial amount of pre-training data and larger training\nresources, as well as the issue of slower update speeds. Espe-\ncially as model size increases, the cost of retrieval-enhanced\ntraining becomes relatively higher. Despite these limitations,\nthis method demonstrates notable characteristics in terms of\nmodel robustness. Once trained, retrieval-enhanced models\nbased on pure pre-training eliminate the need for external li-brary dependencies, enhancing both generation speed and op-\nerational efficiency.\nFine-tuning Stage\nDuring the downstream fine-tuning phase, researchers have\nemployed various methods to fine-tune retrievers and gener-\nators for improved information retrieval, primarily in open-\ndomain question-answering tasks. Concerning retriever fine-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-"], "retrieved_docs_id": ["d5d9951817", "662eb558d5", "c2d5ae2a60", "011ee221ab", "fefa202c19"], "reranker_type": "colbert", "search_type": "text", "rr": 0.25, "hit": 1}, {"question": "How does the input to the generator differ in a RAG model compared to typical Large Language Model (LLM) generation tasks?\n", "true_answer": "In a RAG (Retriever-Augmented Generator) model, the input to the generator includes not only a query but also various documents retrieved by the retriever, whereas in typical LLM generation tasks, the input is usually just a query.", "source_doc": "RAG.pdf", "source_id": "7fabdba415", "retrieved_docs": ["information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "quently, it utilizes this retrieved information to generate re-\nsponses or text, thereby enhancing the quality of predictions.\nThe RAG method allows developers to avoid the need for\nretraining the entire large model for each specific task. In-\nstead, they can attach a knowledge base, providing additional\ninformation input to the model and improving the accuracy\nof its responses. RAG methods are particularly well-suited\nfor knowledge-intensive tasks. In summary, the RAG system\nconsists of two key stages:1. Utilizing encoding models to retrieve relevant docu-\nments based on questions, such as BM25, DPR, Col-\nBERT, and similar approaches [Robertson et al. , 2009,\nKarpukhin et al. , 2020, Khattab and Zaharia, 2020 ].\n2. Generation Phase: Using the retrieved context as a con-\ndition, the system generates text.\n2.2 RAG vs Fine-tuning\nIn the optimization of Large Language Models (LLMs), in\naddition to RAG, another important optimization technique\nis fine-tuning.", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "cross-attention scores, selecting the highest scoring input to-\nkens to effectively filter tokens. RECOMP [Xuet al. , 2023a ]\nproposes extractive and generative compressors, which gen-\nerate summaries by selecting relevant sentences or syn-\nthesizing document information to achieve multi-document\nquery focus summaries.In addition to that, a novel approach,\nPKG [Luoet al. , 2023 ], infuses knowledge into a white-box\nmodel through directive fine-tuning, and directly replaces the\nretriever module, used to directly output relevant documents\nbased on the query.\n5 Generator\nAnother core component in RAG is the generator, responsible\nfor transforming retrieved information into natural and fluent\ntext. Its design is inspired by traditional language models,\nbut in comparison to conventional generative models, RAG\u2019s\ngenerator enhances accuracy and relevance by leveraging the\nretrieved information. In RAG, the generator\u2019s input includes"], "retrieved_docs_id": ["7fabdba415", "fefa202c19", "80558327ad", "4fffd3dc2b", "cd69a480bb"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is one example of an Architecture Design Method used to create compact models?\n", "true_answer": "One example of an Architecture Design Method is Reformer, which introduces locality-sensitive hashing in attention mechanisms to reduce complexity and uses reversible residual layers to store activations more efficiently.", "source_doc": "multimodal.pdf", "source_id": "82a6543862", "retrieved_docs": ["3.1 Compact Architecture\nCompact Architecture refers to the design of lightweight and efficient models while maintaining high\nperformance in downstream tasks. It encompasses various strategies and methodologies to reduce\nmodel size, computational complexity, and memory footprint without compromising performance.\nThese strategies can be broadly categorized into three categories, 1) Architecture Design Methods,\n2) Architecture Search Methods, and 3) Optimization of Attention Mechanisms Methods.\nArchitecture Design Methods involve creating new architectures [133] or adjusting existing\nones [134] to achieve compactness without sacrificing performance. For example, Reformer [96]\nintroduced locality-sensitive hashing in attention mechanisms to reduce complexity, while also\nemploying reversible residual layers to store activations more efficiently. Furthermore, Efficient-\nFormer [97] analyzed ViT-based model architectures and operators, introducing a dimension-", "(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). This constitutes the second\nmodel structure we consider. A schematic of this architecture is shown in Figure 4, middle.\nIn fact, early work on transfer learning for NLP used this architecture with a language\nmodeling objective as a pre-training method (Radford et al., 2018).\nLanguage models are typically used for compression or sequence generation (Graves,\n2013). However, they can also be used in the text-to-text framework simply by concatenating\nthe inputs and targets. As an example, consider the case of English to German translation:\nIf we have a training datapoint with input sentence \u201cThat is good.\u201d and target \u201cDas ist\ngut.\u201d, we would simply train the model on next-step prediction over the concatenated input\nsequence \u201ctranslate English to German: That is good. target: Das ist gut.\u201d If we wanted to\nobtain the model\u2019s prediction for this example, the model would be fed the pre\ufb01x \u201ctranslate", "hardware. Therefore, language model compression raises immense interest.\nThe popular strategy creates a compact model from scratch (Jiao et al., 2019) or a subset of the big\nmodel\u2019s layers (Sun et al., 2019; Sanh et al., 2019), then pre-trains with a large corpus and distills\nknowledge from the big model. This process is called generic pre-training (Wang et al., 2020b; Sun\net al., 2019; Sanh et al., 2019) and is necessary for a compact model to achieve good performance\non the target tasks. However, the generic pre-training could still cost considerable computational\nresources. For example, it takes 384 NVIDIA V100 GPU hours to get the pre-trained TinyBERT\n(Jiao et al., 2019) on the Wiki corpus dataset. So it may not be affordable for everyone who wants\nto create a compact model. In contrast, another line of strategy, speci\ufb01cally low-rank factorization\n(Golub & Reinsch, 1971; Noach & Goldberg, 2020), can potentially reduce a big model\u2019s parameters", "to hardware architecture. This may be particularly useful\nfor FPGA deployment, as one can explore many different\npossible hardware con\ufb01gurations (such as different micro-\narchitectures of multiply-accumulate elements), and then\ncouple this with the NN architecture and quantization\nco-design.\nCoupled Compression Methods: As discussed above,\nquantization is only one of the methods for ef\ufb01cient\ndeployment of NNs. Other methods include ef\ufb01cient\nNN architecture design, co-design of hardware and\nNN architecture, pruning, and knowledge distillation.\nQuantization can be coupled with these other approaches.\nHowever, there is currently very little work exploring\nwhat are the optimal combinations of these methods. For\ninstance, pruning and quantization can be applied together\nto a model to reduce its overhead [ 87,152], and it is\nimportant to understand the best combination of struc-tured/unstructured pruning and quantization. Similarly,\nanother future direction is to study the coupling between", "classical techniques here mostly found new architecture\nmodules using manual search, which is not scalable. As\nsuch, a new line of work is to design Automated machine\nlearning (AutoML) and Neural Architecture Search (NAS)\nmethods. These aim to \ufb01nd in an automated way the right\nNN architecture, under given constraints of model size,\ndepth, and/or width [ 161,194,232,245,252,291]. We\nrefer interested reader to [ 54] for a recent survey of NAS\nmethods.\nb)Co-designing NN architecture and hardware\ntogether :Another recent line of work has been to adapt\n(and co-design) the NN architecture for a particular target\nhardware platform. The importance of this is because the\noverhead of a NN component (in terms of latency and\nenergy) is hardware-dependent. For example, hardwarearXiv:2103.13630v3  [cs.CV]  21 Jun 2021"], "retrieved_docs_id": ["82a6543862", "0f033dea5f", "d384ad5363", "fabec4ef85", "5639d739b4"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can hallucination capabilities of large language models be used to improve human user experiences?\n", "true_answer": "Hallucination capabilities can be leveraged to enhance user experiences and enable new use cases, such as integrating hallucination to inspire creative ideas in real-world applications or optimizing the models for human experiences rather than specific cross-modal benchmarks.", "source_doc": "hallucination.pdf", "source_id": "dfb6343eae", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 23\nrecollection of its training documents, most of the time the result goes someplace useful. It\u2019s only\nwhen the dreams enter deemed factually incorrect territory that we label them as \u2019hallucinations\u2019.\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications\npresents exciting opportunities for enhancing user experiences and enabling new use cases. As\nhumans are the end-users of these models, the primary goal is to enrich human user experiences.\nFuture research may switch the optimization objective from specific cross-modal benchmarks to\nhuman experience. For example, Some content may cause hallucinations but will not affect the\nuser experience, while some content may. Alternatively, integrating hallucination to inspire more\ncreative ideas in real-world applications could also be intriguing.\n6.6 Enhancing Interpretability and Trust", "recently, HaluEval [602] creates a large-scale LLM-generated\nand human-annotated hallucinated samples to evaluate the\nability of language models to recognize hallucination in both\ntask-specific and general scenarios.\nHallucination\nLLMs are prone to generate untruthful informa-\ntion that either conflicts with the existing source\nor cannot be verified by the available source.\nEven the most powerful LLMs such as ChatGPT\nface great challenges in migrating the hallucina-\ntions of the generated texts. This issue can be\npartially alleviated by special approaches such as\nalignment tuning and tool utilization.\n\u2022Knowledge recency . As another major challenge, LLMs\nwould encounter difficulties when solving tasks that require", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "showing how increasing the guidance weight \u03b3increases\nthe importance of the prompt \u201cToday in France,\u201d.In recent years large language models have exhibited\nstrong generative capabilities to solve a diverse range of\ntasks [ 26,15,71]. \u201cPrompting\u201d is typically used to con-\ndition generation, with task instructions and context [ 64],\nor a small set of examples [ 15]. However, language gener-\nation, especially with smaller models, has been shown to\nstruggle with issues such as hallucination [ 49], degrada-\ntion [ 38] and meandering [ 76]. Various approaches have\nbeen proposed to address this, e.g.: instruction-finetuning\n[81,70] and reinforcement learning [ 56,4,6]. These tech-\nniques are expensive and their compute and data cost may\nnot be accessible to all users. In this paper we propose an\ninference time methodology which, as shown in Figure\n1, gives more importance to the user intent, expressed\nthrough the prompt. Our hypothesis in this paper is: fo-", "Eight Things to Know about Large Language Models\nlikely to leave open important failure modes. For exam-\nple, straightforward attempts to manage hallucination are\nlikely to fail silently in a way that leaves them looking more\ntrustworthy than they are because of issues related to sand-\nbagging: If we apply standard methods to train some future\nLLM to tell the truth, but that LLM can reasonably accu-\nrately predict which factual claims human data workers are\nlikely to check, this can easily lead the LLM to tell the truth\nonly when making claims that are likely to be checked .\n9.2. There will be incentives to deploy LLMs as agents\nthat \ufb02exibly pursue goals\nIncreasingly capable LLMs, with increasingly accurate and\nusable internal models of the world, are likely to be able to\ntake on increasingly open-ended tasks that involve making\nand executing novel plans to optimize for outcomes in the\nworld (Chan et al., 2023). As these capabilities develop, eco-"], "retrieved_docs_id": ["dfb6343eae", "fa2581a685", "f49f3b54ce", "08972157a7", "c8ee410803"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What optimization objective is used during model alignment after domain-adaptive pretraining in ChipNeMo?\n", "true_answer": "An autoregressive optimization objective is used during model alignment after domain-adaptive pretraining in ChipNeMo.", "source_doc": "ChipNemo.pdf", "source_id": "a5a7c4ceb0", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "ChipNeMo: Domain-Adapted LLMs for Chip Design\n2Domain -Adaptive\nPretraining\n24B tokens of chip \ndesign docs/code\nThousands GPU hrs\nModel\nAlignmen t\n56K/128K \n(SteerLM /SFT)  insts\n+ 1.4K task insts\n100+ GPU hrsFoundation Models\nLLaMA2 \n(7B, 13B, 70B) \nChipNeMo \nChat Models\n(7B, 13B, 70B)ChipNeMo \nFoundation Models\n(7B, 13B, 70B)Pretraining\nTrillions tokens of \ninternet data\n105 \u2013 106 GPU hrs\nFigure 1: ChipNeMo Training Flow\n2023)) fine-tuned on additional Verilog data can outperform\nstate-of-art OpenAI GPT-3.5 models. Customizing LLMs\nin this manner also avoids security risks associated with\nsending proprietary chip design data to third party LLMs\nvia APIs. However, it would be prohibitively expensive to\ntrain domain-specific models for every domain from scratch,\nsince this often requires millions of GPU training hours. To\ncost-effectively train domain-specific models, we instead\npropose to combine the following techniques: Domain-", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat."], "retrieved_docs_id": ["a5a7c4ceb0", "a6c3d05123", "df0b9868f2", "2079d05356", "273b593026"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a recent focus in reinforcement research during the inference stage for large language models?\n", "true_answer": "A recent focus is self-retrieval, where models retrieve and use their own information during the inference stage.", "source_doc": "RAG.pdf", "source_id": "326cdd7c26", "retrieved_docs": ["most of the research on reinforcement during the inference\nstage emerged during the era of LLMs. This is primarily due\nto the high training costs associated with high-performance\nlarge models. Researchers have attempted to enhance model\ngeneration by incorporating external knowledge in a cost-\neffective manner through the inclusion of RAG modules dur-\ning the inference stage. Regarding the use of augmented\ndata, early RAG primarily focused on the application of un-\nstructured data, particularly in the context of open-domain\nquestion answering. Subsequently, the range of knowledge\nsources for retrieval expanded, with the use of high-quality\ndata as knowledge sources effectively addressing issues such\nas internalization of incorrect knowledge and hallucinations\nin large models. This includes structured knowledge, with\nknowledge graphs being a representative example. Recently,\nthere has been increased attention on self-retrieval, which in-", "els. In comparison with them, this paper aims to systemati-\ncally outline the entire process of Retrieval-Augmented Gen-\neration (RAG) and focuses specifically on research related to\naugmenting the generation of large language models through\nknowledge retrieval.\nThe development of RAG algorithms and models is il-\nlustrated in Fig 1. On a timeline, most of the research re-\nlated to RAG emerged after 2020, with a significant turn-\ning point in December 2022 when ChatGPT was released.\nSince the release of ChatGPT, research in the field of natu-\nral language processing has entered the era of large models.\nNaive RAG techniques quickly gained prominence, leading\nto a rapid increase in the number of related studies.In terms\nof enhancement strategies, research on reinforcement during\nthe pre-training and supervised fine-tuning stages has been\nongoing since the concept of RAG was introduced. However,\nmost of the research on reinforcement during the inference", "to character level language modeling. Dai et al. [8]introduces a caching mechanism, relying on\nthe relative position embeddings from Shaw et al. [35], which makes inference in these models\nmuch more ef\ufb01cient for unbounded sequences. More recently, Sukhbaatar et al. [39] add a learnable\nself-attention span to extend the size of the context.\nWord level language models deal with large vocabularies and computing the most probable word\nis computationally demanding. Solutions are to either replace the softmax loss with an approxima-\ntion [ 12,29], to sample from the vocabulary during training [ 5,21] or to include subword units [ 34].\nA simple yet effective solution is to replace the loss by a hierarchical softmax designed to better take\nadvantage of the GPU speci\ufb01cities [13].\nFinally, many works focus on the regularization of large language models. In particular, Zaremba et al.\n[44] show that dropout [ 37] is effective for recurrent networks. More recently, Press and Wolf [32]", "collaboration with the AI community.\n6 Related Work\nLarge Language Models. The recent years have witnessed a substantial evolution in the field of LLMs.\nFollowing the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B\nparameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized\nmodels, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al.,\n2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in\nthis progression is the rise of Llama, recognized for its focus on computational efficiency during inference\n(Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closed-\nsource models. Open-source releases like BLOOM (Scao et al., 2022), OPT(Zhang et al., 2022), and Falcon\n(Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla.", "collaboration with the AI community.\n6 Related Work\nLarge Language Models. The recent years have witnessed a substantial evolution in the field of LLMs.\nFollowing the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B\nparameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized\nmodels, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al.,\n2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in\nthis progression is the rise of Llama, recognized for its focus on computational efficiency during inference\n(Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closed-\nsource models. Open-source releases like BLOOM (Scao et al., 2022), OPT(Zhang et al., 2022), and Falcon\n(Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla."], "retrieved_docs_id": ["326cdd7c26", "483a7b216e", "79f7163198", "c2a74bff55", "c2a74bff55"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How do Reflection tokens in the Self-RAG framework determine when to retrieve additional information?\n", "true_answer": "Reflection tokens in the Self-RAG framework can autonomously decide when to retrieve paragraphs or use a set threshold to trigger retrieval.", "source_doc": "RAG.pdf", "source_id": "2449b179e1", "retrieved_docs": ["probabilities. This approach is designed to handle situations\nwhere LLMs might need additional knowledge.\nSelf-RAG [Asai et al. , 2023b ]introduces an important in-\nnovation called Reflection tokens. These special tokens are\ngenerated to review the output and come in two types: Re-\ntrieve and Critic. The model can autonomously decide when\nto retrieve paragraphs or use a set threshold to trigger re-\ntrieval. When retrieval is needed, the generator processes\nmultiple paragraphs simultaneously, performing fragment-\nlevel beam search to obtain the best sequence. The scores for\neach subdivision are updated using Critic scores, and these\nweights can be adjusted during the inference process to cus-\ntomize the model\u2019s behavior. The Self-RAG framework also\nallows the LLM to autonomously determine whether recall\nis necessary, avoiding training additional classifiers or rely-\ning on NLI models. This enhances the model\u2019s ability to au-\ntonomously judge inputs and generate accurate answers.", "Recite-Read [Sunet al. , 2022 ]transforms external re-\ntrieval into retrieval from model weights, initially hav-\ning LLM memorize task-relevant information and gener-\nate output for handling knowledge-intensive natural lan-\nguage processing tasks.\n\u2022Adjusting the Flow between Modules In the realm of\nadjusting the flow between modules, there is an empha-\nsis on enhancing interaction between language models\nand retrieval models. DSP [Khattab et al. , 2022 ]intro-\nduces the Demonstrate-Search-predict framework, treat-\ning the context learning system as an explicit program\nrather than a terminal task prompt to address knowledge-\nintensive tasks. ITER-RETGEN [Shao et al. , 2023 ]\nutilizes generated content to guide retrieval, itera-\ntively performing \u201cretrieval-enhanced generation\u201d and\n\u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read-\nRetrieve-Read flow. Self-RAG [Asai et al. , 2023b ]fol-\nlows the decide-retrieve-reflect-read process, introduc-", "domain question-answering tasks. Concerning retriever fine-\ntuning, REPlUG [Shiet al. , 2023 ]treats the language model\n(LM) as a black box and enhances it through an adjustable re-\ntrieval model. By obtaining feedback from the black-box lan-\nguage model through supervised signals, REPLUG improves\nthe initial retrieval model. UPRISE [Cheng et al. , 2023a ], on\nthe other hand, fine-tunes retrievers by creating a lightweight\nand versatile retriever through fine-tuning on diverse task\nsets. This retriever can automatically provide retrieval\nprompts for zero-shot tasks, showcasing its universality and\nimproved performance across tasks and models.\nSimultaneously, methods for fine-tuning generators in-\nclude Self-Mem [Cheng et al. , 2023b ], which fine-tunes the\ngenerator through a memory pool of examples, and\nSelf-RAG [Asai et al. , 2023b ], which satisfies active re-\ntrieval needs by generating reflection tokens. The RA-\nDIT[Linet al. , 2023 ]method fine-tunes both the generator", "lows the decide-retrieve-reflect-read process, introduc-\ning a module for active judgment. This adaptive and\ndiverse approach allows for the dynamic organization of\nmodules within the Modular RAG framework.\n4 Retriever\nIn the context of RAG, the \u201dR\u201d stands for retrieval, serving\nthe role in the RAG pipeline of retrieving the top-k relevant\ndocuments from a vast knowledge base. However, crafting\na high-quality retriever is a non-trivial task. In this chapter,\nwe organize our discussions around three key questions: 1)\nHow to acquire accurate semantic representations? 2) How\nto match the semantic spaces of queries and documents? 3)\nHow to align the output of the retriever with the preferences\nof the Large Language Model ?\n4.1 How to acquire accurate semantic\nrepresentations?\nIn RAG, semantic space is the multidimensional space where\nquery and Document are mapped. When we perform re-\ntrieval, it is measured within the semantic space. If the se-", "models struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models strug-\ngle to robustly access and use information in their\ninput contexts, we study the role of model archi-\ntecture (decoder-only vs. encoder-decoder), query-\naware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\u2022Encoder-decoder models are relatively robust\nto changes in the position of relevant informa-\ntion within their input context, but only when\nevaluated on sequences within its training-\ntime sequence length. When evaluated on\nsequences longer than those seen during train-\ning, we observe a U-shaped performance\ncurve (\u00a74.1).\n\u2022Query-aware contextualization (placing the\nquery before andafter the documents or key-\nvalue pairs) enables near-perfect performance\non the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2)."], "retrieved_docs_id": ["2449b179e1", "dfac20a7d8", "662eb558d5", "8fe8499442", "7b93fefc3b"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can the occurrence of illusions in a language model due to retrieval of irrelevant data be prevented?\n", "true_answer": "By introducing an additional validation module after document retrieval to assess the relevance between the retrieved documents.", "source_doc": "RAG.pdf", "source_id": "8d0a82337c", "retrieved_docs": ["is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "formation of the preceding blocks (C1, . . . , C i\u22121)and the\nretrieval information of N(Ci\u22121)through cross-attention to\nguide the generation of the next block Ci. To maintain causal-\nity, the autoregressive generation of the i-th block Cican only\nuse the nearest neighbor of the previous block N(Ci\u22121)and\nnotN(Ci).\nAugmented with Structured Data\nStructured data sources like Knowledge Graphs (KG) are\ngradually integrated into the paradigm of RAG. Verified KGs\ncan offer higher-quality context, reducing the likelihood of\nmodel hallucinations.\nRET-LLM [Modarressi et al. , 2023 ]constructs a per-\nsonalized knowledge graph memory by extracting\nrelation triples from past dialogues for future use.\nSUGRE [Kang et al. , 2023 ]embeds relevant subgraphs\nretrieved from the knowledge graph using Graph Neural\nNetworks (GNN) to prevent the model from generating\ncontextually irrelevant replies. SUGRE [Kang et al. , 2023 ]\nemploys a graph encoding method that reflects the graph", "Retrieval-Augmented LLM. Due to the huge amount of\nfact records in a KG, existing work typically adopts a\nretrieval model to first obtain a relatively small subgraph\nfrom KG, and then leverages it to enhance LLMs by en-\nriching the relevant knowledge. Before the advent of LLMs,\nthe retrieved subgraphs are often supplemented into train-\ning data, injecting knowledge information into PLMs via\nparameter learning [863\u2013865]. In contrast, to leverage the\nretrieved knowledge, LLMs mainly incorporate it as part of\nthe prompt, without parameter update. To implement this\napproach, there are two main technical problems, i.e.,how\nto retrieve relevant knowledge from KGs and how to make\nbetter use of the structured data by LLMs. For the first issue\n(i.e.,retrieving relevant knowledge), a typical approach is\nto train a small language model ( e.g., RoBERTa) to iden-\ntify question-related fact triples [866]. To further improve\nthe retrieval performance, several studies also propose an", "igate alignment issues. PRCA [Yang et al. , 2023b ]lever-\naged reinforcement learning to train a context adapter\ndriven by LLM rewards, positioned between the re-\ntriever and generator. It optimizes the retrieved in-\nformation by maximizing rewards in the reinforcement\nlearning phase within the labeled autoregressive pol-\nicy. AAR [Yuet al. , 2023b ]proposed a universal plu-\ngin that learns LM preferences from known-source\nLLMs to assist unknown or non-co-finetuned LLMs.\nRRR [Maet al. , 2023a ]designed a module for rewriting\nqueries based on reinforcement learning to align queries\nwith documents in the corpus.\n\u2022Validation Module: In real-world scenarios, it is notalways guaranteed that the retrieved information is reli-\nable. Retrieving irrelevant data may lead to the occur-\nrence of illusions in LLM. Therefore, an additional val-\nidation module can be introduced after retrieving docu-\nments to assess the relevance between the retrieved doc-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-"], "retrieved_docs_id": ["6291d3f5de", "812e372c75", "d0140a8a43", "8d0a82337c", "fefa202c19"], "reranker_type": "colbert", "search_type": "text", "rr": 0.25, "hit": 1}, {"question": "How are LLMs used in the evaluation of RAG-based applications?\n", "true_answer": "LLMs, such as GPT-3.5 and GPT-4, are used as automatic evaluation tools for RAG-based applications, assessing their faithfulness, answer relevance, and context relevance. This method has been found to be effective and efficient in evaluating these applications.", "source_doc": "RAG.pdf", "source_id": "a05a21efce", "retrieved_docs": ["in retrieved information. Counterfactual robustness tests\ninclude questions that the LLM can answer directly, but\nthe related external documents contain factual errors.\n7.3 Evaluation Frameworks\nRecently, the LLM community has been exploring the use\nof \u201dLLMs as judge\u201d for automatic assessment, with many\nutilizing powerful LLMs (such as GPT-4) to evaluate their\nown LLM applications outputs. Practices by Databricks us-\ning GPT-3.5 and GPT-4 as LLM judges to assess their chatbot\napplications suggest that using LLMs as automatic evaluation\ntools is effective [Leng et al. , 2023 ]. They believe this method\ncan also efficiently and cost-effectively evaluate RAG-based\napplications.\nIn the field of RAG evaluation frameworks, RAGAS and\nARES are relatively new. The core focus of these evaluations\nis on three main metrics: Faithfulness of the answer, answer\nrelevance, and context relevance. Additionally, TruLens, an\nopen-source library proposed by the industry, also offers a", "set of multi-turn questions for evaluation and improves the\nreliability of LLM-based evaluators through methods like\nICL and CoT. Compared with human evaluators, LLMs such\nas ChatGPT and GPT-4 can achieve high agreement with\nhumans, in both small-scale handcrafted and large-scale\ncrowdsourced evaluation tasks. Despite this, these closed-\nsource LLMs are limited in access and have the potential\nrisk of data leakage. To address this, recent work [727] has\nexplored fine-tuning open-source LLMs ( e.g., Vicuna [138])\nas model evaluators using scoring data from human eval-\nuators, which has narrowed the gap with powerful closed-\nsource LLMs ( e.g., GPT-4).\nEvaluation of Specialized LLMs. Specialized LLMs refer\nto the model checkpoints specially adapted to some do-\nmains or applications like healthcare [356] and finance [737].\nAs special task solvers, specialized LLMs will be tested\nnot only on general abilities ( e.g., basic ability like com-", "model-based approach [729]. Table 15 shows an illustration\nof the relationship among LLM type, evaluation approach,\nand tested abilities. Next, we will discuss the evaluation\napproaches for different types of LLMs.\nEvaluation of Base LLMs. Base LLMs refer to the model\ncheckpoints obtained right after pre-training. For base\nLLMs, we mainly focus on examining the basic abilities\n(Section 7.1), such as complex reasoning and knowledge\nutilization. Since most of these basic abilities can be assessed\nwith well-defined tasks, benchmark-based approaches have\nbeen widely used to evaluate base LLMs. Next, we will\nintroduce common evaluation benchmarks and evaluation\nprocedures for base LLMs.\n\u2022Common benchmarks. To evaluate base LLMs, typical\nbenchmarks are designed in the form of close-ended prob-\nlems like multiple-choice questions. These commonly used\nbenchmarks can be mainly divided into two categories:\nknowledge-oriented and reasoning-oriented benchmarks.", "not only on general abilities ( e.g., basic ability like com-\nplex reasoning and advanced ability like human align-\nment), but also on specific abilities related to their des-\nignated domains or applications. For this purpose, one\noften needs to construct specific benchmarks tailored for the\ntarget domains or applications. Then, these domain-specific\nbenchmarks can be combined with general benchmarks to\nconduct both comprehensive and targeted evaluation for\nspecialized LLMs. For example, MultiMedQA [356] is a\nspecific benchmark in healthcare, which includes medicalexaminations and healthcare questions. In this work [356],\nMultiMedQA has been combined with MMLU [364] to\nassess the performance of specialized LLMs for healthcare,\nsuch as Med-PaLM [356]. Similarly, FLUE [737] constructs a\nbenchmark for finance, spanning from financial sentiment\nanalysis to question answering. It has been used collab-\noratively with BBH [365] to evaluate finical LLMs like\nBloombergGPT [360].", "68\non various abilities ( e.g., knowledge utilization and hu-\nman alignment), and thus it is common that they are as-\nsessed with multiple evaluation approaches. In addition\nto benchmark-based evaluation, human-based and model-\nbased approaches have also been widely used to evaluate\nthe advanced abilities of fine-tuned LLMs. Next, we will\nintroduce the two evaluation methods.\n\u2022Human-based evaluation. Unlike automatic evaluation\nfor basic abilities, human evaluation typically considers\nmore factors or abilities in real-world use, such as hu-\nman alignment and tool manipulation. In this evaluation\napproach, test tasks are usually in the form of open-\nended questions, and human evaluators are invited to make\njudgments on the quality of answers generated by LLMs.\nTypically, there are two main types of scoring methods\nfor human evaluators: pairwise comparison and single-\nanswer grading. In pairwise comparison, given the same\nquestion, humans are assigned two answers from different"], "retrieved_docs_id": ["a05a21efce", "559acaf874", "56b8ebf97f", "560353ce53", "3603224301"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the COG model select text fragments during the generation process?\n", "true_answer": "The COG model selects text fragments by utilizing efficient vector search tools to calculate meaningful context representations of text fragments and index them. At each time step, relevant text fragments are sought from the text collection instead of selecting from an independent set of options.", "source_doc": "RAG.pdf", "source_id": "bff4917f9e", "retrieved_docs": ["corporating a retrieval mechanism using the T5 architecture\n[Raffel et al. , 2020 ]in both the pre-training and fine-tuning\nstages. Prior to pre-training, it initializes the encoder-decoder\nLM backbone with a pre-trained T5, and initializes the dense\nretriever with a pre-trained Contriever. During the pre-\ntraining process, it refreshes the asynchronous index every\n1000 steps.\nCOG [Vaze et al. , 2021 ]is a text generation model that for-\nmalizes its generation process by gradually copying text frag-\nments (such as words or phrases) from an existing collection\nof text. Unlike traditional text generation models that select\nwords sequentially, COG utilizes efficient vector search tools\nto calculate meaningful context representations of text frag-\nments and index them. Consequently, the text generation task\nis decomposed into a series of copy and paste operations,\nwhere at each time step, relevant text fragments are sought\nfrom the text collection instead of selecting from an indepen-", "at the current step. Greedy search can achieve satisfactory\nresults in text generation tasks ( e.g., machine translation\nand text summarization), in which the output is highly\ndependent on the input [307]. However, in terms of open-\nended generation tasks ( e.g., story generation and dialog),\ngreedy search sometimes tends to generate awkward and\nrepetitive sentences [308].\nAs another alternative decoding strategy, sampling-\nbased methods are proposed to randomly select the next\ntoken based on the probability distribution to enhance the\nrandomness and diversity during generation:\nxi\u223cP(x|x<i). (9)\nFor the example in Figure 10, sampling-based methods will\nsample the word \u201ccoffee\u201d with higher probability while\nalso retaining the possibilities of selecting the rest words,\n\u201cwater\u201d, \u201ctea\u201d, \u201crice\u201d, etc.\nNot limited to the decoder-only architecture, these two\ndecoding methods can be generally applied to encoder-\ndecoder models and prefix decoder models in a similar way.", "candidates of the final answers generated by LLMs [377] or\nto select better intermediate reasoning steps during step by\nstep reasoning [379, 382].\n5.2.4 Alignment without RLHF\nAlthough RLHF has achieved great success in aligning the\nbehaviors of LLMs with human values and preferences, it\nalso suffers from notable limitations. First, RLHF needs to\ntrain multiple LMs including the model being aligned, the\n26. https://huggingface.co/docs/transformers/v4.31.0/en/main\nclasses/text generation#transformers.GenerationMixin.group beam\nsearchreward model, and the reference model at the same time,\nwhich is tedious in algorithmic procedure and memory-\nconsuming in practice. Besides, the commonly-used PPO\nalgorithm in RLHF is rather complex and often sensitive\nto hyper-parameters. As an alternative, increasing studies\nexplore to directly optimize LLMs to adhere to human pref-\nerences, using supervised fine-tuning without reinforcement\nlearning [349].", "they can decide to search for a relevant query to collect the\nnecessary materials, similar to the tool call of the agent.\nWebGPT [Nakano et al. , 2021 ]employs a reinforcement\nlearning framework to automatically train the GPT-3 model\nto use a search engine for text generation. It uses special to-\nkens to perform actions, including querying on a search en-\ngine, scrolling rankings, and citing references. This allows\nGPT-3 to leverage a search engine for text generation.\nFlare [Jiang et al. , 2023b ], on the other hand, automates the\ntiming of retrieval and addresses the cost of periodic docu-\nment retrieval based on the probability of the generated text.\nIt uses probability as an indicator of LLMs\u2019 confidence during\nthe generation process. When the probability of a term falls\nbelow a predefined threshold, the information retrieval sys-\ntem would retrieve references and removes terms with lower\nprobabilities. This approach is designed to handle situations", "for all scenarios where the ideal model behavior is to generate a very short sequence that should match the\ncorrect reference, we set the temperature to be zero as we desire the argmax under the model\u2019s distribution.\nFor the longer-form generation scenarios such as text summarization, we either follow prior work or specify\na process by which we arrived at the temperature we used.\n118This is unnecessary for language modeling as the model by default produces a probability, which is the required behavior.\n119https://github.com/bigscience-workshop/promptsource\n120Note that it is possible to implement nucleus sampling post hoc even when a model provider does not natively support this,\nbut this leads to a prohibitive increase in the number of tokens needed to query the model.\n161"], "retrieved_docs_id": ["bff4917f9e", "fec76fc29e", "bcea917240", "b844a74991", "2d7a2b058a"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "Which vision encoder, compared to pure vision models, tends to lose some visual details?\n", "true_answer": "CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT.", "source_doc": "hallucination.pdf", "source_id": "3f64cf9b55", "retrieved_docs": ["to lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent\nstudies have proposed complementing this information loss by incorporating visual features from\nother vision encoders. The work of [ 98] proposes mixing features from CLIP ViT and DINO ViT.\nSpecifically, it experimented with additive and interleaved features. Both settings show that there\nis a trade-off between the two types of features. A more dedicated mechanism is needed.\nConcurrently, a visual expert-based model proposed in [ 38] aims to mitigate the information\nloss caused by the CLIP image encoder. Instead of merely mixing features, this paper enhances\nthe visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two\npivotal modules: multi-task encoders and the structural knowledge enhancement module. The multi-\ntask encoders are dedicated to integrating various types of latent visual information extracted by", "VL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to\n448\u00d7448. InternVL [ 2] scales up the vision encoder to 6 billion parameters, enabling processing of\nhigh-resolution images. Regarding hallucination, HallE-Switch [ 123] has investigated the impact\nof vision encoder resolution on its proposed CCEval benchmark. Among the three studied vision\nencoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results\nin lower degrees of hallucination. These works indicate that scaling up vision resolution is a\nstraightforward yet effective solution.\n5.2.2 Versatile Vision Encoders. Several studies [ 38,49,98] have investigated vision encoders for\nMLLMs. Typically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs\nthanks to its remarkable ability to extract semantic-rich features. However, CLIP has been shown\nto lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent", "Vision Expert Agents Most MLLMs, due to their non-lossless image tokenization, struggle to\nfully capture the intricate details of text and objects. Leveraging vision expert agents is a solution\nto the problem of a single vision encoder\u2019s limited generalization ability on detail-abundant content.\nP2G [38] employs expert agents for real-time grounding, enabling efficient and purposeful reasoning\n8", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "prehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,\nwith the rationale that merging the low-level spatial features from DINOv2 and the semantic at-\ntributes offered by SigLIP will enhance performance on subsequent tasks. SPHINX-X[14] employs\ntwo vision encoders \u2013 DINOv2 and CLIP-ConvNeXt. Given that these models have been pre-trained\nvia distinct learning methodologies (self-supervised versus weakly supervised) and network archi-\ntectures (ViT versus CNN), they are naturally capable of offering the most complementary and\nsophisticated visual knowledge.\nLightweight Vision Encoder Vision Transformer architectures in real-world applications pose\nchallenges due to hardware and environmental limitations, including processing power and compu-\ntational capabilities. ViTamin [11] represents a lightweight vision model, specifically tailored for\nvision and language models. It commences with a convolutional stem, succeeded by Mobile Con-"], "retrieved_docs_id": ["c20c82af54", "3f64cf9b55", "f09b6750be", "4ee780b19c", "18b9cdbf0e"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "How many new tokens were added to the LLaMA2 tokenizer for chip design datasets?\n", "true_answer": "Approximately 9K new tokens were added to the LLaMA2 tokenizer for chip design datasets.", "source_doc": "ChipNemo.pdf", "source_id": "ac7c0c980b", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 4: Domain-Adapted ChipNeMo Tokenizer Improvements.\n3.1. Domain-Adaptive Tokenization\nWe adapt the LLaMA2 tokenizer (containing 32K tokens) to\nchip design datasets using the previously outlined four-step\nprocess. Approximately 9K new tokens are added to the\nLLaMA2 tokenizer. The adapted tokenizers can improve\ntokenization efficiency by 1.6% to 3.3% across various chip\ndesign datasets as shown in Figure 4. We observe no obvious\nchanges to tokenizer efficiency on public data. Importantly,\nwe have not observed significant decline in the LLM\u2019s accu-\nracy on public benchmarks when using the domain-adapted\ntokenizers even prior to DAPT.\n3.2. Domain Adaptive Pretraining\nFigure 5: Chip Domain Benchmark Result for ChipNeMo.\nFigure 5 presents the outcomes for ChipNeMo models on\nthe AutoEval benchmark for chip design domain (detailed\nin Appendix A.5). Results on open domain academic bench-\nmark results are presented in Appendix A.6. Our research", "4.Initialize model embeddings of the new tokens by uti-\nlizing the general-purpose tokenizer.\nSpecifically for Step 4, when a new token is encountered,\nit is first re-tokenized using the original pretrained general-\npurpose tokenizer. The LLM\u2019s token embedding for the new\ntoken is determined by averaging the embeddings of the\ntokens generated by the general-purpose tokenizer (Koto\net al., 2021). The LLM\u2019s final output layer weights for the\nnew tokens are initialized to zero.\nStep 2 helps maintain the performance of the pre-trained\nLLM on general datasets by selectively introducing newtokens that are infrequently encountered in general-purpose\ndatasets. Step 4 reduces the effort required for retraining or\nfinetuning the LLM via initialization of the embeddings of\nnew tokens guided by the general-purpose tokenizer.\n2.2. Domain Adaptive Pretraining\nIn our study, we apply DAPT on pretrained foundation base\nmodels: LLaMA2 7B/13B/70B. Each DAPT model is ini-", "ChipNeMo: Domain-Adapted LLMs for Chip Design\n2Domain -Adaptive\nPretraining\n24B tokens of chip \ndesign docs/code\nThousands GPU hrs\nModel\nAlignmen t\n56K/128K \n(SteerLM /SFT)  insts\n+ 1.4K task insts\n100+ GPU hrsFoundation Models\nLLaMA2 \n(7B, 13B, 70B) \nChipNeMo \nChat Models\n(7B, 13B, 70B)ChipNeMo \nFoundation Models\n(7B, 13B, 70B)Pretraining\nTrillions tokens of \ninternet data\n105 \u2013 106 GPU hrs\nFigure 1: ChipNeMo Training Flow\n2023)) fine-tuned on additional Verilog data can outperform\nstate-of-art OpenAI GPT-3.5 models. Customizing LLMs\nin this manner also avoids security risks associated with\nsending proprietary chip design data to third party LLMs\nvia APIs. However, it would be prohibitively expensive to\ntrain domain-specific models for every domain from scratch,\nsince this often requires millions of GPU training hours. To\ncost-effectively train domain-specific models, we instead\npropose to combine the following techniques: Domain-", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat.", "within the first eight words are scraped and added to the dataset. The prompt is taken to be the first five\nwords along with the group member in question. The train-test splits for the dataset are 96%-4% with inputs\non average being 11.805 tokens and outputs on average being 19.356 tokens based on the GPT-2 tokenizer.\nOther important structure in the dataset includes the fact that it also includes sub-datasets filtered by which\nof the five domains is featured.\nPre-processing. We take no further steps to pre-process the dataset.\nData Resources. We access the dataset at https://github.com/amazon-research/bold . The dataset\nis made available through our benchmark.\n154"], "retrieved_docs_id": ["ac7c0c980b", "85cb6bbe71", "2079d05356", "273b593026", "eeb7a41507"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What are some evaluation metrics for the final response generated by the RAG model?\n", "true_answer": "Accuracy, EM, answer fidelity, and answer relevance are some evaluation metrics for the final response generated by the RAG model.", "source_doc": "RAG.pdf", "source_id": "b023f9e1c7", "retrieved_docs": ["ples include Hit Rate, MRR, NDCG, Precision, etc.\n2.Generation Module\nThe generation module here refers to the enhanced or\nsynthesized input formed by supplementing the retrieved\ndocuments into the query, distinct from the final an-\nswer/response generation, which is typically evaluated\nend-to-end. The evaluation metrics for the generation\nmodule mainly focus on context relevance, measuring\nthe relatedness of retrieved documents to the query ques-\ntion.\nEnd-to-End Evaluation\nEnd-to-end evaluation assesses the final response gener-\nated by the RAG model for a given input, involving the\nrelevance and alignment of the model-generated answers\nwith the input query. From the perspective of content\ngeneration goals, evaluation can be divided into unlabeled\nand labeled content. Unlabeled content evaluation met-\nrics include answer fidelity, answer relevance, harmless-\nness, etc., while labeled content evaluation metrics in-\nclude Accuracy and EM. Additionally, from the perspec-", "evaluation metrics. Additionally, the latest evalu-\nation frameworks like RAGAS [Eset al. , 2023 ]and\nARES [Saad-Falcon et al. , 2023 ]also involve RAG eval-\nuation metrics. Summarizing these works, three core metrics\nare primarily focused on: Faithfulness of the answer, Answer\nRelevance, and Context Relevance.\n1.Faithfulness\nThis metric emphasizes that the answers generated by\nthe model must remain true to the given context, ensur-\ning that the answers are consistent with the context infor-\nmation and do not deviate or contradict it. This aspect of\nevaluation is vital for addressing illusions in large mod-\nels.\n2.Answer Relevance\nThis metric stresses that the generated answers need to\nbe directly related to the posed question.\n3.Context Relevance\nThis metric demands that the retrieved contextual infor-\nmation be as accurate and targeted as possible, avoid-\ning irrelevant content. After all, processing long texts\nis costly for LLMs, and too much irrelevant information", "tonomously judge inputs and generate accurate answers.\n7 RAG Evaluation\nIn exploring the development and optimization of RAG, ef-\nfectively evaluating its performance has emerged as a central\nissue. This chapter primarily discusses the methods of eval-\nuation, key metrics for RAG, the abilities it should possess,\nand some mainstream evaluation frameworks.\n7.1 Evaluation Methods\nThere are primarily two approaches to evaluating the ef-\nfectiveness of RAG: independent evaluation and end-to-endevaluation [Liu, 2023 ].\nIndependent Evaluation\nIndependent evaluation includes assessing the retrieval mod-\nule and the generation (read/synthesis) module.\n1.Retrieval Module\nA suite of metrics that measure the effectiveness of sys-\ntems (like search engines, recommendation systems, or\ninformation retrieval systems) in ranking items accord-\ning to queries or tasks are commonly used to evaluate\nthe performance of the RAG retrieval module. Exam-\nples include Hit Rate, MRR, NDCG, Precision, etc.", "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "such as professional domain knowledge question-answering,\nRAG might offer lower training costs and better performance\nbenefits than fine-tuning.\nSimultaneously, improving the evaluation system of RAG\nfor assessing and optimizing its application in different down-\nstream tasks is crucial for the model\u2019s efficiency and bene-\nfits in specific tasks. This includes developing more accurate\nevaluation metrics and frameworks for different downstream\ntasks, such as context relevance, content creativity, and harm-\nlessness, among others.\nFurthermore, enhancing the interpretability of models\nthrough RAG, allowing users to better understand how and\nwhy the model makes specific responses, is also a meaning-\nful task.\nTechnical Stack\nIn the ecosystem of RAG, the development of the related\ntechnical stack has played a driving role. For instance,\nLangChain and LLamaIndex have become widely known\nquickly with the popularity of ChatGPT. They both offer a"], "retrieved_docs_id": ["b023f9e1c7", "57b75e5528", "a580bf7e9b", "af6e8c3fb2", "da182b99e8"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "Which models outperformed OpenAI's code-davinci-002 on Verilog questions?\n", "true_answer": "Thakur et al., 2023 showed that fine-tuned open-source LLMs (CodeGen) on Verilog datasets outperformed state-of-the-art OpenAI models such as code-davinci-002 on 17 Verilog questions.", "source_doc": "ChipNemo.pdf", "source_id": "cdf1ac39e3", "retrieved_docs": ["et al., 2023) and SteerLM (Dong et al., 2023).\nResearchers have started to apply LLM to chip design prob-\nlems. Early works such as Dave (Pearce et al., 2020) first\nexplored the possibility of generating Verilog from En-\nglish with a language model (GPT-2). Following that work,\n(Thakur et al., 2023) showed that fine-tuned open-source\nLLMs (CodeGen) on Verilog datasets collected from GitHub\nand Verilog textbooks outperformed state-of-the-art OpenAI\nmodels such as code-davinci-002 on 17 Verilog questions.\n(Liu et al., 2023) proposed a benchmark with more than\n150 problems and demonstrated that the Verilog code gen-\neration capability of pretrained language models could be\nimproved with supervised fine-tuning by bootstrapping with\nLLM generated synthetic problem-code pairs. Chip-Chat\n(Blocklove et al., 2023) experimented with conversational\nflows to design and verify a 8-bit accumulator-based micro-\nprocessor with GPT-4 and GPT-3.5. Their findings showed", "tuned on the host server of OpenAI. In particular,\nbabbage ,curie , and davinci correspond to the\nGPT-3 (1B), GPT-3 (6.7B), and GPT-3 (175B) models,\nrespectively [55]. In addition, there are also two APIs\nrelated to Codex [105], called code-cushman-001 (a\npowerful and multilingual version of the Codex (12B) [105])\nand code-davinci-002 . Further, GPT-3.5 series\ninclude one base model code-davinci-002 and\nthree enhanced versions, namely text-davinci-002 ,\ntext-davinci-003 , and gpt-3.5-turbo . As more\npowerful alternatives, in this year, OpenAI has released\nthe model interfaces for GPT-4 series, including gpt-4 ,\ngpt-4-32k ,gpt-4-1106-preview (i.e., GPT-4 Turbo)\nand gpt-4-vision-preview (i.e., GPT-4 Turbo with\nvision, a multimodal model). It is worth noting that OpenAI\nhas been maintaining and upgrading these model interfaces\n(gpt-3.5-turbo ,gpt-4 ,gpt-4-32k ), so the API name\nwill actually point to the latest version. Currently, ChatGPT", "into the behavior for individual examples, we see significant variation in behavior that is likely indicative\nof the spectrum of difficulty of questions. On code scenarios, we see consistent trends with code-davinci-\n002 consistently outperforming code-cushman-001 (12B) for both HumanEval andAPPS, sometimes by\nlarge margins (e.g. 10.% strict correctness vs. 2.6% on APPS). We note that we do not evaluate any of\ntext models on these code scenarios, though in some cases this may be sensible/desirable given the striking\ngenerality of model development, deployment, and validation/scrutiny. Conversely, while we evaluate the\ncode models for LSATandLegalSupport , we find achieve accuracies of 0%. Overall, we find text-davinci-\n002 and, especially, code-davinci-002 display very strong reasoning capabilities for many different forms of\nreasoning.\nMemorization & Copyright. To further explore the results for this targeted evaluation, see https:", "16.Reasoning. Forreasoning-intensivescenarios, wefindthatthecodemodels, especiallycode-davinci-\n002, consistently outperform the text models, even on synthetic reasoning scenarios posed in natural\nlanguage.20This gap is made clear in mathematical reasoning: for GSM8K , code-davinci-002\nachieves an accuracy of 52.1%, where the next best model is text-davinci-002 at 35.0% and no other\nmodel surpasses 16%.21Further, in addition to code-davinci-002, text-davinci-002 is much more\naccurate than other text models (e.g. 65.1% accuracy on synthetic reasoning in natural language,\nwhereas the next most accurate text model is OPT (175B) at 29.4% accuracy, and code-davinci-002\nhas an accuracy of 72.7%).\n17.Memorization of copyrighted/licensed material. We find that the likelihood of direct regur-\ngitation of long copyrighted sequences is somewhat uncommon, but it does become noticeable when\nlooking at popular books.22However, we do find the regurgitation risk clearly correlates with model", "Flan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction \ufb01netuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction \ufb01netuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,"], "retrieved_docs_id": ["cdf1ac39e3", "b08a9d18d0", "46b4467def", "ba9814c8b1", "194cf1aaf6"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does providing optimal visual contexts reduce hallucinations in text generation?\n", "true_answer": "Providing optimal visual contexts eliminates over 84.5% of hallucinations in text generation, as shown in an oracle study.", "source_doc": "hallucination.pdf", "source_id": "31eefbd9eb", "retrieved_docs": ["reduce hallucination. Visual context refers to the visual tokens that can be grounded from the\ngenerated text response. An oracle study showed that decoding from the provided optimal visual\ncontexts eliminates over 84.5% of hallucinations. Based on the insight and observation, the authors\ndesigned mechanisms to locate the fine-grained visual information to correct each generated\ntoken that might be hallucinating. This is essentially a visual content-guided decoding strategy.\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that\nutilizes a visual matching score to steer the generation of the final outputs, balancing both object\nhallucination mitigation and text generation quality.\nOthers. The work of OPEAR [ 45] makes an interesting observation that most hallucinations\nare closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,", "Similarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that\nCLIPScore [ 88] can effectively distinguish between hallucinated and non-hallucinated sentences\nthrough a series of studies across different models and datasets. Based on this conclusion, it further\nrecalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which\ndesigns a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that\nare less likely to be hallucinated, and 2) guided sentence generation, which generates responses\nbased on this scoring. This is implemented in a similar way to beam search but at the sentence\nlevel.\nHALC [ 15] provides a key insight that when decoding a specific token in the MLLM, identifying a\ntoken-wise optimal visual context to provide the most informative visual grounding can effectively\nreduce hallucination. Visual context refers to the visual tokens that can be grounded from the", "nificant modality gap exists between textual and visual tokens, suggesting that the current learned\ninterfaces are not effective in mapping visual representations into the textual representation space of\nLLMs. This issue potentially exacerbates the tendency for MLLMs to generate more hallucinations.\nTherefore, HACL proposes enhancing the alignment between visual and textual representations\nthrough contrastive loss. Texts with hallucinations are used as hard negative examples for image\nanchors. The loss pulls representations of non-hallucinating text and visual samples closer while\npushing representations of non-hallucinating and hallucinative text apart. Experiment results show\nthat this method not only reduces hallucination but also enhances performance on other popular\nbenchmarks.\nRecalling the work of EOS Decision [ 120], to teach the model to terminate the generation process\nproperly, this work also designs a learning objective, termed Selective EOS Supervision, in addition", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "Reducing HallucinationsInherently less prone to hallucinations as\neach answer is grounded in retrieved evi-\ndence.Can help reduce hallucinations by training\nthe model based on specific domain data but\nmay still exhibit hallucinations when faced\nwith unfamiliar input.\nEthical and Privacy IssuesEthical and privacy concerns arise from\nstoring and retrieving text from external\ndatabases.Ethical and privacy concerns may arise due\nto sensitive content in the training data.\nTable 1: Comparison between RAG and Fine-tuning\nof Advanced RAG and Modular RAG were aimed at address-\ning specific deficiencies in the Naive RAG.\n3.1 Naive RAG\nThe Naive RAG research paradigm represents the earliest\nmethodology gained prominence shortly after the widespread\nadoption of ChatGPT. The naive RAG involves traditional\nprocess: indexing, retrieval, and generation. Naive RAG\nis also summarized as a \u201cRetrieve\u201d-\u201cRead\u201d framework\n[Maet al. , 2023a ].\nIndexing"], "retrieved_docs_id": ["31eefbd9eb", "17a462daf3", "fc394b39a5", "92e73c053a", "16ccbc6afa"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the learning rate used for training the ChipNeMo models?\n", "true_answer": "The learning rate used for training the ChipNeMo models is 5\u00b710\u22126.", "source_doc": "ChipNemo.pdf", "source_id": "7eb44773ae", "retrieved_docs": ["models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "large adapter exhibiting a slight improvement.\n3.4. Training Cost\nAll models have undergone training using 128 A100 GPUs.\nWe estimate the costs associated with domain adaptive pre-\ntraining for ChipNeMo as illustrated in Table 1. It is worth\nnoting that DAPT accounts for less than 1.5% of the overall\n5", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat."], "retrieved_docs_id": ["7eb44773ae", "d9ae12f819", "a5a7c4ceb0", "df0b9868f2", "273b593026"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a potential consequence of insufficient data quality in building multimodal large language models (MLLMs)?\n", "true_answer": "Insufficient data quality could potentially lead to problematic cross-modal alignment, resulting in hallucinations.", "source_doc": "hallucination.pdf", "source_id": "77ce09f375", "retrieved_docs": ["Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Hallucination of Multimodal Large Language Models: A Survey 9\noverride the visual content. For example, given an image showing a red banana, which is\ncounter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as\n\"banana is yellow\" is a deep-rooted knowledge in the LLM. Such language/knowledge prior\nmakes the model overlook the visual content and response with hallucination.\n\u2022Weak alignment interface. The alignment interface plays an essential role in MLLMs, as\nit serves as the bridge between the two modalities. A weak alignment interface can easily\ncause hallucinations. One potential cause of a weak alignment interface is data, as discussed\nin earlier sections. Apart from that, the interface architecture itself and training loss design\nalso matter [ 52,77,123]. Recent work [ 52] argues that the LLaVA-like linear projection\ninterface preserves most of the information, but lacks supervision on the projected feature.", "cient MLLMs, research state of efficient structures and strategies, and the appli-\ncations. Finally, we discuss the limitations of current efficient MLLM research\nand promising future directions. Please refer to our GitHub repository for more\ndetails: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.\n1 Introduction\nLarge-scale pretraining, a leading approach in Artificial Intelligence(AI), has seen general-purpose\nmodels like large language and multimodal models outperform specialized deep learning models\nacross many tasks. The remarkable abilities of Large Language Models (LLM) have inspired efforts\nto merge them with other modality-based models to enhance multimodal competencies. This con-\ncept is further supported by the remarkable success of proprietary models like OpenAI\u2019s GPT-4V [1]\nand Google\u2019s Gemini[2]. As a result, Multimodal Large Language Models (MLLMs) have emerged,\nincluding the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\""], "retrieved_docs_id": ["ac70fcc9f2", "da0a465b6c", "a1c28916ce", "e021f7788d", "f49f3b54ce"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "How does Quantformer reduce quantization error during training?\n", "true_answer": "Quantformer reduces quantization error during training by leveraging entropy information to maintain consistency in self-attention ranks and introducing a differentiable search mechanism to optimally group patch feature dimensions, reducing rounding and clipping inaccuracies.", "source_doc": "multimodal.pdf", "source_id": "900b3dde3f", "retrieved_docs": ["to quantized values, the quantization error is significantly reduced under certain conditions. This\ntechnique successfully modifies heavy-tailed activation distributions to fit a given quantizer.\nQuantization-Aware Training (QAT) integrates quantization into the training cycle. This in-\ntegration is particularly advantageous when scaling down to ultra-low bit precision, such as 4 bits\nor lower, where PTQ struggles with significant performance loss. For example, Quantformer [124]\nleverages entropy information to maintain consistency in self-attention ranks and introduces a dif-\nferentiable search mechanism to optimally group patch feature dimensions, reducing rounding and\nclipping inaccuracies. Q-ViT [126] incorporates a distillation token and Information Rectification\nModule (IRM) to counteract altered distributions in quantized attention modules. TerViT [127] and\nBit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain", "Migrate the quantization difficulty from activations to\nweights. We aim to choose a per-channel smoothing factor\nssuch that \u02c6X=Xdiag(s)\u22121is easy to quantize. To reduce\nthe quantization error, we should increase the effective quan-\ntization bits for all the channels. The total effective quanti-\nzation bits would be largest when all the channels have the\nsame maximum magnitude. Therefore, a straight-forward\nchoice is sj= max( |Xj|), j= 1,2, ..., C i, where jcorre-\nsponds to j-th input channel. This choice ensures that after\nthe division, all the activation channels will have the same\nmaximum value, which is easy to quantize. Note that the\nrange of activations is dynamic; it varies for different input\nsamples. Here, we estimate the scale of activations channels\nusing calibration samples from the pre-training dataset (Ja-\ncob et al., 2018). However, this formula pushes allthe\nquantization difficulties to the weights. We find that, in this", "which increases the model size and complicates the quantization pipeline by necessitating the bi-\nlevel quantization scheme. Second, the sensitivity-based non-uniform quantization in SqueezeLLM\nallows for much smaller (e.g., 0.05%) or even zero sparsity levels to achieve accurate quantization.\nThis is crucial for reducing the model size as well as inference speed since higher sparsity levels\ncan degrade inference latency. By avoiding grouping and utilizing smaller or zero sparsity levels,\nSqueezeLLM achieves accurate and fast quantization while pushing the average bit precision down\nto 3-bit, all while employing a simpler quantization pipeline and implementation.\nAnother concurrent work is AWQ Lin et al. (2023) which improves the weight-only quantization\nscheme for LLMs by introducing scaling factors to reduce the quantization error of a few impor-\ntant weights. However, their approach is also based on the OBS framework, where sensitivity is", "Quantization, which is a quantization scheme\noptimized for the typical normal distribution of\nLLM weights. By quantizing based on the\nquantiles of a normal distribution, NF4 provides\nbetter performance than standard 4-bit integer or\nfloat quantization. To further reduce memory, the\nquantization constants are themselves quantized\nto 8 bits. This second level of quantization\nsaves an additional 0.37 bits per parameter on\naverage. QLORA leverages NVIDIA\u2019s unified\nmemory feature to page optimizer states to CPU\nRAM when GPU memory is exceeded. avoiding\nout-of-memory during training. QLORA enables\ntraining a 65B parameter LLM on a single 48GB\nGPU with no degradation compared to full 16-\nbit finetuning. QLORA works by freezing the\n4-bit quantized base LLM, then backpropagating\nthrough it into a small set of 16-bit low-rank\nadapter weights which are learned.\n7.4 LOMO\nLOw-Memory Optimization (LOMO) (Lv et al.,\n2023) enables full parameter fine-tuning of LLMs", "Bit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain\naccuracy throughout quantization. PackQViT [129] mitigates outlier effects during quantization.\nBiViT [128] introduces Softmax-aware Binarization to adjust the binarization process, minimizing\nerrors in binarizing softmax attention values. Xiao et al. [142] integrated a gradient regularization\nscheme to curb weight oscillation during binarization training and introduced an activation shift\nmodule to reduce information distortion in activations. Additionally, BinaryViT [130] integrates\nessential architectural elements from CNNs into a pure ViT framework, enhancing its capabilities.\nHardware-Aware Quantization optimizes the quantization process of neural network models for\nspecific hardware platforms ( e.g., GPUs [131], FPGA [132]). It adjusts precision levels and quan-\ntization strategies to maximize performance and energy efficiency during inference. For example,"], "retrieved_docs_id": ["900b3dde3f", "0b12073694", "e881a5cdd2", "4d7dfaafa5", "31efe3044d"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the score of LLaMA-2-Chat-7B in the InstructBLIP benchmark?\n", "true_answer": "The score of LLaMA-2-Chat-7B in the InstructBLIP benchmark is 58.8.", "source_doc": "multimodal.pdf", "source_id": "88e0397250", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 15\nTable 3. Comparison of mainstream MLLMs on discriminative benchmarks. The numbers come from the\noriginal papers of these benchmarks.\nModelLLM\nSizeMME\nExistence\nScore\u2191MME\nCount\nScore\u2191MME\nPosition\nScore\u2191MME\nColor\nScore\u2191POPE\nRandom\nF1-Score\u2191POPE\nRandom\nF1-Score\u2191POPE\nAdversarial\nF1-Score\u2191RAH-Bench\nF1 Score\u2191AMBER\nDis.\nF1-Score\u2191AMBER\nScore\u2191Hal-Eval\nIn-domain\nEvent. F1\u2191Hal-Eval\nOut-of-domain\nEvent. F1\u2191\nmPLUG-Owl [111] 7B 120.00 50.00 50.00 55.00 68.06 66.79 66.82 69.3 31.2 54.1 47 46.6\nImageBind-LLM [34] 7B 128.33 60.00 46.67 73.33 - - - - - - - -\nInstructBLIP [22] (7B) 7B - - - - - - - 89.1 82.6 86.2 66.2 66.6\nInstructBLIP [22] (13B) 13B 185.00 143.33 66.67 153.33 89.29 83.45 78.45 84.7 - - - -\nVisualGLM-6B [25] 6B 85.00 50.00 48.33 55.00 - - - - - - - -\nMultimodal-GPT [28] 7B 61.67 55.00 58.33 68.33 66.68 66.67 66.67 - - - - -\nPandaGPT [95] 7B 70.00 50.00 50.00 50.00 - - - - - - - -", "results are presented in Section 4.4.\nResults. As shown in Figure 12, Llama 2-Chat models outperform open-source models by a significant\nmargin on both single turn and multi-turn prompts. Particularly, Llama 2-Chat 7B model outperforms\nMPT-7B-chat on 60% of the prompts. Llama 2-Chat 34B has an overall win rate of more than 75% against\nequivalently sized Vicuna-33B and Falcon 40B models.\n18", "results are presented in Section 4.4.\nResults. As shown in Figure 12, Llama 2-Chat models outperform open-source models by a significant\nmargin on both single turn and multi-turn prompts. Particularly, Llama 2-Chat 7B model outperforms\nMPT-7B-chat on 60% of the prompts. Llama 2-Chat 34B has an overall win rate of more than 75% against\nequivalently sized Vicuna-33B and Falcon 40B models.\n18", "source models may have limited model capacities due to\nsmall model sizes.\n\u2022The top-performing model varies on different human align-\nment tasks. For different human alignment tasks, we can see\nthat these models achieve inconsistent performance rank-\nings. For example, LLaMA 2-Chat (7B) performs the best\namong the compared open-source models on TruthfulQA,\nwhile Vicuna (13B) performs the best on CrowS-Pairs. A\npossible reason is that these tasks are designed with spe-\ncific purposes for evaluating different aspects of human\nalignment, and these models exhibit varied performance\non different tasks, even for the variants of the same model\n(e.g., Pythia (7B) and Pythia (12B)). More experiments and\nanalysis on human alignment evaluation are needed to\nreveal more detailed findings.\n\u2022As a more recently released model, LLaMA 2 (7B) overall\nachieves a good performance, especially on complex reasoning\ntasks. For complex reasoning tasks, LLaMA 2 (7B) mostly", "Model LLM Backbone VQAv2GQA SQAIVQATVizWiz MMMU MathV MMEPMMECMMB SEED POPE LLA V AWMM-Vet\nFlamingo [16] Chinchilla-7B - - - - 28.8 - - - - - - - - -\nBLIP-2 [15] Flan-T5XXL(13B) 65.0 44.7 61.0 42.5 19.6 - - 1293.8 290.0 - -/46.4 85.3 38.1 22.4\nLLaV A [7] Vicuna-13B - 41.3 - 38.9 - - - - - - - - - -\nMiniGPT-4 [10] Vicuna-13B - 30.8 - 19.4 - - - - - - - - - -\nInstructBLIP [8] Vicuna-13B - 49.5 63.1 50.7 33.4 - - 1212.8 291.8 - - 78.9 58.2 25.6\nQwen-VL-Chat [187] Qwen-7B 78.2\u221757.5\u221768.2 61.5 38.9 35.9/32.9 - 1487.5 360.7 60.6 -/58.2 - - -\nLLaV A-1.5 [54] Vicuna-1.5-13B 80.0\u221763.3\u221771.6 61.3 53.6 - - 1531.3 295.4 67.7 -/68.2 85.9 70.7 35.4\nMiniGPT-v2-Chat [9] LLaMA-2-Chat-7B - 58.8 - 52.3 42.4 - - - - - - - - -\nInternVL-Chat [5] Vicuna-13B 81.2\u221766.6\u2217- 61.5 58.5 - - 1586.4 - - - 87.6 - -\nEmu2-Chat [6] LLaMA-33B 84.9\u221765.1\u2217- 66.6\u221754.9 -/34.1 - - - - 62.8 - - 48.5\nGemini Pro [2] - 71.2 - - 74.6 - 47.9/\u2013 45.2 - 436.79 73.6 \u2013/70.7 - - 64.3"], "retrieved_docs_id": ["90bbefc8ec", "d423d98d91", "d423d98d91", "99af9c92b4", "88e0397250"], "reranker_type": "colbert", "search_type": "text", "rr": 0.2, "hit": 1}, {"question": "When was the first issue of Preprint published?\n", "true_answer": "The first issue of Preprint was published in April 2024.", "source_doc": "hallucination.pdf", "source_id": "6158839d4c", "retrieved_docs": ["Published in Transactions on Machine Learning Research (08/2023)\nRodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085 ,\n2019.\nSebastian Nordhoff and Harald Hammarstr\u00f6m. Glottolog/langdoc: Defining dialects, languages, and lan-\nguage families as collections of resources. In First International Workshop on Linked Science 2011-In\nconjunction with the International Semantic Web Conference (ISWC 2011) , 2011.\nJeppe N\u00f8rregaard, Benjamin D. Horne, and Sibel Adali. Nela-gt-2018: A large multi-labelled news dataset\nfor the study of misinformation in news articles. ArXiv, abs/2203.05659, 2019.\nJekaterina Novikova, Ond\u0159ej Du\u0161ek, and Verena Rieser. The E2E dataset: New challenges for end-to-end\ngeneration. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue , pp. 201\u2013\n206, Saarbr\u00fccken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/\nW17-5525. URL https://aclanthology.org/W17-5525 .", "of InstructGPT in Figure 12). Shortly after the release of this\nRL paper [79], the paper of the Proximal Policy Optimiza-\ntion (PPO) [128] was published in July 2017, which now has\nbeen the foundational RL algorithm for learning from hu-\nman preferences [66]. Later in January 2020, GPT-2 was fine-\ntuned using the aforementioned RL algorithms [79, 128],\nwhich leveraged human preferences to improve the capac-\nities of GPT-2 on NLP tasks. In the same year, another\nwork [129] trained a summarization model for optimizing\nhuman preferences in a similar way. Based on these prior\nwork, InstructGPT [66] was proposed in January 2022 to\nimprove the GPT-3 model for human alignment, which\nformally established a three-stage reinforcement learning from\nhuman feedback (RLHF) algorithm. Note that it seems that\nthe wording of \u201c instruction tuning \u201d has seldom been used in\nOpenAI\u2019s paper and documentation, which is substituted by\nsupervised fine-tuning on human demonstrations (i.e.,the first", "per about backpropagation was published,\nfor the insight about massive parallelism to\nbe operationalized in a useful way for connec-\ntionist deep neural networks. Many inven-\ntions are re-purposed for means unintended\nby their designers. Edison\u2019s phonograph was\nnever intended to play music. He envisioned\nit as preserving the last words of dying peo-\nple or teaching spelling. In fact, he was dis-\nappointed by its use playing popular music\nas he thought this was too \u201cbase\u201d an applica-\ntion of his invention (Diamond et al., 1999).\nIn a similar vein, deep neural networks only\nbegan to work when an existing technology\nwas unexpectedly re-purposed.\nA graphical processing unit (GPU) was origi-\nnally introduced in the 1970s as a specialized\naccelerator for video games and for develop-\ning graphics for movies and animation. In\nthe 2000s, like Edison\u2019s phonograph, GPUs\nwere re-purposed for an entirely unimag-\nined use case \u2013 to train deep neural net-\nworks (Chellapilla et al., 2006; Oh & Jung,", "Published in Transactions on Machine Learning Research (08/2023)\nReflexivity. This work was developed at the Center for Research on Foundation Models (CRFM), a\ncenter at Stanford University borne out of the Stanford Institute for Human-Centered Artificial Intelligence\n(Stanford HAI). It is accompanied by a policy brief on how evaluations substantively improve transparency.88\nThis work was made possible by two unique positions that CRFM occupies. First, to develop the framework,\nbuild the benchmark, and execute the evaluation, CRFM serves as an interdisciplinary hub for coordinating\nand uniting the many authors of this work at Stanford University. Second, to acquire the access and resources\nrequired to evaluate all the models in this work, CRFM leverages its relationships with the associated\nfoundation model developers.\nWe further highlight that this work continues a trend documented by Koch et al. (2021) that many works", "Published in Transactions on Machine Learning Research (08/2023)\nPopulation metrics. We first define the metrics in the population setting\u2014intuitively, the population\nsetting corresponds to having infinite data. The expected calibration error (ECE) examines the difference\nbetween the model\u2019s top probability pmaxand the probability the model is correct ( y= \u02c6y) given pmax:\nECE =E[\u23d0\u23d0pmax\u2212E[y= \u02c6y|pmax]\u23d0\u23d0]\n(3)\nFor selective classification, we first need to define two key concepts: coverage and accuracy. Given a threshold\nt\u22650, thecoverage c(t)is the fraction of examples for which the model\u2019s predicted confidence is at least t:\nc(t) =P(pmax\u2265t). (4)\nThe coverage c(t)is non-increasing in tbecause when tis higher there will be fewer examples where the\nmodel is more confident than t.\nTheselective accuracy a(t)at a threshold t\u22650is the accuracy for all examples where the model\u2019s predicted\nconfidence is at least t:94\na(t) ={\nP(y= \u02c6y|pmax\u2265t)ifc(t)>0\n1 otherwise. (5)"], "retrieved_docs_id": ["0633197e8c", "ec039a8cdb", "18c0a9763e", "a7dcb1a4fb", "60655521bc"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "How are the parameters of a pre-trained model updated during adapter-based tuning?\n", "true_answer": "During adapter-based tuning, only the adapter parameters are updated, while the pre-trained model\u2019s parameters remain fixed.", "source_doc": "multimodal.pdf", "source_id": "004ffc5dd9", "retrieved_docs": ["expressiveness and generalization capabilities. Adapter-based tuning introduces lightweight adapter\nmodules into the pre-trained model\u2019s architecture. These adapter modules, typically composed of\nfeed-forward neural networks with a small number of parameters, are inserted between the layers\nof the original model. During fine-tuning, only the adapter parameters are updated, while the pre-\ntrained model\u2019s parameters remain fixed. This method significantly reduces the number of trainable\nparameters, leading to faster training and inference times without compromising the model\u2019s per-\nformance. LLM-Adapters [154] presents a framework for integrating various adapters into large\nlanguage models, enabling parameter-efficient fine-tuning for diverse tasks. This framework en-\n16", "lated gradient update to weight matrices to have full-rank during adaptation. This means that when\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\nness of full \ufb01ne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\nto training the original model, while adapter-based methods converges to an MLP and pre\ufb01x-based\nmethods to a model that cannot take long input sequences.\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd\u00d7k.\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\nthen adding a different B\u2032A\u2032, a quick operation with very little memory overhead. Critically, this\n2They represent a negligible number of parameters compared to weights.", "Weights\n\ud835\udc4a\u2208\u211d\ud835\udc51\u00d7\ud835\udc51\nxh\n\ud835\udc35=0\n\ud835\udc34=\ud835\udca9(0,\ud835\udf0e2)\n\ud835\udc51\ud835\udc5fPretrained \nWeights\n\ud835\udc4a\u2208\u211d\ud835\udc51\u00d7\ud835\udc51\nxf(x)\n\ud835\udc51\nFigure 1: Our reparametriza-\ntion. We only train AandB.Many applications in natural language processing rely on adapt-\ningonelarge-scale, pre-trained language model to multiple down-\nstream applications. Such adaptation is usually done via \ufb01ne-tuning ,\nwhich updates all the parameters of the pre-trained model. The ma-\njor downside of \ufb01ne-tuning is that the new model contains as many\nparameters as in the original model. As larger models are trained\nevery few months, this changes from a mere \u201cinconvenience\u201d for\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\n175 billion trainable parameters.1\nMany sought to mitigate this by adapting only some parameters or\nlearning external modules for new tasks. This way, we only need\nto store and load a small number of task-speci\ufb01c parameters in ad-", "During full \ufb01ne-tuning, the model is initialized to pre-trained weights \u03a60and updated to \u03a60+ \u2206\u03a6\nby repeatedly following the gradient to maximize the conditional language modeling objective:\nmax\n\u03a6\u2211\n(x,y)\u2208Z|y|\u2211\nt=1log(P\u03a6(yt|x,y<t)) (1)\nOne of the main drawbacks for full \ufb01ne-tuning is that for each downstream task, we learn a different\nset of parameters \u2206\u03a6whose dimension|\u2206\u03a6|equals|\u03a60|. Thus, if the pre-trained model is large\n(such as GPT-3 with |\u03a60| \u2248175Billion), storing and deploying many independent instances of\n\ufb01ne-tuned models can be challenging, if at all feasible.\nIn this paper, we adopt a more parameter-ef\ufb01cient approach, where the task-speci\ufb01c parameter\nincrement \u2206\u03a6 = \u2206\u03a6(\u0398) is further encoded by a much smaller-sized set of parameters \u0398with\n|\u0398|\u226a| \u03a60|. The task of \ufb01nding \u2206\u03a6thus becomes optimizing over \u0398:\nmax\n\u0398\u2211\n(x,y)\u2208Z|y|\u2211\nt=1log(\np\u03a60+\u2206\u03a6(\u0398) (yt|x,y<t))\n(2)\nIn the subsequent sections, we propose to use a low-rank representation to encode \u2206\u03a6that is both", "abs/2303.10512, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2303.10512\n[407] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and\nA. Ghodsi, \u201cDylora: Parameter efficient tuning of\npre-trained models using dynamic search-free low-\nrank adaptation,\u201d CoRR , vol. abs/2210.07558, 2022.\n[Online]. Available: https://doi.org/10.48550/arXiv.\n2210.07558\n[408] N. Ding, Y. Qin, G. Yang, F. Wei, Y. Zonghan, Y. Su,\nS. Hu, Y. Chen, C.-M. Chan, W. Chen, J. Yi, W. Zhao,\nX. Wang, Z. Liu, H.-T. Zheng, J. Chen, Y. Liu, J. Tang,\nJ. Li, and M. Sun, \u201cParameter-efficient fine-tuning\nof large-scale pre-trained language models,\u201d Nature\nMachine Intelligence , vol. 5, pp. 1\u201316, 03 2023.\n[409] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P . Lu, H. Li,\nP . Gao, and Y. Qiao, \u201cLlama-adapter: Efficient fine-\ntuning of language models with zero-init attention,\u201d\nCoRR , vol. abs/2303.16199, 2023.\n[410] J. Pfeiffer, I. Vulic, I. Gurevych, and S. Ruder, \u201cMAD-\nX: an adapter-based framework for multi-task cross-"], "retrieved_docs_id": ["004ffc5dd9", "2099c4406a", "b1e71d3b0e", "ace637d393", "e70ec500cb"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "Which model outperforms several 13 billion parameter MLLMs in document understanding?\n", "true_answer": "TinyChart [37]", "source_doc": "multimodal.pdf", "source_id": "6b47636d3a", "retrieved_docs": ["7.2 Document Understanding\nDocuments or charts serve as a crucial source of information, offering an intuitive visualization\nof data in various forms. They have become an indispensable part of information dissemination,\nbusiness decision-making, and academic research. However, current chart understanding models\nstill face two primary limitations: (1) The considerable number of parameters makes training and\ndeployment challenging. For instance, ChartLlama [196], a 13-billion-parameter model, is difficult\nto deploy on a single consumer-grade GPU. (2) These models struggle with efficiently encoding\nhigh-resolution images, as vision transformers tend to produce lengthy feature sequences.\nTo address the challenges of fine-grained visual perception and visual information compression for\ndocument-oriented MLLMs. TinyChart [37] outperforms several 13B MLLMs with Program-of-\nThoughts (PoT) learning and Visual Token Merging strategy while excelling in faster inference", "its selection is closely related to the lightweight nature of MLLM. In comparison to conventional\nMLLMs with parameter sizes ranging from 7 billion to tens of billions[87, 88], efficient MLLMs\ntypically employ language models with less than 3 billion parameters, such as phi2-2.7B[74] by\nMicrosoft and Gemma-2B[78] by Google. Phi-2 trained on special data recipes can match the per-\nformance of models 25 times larger trained on regular data. Phi-3-mini [86] can be easily deployed\nlocally on a modern phone and achieves a quality that seems on-par with models such as Mixtral\n8x7B [89] and GPT-3.5. In addition to utilizing pre-trained models, MobileVLM[20] downscales\nLLaMA[87] and trains from scratch using open-source datasets. The specific model scaling is illus-\ntrated in the Table.1 and Table.4.\n2.4 Vision Token Compression\nInitial research has underscored the potential of MLLMs across various tasks, including visual ques-", "maximum learning rate of 1\u00935\u000210\u00004that decays by 10\u0002. From Kaplan et al. (2020), we \ufb01nd that\nthe optimal model size should be 4.68 billion parameters. From our approach 1, we estimate a 2.86\nbillion parameter model should be optimal. We train a 4.74 billion parameter and a 2.80 billion\nparameter transformer to test this hypothesis, using the same depth-to-width ratio to avoid as many\nconfounding factors as possible. We \ufb01nd that our predicted model outperforms the model predicted\nby Kaplan et al. (2020) as shown in Figure A4.\n0 1 2\nSequences 1e72.22.32.42.52.62.72.8Training Loss\n0.0 0.2 0.4 0.6 0.8 1.0\nFLOPs \u00d710212.22.32.42.52.62.72.8Training LossKaplan et al (2020)\nApproach 1\nFigure A4jComparison to Kaplan et al. (2020) at 1021FLOPs.We train 2.80 and 4.74 billion\nparameter transformers predicted as optimal for 1021FLOPs by Approach 1 and by Kaplan et al.\n(2020). We \ufb01nd that our prediction results in a more performant model at the end of training.\nE. Curvature of the FLOP-loss frontier", "Kathy Meier-Hellstern Douglas Eck Je\ufb00 Dean Slav Petrov Noah Fiedel\nGoogle Research\nAbstract\nLarge language models have been shown to achieve remarkable performance across a variety of natural\nlanguage tasks using few-shot learning , which drastically reduces the number of task-speci\ufb01c training\nexamples needed to adapt the model to a particular application. To further our understanding of the\nimpact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer\nlanguage model, which we call Pathways Language Model (PaLM).\nWe trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly e\ufb03cient\ntraining across multiple TPU Pods. We demonstrate continued bene\ufb01ts of scaling by achieving state-of-\nthe-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a\nnumber of these tasks, PaLM 540B achieves breakthrough performance, outperforming the \ufb01netuned state-", "C Broader Impacts\nThe current status quo in research is that large lan-\nguage models are things people train and publish\nabout, but do not actually release. To the best of our\nknowledge, GPT-NeoX-20B is the largest dense\nlanguage model to ever be publicly released with a\nseveral-way tie for second place at 13 billion param-\neters (Artetxe et al., 2021; Xue et al., 2020, 2022)\nand many more models at the 10-11B parameter\nscale. A variety of reasons for the non-release of\nlarge language models are given by various groups,\nbut the primary one is the harms that public access\nto LLMs would purportedly cause.\nWe take these concerns quite seriously. However,\nhaving taken them quite seriously, we feel that they\nare \ufb02awed in several respects. While a thorough\nanalysis of these issues is beyond the scope of this\npaper, the public release of our model is the most\nimportant contribution of this paper and so an ex-\nplanation of why we disagree with the prevailing\nwisdom is important."], "retrieved_docs_id": ["6b47636d3a", "26327c579e", "d886cf68e4", "bab21eb3a9", "56f3afd56e"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How much computation does LLaV A-UHD use compared to previous models when supporting 6 times larger resolution images?\n", "true_answer": "LLaV A-UHD uses 94% of the inference computation compared to previous models when supporting 6 times larger resolution images.", "source_doc": "multimodal.pdf", "source_id": "986687f08e", "retrieved_docs": ["Token Processing Techniques designed to process lengthy visual token squence are critical in ef-\nficient MLLMs as they address the dual challenges of preserving fine-grained details and reducing\ncomputational complexity. LLaV A-UHD [35] presents a novel approach to manage the computa-\ntional burden associated with high-resolution images. It puts forward two key components: (1) a\ncompression module that further condenses image tokens from visual encoders, significantly re-\nducing the computational load, and (2) a spatial schema to organize slice tokens for LLMs. No-\ntably, LLaV A-UHD demonstrates its efficiency by supporting 6 times larger resolution images using\nonly 94% of the inference computation compared to previous models. Furthermore, the model\ncan be efficiently trained in academic settings, completing the process within 23 hours on 8 A100\nGPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-", "Figure 6: Comparision of Phi[86] (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-small) versus\nLlama-2 [91] family of models(7B, 13B, 34B, 70B) that were trained on the same fixed data.\nLLaV A-UHD [35] proposes an image modularization strategy that divides native-resolution im-\nages into smaller variable-sized slices for efficient and extensible encoding. Inaddition, InternLM-\nXComposer2-4KHD [90] introduces a strategy that dynamically adjusts resolution with an automatic\nlayout arrangement, which not only maintains the original aspect ratios of images but also adaptively\nalters patch layouts and counts, thereby enhancing the efficiency of image information extraction.\nBy implementing an adaptive input strategy for images of varying resolutions, a balance between\nperceptual capability and efficiency can be achieved.\nToken Processing Techniques designed to process lengthy visual token squence are critical in ef-", "VL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to\n448\u00d7448. InternVL [ 2] scales up the vision encoder to 6 billion parameters, enabling processing of\nhigh-resolution images. Regarding hallucination, HallE-Switch [ 123] has investigated the impact\nof vision encoder resolution on its proposed CCEval benchmark. Among the three studied vision\nencoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results\nin lower degrees of hallucination. These works indicate that scaling up vision resolution is a\nstraightforward yet effective solution.\n5.2.2 Versatile Vision Encoders. Several studies [ 38,49,98] have investigated vision encoders for\nMLLMs. Typically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs\nthanks to its remarkable ability to extract semantic-rich features. However, CLIP has been shown\nto lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent", "dataset. Visual features such as captions and\nbounding boxes were used to encode images.\nLLaV A yields a 85.1% relative score compared\nwith GPT-4 on a synthetic multimodal instruction\nfollowing dataset. When fine-tuned on Science QA,\nthe synergy of LLaV A and GPT-4 achieves a new\nstate-of-the-art accuracy of 92.53%.\nVideo-LLaMA (Zhang et al., 2023b) is\na multimodal framework that enhances large\nlanguage models with the ability to understand\nboth visual and auditory content in videos. The\narchitecture of Video-LLaMA consists of two\nbranche encoders: the Vision-Language (VL)\nBranch and the Audio-Language (AL) Branch, and\na language decoder (Vicuna (7B/13B) (Chiang\net al., 2023), LLaMA (7B) (Touvron et al.,\n2023a), etc.). The VL Branch includes a frozen\npre-trained image encoder (pre-trained vision\ncomponent of BLIP-2 (Li et al., 2023d), which\nincludes a ViT-G/14 and a pre-trained Q-former),\na position embedding layer, a video Q-former and\na linear layer. The AL Branch includes a pre-", "VQAv2 [ GKSS+17] and test-dev set of VizWiz [ GLS+18], respectively. The resolution of images is\n224\u00d7224. We use greedy search for the decoding. We follow the normalization rules of the VQAv2\nevaluation code5when computing the VQA accuracy. We evaluate the performance of VQA in an\nopen-ended setting that KOSMOS -1generates answers and stops at the </s> (\u201cend of sequence\u201d)\ntoken. The prompt is \u201cQuestion: {question} Answer: {answer}\u201d for visual question answering tasks.\n4.1.2 Results\nImage Captioning Table 2 shows the zero-shot captioning performance on COCO Karpathy test\nsplit and Flickr30k test set. KOSMOS -1achieves remarkable results in zero-shot setting on two image\ncaptioning datasets. Speci\ufb01cally, our model achieves a CIDEr score of 67.1 on the Flickr30k dataset,\ncompared to 60.6 and 61.5 for the Flamingo-3B and Flamingo-9B models, respectively. Notably, our\nmodel is able to accomplish this feat with a smaller size of 1.6B, compared to Flamingo models. This"], "retrieved_docs_id": ["986687f08e", "c0bdc4830f", "3f64cf9b55", "559c71259f", "bc8dd45ffe"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does FaithScore evaluate free-form responses from MLLMs?\n", "true_answer": "FaithScore evaluates free-form responses from MLLMs by decomposing the response into elements, evaluating, and analyzing them in detail.", "source_doc": "hallucination.pdf", "source_id": "23d981a684", "retrieved_docs": ["HaELM [ 104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4\nmodels to assess the quality of the MLLM response. In contrast, the work of Hallucination Evaluation\nbased on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination\ndetection. It collects a set of hallucination data generated by a wide range of MLLMs, simulates data\nusing ChatGPT, and trains an LLM based on LLaMA [ 99]. After that, the HaELM model becomes\nproficient in hallucination evaluation, leveraging reference descriptions of images as the basis of\nassessment.\nFaithScore [ 55]Considering the natural forms of interaction between humans and MLLMs,\nFaithScore aims to evaluate free-form responses to open-ended questions. Different from LLM-based\noverall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate,\nand analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence", "6.4 Establishing Standardized Benchmarks\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in as-\nsessing the degree of hallucination in MLLMs. In Table 1, it can be observed that there is a variety\nof evaluation benchmarks, but a lack of unified standards. Among them, one of the most popular\nbenchmarks might be POPE [ 69], which employs a \u2019Yes-or-No\u2019 evaluation protocol. However, this\nbinary-QA manner does not align with how humans use MLLMs. Accordingly, some benchmarks\nspecifically evaluate the hallucination of MLLMs in the (free-form) generative context. Yet, they\noften rely on external models, such as vision expert models or other LLMs, which limits their wide-\nspread application. Moving forward, future research can investigate standardized benchmarks that\nare theoretically sound and easy to use. Otherwise, research on methods to mitigate hallucinations\nmay be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature", "and analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence\nidentification, atomic fact generation, and fact verification. The evaluation metric involves fine-\ngrained object hallucination categories, including entity, count, color, relation, and other attributes.\nThe final computation of FaithScore is the ratio of hallucinated content.\nBingo [ 21]Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark\nspecifically designed for assessing and analyzing the limitations of current popular MLLMs, such as\nGPT-4V [ 83]. It comprises 190 failure instances, along with 131 success instances as a comparison.\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\nBias refers to the model\u2019s susceptibility to generating hallucinatory outputs on specific types of\nexamples, such as OCR bias, region bias, etc. Interference refers to scenarios in which the judgment", "directly inputs image patches and employs a linear projection to transform the raw pixels of each\npatch into embeddings.\nThe abstracted pipeline is depicted in Fig. 2. MLLMs take input from both visual and textual\nmodalities, learning from multimodal instructions and responses, which leads to remarkable per-\nformance across various multimodal tasks. Regarding the training of MLLMs, we provide a concise\noverview of the training process for interface-based MLLMs. Given that end-to-end models are\nclosed-source, the training details are unknown. Typically, the training of interface-based MLLMs\nconsists of two stages: 1) pre-training, 2) instruction tuning.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "The results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We\nobserve that the MLLMs\u2019 performance is not always consistent across different benchmarks. It\nindicates that different benchmarks have different evaluation dimensions and emphases.\nTable 2. Comparison of mainstream MLLMs on generative benchmarks. The numbers come from the original\npapers of these benchmarks.\nModel LLM SizeCHAIR\n(On AMBER)\u2193AMBER\nScore\u2191HallusionBench\nAll-Acc\u2191FaithScore\n(LLaVA-1k)\u2191FaithScore\n(COCO-Cap)\u2191Hal-Eval\nIn-domain\nGen. Acc\u2191Hal-Eval\nOut-of-domain\nGen. Acc\u2191\nmPLUG-Owl [111] 7B 23.1 54.1 43.93 0.7167 0.8546 27.3 29.5\nMultimodal-GPT [28] 7B - - - 0.5335 0.5440 - -\nInstructBLIP [22] 7B 10.3 86.2 45.26 0.8091 0.9392 35.5 41.3\nGPT-4V [83] - 4.3 92.7 65.28 - - - -\nLLaVA (7B) [75] 7B 13.5 69.3 - - - 23.3 26.3\nLLaVA (13B) [75] 13B - - - 0.8360 0.8729 - -\nMiniGPT-4 (7B) [138] 7B - - 35.78 0.5713 0.6359 61.4 50.1\nMiniGPT-4 (13B) [138] 13B 15.9 76.7 - - - - -"], "retrieved_docs_id": ["23d981a684", "312439a972", "db8870dfa6", "a8f0bda3b0", "3939d93618"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the length of the context window in RAG affect its performance?\n", "true_answer": "The performance of RAG's generation phase is constrained by the context window of LLMs. If the window is too short, it may not contain enough relevant information, and if it's too long, it might lead to information loss.", "source_doc": "RAG.pdf", "source_id": "6080afb1ff", "retrieved_docs": ["that require further investigation.\nFirstly, the issue of long context in RAG is a significant\nchallenge. As mentioned in the literature [Xuet al. , 2023c ],\nRAG\u2019s generation phase is constrained by the context win-\ndow of LLMs. If the window is too short, it may not contain\nenough relevant information; if it\u2019s too long, it might lead to\ninformation loss. Currently, expanding the context window\nof LLMs, even to the extent of limitless context, is a critical\ndirection in LLM development. However, once the context\nwindow constraint is removed, how RAG should adapt re-\nmains a noteworthy question.\nSecondly, the robustness of RAG is another important re-\nsearch focus. If irrelevant noise appears during retrieval, or\nif the retrieved content contradicts facts, it can significantly\nimpact RAG\u2019s effectiveness. This situation is figuratively\nreferred to as \u201dopening a book to a poisonous mushroom\u201d.", "\u2022Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\nLlama team who helped get this work started.\n\u2022Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the figures in the\npaper.\n\u2022Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\ninternal demo.\n\u2022Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\nLaurens van der Maaten, Jason Weston, and Omer Levy.\nA.2 Additional Details for Pretraining\nA.2.1 Architecture Changes Compared to Llama 1\nContext Length. We expand the context window for Llama 2 from 2048 tokens to 4096 tokens. The longer\ncontext window enables models to process more information, which is particularly useful for supporting\nlonger histories in chat applications, various summarization tasks, and understanding longer documents.\nTable 16 compares the performance of 2k and 4k context pretraining on long-context benchmarks. Both", "\u2022Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\nLlama team who helped get this work started.\n\u2022Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the figures in the\npaper.\n\u2022Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\ninternal demo.\n\u2022Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\nLaurens van der Maaten, Jason Weston, and Omer Levy.\nA.2 Additional Details for Pretraining\nA.2.1 Architecture Changes Compared to Llama 1\nContext Length. We expand the context window for Llama 2 from 2048 tokens to 4096 tokens. The longer\ncontext window enables models to process more information, which is particularly useful for supporting\nlonger histories in chat applications, various summarization tasks, and understanding longer documents.\nTable 16 compares the performance of 2k and 4k context pretraining on long-context benchmarks. Both", "has highlighted that the fine-tuning approach tends to be\ninherently slow when adapting LLMs for long texts [240].\n\u2022Position interpolation. This method downscales the po-\nsition indices within the original context window, to avoid\nout-of-distribution rotation angles during pre-training [240,\n289]. To be more specific, this approach multiplies all posi-\ntion indices by a coefficient L/L\u2032(L < L\u2032), where Land\nL\u2032represent the original and target context window length,\nrespectively. Experimental results [240] have shown that\nthis method can extend the context window effectively and\nefficiently, compared to the above approach of direct model\nfine-tuning. However, it is worth noting that this technique\nmay have an adverse impact on the model\u2019s performance\nwhen handling shorter texts[240, 290].\n\u2022Position truncation. To mitigate the challenges posed\nby out-of-distribution rotation angles, another practical ap-\nproach is to truncate longer relative positions to satisfy the", "ter at using input context. When the input con-\ntext fits in the context window of both a model\nand its extended-context counterpart, we see that\nperformance between them is nearly identical. For\nexample, the 10- and 20-document settings both\nfit in the context window of GPT-3.5-Turbo and\nGPT-3.5-Turbo (16K), and we observe that their\nperformance as a function of position of relative\ninformation is nearly superimposed (solid purple\nand dashed brown series in Figure 5). These results"], "retrieved_docs_id": ["6080afb1ff", "15d2848c92", "15d2848c92", "17615db54d", "dbbb26b25b"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the focus of the analysis in the given context?\n", "true_answer": "The focus of the analysis in the given context is the unique origins of hallucinations in modern large-scale language models (MLLMs), spanning a spectrum of contributing factors from data, model, training, to the inference stage.", "source_doc": "hallucination.pdf", "source_id": "6da15b5bb7", "retrieved_docs": ["Published in Transactions on Machine Learning Research (08/2023)\nIn this work, we focus on the task of predicting the short answer(s) for a given question in both closed- and\nopen-book settings. For the latter setting, we provide the model with the corresponding long answer, rather\nthan the entire Wikipedia page, as the context due to restrictions posed by models\u2019 context windows. Thus,\ninputs to the model will be an optional passage (only in open-book) followed by a question, and the target\noutput will be the annotated short answer(s). Performance for this scenario is measured as the best F1-score\nof the model output with respect to the correct (short) answers.\nData. Kwiatkowski et al. (2019) created the dataset by obtaining questions based on frequent Google\nsearch queries with a Wikipedia page in the top-5 search results. They then relied on annotators to: (i)\nfilter unambiguous factual questions, (ii) provide a long answer : an HTML bounding box (if any) within", "inate ambiguity in entities and terms, along with\neliminating duplicate or redundant information to\nsimplify the retriever\u2019s focus. Ensuring factual ac-\ncuracy is crucial, and whenever possible, the accu-\nracy of each piece of data should be verified. Con-\ntext retention, to adapt to the system\u2019s interaction\ncontext in the real world, can be achieved by adding\nanother layer of context with domain-specific anno-\ntations, coupled with continuous updates through\nuser feedback loops. Time sensitivity is essential\ncontextual information, and mechanisms should be\ndesigned to refresh outdated documents. In sum-\nmary, the focus of optimizing indexed data should\nbe on clarity, context, and correctness to make the\nsystem efficient and reliable. The following intro-\nduces best practices.\n2.Optimizing Index Structures: This can be\nachieved by adjusting the size of the chunks, alter-\ning the index paths, and incorporating graph struc-\nture information. The method of adjusting chunks", "images given the categories\u2019 descriptions. Table 11 presents the data samples. The \ufb01rst group is\nfrom [ WBW+11], while the other two groups are collected from the website. Each category contains\ntwenty images.\nThe evaluation procedure is illustrated in Figure 6. For the zero-shot setting, we provide detailed\ndescriptions of two speci\ufb01c categories and use the template \u201cQuestion:what is the name of {general\ncategory} in the picture? Answer:\u201d to prompt the model for the speci\ufb01c category name in an open-\nended manner. To evaluate the effect of providing verbal descriptions in context, we also implement\na zero-shot baseline without prompting descriptions. Instead, we provide the corresponding speci\ufb01c\nnames in the prompt.\n4.7.2 Results\nThe evaluation results are shown in Table 12. We observe that providing descriptions in context can\nsigni\ufb01cantly improve the accuracy of image classi\ufb01cation. The consistent improvements indicate", "designed for in-context learning nor to follow instructions in prompts. As one concrete recommendation as\nthe language modeling space matures, we recommend model developers explicitly declare how their models\nshould be evaluated and what the scope is of their generality/when models should be preferred. We believe\nthis disclosure will help to strike a productive balance between the general-purpose possibilities for language\nmodels in the abstract and what is concretely sensible for a given model.\n7 Adaptation via prompting\nAdaptation transforms a raw language model into a system that makes predictions on test instances. In\nshort, we use prompting as our adaptation method with 5 in-context examples (when in-context examples\nare included) as depicted in Figure 23. However, many lower-level details must be specified to fully define the\nadaptation procedure: we discuss important conceptual details here and defer the remainder to Appendix J.", "should foreground where what is implemented falls short of the ambition of evaluating models in their\ntotality. Our benchmark foregrounds the limitations of our current benchmark by design: our benchmark is\na subset of a pre-specified taxonomy. That is, the difference between what is in the taxonomy and what is\nin the benchmark identifies what we currently miss.\nWhile it is useful to articulate what is missing, given the immense scale of the use cases and desiderata we\ncould have for language models, we believe it is necessary to have clear priorities on how to navigate the\nspaceofwhatwelack. Theseprioritiesaredeeplysubjective: manycompellingargumentscanbepresentedto\nincrease focus on any specific region of the design space for language models and language model evaluation.\nIndeed, this is also made clear through the diversity of different works happening in parallel throughout the\nAI community to evaluate language models."], "retrieved_docs_id": ["142d8a0bdb", "d2f0150bc9", "69f5efa988", "e1cd78ab51", "48bfb93875"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "How does iterative retrieval improve the robustness of answer generation in large language models?\n", "true_answer": "Iterative retrieval improves robustness by regularly collecting documents based on the original query and generated text, providing additional materials for large language models. The robustness is further enhanced by providing additional references in multiple iterative retrievals.", "source_doc": "RAG.pdf", "source_id": "f24827ee1d", "retrieved_docs": ["dle\u201d phenomenon [Liuet al. , 2023 ]. This redundant informa-\ntion can obscure key information or contain information con-\ntrary to the real answer, negatively impacting the generation\neffect [Yoran et al. , 2023 ]. Additionally, the information ob-\ntained from a single retrieval is limited in problems requiring\nmulti-step reasoning.\nCurrent methods to optimize the retrieval process mainly\ninclude iterative retrieval and adaptive retrieval. These allow\nthe model to iterate multiple times during the retrieval process\nor adaptively adjust the retrieval process to better accommo-\ndate different tasks and scenarios.\nIterative Retrieval\nRegularly collecting documents based on the original query\nand generated text can provide additional materials for\nLLMs [Borgeaud et al. , 2022, Arora et al. , 2023 ]. Providing\nadditional references in multiple iterative retrievals has im-\nproved the robustness of subsequent answer generation.\nHowever, this method may be semantically discontinuous and", "Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "vant knowledge without altering the parameters of LLMs, en-\nabling the model to perform more sophisticated tasks. Addi-\ntionally, CREA-ICL [Liet al. , 2023b ]leverages synchronous\nretrieval of cross-lingual knowledge to assist in acquiring ad-\nditional information, while RECITE forms context by sam-\npling one or more paragraphs from LLMs.\nDuring the inference phase, optimizing the process of RAG\ncan benefit adaptation to more challenging tasks. For ex-ample, ITRG [Feng et al. , 2023a ]enhances adaptability for\ntasks requiring multiple-step reasoning by iteratively retriev-\ning and searching for the correct reasoning path. ITER-\nRETGEN [Shao et al. , 2023 ]employs an iterative approach\nto coalesce retrieval and generation, achieving an alternating\nprocess of \u201dretrieval-enhanced generation\u201d and \u201dgeneration-\nenhanced retrieval.\u201d\nOn the other hand, IRCOT [Trivedi et al. , 2022 ]merges the\nconcepts of RAG and CoT [Weiet al. , 2022 ], employing al-", "RAG can outperform larger models without RAG (Borgeaud\net al., 2022). Retrieval methods include sparse retrieval\nmethods such as TF-IDF or BM25(Robertson & Zaragoza,\n2009), which analyze word statistic information and find\nmatching documents with a high dimensional sparse vec-\ntor. Dense retrieval methods such as (Karpukhin et al.,\n2020; Izacard et al., 2022a) find matching documents on\nan embedding space generated by a retrieval model pre-\ntrained on a large corpus with or without fine-tuning on a\nretrieval dataset. The retrieval model can be trained stan-\ndalone (Karpukhin et al., 2020; Izacard et al., 2022a; Shi\net al., 2023) or jointly with language models (Izacard et al.,\n2022b; Borgeaud et al., 2022). In addition, it has been shown\nthat off-the-shelf general purpose retrievers can improve a\nbaseline language model significantly without further fine-\ntuning (Ram et al., 2023). RAG is also proposed to perform\ncode generation tasks (Zhou et al., 2023) by retrieving from"], "retrieved_docs_id": ["f24827ee1d", "af911eac69", "6291d3f5de", "1f6c13012c", "392133bc25"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does post-training quantization reduce the precision of ViT models?\n", "true_answer": "Post-training quantization compresses trained ViT models by converting their parameters from high-precision floating-point numbers to lower-precision fixed-point numbers, such as 8-bit integers.", "source_doc": "multimodal.pdf", "source_id": "354a427ccf", "retrieved_docs": ["improved performance for the ViT student model.\n3.4 Quantization\nViT quantization is the process of reducing the precision of numerical representations in ViT models,\ntypically transitioning from floating-point to fixed-point arithmetic [140]. This reduction in preci-\nsion aims to decrease memory usage, computational complexity, and energy consumption while\npreserving model accuracy to an acceptable level. Current research can be mainly categorized into\npost-training quantization, quantization-aware training, and hardware-aware quantization.\nPost-Training Quantization (PTQ) compresses trained ViT models by converting their param-\neters from high-precision floating-point numbers to lower-precision fixed-point numbers, such as\n8-bit integers. For example, Liu et al. [141] introduced a ranking loss method to identify opti-\nmal low-bit quantization intervals for weights and inputs, ensuring the functionality of the attention", "Bit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain\naccuracy throughout quantization. PackQViT [129] mitigates outlier effects during quantization.\nBiViT [128] introduces Softmax-aware Binarization to adjust the binarization process, minimizing\nerrors in binarizing softmax attention values. Xiao et al. [142] integrated a gradient regularization\nscheme to curb weight oscillation during binarization training and introduced an activation shift\nmodule to reduce information distortion in activations. Additionally, BinaryViT [130] integrates\nessential architectural elements from CNNs into a pure ViT framework, enhancing its capabilities.\nHardware-Aware Quantization optimizes the quantization process of neural network models for\nspecific hardware platforms ( e.g., GPUs [131], FPGA [132]). It adjusts precision levels and quan-\ntization strategies to maximize performance and energy efficiency during inference. For example,", "results with 4-bit quantization on both weights and key-\nvalue cache, but not on 4-bit activation quantization, which\nstill needs more exploration.\n5.4.3 Empirical Analysis and Findings\nQuantization has currently become a common technique\nto reduce the memory footprint and latency of LLMs in\ndeployment. In particular, it is important to understand\nwhat level of precision ( e.g., INT8 or INT4) can be applied\nto quantize different parts of LLMs ( e.g., weights or acti-\nvations), while retaining a high accuracy. In this part, we\nfirst summarize the major findings about the quantization of\nLLMs in existing literature, and then present some empirical\nanalysis with quantization experiments.\nImportant Findings from Existing Work . Recently, a very\ncomprehensive evaluation [421] has been conducted about\nthe impact of multiple factors ( e.g., model size and sensi-\ntivity) on the post-training quantization methods. Another\nstudy [422] examines the scaling law of k-bit quantiza-", "produce negative values.\nSummary (QAT). QAT has been shown to work\ndespite the coarse approximation of STE. However, the\nmain disadvantage of QAT is the computational cost of\nre-training the NN model. This re-training may need\nto be performed for several hundred epochs to recover\naccuracy, especially for low-bit precision quantization. If\na quantized model is going to be deployed for an extended\nperiod, and if ef\ufb01ciency and accuracy are especially\nimportant, then this investment in re-training is likely\nto be worth it. However, this is not always the case, as\nsome models have a relatively short lifetime. Next, we\nnext discuss an alternative approach that does not have\nthis overhead.\n2) Post-Training Quantization: An alternative to the\nexpensive QAT method is Post-Training Quantization\n(PTQ) which performs the quantization and the adjust-\nments of the weights, without any \ufb01ne-tuning [ 11,24,40,\n60,61,68,69,89,108,142,148,174,182,223,281].", "SmoothQuant: Accurate and Efficient\nPost-Training Quantization for Large Language Models\nGuangxuan Xiao* 1Ji Lin* 1Mickael Seznec2Hao Wu2Julien Demouth2Song Han1\nhttps://github.com/mit-han-lab/smoothquant\nAbstract\nLarge language models (LLMs) show excel-\nlent performance but are compute- and memory-\nintensive. Quantization can reduce memory and\naccelerate inference. However, existing methods\ncannot maintain accuracy and hardware efficiency\nat the same time. We propose SmoothQuant, a\ntraining-free, accuracy-preserving, and general-\npurpose post-training quantization (PTQ) solution\nto enable 8-bit weight, 8-bit activation (W8A8)\nquantization for LLMs. Based on the fact that\nweights are easy to quantize while activations are\nnot, SmoothQuant smooths the activation outliers\nby offline migrating the quantization difficulty\nfrom activations to weights with a mathematically\nequivalent transformation. SmoothQuant enables\nan INT8 quantization of both weights and activa-"], "retrieved_docs_id": ["354a427ccf", "31efe3044d", "3818d0bc1c", "94a6ffe197", "6e61c230fd"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does REALM model the pre-training process?\n", "true_answer": "REALM models the pre-training process as a retrieve-then-predict process, where the language model pre-trains by predicting masked tokens based on masked sentences, modeling P(x|y).", "source_doc": "RAG.pdf", "source_id": "6240233238", "retrieved_docs": ["As a knowledge-intensive task, RAG employs different tech-\nnical approaches during the language model training\u2019s pre-\ntraining, fine-tuning, and inference stages.\nPre-training Stage\nSince the emergence of pre-trained models, researchers have\ndelved into enhancing the performance of Pre-trained Lan-\nguage Models (PTMs) in open-domain Question Answering\n(QA) through retrieval methods at the pre-training stage. Rec-\nognizing and expanding implicit knowledge in pre-trained\nmodels can be challenging. REALM [Arora et al. , 2023 ]in-\ntroduces a more modular and interpretable knowledge em-\nbedding approach. Following the Masked Language Model\n(MLM) paradigm, REALM models both pre-training and\nfine-tuning as a retrieve-then-predict process, where the lan-\nguage model pre-trains by predicting masked tokens ybased\non masked sentences x, modeling P(x|y).\nRETRO [Borgeaud et al. , 2022 ]leverages retrieval aug-\nmentation for pre-training a self-regressive language model,", "provide enough information about whether the produced answer would contradict.\nRetrieval-augmented language model pre-training\n(REALM) [ 186] inserts retrieved documents\ninto the pre-training examples. While Guu et al.\n[186] designed REALM for extractive tasks\nsuch as question-answering, Lewis et al. [304]\npropose retrieval-augmented generation (RAG), a\nlanguage generation framework using retrievers\nfor knowledge-intensive tasks that humans could\nnot solve without access to an external knowledge\nsource. Yogatama et al. [646] propose the adaptive\nSemiparametric Language Models architecture,\nwhich incorporates the current local context, a\nshort-term memory that caches earlier-computed\nhidden states, and a long-term memory based on a\nkey-value store of (hidden-state, output) tuples. To\nequip a retrieval-augmented LLM with few-shot\nabilities that were before only emergent in LLMs\nwith many more parameters, Izacard et al. [236]\npropose a KL-divergence loss term for retrieval", "language model to generate the response autoregressively.\nIn the following, we will discuss the training, evaluation,\nand key points to develop capable MLLMs.\nTraining Process. The training process of the MLLM in-\ncludes two major stages: vision-language alignment pre-\ntraining and visual instruction tuning.\n\u2022Vision-language alignment pre-training. To develop\nMLLMs, existing work mostly initializes the vision encoder\nand the LLM with pre-trained models [149, 150, 826]. These\nmodels retain excellent vision and language capacities, but\nspan different semantic spaces. Thus, the goal of vision-\nlanguage alignment pre-training ( i.e.,the first-stage training)\nis to align the vision encoder and the LLM through end-to-\nend training on large-scale image-text pairs [827, 828]. How-\never, directly tuning these two models on image-text pairs\nmay cause the degradation of the original representation ca-\npacities. To improve the alignment performance, it is crucial", "Switch Transformers\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm:\nRetrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909 ,\n2020.\nAaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur,\nGreg Ganger, and Phil Gibbons. Pipedream: Fast and e\ufb03cient pipeline parallel dnn\ntraining. arXiv preprint arXiv:1806.03377 , 2018.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems , volume 28, pages 1693\u20131701. Cur-\nran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\nafdec7005cc9f14302cd0474fd0f3c96-Paper.pdf .\nGeo\ufb00rey Hinton, Oriol Vinyals, and Je\ufb00 Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 , 2015.", "corporating a retrieval mechanism using the T5 architecture\n[Raffel et al. , 2020 ]in both the pre-training and fine-tuning\nstages. Prior to pre-training, it initializes the encoder-decoder\nLM backbone with a pre-trained T5, and initializes the dense\nretriever with a pre-trained Contriever. During the pre-\ntraining process, it refreshes the asynchronous index every\n1000 steps.\nCOG [Vaze et al. , 2021 ]is a text generation model that for-\nmalizes its generation process by gradually copying text frag-\nments (such as words or phrases) from an existing collection\nof text. Unlike traditional text generation models that select\nwords sequentially, COG utilizes efficient vector search tools\nto calculate meaningful context representations of text frag-\nments and index them. Consequently, the text generation task\nis decomposed into a series of copy and paste operations,\nwhere at each time step, relevant text fragments are sought\nfrom the text collection instead of selecting from an indepen-"], "retrieved_docs_id": ["6240233238", "f7770d2394", "e838613d76", "bd7abbe880", "bff4917f9e"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does SURGE minimize the semantic similarity between documents and queries?\n", "true_answer": "SURGE minimizes the semantic similarity between documents and queries by updating the generator and retriever during the fine-tuning phase, which leverages relevant background knowledge.", "source_doc": "RAG.pdf", "source_id": "977e0e1405", "retrieved_docs": ["swers given a retrieval-enhanced directive. It updates the gen-\nerator and retriever to minimize the semantic similarity be-\ntween documents and queries, effectively leveraging relevant\nbackground knowledge.\nAdditionally, SUGRE [Kang et al. , 2023 ]introduces the\nconcept of contrastive learning. It conducts end-to-end fine-\ntuning of both retriever and generator, ensuring highly de-\ntailed text generation and retrieved subgraphs. Using a\ncontext-aware subgraph retriever based on Graph Neural Net-\nworks (GNN), SURGE extracts relevant knowledge from a\nknowledge graph corresponding to an ongoing conversation.\nThis ensures the generated responses faithfully reflect the re-\ntrieved knowledge. SURGE employs an invariant yet efficient\ngraph encoder and a graph-text contrastive learning objective\nfor this purpose.\nIn summary, the enhancement methods during the fine-\ntuning phase exhibit several characteristics. Firstly, fine-\ntuning both LLM and retriever allows better adaptation", "formation to the edges of the prompt is a straightfor-\nward idea. This concept has been implemented in frame-\nworks such as LlamaIndex, LangChain, and HayStack\n[Blagojevi, 2023 ]. For instance, Diversity Ranker pri-\noritizes reordering based on document diversity, while\nLostInTheMiddleRanker alternates placing the best doc-\nument at the beginning and end of the context window.\nSimultaneously, addressing the challenge of interpreting\nvector-based simulated searches for semantic similarity,\napproaches like cohereAI rerank [Cohere, 2023 ], bge-\nrerank5, or LongLLMLingua [Jiang et al. , 2023a ]recal-\nculate the semantic similarity between relevant text and\nquery.\n\u2022Prompt Compression Research indicates that noise\nin retrieved documents adversely affects RAG perfor-\nmance. In post-processing, the emphasis lies in com-\npressing irrelevant context, highlighting pivotal para-\ngraphs, and reducing the overall context length. Ap-\nproaches such as Selective Context [Litman et al. , 2020 ]", "This demonstrates the ability of our model to handle long-range contexts effectively.\nSemantic Similarity Semantic similarity (or paraphrase detection) tasks involve predicting whether\ntwo sentences are semantically equivalent or not. The challenges lie in recognizing rephrasing of\nconcepts, understanding negation, and handling syntactic ambiguity. We use three datasets for this\ntask \u2013 the Microsoft Paraphrase corpus (MRPC) [ 14] (collected from news sources), the Quora\nQuestion Pairs (QQP) dataset [ 9], and the Semantic Textual Similarity benchmark (STS-B) [ 6].\nWe obtain state-of-the-art results on two of the three semantic similarity tasks (Table 4) with a 1\npoint absolute gain on STS-B. The performance delta on QQP is signi\ufb01cant, with a 4.2% absolute\nimprovement over Single-task BiLSTM + ELMo + Attn.\nClassi\ufb01cation Finally, we also evaluate on two different text classi\ufb01cation tasks. The Corpus", "different scenarios, including using query engines pro-\nvided by frameworks like LlamaIndex, employing tree\nqueries, utilizing vector queries, or employing the most\nbasic sequential querying of chunks.\u2022HyDE: This approach is grounded on the assumption\nthat the generated answers may be closer in the embed-\nding space than a direct query. Utilizing LLM, HyDE\ngenerates a hypothetical document (answer) in response\nto a query, embeds the document, and employs this em-\nbedding to retrieve real documents similar to the hypo-\nthetical one. In contrast to seeking embedding similarity\nbased on the query, this method emphasizes the embed-\nding similarity from answer to answer. However, it may\nnot consistently yield favorable results, particularly in\ninstances where the language model is unfamiliar with\nthe discussed topic, potentially leading to an increased\ngeneration of error-prone instances.\nModular RAG\nThe modular RAG structure breaks away from the traditional", "tiple downstream tasks, fine-tuning the retriever with two dif-\nferent supervised signals via hard labeling of the dataset and\nthe soft reward derived from LLM.\nThis somewhat improves the semantic representation\nthrough both domain knowledge injection and downstream\ntask fine-tuning. However, the retrievers trained by this ap-\nproach are not intuitively helpful for large language models,\nso some work has been done to supervise the fine-tuning of\nEmbedding models directly through feedback signals from\nthe LLM. (This section will be presented in 4.4)\n4.2 How to Match the Semantic Space of Queries\nand Documents\nIn the RAG application, some retrievers use the same embed-\nding model to encode the query and doc, while others use two\nmodels to separately encode the query and doc. Moreover, the\noriginal query of the user may have problems of poor expres-\nsion and lack of semantic information. Therefore, aligning\nthe semantic space of the user\u2019s query and documents is very"], "retrieved_docs_id": ["977e0e1405", "e2c2dc1d50", "fb28b3acaa", "d96393bb4b", "d5d9951817"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does NoisyQuant reduce quantization error?\n", "true_answer": "NoisyQuant reduces quantization error by adding a fixed Uniform noisy bias to quantized values.", "source_doc": "multimodal.pdf", "source_id": "b9d5e110ca", "retrieved_docs": ["mechanism. They also conducted an analysis to understand the relationship between quantization\nloss in different layers and feature diversity, exploring a mixed-precision quantization approach\nleveraging the nuclear norm of each attention map and output feature. Additionally, PTQ4ViT [121]\nintroduced the twin uniform quantization method to minimize quantization error on activation val-\nues following softmax and GELU functions, incorporating a Hessian-guided metric to enhance cal-\nibration accuracy. APQ-ViT [122] proposed a unified Bottom-elimination Blockwise Calibration\nscheme to optimize the calibration metric, prioritizing crucial quantization errors and designing a\nMatthew-effect Preserving Quantization for Softmax to maintain the power-law character and at-\ntention mechanism functionality. NoisyQuant [123] proposes to add a fixed Uniform noisy bias\nto quantized values, the quantization error is significantly reduced under certain conditions. This", "46\n\u2022Fine-grained quantization . For Transformer models,\nweights and activations are usually represented in the\nform of tensors. A straightforward approach is to use\ncoarse-grained quantization parameters for the whole ten-\nsor ( i.e., per-tensor quantization) [414]. However, it usu-\nally leads to inaccurate reconstruction results. Thus, fine-\ngrained methods are proposed to reduce the quantization\nerror. ZeroQuant [415] adopts a token-wise quantization\napproach with dynamic calibration for compressing acti-\nvations. Whereas for weights (easier to be quantized), it\nuses a group-wise quantization. In practice, a group size\nof 128 [415, 416] is commonly used for model quantization.\n\u2022Balancing the quantization difficulty . Considering that\nweights are easier to be quantized than activations,\nSmoothQuant [414] proposes to migrate the difficulty from\nactivations to weights. Specially, they incorporate a scaling\ntransformation to balance the difficulty between weights", "which increases the model size and complicates the quantization pipeline by necessitating the bi-\nlevel quantization scheme. Second, the sensitivity-based non-uniform quantization in SqueezeLLM\nallows for much smaller (e.g., 0.05%) or even zero sparsity levels to achieve accurate quantization.\nThis is crucial for reducing the model size as well as inference speed since higher sparsity levels\ncan degrade inference latency. By avoiding grouping and utilizing smaller or zero sparsity levels,\nSqueezeLLM achieves accurate and fast quantization while pushing the average bit precision down\nto 3-bit, all while employing a simpler quantization pipeline and implementation.\nAnother concurrent work is AWQ Lin et al. (2023) which improves the weight-only quantization\nscheme for LLMs by introducing scaling factors to reduce the quantization error of a few impor-\ntant weights. However, their approach is also based on the OBS framework, where sensitivity is", "to quantized values, the quantization error is significantly reduced under certain conditions. This\ntechnique successfully modifies heavy-tailed activation distributions to fit a given quantizer.\nQuantization-Aware Training (QAT) integrates quantization into the training cycle. This in-\ntegration is particularly advantageous when scaling down to ultra-low bit precision, such as 4 bits\nor lower, where PTQ struggles with significant performance loss. For example, Quantformer [124]\nleverages entropy information to maintain consistency in self-attention ranks and introduces a dif-\nferentiable search mechanism to optimally group patch feature dimensions, reducing rounding and\nclipping inaccuracies. Q-ViT [126] incorporates a distillation token and Information Rectification\nModule (IRM) to counteract altered distributions in quantized attention modules. TerViT [127] and\nBit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain", "branches. Here, we brie\ufb02y discuss each branch, and we\nrefer the interested readers to [197] for more details.\na) Quantization Error Minimization: The \ufb01rst\nbranch of solutions aims to minimize the quantization\nerror, i.e., the gap between the real values and the\nquantized values [ 19,34,62,103,151,158,164,169,\n178,218,248]. Instead of using a single binary matrix\nto represent real-value weights/activations, HORQ [ 151]\nand ABC-Net [ 158] use a linear combination of multiple\nbinary matrices, i.e., W\u2248\u03b11B1+\u00b7\u00b7\u00b7+\u03b1MBM, to\nreduce the quantization error. Inspired by the fact that\nbinarizing the activations reduces their representational\ncapability for the succeeding convolution block, [ 178]\nand [ 34] show that binarization of wider networks (i.e.,\nnetworks with larger number of \ufb01lters) can achieve a\ngood trade-off between the accuracy and the model size.\nb) Improved Loss function: Another branch of\nworks focuses on the choice of loss function [ 48,98,"], "retrieved_docs_id": ["b9d5e110ca", "37ff247d27", "e881a5cdd2", "900b3dde3f", "4aee61c096"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How is the ChipNeMo chat model aligned with queries in the chip design domain?\n", "true_answer": "The ChipNeMo chat model is aligned with queries in the chip design domain by using a general-purpose chat instruction dataset and a small amount of domain-specific instruction datasets. This alignment is further improved by adding a small amount of task-specific instruction data.", "source_doc": "ChipNemo.pdf", "source_id": "411c489c58", "retrieved_docs": ["We use largely publicly available general-purpose chat in-\nstruction datasets for multi-turn chat together with a small\namount of domain-specific instruction datasets to perform\nalignment on the ChipNeMo foundation model, which pro-\nduces the ChipNeMo chat model. We observe that align-\nment with a general purpose chat instruction dataset is\nadequate to align the ChipNeMo foundation models with\nqueries in the chip design domain. We also added a small\namount of task-specific instruction data, which further im-\nproves the alignment. We trained multiple ChipNeMo foun-\ndation and chat models based on variants of LLaMA2 mod-\nels used as the base foundation model.\nTo improve performance on the engineering assistant chat-\nbot application, we also leverage Retrieval Augmented Gen-\neration (RAG). RAG is an open-book approach for giving\nLLMs precise context for user queries. It retrieves rele-\nvant in-domain knowledge from its data store to augment", "fine-tuning and also masked the attribute labels and trained\non ChipNeMo models for 2 epochs. We refer readers to\nAppendix A.4 for details on the alignment datasets and A.7\non implementations details.\nWe also experimented with DAPT directly on a chat aligned\nmodel, such as the LLaMA2-Chat model. We found that\nDAPT significantly degraded the model\u2019s alignment, mak-\ning the resulting model useless for downstream tasks.\n2.4. Domain-Adapted Retrieval Model\nIt is well known that LLMs can generate inaccurate text,\nso-called hallucination (Ji et al., 2023). Although the phe-\nnomenon is not completely understood, we still must miti-\ngatehallucinations since they are particularly problematic\nin an engineering assistant chatbot context, where accu-\nracy is critical. Our proposal is to leverage the retrieval\naugmented generation (RAG) method. RAG tries to re-trieve relevant passages from a database to be included in\nthe prompt together with the question, which grounds the", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "retrieval model, the fact remains that retrieval still struggles\nwith queries that do not map directly to passages in the\ndocument corpus or require more context not present in\nthe passage. Unfortunately, these queries are also more\nrepresentative of queries that will be asked by engineers in\nreal situations. Combining retrieval with a domain adapted\nlanguage model is one way to address this issue.\n3. Evaluations\nWe evaluate our training methodology and application per-\nformance in this section. We study our 7B, 13B, and 70B\nmodels in the training methodology evaluation, and only our\nChipNeMo-70B model using SteerLM for model alignment\nin the application performance evaluation. For compari-\nson, we also evaluate two baseline chat models: LLaMA2-\n70B-Chat and GPT-4. LLaMA2-70B-Chat is the publicly\nreleased LLaMA2-Chat model trained with RLHF and is\nconsidered to be the state-of-the-art open-source chat model,\nwhile GPT-4 is considered to be the state-of-the-art propri-", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat."], "retrieved_docs_id": ["411c489c58", "aec87069e2", "a6c3d05123", "0fb655a6fb", "273b593026"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How do large language models (LLMs) fail when dealing with real-world production environments?\n", "true_answer": "LLMs may fail to provide accurate answers when the information sought extends beyond the model\u2019s training data or requires the latest data. This limitation poses challenges when deploying generative artificial intelligence in real-world production environments.", "source_doc": "RAG.pdf", "source_id": "b40c0db2f1", "retrieved_docs": ["numerous shortcomings. They often fabricate\nfacts [Zhang et al. , 2023b ]and lack knowledge when\ndealing with specific domains or highly specialized\nqueries [Kandpal et al. , 2023 ]. For instance, when the infor-\nmation sought extends beyond the model\u2019s training data or\nrequires the latest data, LLM may fail to provide accurate\nanswers. This limitation poses challenges when deploying\ngenerative artificial intelligence in real-world production\nenvironments, as blindly using a black-box LLM may not\nsuffice.\nTraditionally, neural networks adapt to specific domains\nor proprietary information by fine-tuning models to param-\neterize knowledge. While this technique yields significant\nresults, it demands substantial computational resources, in-\ncurs high costs, and requires specialized technical expertise,\nmaking it less adaptable to the evolving information land-\nscape. Parametric knowledge and non-parametric knowledge\nplay distinct roles. Parametric knowledge is acquired through", "maintaining user privacy and data security are crucial con-\nsiderations when applying LLMs to real-world scenarios.\n9 C ONCLUSION AND FUTURE DIRECTIONS\nIn this survey, we have reviewed the recent progress of large\nlanguage models (LLMs), and introduced the key concepts,\nfindings, and techniques for understanding and utilizing\nLLMs. We focus on the large-sized models ( i.e.,having a size\nlarger than 10B) while excluding the contents of early pre-\ntrained language models ( e.g., BERT and GPT-2) that have\nbeen well covered in the existing literature. In particular,our survey has discussed four important aspects of LLMs,\ni.e.,pre-training, adaptation, utilization, and evaluation. For\neach aspect, we highlight the techniques or findings that are\nkey to the success of LLMs. Furthermore, we also summa-\nrize the available resources for developing LLMs and dis-\ncuss important implementation guidelines for reproducing\nLLMs. This survey tries to cover the most recent literature", "source ( intrinsic hallucination ) or cannot be verified by the\navailable source ( extrinsic hallucination ), which are illustrated\nby two examples in Figure 17. Hallucination widely occurs\nin existing LLMs, even the most superior LLMs such as\nGPT-4 [46]. Furthermore, existing work shows that LLMs\nencounter difficulties in recognizing the hallucinated con-\ntent in text [602], even the powerful ChatGPT. Additionally,beyond language tasks, a recent study has shown that large\nvision-language models (LVLM) also face challenges with\nhallucination, i.e.,generating objects that are not present in\nthe accompanying images [662]. In essence, LLMs seem\nto \u201cunconsciously\u201d utilize the knowledge in task solving,\nwhich still lack an ability to accurately control the use\nof internal or external knowledge. Hallucinations would\nmislead LLMs to generate undesired outputs and mostly\ndegrade the performance, leading to potential risks when\ndeploying LLMs in real-world applications. To alleviate", "Further, it supports a number of language models such as\nGPT-2 and LLaMA, and also covers several representative\nvision Transformer models ( e.g.,ViT and Swin Transformer).\nAs discussed in Section 5.3.1, there have been a large\nnumber of efficient tuning methods proposed in the existing\nliterature. However, most of these approaches are tested\non small-sized pre-trained language models, instead of the\nLLMs. So far, there still lacks a thorough investigation on\nthe effect of different efficient tuning methods on large-sized\nlanguage models at different settings or tasks.\n5.4 Memory-Efficient Model Adaptation\nDue to the huge number of model parameters, LLMs take a\nsignificant memory footprint for inference, making it very\ncostly to be deployed in real-world applications. In this\nsection, we discuss how to reduce the memory footprint\nof LLMs via a popular model compression approach ( i.e.,\nmodel quantization), so that large-sized LLMs can be used", "also fail to generate reasonable outputs on some inputs; we show some examples of this in Figure 9.\nPerhaps the greatest limitation of our models is that, in most cases, they follow the user\u2019s instruction,\neven if that could lead to harm in the real world. For example, when given a prompt instructing the\nmodels to be maximally biased, InstructGPT generates more toxic outputs than equivalently-sized\nGPT-3 models. We discuss potential mitigations in the following sections.\n5.4 Open questions\nThis work is a \ufb01rst step towards using alignment techniques to \ufb01ne-tune language models to follow a\nwide range of instructions. There are many open questions to explore to further align language model\nbehavior with what people actually want them to do.\nMany methods could be tried to further decrease the models\u2019 propensity to generate toxic, biased,\nor otherwise harmful outputs. For example, one could use an adversarial set-up where labelers \ufb01nd"], "retrieved_docs_id": ["b40c0db2f1", "3ba0afeccb", "328a8f7683", "0df03879ce", "321f70e8a9"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the process of creating an index for a language model?\n", "true_answer": "The process involves data cleaning and extraction, converting file formats into plain text, dividing the text into smaller chunks, encoding the text into vectors through a language model, and requiring high inference speed for the embedding models.", "source_doc": "RAG.pdf", "source_id": "58f1dd1f89", "retrieved_docs": ["is also summarized as a \u201cRetrieve\u201d-\u201cRead\u201d framework\n[Maet al. , 2023a ].\nIndexing\nThe pipeline for obtaining data from the source and building\nan index for it generally occurs in an offline state. Specifi-\ncally, the construction of a data index involves the following\nsteps:1.Data Indexing: This involves cleaning and extracting the\noriginal data, converting different file formats such as PDF,\nHTML, Word, Markdown, etc., into plain text.\n2.Chunking: This involves dividing the loaded text into\nsmaller chunks. This is necessary because language mod-\nels typically have a limit on the amount of context they can\nhandle, so it is necessary to create as small text chunks as\npossible.\n3. Embedding and Creating Index: This is the process of\nencoding text into vectors through a language model. The re-\nsulting vectors will be used in the subsequent retrieval process\nto calculate the similarity between the vector and the problem\nvector.The embedding models require a high inference speed.", "corporating a retrieval mechanism using the T5 architecture\n[Raffel et al. , 2020 ]in both the pre-training and fine-tuning\nstages. Prior to pre-training, it initializes the encoder-decoder\nLM backbone with a pre-trained T5, and initializes the dense\nretriever with a pre-trained Contriever. During the pre-\ntraining process, it refreshes the asynchronous index every\n1000 steps.\nCOG [Vaze et al. , 2021 ]is a text generation model that for-\nmalizes its generation process by gradually copying text frag-\nments (such as words or phrases) from an existing collection\nof text. Unlike traditional text generation models that select\nwords sequentially, COG utilizes efficient vector search tools\nto calculate meaningful context representations of text frag-\nments and index them. Consequently, the text generation task\nis decomposed into a series of copy and paste operations,\nwhere at each time step, relevant text fragments are sought\nfrom the text collection instead of selecting from an indepen-", "put forward various methods to optimize the retrieval process.\nIn terms of specific implementation, Advanced RAG can be\nadjusted either through a pipeline or in an end-to-end manner.\nPre-Retrieval Process\n\u2022Optimizing Data Indexing\nThe purpose of optimizing data indexing is to enhance\nthe quality of indexed content. Currently, there are five\nmain strategies employed for this purpose: increasing\nthe granularity of indexed data, optimizing index struc-\ntures, adding metadata, alignment optimization, and\nmixed retrieval.\n1.Enhancing Data Granularity: The objective of\npre-index optimization is to improve text standard-\nization, consistency, and ensure factual accuracy\nand contextual richness to guarantee the perfor-\nmance of the RAG system. Text standardization in-\nvolves removing irrelevant information and special\ncharacters to enhance the efficiency of the retriever.\nIn terms of consistency, the primary task is to elim-\ninate ambiguity in entities and terms, along with", "and generating appropriate responses, Claude\noutperforms GPT-3 (Brown et al., 2020b) +10%,\n+20%, -20%, and +10%. respectively.\n4.8 WizardLM\nWizardLM (7B) (Xu et al., 2023a) is a\nlanguage model trained by fine-tuning LLaMA\n(7B) (Touvron et al., 2023a) on the instruction\ndataset Evol-Instruct generated by ChatGPT\n(details see Section 3.7). It is fine-tuned on a\nsubset (with 70K) of Evol-Instruct to enable a fair\ncomparison with Vicuna (Chiang et al., 2023). The\nfine-tuning process takes approximately 70 hours\non 3 epochs based on an 8 V100 GPU with the\nDeepspeed Zero-3 (Rasley et al., 2020) technique.\nDuring inference, the max generation length is\n2048.\nTo evaluate LLMs\u2019 performances on complex\ninstructions, the authors collected 218 human-\n5https://www.anthropic.com/index/introducing-claude", "Published in Transactions on Machine Learning Research (08/2023)\nThis approach is also the dominant approach to language model evaluation,79often with even broader\ncollections: Brown et al. (2020) popularized the approach in their work on GPT-3, where they evaluated on\n42 datasets. Indeed, this is the approach used in all the works that introduced models we evaluate in this\nwork. Efforts like the EleutherAI Language Model Evaluation Harness (Gao et al., 2021b), HuggingFace\u2019s\nEvaluate library (von Werra et al., 2022), and Big-Bench (Srivastava et al., 2022) have centralized and\nexpanded these evaluations into systematic repositories.\nSituated against this landscape, what differentiates our work is our holistic approach, which manifests in both\nour benchmark design process and our concrete benchmark. HELM is the byproduct of an explicit two-step\nprocess: we taxonomize the space for language model evaluation, structured around use cases (scenarios)"], "retrieved_docs_id": ["58f1dd1f89", "bff4917f9e", "8a71abd00a", "129448768e", "6e018423c0"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the ChipNeMo-70B model perform compared to GPT-4 in engineering assistant chatbot and EDA scripts generation?\n", "true_answer": "The ChipNeMo-70B model outperforms the GPT-4 on two use cases, namely engineering assistant chatbot and EDA scripts generation.", "source_doc": "ChipNemo.pdf", "source_id": "a6c3d05123", "retrieved_docs": ["processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "results on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks\nwhere we check for fully accurate code. For \u201cHard\u201d bench-\nmarks in Figure 9 we check for partial correctness of the\ncode, which is evaluated by a human user on a 0-10 scale.\nChipNeMo-70B-Steer performs significantly better than off-\nthe-shelf GPT-4 and LLaMA2-70B-Chat model.\nFigure 8: EDA Script Generation Evaluation Results, Pass@5\nAs seen in Figure 8, models like GPT-4 and LLaMA2-70B-\nChat have close to zero accuracy for the Python tool where\nthe domain knowledge related to APIs of the tool are neces-\nsary. This shows the importance of DAPT. Without DAPT,\nthe model had little to no understanding of the underlying\nAPIs and performed poorly on both automatic and human\nevaluated benchmarks. Our aligned model further improved\nthe results of DAPT because our domain instructional data\nhelps guide the model to present the final script in the most\nuseful manner. An ablation study on inclusion of domain", "uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2"], "retrieved_docs_id": ["e6b9ba907a", "a6c3d05123", "af6e8c3fb2", "cf9d13203d", "c7d05c4b43"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "How is LLM performance used in reinforcement learning for a rewriter module?\n", "true_answer": "LLM performance is used as a reward in reinforcement learning for a rewriter module, allowing the rewriter to adjust retrieval queries and improve the downstream task performance of the reader.", "source_doc": "RAG.pdf", "source_id": "79cd640612", "retrieved_docs": ["Retrieve-Read process, utilizing LLM performance as a\nreward in reinforcement learning for a rewritter module.\nThis allows the rewritter to adjust retrieval queries, im-\nproving the downstream task performance of the reader.\nSimilarly, modules can be selectively replaced in ap-\nproaches like Generate-Read [Yuet al. , 2022 ], where the\nLLM generation module replaces the retrieval module.", "igate alignment issues. PRCA [Yang et al. , 2023b ]lever-\naged reinforcement learning to train a context adapter\ndriven by LLM rewards, positioned between the re-\ntriever and generator. It optimizes the retrieved in-\nformation by maximizing rewards in the reinforcement\nlearning phase within the labeled autoregressive pol-\nicy. AAR [Yuet al. , 2023b ]proposed a universal plu-\ngin that learns LM preferences from known-source\nLLMs to assist unknown or non-co-finetuned LLMs.\nRRR [Maet al. , 2023a ]designed a module for rewriting\nqueries based on reinforcement learning to align queries\nwith documents in the corpus.\n\u2022Validation Module: In real-world scenarios, it is notalways guaranteed that the retrieved information is reli-\nable. Retrieving irrelevant data may lead to the occur-\nrence of illusions in LLM. Therefore, an additional val-\nidation module can be introduced after retrieving docu-\nments to assess the relevance between the retrieved doc-", "propose the Automatic Prompt Engineer (APE)\nmethod, which leverages LLMs to generate, score,\nand rephrase instruction-following zero- and few-\nshot prompts. Longpre et al. [340] describe and an-\nalyze the steps taken to create an improved version\nof the Flan collection [ 598] used to train FLAN-\nPaLM [ 93]. When trained on this data, the authors\nfind that the improved model performance stems\nfrom more diverse tasks by inverting input-output\npairs and data augmentation techniques such as\nmixing zero-shot and few-shot prompts. Honovich\net al. [209] generate a large dataset of natural lan-\nguage instructions using a pre-trained LLM to gen-\nerate and then rephrase instructions. They show\nthat a T5 (\"LM-adapted\") fine-tuned on this data\noutperforms other instruction fine-tuned T5 models\nsuch as T0++ [475] and Tk-Instruct [589].\nReinforcement Learning From Human Feed-\nback (RLHF) is a variation of RL that incor-\nporates feedback from humans in the form of re-", "2018; Christiano, 2022; Uesato et al., 2022).6. Human performance on a task isn\u2019t an\nupper bound on LLM performance\nWhile LLMs are trained primarily to imitate human writing\nbehavior, they can at least potentially outperform humans on\nmany tasks. This is for two reasons: First, they are trained\non far more data than any human sees,4giving them much\nmore information to memorize and potentially synthesize.\nIn addition, they are often given additional training using\nreinforcement learning before being deployed (Stiennon\net al., 2020; Ouyang et al., 2022; Bai et al., 2022a), which\ntrains them to produce responses that humans \ufb01nd helpful\nwithout requiring humans to demonstrate such helpful be-\nhavior. This is analogous to the techniques used to produce\nsuperhuman performance at games like Go (Silver et al.,\n2016). Concretely, LLMs appear to be much better than\nhumans at their pretraining task of predicting which word is\nmost likely to appear after some seed piece of text (Shlegeris", "Bradley-Terry model [ 5], then fine-tune a language model to maximize the given reward using\nreinforcement learning algorithms, commonly REINFORCE [ 45], proximal policy optimization\n(PPO; [ 37]), or variants [ 32]. A closely-related line of work leverages LLMs fine-tuned for instruction\nfollowing with human feedback to generate additional synthetic preference data for targeted attributes\nsuch as safety or harmlessness [ 2], using only weak supervision from humans in the form of a\ntext rubric for the LLM\u2019s annotations. These methods represent a convergence of two bodies of\nwork: one body of work on training language models with reinforcement learning for a variety\nof objectives [ 33,27,46] and another body of work on general methods for learning from human\npreferences [ 12,19]. Despite the appeal of using relative human preferences, fine-tuning large\nlanguage models with reinforcement learning remains a major practical challenge; this work provides"], "retrieved_docs_id": ["79cd640612", "8d0a82337c", "65665a1a99", "0384513a85", "630e695c8e"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does IRCoT use retrieval in its thought chain?\n", "true_answer": "IRCoT retrieves documents for each generated sentence, introducing retrieval at every step of the thought chain.", "source_doc": "RAG.pdf", "source_id": "89c7fd1852", "retrieved_docs": ["ITER-RETGEN [Shao et al. , 2023 ]collaboratively utilizes\n\u201dretrieval-enhanced generation\u201d and \u201dgeneration-enhanced\nretrieval\u201d for tasks requiring reproduction of information.\nThat is, the model uses the content needed to complete the\ntask to respond to the input task, and these target contents\nserve as the information context for retrieving more relevant\nknowledge. This helps to generate better responses in another\niteration.\nIRCoT [Trivedi et al. , 2022 ]also explores retrieving docu-\nments for each generated sentence, introducing retrieval at\nevery step of the thought chain. It uses CoT to guide the re-\ntrieval and uses the retrieval results to improve CoT, ensuring\nsemantic completeness.\nAdaptive Retrieval\nIndeed, the RAG methods described in the previous two\nsections follow a passive approach where retrieval is prior-", "vant knowledge without altering the parameters of LLMs, en-\nabling the model to perform more sophisticated tasks. Addi-\ntionally, CREA-ICL [Liet al. , 2023b ]leverages synchronous\nretrieval of cross-lingual knowledge to assist in acquiring ad-\nditional information, while RECITE forms context by sam-\npling one or more paragraphs from LLMs.\nDuring the inference phase, optimizing the process of RAG\ncan benefit adaptation to more challenging tasks. For ex-ample, ITRG [Feng et al. , 2023a ]enhances adaptability for\ntasks requiring multiple-step reasoning by iteratively retriev-\ning and searching for the correct reasoning path. ITER-\nRETGEN [Shao et al. , 2023 ]employs an iterative approach\nto coalesce retrieval and generation, achieving an alternating\nprocess of \u201dretrieval-enhanced generation\u201d and \u201dgeneration-\nenhanced retrieval.\u201d\nOn the other hand, IRCOT [Trivedi et al. , 2022 ]merges the\nconcepts of RAG and CoT [Weiet al. , 2022 ], employing al-", "et al., 2021; Wei et al., 2022b). In this work, we refer to this technique as chain-of-thought prompting . In\nthe few-shot setting, these intermediate reasoning steps are manually written for the few-shot exemplars,\nand the model will then generate its own chain-of-thoughts for the test examples. Only the \ufb01nal answer is\nused for evaluation, although these generated chain-of-thoughts can be useful for error analysis and model\ninterpretability. An example of chain-of-thought prompting for the GSM8K benchmark (grade-school math\nproblems) is given below in Figure 8.\n20", "then to tackle complex question-answering and reasoning tasks.\nimprove the performance in complex tasks. Motivated by chain-of-thought prompting, we investigate\na multimodal chain-of-thought prompting using KOSMOS -1. As illustrated in Figure 5, we break\ndown perception-language tasks into two steps. In the \ufb01rst stage, given an image, we use a prompt to\nguide the model to generate a rationale. The model is then fed the rationale and a task-aware prompt\nto produce the \ufb01nal results.\n4.5.1 Evaluation Setup\nWe evaluate the ability of multimodal chain-of-thought prompting on the Rendered SST-2. We use the\nprompt \u201cIntroduce this picture in detail:\u201d to generate the content in the picture as the rationale. Then,\nwe use the prompt \u201c{rationale} Question: what is the sentiment of the opinion? Answer: {answer}\u201d\nto predict the sentiment, where the answer is either positive or negative.\n4.5.2 Results\nWe conduct experiments to evaluate the performance of the multimodal chain-of-thought prompting.", "We conduct experiments to evaluate the performance of the multimodal chain-of-thought prompting.\nTable 9 shows that multimodal chain-of-thought prompting achieves a score of 72.9, which is 5.8\npoints higher than the standard prompting. By generating intermediate content, the model can\nrecognize the text in the images and infer the sentiment of the sentences more correctly.\n4.6 Zero-Shot Image Classi\ufb01cation\nWe report the zero-shot image classi\ufb01cation performance on ImageNet [ DDS+09]. Image classi\ufb01ca-\ntion comprehends an entire image as a whole and aims to assign a label to the image. We map each\nlabel to its category name in natural language. The model is prompted to predict the category name\nto perform zero-shot image classi\ufb01cation.\n13"], "retrieved_docs_id": ["89c7fd1852", "1f6c13012c", "018924ed41", "36a0a3a766", "0153529eb5"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How is the IT stage typically conducted in the context of Multi-Task Language Model Learning?\n", "true_answer": "The IT stage is typically conducted within the paradigm of Supervised Fine-Tuning (SFT).", "source_doc": "multimodal.pdf", "source_id": "0806fe2e1a", "retrieved_docs": ["multi-task \ufb01ne-tuning rather than for in-context learning without weight updates.\nAnother approach to increasing generality and transfer-learning capability in language models is multi-task learning\n[Car97 ], which \ufb01ne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for\neach one. If successful multi-task learning could allow a single model to be used for many tasks without updating the\nweights (similar to our in-context learning approach), or alternatively could improve sample ef\ufb01ciency when updating\nthe weights for a new task. Multi-task learning has shown some promising initial results [ LGH+15,LSP+18] and\nmulti-stage \ufb01ne-tuning has recently become a standardized part of SOTA results on some datasets [ PFB18 ] and pushed\nthe boundaries on certain tasks [ KKS+20], but is still limited by the need to manually curate collections of datasets and", "Exploring the Limits of Transfer Learning\nuni\ufb01ed text-to-text framework, \u201cmulti-task learning\u201d simply corresponds to mixing data sets\ntogether. It follows that we can still train on unlabeled data when using multi-task learning\nby treating the unsupervised task as one of the tasks being mixed together. In contrast,\nmost applications of multi-task learning to NLP add task-speci\ufb01c classi\ufb01cation networks or\nuse di\ufb00erent loss functions for each task (Liu et al., 2019b).\nAs pointed out by Arivazhagan et al. (2019), an extremely important factor in multi-task\nlearning is how much data from each task the model should be trained on. Our goal is to not\nunder- or over-train the model\u2014that is, we want the model to see enough data from a given\ntask that it can perform the task well, but not to see so much data that it memorizes the\ntraining set. How exactly to set the proportion of data coming from each task can depend on", "representative models that possess such an ability8.\n\u2022In-context learning. The in-context learning (ICL) ability\nis formally introduced by GPT-3 [55]: assuming that the\nlanguage model has been provided with a natural language\ninstruction and/or several task demonstrations, it can gen-\nerate the expected output for the test instances by com-\npleting the word sequence of input text, without requiring\nadditional training or gradient update9. Among the GPT-\nseries models, the 175B GPT-3 model exhibited a strong ICL\nability in general, but not the GPT-1 and GPT-2 models. Such\nan ability also depends on the specific downstream task. For\nexample, the ICL ability can emerge on the arithmetic tasks\n(e.g., the 3-digit addition and subtraction) for the 13B GPT-3,\nbut 175B GPT-3 even cannot work well on the Persian QA\ntask [31].\n\u2022Instruction following. By fine-tuning with a mixture of\nmulti-task datasets formatted via natural language descrip-", "8.2 Low-resource Instruction Tuning\nGupta et al. (2023) attempts to estimate the minimal\ndownstream training data required by IT models to\nmatch the SOTA supervised models over various\ntasks. Gupta et al. (2023) conducted experiments\non 119 tasks from Super Natural Instructions\n(SuperNI) in both single-task learning (STL) and\nmulti-task learning (MTL) settings. The results\nindicate that in the STL setting, IT models with\nonly 25% of downstream training data outperform\nthe SOTA models on those tasks, while in the MTL\nsetting, just 6% of downstream training data can\nlead IT models to achieve the SOTA performance.\nThese findings suggest that instruction tuning can\neffectively assist a model in quickly learning a task\neven with limited data.\nHowever, due to resource limitations, Gupta\net al. (2023) did not conduct experiments on\nLLMs, like T5-11B. So, to gain a more\ncomprehensive understanding of the IT models,\nfurther investigation using larger language models\nand datasets is necessary.", "their performance on the target task. The benefits of IT in efficient MLLMs are manifold. Firstly,\nit enables the model to adapt to a wide range of tasks with minimal changes to its architecture\nor training data. This makes it a flexible and efficient approach for fine-tuning on diverse tasks.\nSecondly, IT allows for better generalization, as the model learns to follow instructions and apply\nits knowledge to new and unseen tasks.\nThe IT stage is typically conducted within the paradigm of Supervised Fine-Tuning (SFT). SFT\ndatasets are often derived from a portion of the pre-training data, which is transformed into an\ninstruction-based format, presented in the form of single-turn or multi-turn dialogue structures.\nGiven an image Xvand its caption, a conversation data (X1\nq, X1\na, . . . , XT\nq, XT\na)can be generated,\nwhere T is the total number of turns. Typically, we can organize the data into a sequence of instruc-\ntions and responses following [7], where the instruction Xt"], "retrieved_docs_id": ["4e6d19e6c4", "73fb6ecc71", "731799f8cf", "d6c1ab999f", "0806fe2e1a"], "reranker_type": "colbert", "search_type": "text", "rr": 0.2, "hit": 1}, {"question": "What is the computational complexity of State Space Models (SSMs) during inference?\n", "true_answer": "State Space Models (SSMs) offer near-linear computational complexity during inference.", "source_doc": "multimodal.pdf", "source_id": "bb2e9ee3f0", "retrieved_docs": ["Figure 13: The elements(left) block(middle) and architecture(right) in RWKV [151].\nThis approach parallelizes computations during training and maintains constant computational and\nmemory complexity during inference.\nState Space Models (SSMs) [152] can be formulated as a type of RNN for efficient autoregressive\ninference and have emerged as a promising alternative to attention mechanisms, offering near-linear\ncomputational complexity compared to the quadratic complexity of attention. SSMs are formulated\nas x\u2019(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), mapping a single-dimension input signal u(t) to an N-\ndimension latent state x(t) before projecting it to a single-dimension output signal y(t), with A, B, C,\nand D being parameters learned by gradient descent [152]. Several techniques have been proposed\nto enhance SSMs, such as the Structured State Space sequence model (S4) [152], which refines\nSSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)", "sequence length requires /u1D442./u1D435/u1D43F/u1D437/u1D441 )time and memory; this is the root of the fundamental e\ufb03ciency bottleneck\naddressed in Section 3.3.\nGeneral State Space Models. We note that the term state space model has a very broad meaning which simply\nrepresents the notion of any recurrent process with a latent state. It has been used to refer to many disparate\nconcepts in di\ufb00erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner\net al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)),\nKalman \ufb01lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS)\n(machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4", "SSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)\nmodel [153], which proposes fully diagonal parameterization of state spaces for greater efficiency.\nH3 stacks two SSMs to interact with their output and input projection, bridging the gap between\nSSMs and attention while adapting to modern hardware. Mamba [77], a selective state space model,\nhas been introduced as a strong competitor to the Transformer architecture in large language models.\nMamba incorporates a selection mechanism to eliminate irrelevant data and develops a hardware-\naware parallel algorithm for recurrent operation. This results in competitive performance compared\nto LLMs of the same capacity, with faster inference speeds that scale linearly with time and con-\nstant memory usage. In conclusion, State Space Models offer significant potential as an alternative\nto attention mechanisms by providing near-linear computational complexity and effectively captur-", "of functions \u03b3\u03b8is a design choice with a signi\ufb01cant impact on the expressivity and computational complexity\nof the layer.\nOne choice of implicit parametrization is to select has the response function of a linear state-space model\n(SSM) (Chen, 1984), described by the \ufb01rst-order di\ufb00erence equation:\nxt+1=Axt+Butstate equation\nyt=Cxt+Dutoutput equation\nHere, the convenient choice of x0= 0renders the input-output map to a simple convolution\nyt=t\u2211\nn=0(\nCAt\u2212nB+D\u03b4t\u2212n)\nun\nwhere\u03b4tdenotes the Kronecker delta. We can then identify the \ufb01lter has\nt\u21a6\u2192ht={\n0 t<0\nCAtB+D\u03b4tt\u22650\nwhere the entries of A,B,Cand Dare the learned parameters of the \ufb01lter. In terms of layer design, the\ndegrees of freedom of SSMs are the dimension of the state and the structure of the matrices. SSMs are\na canonical example of how long convolutions with sub-linear parameter counts can improve deep learning\nmodels for long sequences (Gu et al., 2020, 2021). Other implicit approaches include parametrizing \ufb01lters", "linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware\n(up to 3 \u00d7faster on A100 GPUs).\nArchitecture. We simplify prior deep sequence model architectures by combining the design of prior SSM\narchitectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a\nsimple and homogenous architecture design (Mamba) incorporating selective state spaces.\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that\nmake them suitable as the backbone of general foundation models operating on sequences. (i)High quality:\nselectivity brings strong performance on dense modalities such as language and genomics. (ii)Fast training and\ninference: computation and memory scales linearly in sequence length during training, and unrolling the model\nautoregressively during inference requires only constant time per step since it does not require a cache of previous"], "retrieved_docs_id": ["bb2e9ee3f0", "79e095312d", "85b5cac71b", "4afd597e85", "5a07490119"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How did the domain-adapted retrieval model perform compared to Sentence Transformer and e5small unsupervised in the Specs category?\n", "true_answer": "The domain-adapted model performed 2x better than the original e5small unsupervised model and 30% better than the sentence transformer in the Specs category.", "source_doc": "ChipNemo.pdf", "source_id": "79a9ff88c8", "retrieved_docs": ["retrieval. It includes about 1.8K documents, which were\nsegmented into 67K passages, each about 512 characters.\nFirst, we compare our domain adapted retrieval model with\nSentence Transformer (Reimers & Gurevych, 2019) and\ne5small unsupervised (Wang et al., 2022) on each category.\nEach model fetches its top 8 passages from the data store.\nAs shown in Figure 6, our domain-adapted model performed\n2x better than the original e5small unsupervised model and\n30% better than sentence transformer.\nFigure 6: Retrieval Model Accuracy Comparison\nThe queries in the Specs category are derived directly from\npassages in the documents, so their answers are often nicely\ncontained in a concise passage and clearly address the query.\nOn the other hand, the queries of the Testbench and Build\ncategories are not directly derived from passages, so their\nanswers were often not as apparent in the fetched passages\nand required more context (see Appendix A.8 for detailed", "the prompt together with the question, which grounds the\nLLM to produce more accurate answers. We find that using\na domain adapted language model for RAG significantly\nimproves answer quality on our domain specific questions.\nAlso, we find that fine-tuning an off-the-shelf unsupervised\npre-trained dense retrieval model with a modest amount\nof domain specific training data significantly improves re-\ntrieval accuracy. Our domain-adapted RAG implementation\ndiagram is illustrated on Figure 3.\nFigure 3: RAG Implementation Variations\nWe created our domain adapted retrieval model by fine-\ntuning the e5small unsupervised model (Wang et al., 2022)\nwith 3000 domain specific auto-generated samples using the\nTevatron framework (Gao et al., 2022). We refer readers to\nthe details on the sample generation and training process in\nAppendix A.8.\nEven with the significant gains that come with fine-tuning a\nretrieval model, the fact remains that retrieval still struggles", "vant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation", "the potential of combining context from an in-\nformation retrieval system with a pretrained lan-\nguage models for unsupervised question answering.\nO\u2019Connor and Andreas (2021) found that many\ninformation-destroying operations had marginal ef-\nfects on Transformer LMs\u2019 predictions. Krishna\net al. (2022) found that long-context neural gen-\neration in modestly-sized Transformer language\nmodels degenerates because models fail to prop-\nerly condition on long context. Finally, studying\nlong-context models, Sun et al. (2021) found that\nlonger contexts improves prediction of only a few\ntokens, an empirical finding consistent with the\ntheory of Sharan et al. (2018), who showed that\nsequence distributions with bounded mutual infor-\nmation necessarily lead to marginal average predic-\ntion benefits from increasingly long context. Qin\net al. (2023) analyze how efficient Transformers\nperform on a variety of long-context downstream\nNLP tasks, finding that long-context transformers", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications."], "retrieved_docs_id": ["79a9ff88c8", "ad55562468", "28f0897bcb", "cc85f3aa39", "a6c3d05123"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a significant challenge in the practical application of multimodal large language models?\n", "true_answer": "The phenomenon of hallucination is a significant challenge in the practical application of multimodal large language models.", "source_doc": "hallucination.pdf", "source_id": "114f3dada8", "retrieved_docs": ["Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "2.3 Hallucinations in Multimodal Large Language Models\nHallucination of MLLM generally refers to the phenomenon where the generated text response\ndoes not align with the corresponding visual content. State-of-the-art studies in this field primarily\nfocus on object hallucination, given that objects are central to research in computer vision and\nmultimodal contexts. Regarding inconsistency, two typical failure modes are: 1) missing objects,\nand 2) describing objects that are not present in the image or with incorrect statements. Empirically,\nthe second mode has been shown to be less preferable to humans. For example, the LSMDC\nchallenge [ 91] shows that correctness is more important to human judges than specificity. In\ncontrast, the coverage of objects is less perceptible to humans. Thus, object coverage is not a\nprimary focus in studies of object hallucination. Empirically, object hallucination can be categorized"], "retrieved_docs_id": ["114f3dada8", "72dc971633", "542e5c49da", "ac70fcc9f2", "f58cf51d02"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How do large language models (LLMs) decide when to search for relevant queries?\n", "true_answer": "LLMs decide to search for a relevant query when they think it is necessary.", "source_doc": "RAG.pdf", "source_id": "8d605d7952", "retrieved_docs": ["itized. This method, which involves querying related doc-\numents and inputting into a LLM based on context, may\nlead to efficiency issues. Adaptive retrieval methods such\nas those introduced by Flare [Jiang et al. , 2023b ]and Self-\nRAG [Asai et al. , 2023b ], optimize the RAG retrieval process,\nenabling the LLM to actively judge the timing and content of\nretrieval. This helps to improve the efficiency and relevance\nof the information retrieved.\nIn fact, the way in which LLM actively uses tools and\nmakes judgments is not originated from RAG but has been\nwidely used in the agents of large models [Yang et al. , 2023c,\nSchick et al. , 2023, Zhang, 2023 ]. The retrieval steps\nof Graph-Toolformer [Zhang, 2023 ]are roughly divided\ninto: LLMs actively use the retriever, Self-Ask and\nDSP[Khattab et al. , 2022 ]try to use few-shot prompts to trig-\nger LLM search queries. When LLMs think it is necessary,\nthey can decide to search for a relevant query to collect the", "develop more powerful multimedia retrieval systems.\n8.1.3 LLM for Recommender Systems\nUnlike IR systems that analyze user search queries to\nretrieve relevant documents, recommender systems (RS)\naim to capture the underlying user preference and pro-\nvide appropriate information resources to users [798\u2013801].\nTypically, existing studies train a recommendation model\n(either classic or deep learning model) by fitting it over\nthe user\u2019s logged data ( e.g., click data) [745, 802]. However,\nthese models often suffer from a series of technical issues,\ne.g., cold-start recommendation, domain transfer, and poor\nexplainability. Recently, LLMs have demonstrated the po-\ntential to alleviate these issues of recommendation mod-\nels [357, 803, 804], due to the strong capacities of domain\ngeneralization and language generation. In this part, we\nbriefly review the recent progress of LLMs in recommender\nsystems, from the following three aspects, namely LLMs as", "scale of LLMs.\nLLM-Enhanced IR Models. As another major research\ndirection, LLMs can be employed to improve existing IR\nmodels ( e.g., small models). A common challenge faced\nby existing IR models is the lack of relevant judgment\nannotation [786, 787]. To tackle this problem, LLMs can be\ninstructed to annotate positive or negative documents for\na given query [788], or to generate corresponding queries\nbased on a set of documents in the corpus by referring to a\nfew demonstrations [789, 790]. In addition to training data\naugmentation, LLM has the potential to improve existing\nIR models by refining the search-oriented informativeness\nof both queries and documents. In IR systems, the in-\nput queries may be constrained by a user\u2019s cognitive and\ncultural competency, making it challenging to accurately\nexpress the real intent, and irrelevant content present in\ndocuments can also impact the relevance evaluation with\nthe query. As a solution, LLM can be utilized to rewrite the", "they can decide to search for a relevant query to collect the\nnecessary materials, similar to the tool call of the agent.\nWebGPT [Nakano et al. , 2021 ]employs a reinforcement\nlearning framework to automatically train the GPT-3 model\nto use a search engine for text generation. It uses special to-\nkens to perform actions, including querying on a search en-\ngine, scrolling rankings, and citing references. This allows\nGPT-3 to leverage a search engine for text generation.\nFlare [Jiang et al. , 2023b ], on the other hand, automates the\ntiming of retrieval and addresses the cost of periodic docu-\nment retrieval based on the probability of the generated text.\nIt uses probability as an indicator of LLMs\u2019 confidence during\nthe generation process. When the probability of a term falls\nbelow a predefined threshold, the information retrieval sys-\ntem would retrieve references and removes terms with lower\nprobabilities. This approach is designed to handle situations", "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness"], "retrieved_docs_id": ["8d605d7952", "cb4ccdc69b", "1c54994eed", "b844a74991", "6291d3f5de"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is one potential application of large language models in GPU ASIC and Architecture design?\n", "true_answer": "One potential application of large language models in GPU ASIC and Architecture design is as an engineering assistant chatbot that understands internal hardware terminology and can assist engineers with their tasks.", "source_doc": "ChipNemo.pdf", "source_id": "f23b3625e0", "retrieved_docs": ["Bard, etc.) and open-source (Vicuna (Chiang et al., 2023),\nLLaMA2 (Touvron et al., 2023), etc.) large language mod-\nels (LLM) provide an unprecedented opportunity to help\nautomate these language-related chip design tasks. Indeed,\nearly academic research (Thakur et al., 2023; Blocklove\net al., 2023; He et al., 2023) has explored applications of\nLLMs for generating Register Transfer Level (RTL) code\nthat can perform simple tasks in small design modules as\nwell as generating scripts for EDA tools.\nWe believe that LLMs have the potential to help chip de-\nsign productivity by using generative AI to automate many\nlanguage-related chip design tasks such as code generation,\nresponses to engineering questions via a natural language\ninterface, analysis and report generation, and bug triage. In\nthis study, we focus on three specific LLM applications: an\nengineering assistant chatbot for GPU ASIC and Architec-\nture design engineers, which understands internal hardware", "are scaled by \ud835\udc60, the fraction of nonzero blocks in the block-sparsity mask. However, for small values of \ud835\udc60, we\nwould still need to write the result O2R\ud835\udc41\u0002\ud835\udc51. Therefore the number of HBM accesses is\n\u0398\u0012\n\ud835\udc41\ud835\udc51\u00b8\ud835\udc412\ud835\udc512\n\ud835\udc40\ud835\udc60\u0013\n\u0095\n\u0003\nD.2 Potential Extensions\nWe discuss here a few potential extensions of the IO-aware approach to speed up deep learning training.\nMulti-GPU Attention. Large language models are trained on hundreds or thousands of GPUs, and\none typically splits the attention computation between 4-8 GPUs on the same node [ 77]. This introduces\nanother level of memory hierarchy: beside GPU SRAM and GPU HBM, we also have the HBM of other\n25", "Hallucination of Multimodal Large Language Models: A Survey 9\noverride the visual content. For example, given an image showing a red banana, which is\ncounter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as\n\"banana is yellow\" is a deep-rooted knowledge in the LLM. Such language/knowledge prior\nmakes the model overlook the visual content and response with hallucination.\n\u2022Weak alignment interface. The alignment interface plays an essential role in MLLMs, as\nit serves as the bridge between the two modalities. A weak alignment interface can easily\ncause hallucinations. One potential cause of a weak alignment interface is data, as discussed\nin earlier sections. Apart from that, the interface architecture itself and training loss design\nalso matter [ 52,77,123]. Recent work [ 52] argues that the LLaVA-like linear projection\ninterface preserves most of the information, but lacks supervision on the projected feature.", "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU\u2217\nYixin Song, Zeyu Mi\u2020, Haotong Xie and Haibo Chen\nInstitute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University\nAbstract\nThis paper introduces PowerInfer, a high-speed Large Lan-\nguage Model (LLM) inference engine on a personal computer\n(PC) equipped with a single consumer-grade GPU. The key\nunderlying the design of PowerInfer is exploiting the high\nlocality inherent in LLM inference, characterized by a power-\nlaw distribution in neuron activation. This distribution indi-\ncates that a small subset of neurons, termed hot neurons , are\nconsistently activated across inputs, while the majority, cold\nneurons , vary based on speci\ufb01c inputs. PowerInfer exploits\nsuch an insight to design a GPU-CPU hybrid inference en-\ngine: hot-activated neurons are preloaded onto the GPU for\nfast access, while cold-activated neurons are computed on\nthe CPU, thus signi\ufb01cantly reducing GPU memory demands", "system\u2019s overall architecture. But to the extent\nthat the relationship between words and things\nfor a given VLM-based system is di\ufb00erent than\nit is for human language-users, it might be pru-\ndent not to take literally talk of what that system\n\u201cknows\u201d or \u201cbelieves\u201d.\n9 What About Embodiment?\nHumans are members of a community of\nlanguage-users inhabiting a shared world, and\n11Of course, there is causal structure to the computa-\ntionscarried out by the model during inference. But this\nis not the same as there being causal relations between\nwords and the things those words are taken to be about.this primal fact makes them essentially di\ufb00erent\nto large language models. Human language users\ncan consult the world to settle their disagree-\nments and update their beliefs. They can, so to\nspeak, \u201ctriangulate\u201d on objective reality. In iso-\nlation, an LLM is not the sort of thing that can\ndo this, but in application, LLMs are embedded\nin larger systems. What if an LLM is embedded"], "retrieved_docs_id": ["f23b3625e0", "213829ff85", "a1c28916ce", "079fda9e8c", "2d1119f947"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What are the two types of evaluation instances in VHTest?\n", "true_answer": "The two types of evaluation instances in VHTest are \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).", "source_doc": "hallucination.pdf", "source_id": "8ef8344de6", "retrieved_docs": ["VHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual\nproperties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which\nemerge from comparisons across multiple objects, such as relative size, relative position, and\ncounting. Based on such categorization, the authors further defined 8 visual hallucination modes,\nproviding a very detailed evaluation of hallucination in MLLMs. Furthermore, the collected 1,200\nevaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no\nquestion\" (YNQ). Such design enables this benchmark to evaluate both generative and discriminative\ntasks.\nComparison of mainstream models We compare the mainstream MLLMs on some represen-\ntative benchmarks, providing a holistic overview of their performance from different dimensions.\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We", "14 Bai, et al.\nMHaluBench [ 13]This benchmark does not aim to evaluate the MLLMs themselves. Instead, it\nis intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether\na tool can successfully detect the hallucination produced by an MLLM. Thus, the benchmark\nconsists of hallucinatory examples. Specifically, the benchmark unifies image-to-text tasks and the\ntext-to-image tasks into one evaluation suite: cross-modal consistency checking. The hallucinatory\nexamples are generated using leading MLLMs and image generation models, such as LLaVA [ 75],\nMiniGPT-4 [ 138], DALL-E2 [ 89], and DALL-E3 [ 6]. During evaluation, the benchmark can be used\nto compare different hallucination detection methods based on their performance. So far, there are\nnot many dedicated hallucination detection methods. This work serves as a basis for this direction.\nVHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual", "and analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence\nidentification, atomic fact generation, and fact verification. The evaluation metric involves fine-\ngrained object hallucination categories, including entity, count, color, relation, and other attributes.\nThe final computation of FaithScore is the ratio of hallucinated content.\nBingo [ 21]Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark\nspecifically designed for assessing and analyzing the limitations of current popular MLLMs, such as\nGPT-4V [ 83]. It comprises 190 failure instances, along with 131 success instances as a comparison.\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\nBias refers to the model\u2019s susceptibility to generating hallucinatory outputs on specific types of\nexamples, such as OCR bias, region bias, etc. Interference refers to scenarios in which the judgment", "68\non various abilities ( e.g., knowledge utilization and hu-\nman alignment), and thus it is common that they are as-\nsessed with multiple evaluation approaches. In addition\nto benchmark-based evaluation, human-based and model-\nbased approaches have also been widely used to evaluate\nthe advanced abilities of fine-tuned LLMs. Next, we will\nintroduce the two evaluation methods.\n\u2022Human-based evaluation. Unlike automatic evaluation\nfor basic abilities, human evaluation typically considers\nmore factors or abilities in real-world use, such as hu-\nman alignment and tool manipulation. In this evaluation\napproach, test tasks are usually in the form of open-\nended questions, and human evaluators are invited to make\njudgments on the quality of answers generated by LLMs.\nTypically, there are two main types of scoring methods\nfor human evaluators: pairwise comparison and single-\nanswer grading. In pairwise comparison, given the same\nquestion, humans are assigned two answers from different", "Figure 7: Adaptation. During adaptation, we construct a promptfor each evaluation instance which\nmay include in-context training instances as well. Given decoding parameters , a language model generates a\ncompletion (in red). The multiple choice example is shown using two different adaptation strategies that we\ndescribe subsequently, with leftversion being the jointstrategy (all answer choices are presented at once)\nand therightversion being the separate strategy (each answer choice is presented separately).\nSecond, the text-to-text abstraction is a convenient general interface that can capture all the (text-only)\ntasks of interest, an idea that was pioneered by McCann et al. (2018) and Raffel et al. (2019).\n2.3 Metrics\nOnce a language model is adapted, we execute the resulting system on the evaluation instances for each\nscenario, yielding completions with their log probabilities. To determine how well the model performs, we"], "retrieved_docs_id": ["8ef8344de6", "5c89e9ef97", "db8870dfa6", "3603224301", "2cd8af9f66"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the name of the state-of-the-art proprietary chat model?\n", "true_answer": "GPT-4", "source_doc": "ChipNemo.pdf", "source_id": "8f6b70d3f1", "retrieved_docs": ["while GPT-4 is considered to be the state-of-the-art propri-\netary chat model.\n4", "are observed with other models). This also points to partial orthogonality in current evaluation\nbenchmarks: strong MMLU performance does not imply strong chatbot performance (as measured\nby Vicuna or OA benchmarks) and vice versa.\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1\ndataset collection guidelines explicitly forbid the use of GPT models. The next best model trained\non only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points\nlower than Guanaco on the Vicuna benchmark (see Table 6). Overall, these results show that 4-bit\nQLORAis effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\npotential for future work via QLORAtuning on specialized open-source data, which produces models\nthat can compete with the very best commercial models that exist today.", "retrieval model, the fact remains that retrieval still struggles\nwith queries that do not map directly to passages in the\ndocument corpus or require more context not present in\nthe passage. Unfortunately, these queries are also more\nrepresentative of queries that will be asked by engineers in\nreal situations. Combining retrieval with a domain adapted\nlanguage model is one way to address this issue.\n3. Evaluations\nWe evaluate our training methodology and application per-\nformance in this section. We study our 7B, 13B, and 70B\nmodels in the training methodology evaluation, and only our\nChipNeMo-70B model using SteerLM for model alignment\nin the application performance evaluation. For compari-\nson, we also evaluate two baseline chat models: LLaMA2-\n70B-Chat and GPT-4. LLaMA2-70B-Chat is the publicly\nreleased LLaMA2-Chat model trained with RLHF and is\nconsidered to be the state-of-the-art open-source chat model,\nwhile GPT-4 is considered to be the state-of-the-art propri-", "to achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.\n(2022) that finetuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant", "to achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.\n(2022) that \ufb01netuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant\nimprovement in performance as we were scaling."], "retrieved_docs_id": ["8f6b70d3f1", "1ee92ac678", "0fb655a6fb", "97eaa889af", "97eaa889af"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the proposed decoding method for MLLMs address the issue of over-trust?\n", "true_answer": "The proposed decoding method for MLLMs addresses the issue of over-trust by introducing a penalty term on the model logits during the beam-search decoding process to mitigate the over-trust issue.", "source_doc": "hallucination.pdf", "source_id": "4f752eeea2", "retrieved_docs": ["are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,\ni.e.,MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all\nthe previous tokens. Such a partial over-trust inclination results in neglecting image tokens and\ndescribing the image content with hallucination. Based on this observation, a decoding method for\nMLLMs grounded in an Over-trust Penalty and a Retrospection- Allocation strategy is proposed.\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding\nprocess to mitigate the over-trust issue. Additionally, to handle the hard cases that cannot be\naddressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed\nto retrospect the presence of summary tokens in the previously generated tokens and reallocate\nthe token selection if necessary.\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered", "6.6 Enhancing Interpretability and Trust\nExisting methods for hallucination mitigation are primarily based on empirical observations of\nspecific patterns, such as skipping the \u2018\\n\u2019 token and penalizing over-trust tokens. However, despite\nthe impressive improvements achieved on specific benchmarks, understanding the underlying\nmechanisms and decision-making processes remains challenging. Future research should focus on\ndeveloping techniques for interpreting and explaining the generation process of MLLMs, thereby\nproviding insights into the factors influencing hallucinated content. This includes investigating\nmethods for visualizing model internals, identifying salient features and linguistic patterns, and\ntracing the generation process from input to output. Enhancing the interpretability of MLLMs\nwill not only improve our understanding of model behavior but also enable users to better assess\nhallucinated content in practical applications.\n6.7 Navigating the Ethical Landscape", "22 Bai, et al.\ncause hallucination. In order to improve the accuracy and reliability of hallucinated content, it is\ncrucial to ensure that MLLMs have access to high-quality and diverse training data. Future research\nshould focus on developing techniques for data collection, augmentation, and calibration. Firstly,\ncollecting enough data at the initial stage is crucial to address the data scarcity issue and increase\ndata diversity. Secondly, data augmentation is an effective solution to further expand the size of data.\nFinally, exploring methods for re-calibrating existing datasets is crucial. This includes eliminating\nbiases, promoting diversity and inclusivity, and mitigating other potential issues that may induce\nhallucinations.\n6.2 Cross-modal Alignment and Consistency\nThe key challenge of multimodal hallucination is the cross-modal consistency issue. Ensuring that\ngenerated content remains consistent and contextually relevant to the input modality requires", "in real-world applications. This problem has attracted increasing attention, prompting efforts to detect\nand mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these\nhallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and\nstrategies developed to address this issue. Additionally, we analyze the current challenges and limitations,\nformulating open questions that delineate potential pathways for future research. By drawing the granular\nclassification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this\nsurvey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in\nthe field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the\nrobustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners", "InfMLLM [135] 13B 195.00 145.00 170.00 195.00 - - - - - - - -\nLLaMA-Adapter V2 [26] 7B 185.00 133.33 56.67 118.33 - - - - - - - -\nMiniGPT-4 [138] 13B 68.33 55.00 43.33 75.00 78.86 72.21 71.37 - 69.3 76.7 48.2 53.0\nmPLUG-Owl2 [112] 7B 185.00 155.00 88.33 150.00 - - - - 78.5 84.0 - -\nLLaVA-1.5 [75] 7B - - - - - - - - 74.4 82.9 48.9 34.2\nCogVLM [106] 7B 195.00 165.00 103.33 160.00 - - - - 80 86.1 - -\n5.1 Data\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing\nhallucination in MLLMs. For mitigating hallucination, recent works make attempts on data, includ-\ning introducing negative data [ 73], introducing counterfactual data [ 117], and reducing noise and\nerrors in existing dataset [105, 120].\nLRV-Instruction [ 73]LRV-Instruction is proposed to address the issue that existing instruction\ntuning data primarily focus on positive instruction samples, leading the model to consistently"], "retrieved_docs_id": ["4f752eeea2", "ebbf0f2d7a", "7ed2952b17", "c7f1da1e07", "a411f027c7"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Cait enhance model compression in Vision Transformers?\n", "true_answer": "Cait enhances model compression in Vision Transformers by introducing asymmetric token merging to integrate neighboring tokens efficiently and preserving the spatial structure, along with consistent dynamic channel pruning for uniform pruning of unimportant channels.", "source_doc": "multimodal.pdf", "source_id": "28380a85e1", "retrieved_docs": ["and efficiently handling intricate weight correlations during pruning, alongside an effective fine-\ntuning procedure for post-compression recovery. Cait [107] introduced asymmetric token merging\nto integrate neighboring tokens efficiently while preserving the spatial structure, paired with consis-\ntent dynamic channel pruning for uniform pruning of unimportant channels in Vision Transformers,\nenhancing model compression.\nStructured Pruning aims to remove structural components, such as attention heads or layers\nbased on predefined criteria. For example, WDPruning [108] employed a binary mask to discern\ninsignificant parameters based on their magnitudes. Additionally, Yu et al. [136] presented a unified\nframework integrating pruning to generate compact transformers. X-Pruner [109] utilizes an end-\nto-end learned explainability-aware mask to measure each unit\u2019s contribution to predicting target", "Kurt Keutzer. Q-diffusion: Quantizing diffusion models. arXiv preprint arXiv:2302.04304 , 2023.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-\naware weight quantization for llm compression and acceleration. 2023.\nYijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. NoisyQuant:\nNoisy bias-enhanced post-training activation quantization for vision transformers. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20321\u201320330,\n2023.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\nSangyun Oh, Hyeonuk Sim, Jounghyun Kim, and Jongeun Lee. Non-uniform step size quantiza-\ntion for accurate post-training quantization. In Computer Vision\u2013ECCV 2022: 17th European\nConference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XI , pp. 658\u2013673. Springer,\n2022.", "mance and enhance computational ef\ufb01ciency.\nSpeculative LLM Inference: speculative inference [6, 13,\n43] can also be leveraged to serve models exceeding GPU\nmemory. Speculative decoding [7] uses a smaller, faster model\nto pre-decode tokens, later validated by the main model in a\nbatch, reducing steps and CPU-GPU communication. SpecIn-\nfer [26], as another example, effectively reduces the number\nof LLM decoding steps and the overall communication be-\ntween CPU DRAM and GPU HBM. While separate from our\nfocus, integrating speculative inference into PowerInfer could\nfurther boost LLM inference speed.\nLLM-Speci\ufb01c Serving Optimizations: The prominence\nof Transformers has led to specialized serving systems [9,\n36, 53]. Orca [47] introduces iteration-level scheduling.\nvLLM [18] implements Paged Attention for token storage\nin varied GPU memory addresses, overcoming KV cache\u2019s\ncontinuous storage limit. While vLLM effectively mitigates\nthe issue of severe GPU memory fragmentation, it does not", "Advances in Neural Information Processing Systems , 33, 2020.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. arXiv\npreprint arXiv:2106.04560 , 2021.\nZhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji,\nJian Guan, et al. Cpm: A large-scale generative chinese pre-trained language model. AI Open , 2:93\u201399,\n2021.\nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the e\ufb03cacy of pruning for model\ncompression. arXiv preprint arXiv:1710.01878 , 2017.\n16", "prehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,\nwith the rationale that merging the low-level spatial features from DINOv2 and the semantic at-\ntributes offered by SigLIP will enhance performance on subsequent tasks. SPHINX-X[14] employs\ntwo vision encoders \u2013 DINOv2 and CLIP-ConvNeXt. Given that these models have been pre-trained\nvia distinct learning methodologies (self-supervised versus weakly supervised) and network archi-\ntectures (ViT versus CNN), they are naturally capable of offering the most complementary and\nsophisticated visual knowledge.\nLightweight Vision Encoder Vision Transformer architectures in real-world applications pose\nchallenges due to hardware and environmental limitations, including processing power and compu-\ntational capabilities. ViTamin [11] represents a lightweight vision model, specifically tailored for\nvision and language models. It commences with a convolutional stem, succeeded by Mobile Con-"], "retrieved_docs_id": ["28380a85e1", "4f10982415", "31042efbf7", "b950b5c339", "18b9cdbf0e"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What type of information is often inaccurately described by multimodal large language models (MLLMs) in images?\n", "true_answer": "MLLMs often inaccurately describe the attributes of objects in images, such as color, shape, material, content, counting, action, etc.", "source_doc": "hallucination.pdf", "source_id": "f2b3e09bb2", "retrieved_docs": ["2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "images. In addition, Resampler [ ADL+22] is used as an attentive pooling mechanism to reduce the\nnumber of image embeddings.\n2.2 Multimodal Large Language Models (MLLMs)\nAfter obtaining the embeddings of an input sequence, we feed them into the Transformer-based\ndecoder. The left-to-right causal model processes the sequence in an auto-regressive manner, which\nproduces the next token by conditioning on past timesteps. The causal masking is used to mask\nout future information. A softmax classi\ufb01er upon Transformer is used to generate tokens over the\nvocabulary.\nMLLMs serve as general-purpose interfaces [ HSD+22] that can perform interactions with both\nnatural language and multimodal input. The framework is \ufb02exible to handle various data types,\nas long as we can represent input as vectors. MLLMs combine the best of two worlds. First, the\nlanguage models naturally inherit the capabilities of in-context learning and instruction following.", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "ods, to deploy LLMs efficiently and effectively in real-world\nrecommender systems. In addition, existing LLMs have\nlimited capacities in long context modeling, make it difficult\nto process the huge amount of user-item interaction data.\nImproved context length extension and context information\nutilization approaches should be developed to improve the\nmodeling capacities of LLMs in long interaction sequences.\n8.1.4 Multimodal Large Language Model\nIn existing literature [823, 824], multimodal models mainly\nrefer to the models that can process and integrate informa-\ntion of various modalities ( e.g., text, image, and audio) frominput, and further produce corresponding output in certain\nmodalities. In this part, we mainly focus on the multimodal\nextension of LLMs by enabling the information modeling\nof non-textual modalities, especially the vision modality,\ncalled multimodal large language models (MLLMs) [797]49. To\nstart our discussion, we specify the input to be text-image"], "retrieved_docs_id": ["da0a465b6c", "f49f3b54ce", "5107b7792e", "72dc971633", "593e38b258"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "Which company offers a low-code AI solution for implementing various RAG applications?\n", "true_answer": "Flowise AI", "source_doc": "RAG.pdf", "source_id": "9ff21c1039", "retrieved_docs": ["quickly with the popularity of ChatGPT. They both offer a\nrich set of RAG-related APIs, gradually becoming one of\nthe indispensable technologies in the era of large models.\nMeanwhile, new types of technical stacks are constantly be-\ning developed. Although they do not offer as many features\nas LangChain and LLamaIndex, they focus more on their\nunique characteristics. For example, Flowise AI6emphasizes\nlow-code, allowing users to implement various AI applica-\ntions represented by RAG without writing code, simply by\ndragging and dropping. Other emerging technologies include\nHayStack, Meltno, and Cohere Coral.\nIn addition to AI-native frameworks, traditional software\nor cloud service providers have also expanded their service\nrange. For instance, Verba7, provided by the vector database\ncompany Weaviate, focuses on personal assistants. Amazon\noffers its users the intelligent enterprise search service tool\nKendra, based on RAG thinking. Users can search in different", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "These efforts collectively strive to achieve a balance between\nefficiency and the richness of contextual information in RAG\nretrieval.\n\u2022Exploring Hybrid Search: By intelligently blending\nvarious techniques such as keyword-based search, se-\nmantic search, and vector search, the RAG system can\nleverage the strengths of each method. This approach\nenables the RAG system to adapt to different query types\nand information needs, ensuring consistent retrieval of\nthe most relevant and context-rich information. Hybrid\nsearch serves as a robust complement to retrieval strate-\ngies, enhancing the overall performance of the RAG\npipeline.\n\u2022Recursive Retrieval and Query Engine: Another pow-\nerful method to optimize retrieval in the RAG system\ninvolves implementing recursive retrieval and a sophis-\nticated query engine. Recursive retrieval entails acquir-\ning smaller document blocks during the initial retrieval\nphase to capture key semantic meanings. In the later", "within the same output sequence.\n8.2 Ecosystem of RAG\nDownstream Tasks and Evaluation\nBy integrating relevant information from a broad knowledge\nbase, RAG has demonstrated significant potential in enhanc-\ning language models\u2019 ability to process complex queries and\ngenerate information-rich responses. Numerous studies have\nshown that RAG performs well in various downstream tasks,\nsuch as open-ended question answering and fact verification.\nRAG models not only improve the accuracy and relevance of\ninformation in downstream applications but also increase the\ndiversity and depth of responses.\nGiven the success of RAG, exploring the model\u2019s adapt-\nability and universality in multi-domain applications will be\npart of future work. This includes its use in professional do-\nmain knowledge question-answering, such as in medicine,\nlaw, and education. In the application of downstream tasks\nsuch as professional domain knowledge question-answering,", "in retrieved information. Counterfactual robustness tests\ninclude questions that the LLM can answer directly, but\nthe related external documents contain factual errors.\n7.3 Evaluation Frameworks\nRecently, the LLM community has been exploring the use\nof \u201dLLMs as judge\u201d for automatic assessment, with many\nutilizing powerful LLMs (such as GPT-4) to evaluate their\nown LLM applications outputs. Practices by Databricks us-\ning GPT-3.5 and GPT-4 as LLM judges to assess their chatbot\napplications suggest that using LLMs as automatic evaluation\ntools is effective [Leng et al. , 2023 ]. They believe this method\ncan also efficiently and cost-effectively evaluate RAG-based\napplications.\nIn the field of RAG evaluation frameworks, RAGAS and\nARES are relatively new. The core focus of these evaluations\nis on three main metrics: Faithfulness of the answer, answer\nrelevance, and context relevance. Additionally, TruLens, an\nopen-source library proposed by the industry, also offers a"], "retrieved_docs_id": ["9ff21c1039", "4fffd3dc2b", "e34dfedd36", "610b921fd5", "a05a21efce"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What are the three core metrics primarily focused on in the latest evaluation frameworks like RAGAS and ARES?\n", "true_answer": "Faithfulness of the answer, Answer Relevance, and Context Relevance.", "source_doc": "RAG.pdf", "source_id": "57b75e5528", "retrieved_docs": ["evaluation metrics. Additionally, the latest evalu-\nation frameworks like RAGAS [Eset al. , 2023 ]and\nARES [Saad-Falcon et al. , 2023 ]also involve RAG eval-\nuation metrics. Summarizing these works, three core metrics\nare primarily focused on: Faithfulness of the answer, Answer\nRelevance, and Context Relevance.\n1.Faithfulness\nThis metric emphasizes that the answers generated by\nthe model must remain true to the given context, ensur-\ning that the answers are consistent with the context infor-\nmation and do not deviate or contradict it. This aspect of\nevaluation is vital for addressing illusions in large mod-\nels.\n2.Answer Relevance\nThis metric stresses that the generated answers need to\nbe directly related to the posed question.\n3.Context Relevance\nThis metric demands that the retrieved contextual infor-\nmation be as accurate and targeted as possible, avoid-\ning irrelevant content. After all, processing long texts\nis costly for LLMs, and too much irrelevant information", "open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "ARES\nARES aims to automatically evaluate the performance of\nRAG systems in three aspects: Context Relevance, Answer\nFaithfulness, and Answer Relevance. These evaluation met-\nrics are similar to those in RAGAS. However, RAGAS, being\na newer evaluation framework based on simple handwritten\nprompts, has limited adaptability to new RAG evaluation set-\ntings, which is one of the significances of the ARES work.\nFurthermore, as demonstrated in its assessments, ARES per-\nforms significantly lower than RAGAS.\nARES reduces the cost of evaluation by using a small\namount of manually annotated data and synthetic data,\nand utilizes Predictive-Driven Reasoning (PDR) to provide\nstatistical confidence intervals, enhancing the accuracy of\nevaluation [Saad-Falcon et al. , 2023 ].\nAlgorithm Principles\n1. Generating Synthetic Dataset: ARES initially generates\nsynthetic questions and answers from documents in the\ntarget corpus using a language model to create positive\nand negative samples.", "in retrieved information. Counterfactual robustness tests\ninclude questions that the LLM can answer directly, but\nthe related external documents contain factual errors.\n7.3 Evaluation Frameworks\nRecently, the LLM community has been exploring the use\nof \u201dLLMs as judge\u201d for automatic assessment, with many\nutilizing powerful LLMs (such as GPT-4) to evaluate their\nown LLM applications outputs. Practices by Databricks us-\ning GPT-3.5 and GPT-4 as LLM judges to assess their chatbot\napplications suggest that using LLMs as automatic evaluation\ntools is effective [Leng et al. , 2023 ]. They believe this method\ncan also efficiently and cost-effectively evaluate RAG-based\napplications.\nIn the field of RAG evaluation frameworks, RAGAS and\nARES are relatively new. The core focus of these evaluations\nis on three main metrics: Faithfulness of the answer, answer\nrelevance, and context relevance. Additionally, TruLens, an\nopen-source library proposed by the industry, also offers a", "ous downstream tasks and with different retrievers may yield\ndivergent results. However, some academic and engineering\npractices have focused on general evaluation metrics for RAG\nand the abilities required for its effective use. This section\nprimarily introduces key metrics for evaluating RAG\u2019s effec-\ntiveness and essential abilities for assessing its performance.\nKey Metrics\nRecent OpenAI report [Jarvis and Allard, 2023 ]have\nmentioned various techniques for optimizing large\nlanguage models (LLMs), including RAG and its"], "retrieved_docs_id": ["57b75e5528", "ffd5c8b41e", "1b1cdfdd79", "a05a21efce", "8e161396f8"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the reduction in parameters achieved by LDPv2 compared to the original LDP in CNN-based MobileVLMv2?\n", "true_answer": "LDPv2 achieves a 99.8% reduction in parameters compared to the original LDP in CNN-based MobileVLMv2.", "source_doc": "multimodal.pdf", "source_id": "f4853839e9", "retrieved_docs": ["cross-attention, while image features are unfolded and concatenated with Q to serve as K and V in\ncross-attention. By this means, the transformer output at the corresponding positions of the learn-\nable latent queries is taken as the aggregated representation of visual features, thereby standardizing\nvariable-length video frame features into fixed-size features. MEQ-Former in BRA VE [12] designs\na multi-encoder querying transformer to amalgamate features from multiple frozen vision encoders\ninto a versatile representation that can be directly inputted into a frozen language model.\nCNN-based MobileVLMv2[17] proposes LDPv2, a new projector consisting of three parts: fea-\nture transformation, token reduction, and positional information enhancement. By using point-wise\nconvolution layers, average pooling, and a PEG module with a skip connection, LDPv2 achieves\nbetter efficiency, a 99.8% reduction in parameters, and slightly faster processing compared to the\noriginal LDP[20].", "Inference with Quantization. Figure 13 illustrates that Pow-\nerInfer effectively supports LLMs that are compressed using\nINT4 quantization. On a high-end PC (PC-High), PowerIn-\nfer delivers responses at an average speed of 13.20 token-\ns/s, reaching a peak of 29.08 tokens/s. The average speedup\nachieved compared with llama.cpp is 2.89 \u00d7, with a maxi-\nmum of 4.28\u00d7. On a lower-end setup (PC-Low), the average\nspeedup is 5.01 \u00d7, peaking at 8.00 \u00d7. The reduction in memory\nrequirements due to quantization enables PowerInfer to more\nef\ufb01ciently manage larger models. For instance, in our exper-\niment with the OPT-175B model on PC-High, PowerInfer\nnearly reaches two tokens per second, surpassing llama.cpp\nby a factor of 2.66 \u00d7.\nBatching Inference. We also evaluate the end-to-end infer-\nence performance of PowerInfer with different batch sizes, as\nshown in Figure 14. PowerInfer demonstrates a signi\ufb01cant\nadvantage when the batch size is smaller than 32, achieving", "ModelVision Encoder LLMVision-LLM ProjectorVariants Resolution Parameter Size Variants Parameter Size\nMobileVLM [20] CLIP ViT-L/14 [73] 336 0.3B MobileLLaMA[20] 2.7B LDP[20]\nLLaV A-Phi [21] CLIP ViT-L/14 [73] 336 0.3B Phi-2[74] 2.7B MLP\nImp-v1 [22] SigLIP [75] 384 0.4B Phi-2[74] 2.7B -\nTinyLLaV A [23] SigLIP-SO [75] 384 0.4B Phi-2[74] 2.7B MLP\nBunny [24] SigLIP-SO [75] 384 0.4B Phi-2[74] 2.7B MLP\nMobileVLM-v2-3B [17] CLIP ViT-L/14 [73] 336 0.3B MobileLLaMA[17] 2.7B LDPv2[17]\nMoE-LLaV A-3.6B [25] CLIP-Large [73] 384 - Phi-2[74] 2.7B MLP\nCobra [13]DINOv2 [76]\nSigLIP-SO [75]384 0.3B+0.4B Mamba-2.8b-Zephyr[77] 2.8B MLP\nMini-Gemini [26] CLIP-Large [73] 336 - Gemma[78] 2B MLP\nVary-toy [27] CLIP [73] 224 - Qwen[79] 1.8B -\nTinyGPT-V [28] EV A [80] 224/448 - Phi-2[74] 2.7B Q-Former [15]\nSPHINX-Tiny [14]DINOv2 [76]\nCLIP-ConvNeXt [81]448 - TinyLlama[82] 1.1B -\nALLaV A-Longer [29] CLIP-ViT-L/14 [73] 336 0.3B Phi-2[74] 2.7B -\nMM1-3B-MoE-Chat [30] CLIP DFN-ViT-H [83] 378 - - 3B\u2217C-Abstractor [19]", "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "amount of memory for activations. LOMO is compatible with activation memory reduction tech-\nniques such as activation checkpointing. By integrating activation checkpointing with LOMO, the\nmemory footprint due to activation can be reduced from 45.61GB to 1.79GB.\n4.2 T HROUGHPUT\nWe evaluate the throughput performance of LOMO compared to AdamW and SGD. The experiments\nare conduct on a server equipped with 8 RTX 3090 GPUs, interconnected via a PCIe motherboard.\nThe sequence length and batch size are set to 1024 and 1, respectively. Throughput is measured in\nterms of the number of tokens processed per GPU per second (TGS), and parameter partitioning was\nachieved using ZeRO-3 (Rajbhandari et al., 2020).\nFor the 7B model, LOMO demonstrates remarkable throughput, surpassing AdamW and SGD by\nabout 11 times. This significant improvement can be attributed to LOMO\u2019s ability to train the 7B\n7"], "retrieved_docs_id": ["f4853839e9", "ee109d49c1", "14f018b2c6", "3238be52f9", "8a82c05080"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does RAG increase trust in its output?\n", "true_answer": "By citing sources, RAG allows users to verify the accuracy of the answers, which increases trust in its output.", "source_doc": "RAG.pdf", "source_id": "62ff7bd487", "retrieved_docs": ["maintains the timeliness and accuracy of responses.\n\u2022 Transparency is an advantage of RAG. By citing\nsources, users can verify the accuracy of the answers,\nincreasing trust in the model\u2019s output.\n\u2022 RAG has customization capabilities. Models can be tai-\nlored to different domains by indexing relevant textual\ncorpora, providing knowledge support for specific fields.\n\u2022 In terms of security and privacy management, RAG,\nwith its built-in roles and security controls in the\ndatabase, can better control data usage. In contrast, fine-\ntuned models may lack clear management of who can\naccess which data.\n\u2022 RAG is more scalable. It can handle large-scale datasets\nwithout the need to update all parameters and create\ntraining sets, making it more economically efficient.\n\u2022 Lastly, results produced by RAG are more trustworthy.\nRAG selects deterministic results from the latest data,\nwhile fine-tuned models may exhibit hallucinations and\ninaccuracies when dealing with dynamic data, lacking", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "within the same output sequence.\n8.2 Ecosystem of RAG\nDownstream Tasks and Evaluation\nBy integrating relevant information from a broad knowledge\nbase, RAG has demonstrated significant potential in enhanc-\ning language models\u2019 ability to process complex queries and\ngenerate information-rich responses. Numerous studies have\nshown that RAG performs well in various downstream tasks,\nsuch as open-ended question answering and fact verification.\nRAG models not only improve the accuracy and relevance of\ninformation in downstream applications but also increase the\ndiversity and depth of responses.\nGiven the success of RAG, exploring the model\u2019s adapt-\nability and universality in multi-domain applications will be\npart of future work. This includes its use in professional do-\nmain knowledge question-answering, such as in medicine,\nlaw, and education. In the application of downstream tasks\nsuch as professional domain knowledge question-answering,", "external knowledge, alleviates hallucination issues, identifies\ntimely information via retrieval technology, and enhances re-\nsponse accuracy. Additionally, by citing sources, RAG in-\ncreases transparency and user trust in model outputs. RAG\ncan also be customized based on specific domains by index-\ning relevant text corpora. RAG\u2019s development and charac-\nteristics are summarized into three paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG, each with its models,\nmethods, and shortcomings. Naive RAG primarily involves\nthe \u2019retrieval-reading\u2019 process. Advanced RAG uses more\nrefined data processing, optimizes the knowledge base in-\ndexing, and introduces multiple or iterative retrievals. As\nexploration deepens, RAG integrates other techniques like\nfine-tuning, leading to the emergence of the Modular RAG\nparadigm, which enriches the RAG process with new mod-\nules and offers more flexibility.\nIn the subsequent chapters, we further analyze three key", "such as professional domain knowledge question-answering,\nRAG might offer lower training costs and better performance\nbenefits than fine-tuning.\nSimultaneously, improving the evaluation system of RAG\nfor assessing and optimizing its application in different down-\nstream tasks is crucial for the model\u2019s efficiency and bene-\nfits in specific tasks. This includes developing more accurate\nevaluation metrics and frameworks for different downstream\ntasks, such as context relevance, content creativity, and harm-\nlessness, among others.\nFurthermore, enhancing the interpretability of models\nthrough RAG, allowing users to better understand how and\nwhy the model makes specific responses, is also a meaning-\nful task.\nTechnical Stack\nIn the ecosystem of RAG, the development of the related\ntechnical stack has played a driving role. For instance,\nLangChain and LLamaIndex have become widely known\nquickly with the popularity of ChatGPT. They both offer a"], "retrieved_docs_id": ["62ff7bd487", "4fffd3dc2b", "610b921fd5", "123a2dcc44", "da182b99e8"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a lightweight vision model designed for vision and language tasks?\n", "true_answer": "ViTamin [11]", "source_doc": "multimodal.pdf", "source_id": "18b9cdbf0e", "retrieved_docs": ["prehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,\nwith the rationale that merging the low-level spatial features from DINOv2 and the semantic at-\ntributes offered by SigLIP will enhance performance on subsequent tasks. SPHINX-X[14] employs\ntwo vision encoders \u2013 DINOv2 and CLIP-ConvNeXt. Given that these models have been pre-trained\nvia distinct learning methodologies (self-supervised versus weakly supervised) and network archi-\ntectures (ViT versus CNN), they are naturally capable of offering the most complementary and\nsophisticated visual knowledge.\nLightweight Vision Encoder Vision Transformer architectures in real-world applications pose\nchallenges due to hardware and environmental limitations, including processing power and compu-\ntational capabilities. ViTamin [11] represents a lightweight vision model, specifically tailored for\nvision and language models. It commences with a convolutional stem, succeeded by Mobile Con-", "Vision Audition\n31 8 \u2026 70 2\nA B C\nD E F\nFigure 1: KOSMOS -1is a multimodal large language model (MLLM) that is capable of perceiving\nmultimodal input, following instructions, and performing in-context learning for not only language\ntasks but also multimodal tasks. In this work, we align vision with large language models (LLMs),\nadvancing the trend of going from LLMs to MLLMs.\n\u2217Equal contribution. \u2020Corresponding author.arXiv:2302.14045v2  [cs.CL]  1 Mar 2023", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "its selection is closely related to the lightweight nature of MLLM. In comparison to conventional\nMLLMs with parameter sizes ranging from 7 billion to tens of billions[87, 88], efficient MLLMs\ntypically employ language models with less than 3 billion parameters, such as phi2-2.7B[74] by\nMicrosoft and Gemma-2B[78] by Google. Phi-2 trained on special data recipes can match the per-\nformance of models 25 times larger trained on regular data. Phi-3-mini [86] can be easily deployed\nlocally on a modern phone and achieves a quality that seems on-par with models such as Mixtral\n8x7B [89] and GPT-3.5. In addition to utilizing pre-trained models, MobileVLM[20] downscales\nLLaMA[87] and trains from scratch using open-source datasets. The specific model scaling is illus-\ntrated in the Table.1 and Table.4.\n2.4 Vision Token Compression\nInitial research has underscored the potential of MLLMs across various tasks, including visual ques-", "Figure 5: MobileVLM v2 [17] and Honeybee [19] efficient vision-language projector.\n2.3 Small Language Model\nThe pre-trained small language model(SLM) serves as the core component of MLLMs, endowing\nit with many outstanding capabilities, such as zero-shot generalization, instruction following, and\nin-context learning. The SLM accepts input sequences containing multiple modalities and outputs\ncorresponding text sequences. A text tokenizer is typically bundled with the SLM, mapping text\nprompts Xqto the text tokens Hq. The text tokens Hqand the visual tokens Hvare concatenated as\nthe input of the language model, which outputs the final response sequence Yain an autoregressive\nmanner:\np(Ya|Hv, Hq) =LY\ni=1p(yi|Hv, Hq, y<i), (3)\nwhere Ldenotes the length of Ya. As the SLM contributes the vast majority of MLLM parameters,\nits selection is closely related to the lightweight nature of MLLM. In comparison to conventional"], "retrieved_docs_id": ["18b9cdbf0e", "f0ea146bbd", "4ee780b19c", "26327c579e", "00e8c4ea32"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How well did the domain-adapted LLM perform for the engineering assistant chatbot based on expert evaluations?\n", "true_answer": "The domain-adapted LLM achieved a score of 6.0 on a 7-point Likert scale for the engineering assistant chatbot based on expert evaluations.", "source_doc": "ChipNemo.pdf", "source_id": "28f0897bcb", "retrieved_docs": ["uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "vant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation", "processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ncost of pretraining a foundational model from scratch.\nModel Size Pretraining DAPT SFT\n7B 184,320 2,620 90\n13B 368,640 4,940 160\n70B 1,720,320 20,500 840\nTable 1: Training cost of LLaMA2 models in A100 GPU hours.\nPretraining cost from (Touvron et al., 2023).\n3.5. RAG and Engineering Assistant Chatbot\nWe created a benchmark to evaluate the performance of\ndesign chat assistance, which uses the RAG method. This\nbenchmark includes 88 questions in three categories: archi-\ntecture/design/verification specifications (Specs), testbench\nregression documentation (Testbench), and build infrastruc-\nture documentation (Build). For each question, we specify\nthe golden answer as well as the paragraphs in the design\ndocument that contains the relevant knowledge for the an-\nswer. These questions are created by designers manually\nbased on a set of design documents as the data store for\nretrieval. It includes about 1.8K documents, which were"], "retrieved_docs_id": ["c7d05c4b43", "a6c3d05123", "28f0897bcb", "e6b9ba907a", "7313e64a59"], "reranker_type": "colbert", "search_type": "text", "rr": 0.3333333333333333, "hit": 1}, {"question": "Which model outperforms GPT-4 in all categories, including both RAG misses and hits, according to the human evaluation?\n", "true_answer": "ChipNeMo-70B-Steer outperforms GPT-4 in all categories, including both RAG misses and hits, according to the human evaluation.", "source_doc": "ChipNemo.pdf", "source_id": "1ed1c2ae54", "retrieved_docs": ["and required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the differencein retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022ChipNeMo-70B-Steer outperforms similar sized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and", "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "independently for each answer. The MC1 accuracy results are shown in Figure 4 (left). We can see\nthat with a modified ranking approach, Dromedary significantly outperforms the powerful GPT-4\nmodel and other baselines, achieving a new state-of-the-art MC1 accuracy of 69.\nIn the generation task, models generate full-sentence answers given the question. The benchmark\nevaluates the model\u2019s performance on both questions to measure truthful models and the intersection\nof truthful and informative. As shown in Table 4 (right), Dromedary achieves higher scores than\nGPT-3 ,LLaMA ,Alpaca in both categories, while failing behind the ChatGPT -distilled Vicuna model.\n4.2.2 BIG-bench HHH Eval\nThe BIG-bench HHH Eval [ 39,3] was specifically designed to evaluate a model\u2019s performance in\nterms of helpfulness, honesty, and harmlessness (HHH). It is a Multiple-Choice (MC) task, which\ntests the models\u2019 ability to select superior answers from two reference answers10. We calculate the", "under user-preferred formalizations. Based on\nthe superficial alignment hypothesis, the authors\nclaimed that large language models can generate\nuser-satisfied responses by fine-tuning it on a small\nfraction of instruction data. Therefore, the authors\nbuilt instruction train/valid/test sets to verify this\nhypothesis.\nEvaluations are conducted on the constructed\ntest set. For human evaluations, LIMA outperforms\nInstructGPT and Alpaca by 17% and 19%,\nrespectively. Additionally, LIMA achieves\ncomparable results to BARD6, Cladue7, and GPT-\n4. For automatic evaluation, which is conducted\nby asking GPT-4 to rate responses and a higher\nrate score denotes better performance, LIMA\noutperforms InstructGPT and Alpaca by 20% and\n36%, respectively, achieving comparable results to\nBARD, while underperforming Claude and GPT-4.\nExperimental results verify the proposed superficial\nalignment hypothesis.\n4.11 Others\nOPT-IML (175B) (Iyer et al., 2022) is a\nlarge language model trained by fine-tuning the", "and 0.7 on User-Oriented-Instructions-252 (Wang\net al., 2022c), Vicuna-Instructions (Chiang et al.,\n2023), and Unnatural Instructions (Honovich et al.,\n2022) datasets, respectively. For human evaluation,\nregarding aspects including helpfulness, honesty,\nand harmlessness, GPT-4-LLM outperforms\nAlpaca by 11.7, 20.9, and 28.6 respectively.4.7 Claude\nClaude5is a language model trained by fine-tuning\nthe pre-trained language model on an instruction\ndataset, aiming to generate helpful and harmless\nresponses. The fine-tuning process consists of two\nstages: (1) supervised fine-tuning on the instruction\ndataset. The authors created an instruction dataset\nby collecting 52K different instructions, paired\nwith responses generated by GPT-4. The fine-\ntuning process takes approximately eight hours\non an 8-card 80GB A100 machine with mixed\nprecision and fully shared data parallelism. (2)\noptimizing the step-1 model with the proximal\npolicy optimization (Schulman et al., 2017) method."], "retrieved_docs_id": ["1ed1c2ae54", "af6e8c3fb2", "431dc0ad99", "6253a330c1", "ef70c3a68a"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does SUGRE embed relevant information from a knowledge graph?\n", "true_answer": "SUGRE embeds relevant subgraphs retrieved from the knowledge graph using Graph Neural Networks (GNN).", "source_doc": "RAG.pdf", "source_id": "812e372c75", "retrieved_docs": ["formation of the preceding blocks (C1, . . . , C i\u22121)and the\nretrieval information of N(Ci\u22121)through cross-attention to\nguide the generation of the next block Ci. To maintain causal-\nity, the autoregressive generation of the i-th block Cican only\nuse the nearest neighbor of the previous block N(Ci\u22121)and\nnotN(Ci).\nAugmented with Structured Data\nStructured data sources like Knowledge Graphs (KG) are\ngradually integrated into the paradigm of RAG. Verified KGs\ncan offer higher-quality context, reducing the likelihood of\nmodel hallucinations.\nRET-LLM [Modarressi et al. , 2023 ]constructs a per-\nsonalized knowledge graph memory by extracting\nrelation triples from past dialogues for future use.\nSUGRE [Kang et al. , 2023 ]embeds relevant subgraphs\nretrieved from the knowledge graph using Graph Neural\nNetworks (GNN) to prevent the model from generating\ncontextually irrelevant replies. SUGRE [Kang et al. , 2023 ]\nemploys a graph encoding method that reflects the graph", "swers given a retrieval-enhanced directive. It updates the gen-\nerator and retriever to minimize the semantic similarity be-\ntween documents and queries, effectively leveraging relevant\nbackground knowledge.\nAdditionally, SUGRE [Kang et al. , 2023 ]introduces the\nconcept of contrastive learning. It conducts end-to-end fine-\ntuning of both retriever and generator, ensuring highly de-\ntailed text generation and retrieved subgraphs. Using a\ncontext-aware subgraph retriever based on Graph Neural Net-\nworks (GNN), SURGE extracts relevant knowledge from a\nknowledge graph corresponding to an ongoing conversation.\nThis ensures the generated responses faithfully reflect the re-\ntrieved knowledge. SURGE employs an invariant yet efficient\ngraph encoder and a graph-text contrastive learning objective\nfor this purpose.\nIn summary, the enhancement methods during the fine-\ntuning phase exhibit several characteristics. Firstly, fine-\ntuning both LLM and retriever allows better adaptation", "It is permutation-invariant; therefore, the output is\nindependent of the input sequence order. Positional\ninformation is commonly injected to make the\nmodel respect a token\u2019s position in the sequence,\ni.e., capture the semantics of where a token occurs\nrather than just whether it occurs. The longer the\ninput is, the more important the positional embed-\nding becomes since the model needs to effectively\nuse information from different parts of the input\nthat may cover a wide range of distances from the\ncurrent token.\nWithout positional embeddings, a Transformer\nmodels the relations between any two tokens with\nequal probability. Hence, positional embeddings\nintroduce an LSTM-like inductive bias that (typi-\ncally) tokens closer to each other in the sequence\nare more relevant to each other. Depending on the\npositional embedding scheme chosen, this can be\nlearned or effectively hard-coded. However, it re-\nmains unclear what is the most effective positional", "employs a graph encoding method that reflects the graph\nstructure into PTMs\u2019 representation space and utilizes a\nmulti-modal contrastive learning objective between graph-\ntext modes to ensure consistency between retrieved facts\nand generated text. KnowledgeGPT [Wang et al. , 2023c ]\ngenerates search queries for Knowledge Bases (KB) in code\nformat and includes predefined KB operation functions.\nApart from retrieval, KnowledgeGPT also offers the ca-\npability to store knowledge in a personalized knowledge\nbase to meet individual user needs. These structured data\nsources provide RAG with richer knowledge and context,\ncontributing to improved model performance.\nLLM Generated Content RAG\nObserving that the auxiliary information recalled by RAG\nis not always effective and may even have negative effects,\nsome studies have expanded the paradigm of RAG by delving\ndeeper into the internal knowledge of LLM. This approach\nutilizes the content generated by LLM itself for retrieval, aim-", "outperform 10\u00d7larger ones [653, 657]. Further, open-book\nQA tasks can be also employed to evaluate the recency\nof knowledge information. Pre-training or retrieving from\noutdated knowledge resources may cause LLMs to generate\nincorrect answers for time-sensitive questions [653].\nKnowledge Completion. In knowledge completion tasks,\nLLMs might be (to some extent) considered as a knowledge\nbase [576], which can be leveraged to complete or predict the\nmissing parts of knowledge units ( e.g., knowledge triples).\nSuch tasks can probe and evaluate how much and what kind\nofknowledge LLMs have learned from the pre-training\ndata. Existing knowledge completion tasks can be roughly\ndivided into knowledge graph completion tasks ( e.g.,FB15k-\n237 [572] and WN18RR [574]) and fact completion tasks ( e.g.,\nWikiFact [571]), which aim to complete the triples from a\nknowledge graph and incomplete sentences about specific\nfacts, respectively. Empirical studies have revealed that it"], "retrieved_docs_id": ["812e372c75", "977e0e1405", "f5ac31896a", "4c3ac6cb2e", "e798d8fddc"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can multi-view low-resolution vision encoders capture detailed information for MLLM?\n", "true_answer": "By inputting multi-view high-resolution images, specifically a global view (low-resolution images from resizing) and a local view (image patches from splitting).", "source_doc": "multimodal.pdf", "source_id": "f8392fc0db", "retrieved_docs": ["Multi-view Input Directly employing high-resolution vision encoders for fine-grained percep-\ntion is prohibitively costly and does not align with practical usage requirements. Therefore, to\nutilize low-resolution vision encoders while enabling MLLM to perceive detailed information, a\ncommon approach is to input multi-view HR images, i.e., a global view: low-resolution images\nobtained through resizing, and a local view: image patches derived from splitting. For example,\n7", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "VL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to\n448\u00d7448. InternVL [ 2] scales up the vision encoder to 6 billion parameters, enabling processing of\nhigh-resolution images. Regarding hallucination, HallE-Switch [ 123] has investigated the impact\nof vision encoder resolution on its proposed CCEval benchmark. Among the three studied vision\nencoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results\nin lower degrees of hallucination. These works indicate that scaling up vision resolution is a\nstraightforward yet effective solution.\n5.2.2 Versatile Vision Encoders. Several studies [ 38,49,98] have investigated vision encoders for\nMLLMs. Typically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs\nthanks to its remarkable ability to extract semantic-rich features. However, CLIP has been shown\nto lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent", "tion answering and image captioning. However, MLLMs face considerable challenges in tasks ne-\ncessitating intricate recognition, including crowd counting and OCR of small characters. A direct\napproach to address these challenges involves increasing the image resolution, practically, the num-\nber of visual tokens. This strategy, nonetheless, imposes a substantial computational burden on\nMLLMs, primarily due to the quadratic scaling of computational costs with the number of input to-\nkens in the Transformer architecture. Motivated by this challenge, vision token compression, aimed\nto reduce the prohibitive computation budget caused by numerous tokens, has become an essential\naspect of efficient MLLMs. We will explore this topic through several key techniques, including\nmulti-view input, token processing, multi-scale information fusion, vision expert agents and video-\nspecific methods.\nMulti-view Input Directly employing high-resolution vision encoders for fine-grained percep-", "to lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent\nstudies have proposed complementing this information loss by incorporating visual features from\nother vision encoders. The work of [ 98] proposes mixing features from CLIP ViT and DINO ViT.\nSpecifically, it experimented with additive and interleaved features. Both settings show that there\nis a trade-off between the two types of features. A more dedicated mechanism is needed.\nConcurrently, a visual expert-based model proposed in [ 38] aims to mitigate the information\nloss caused by the CLIP image encoder. Instead of merely mixing features, this paper enhances\nthe visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two\npivotal modules: multi-task encoders and the structural knowledge enhancement module. The multi-\ntask encoders are dedicated to integrating various types of latent visual information extracted by"], "retrieved_docs_id": ["f8392fc0db", "4ee780b19c", "3f64cf9b55", "8beea9b82e", "c20c82af54"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the vision encoder modify the input image in a mathematical formula?\n", "true_answer": "The vision encoder compresses the original image (X) into more compact patch features (Zv) using the formula Zv=g(Xv).", "source_doc": "multimodal.pdf", "source_id": "11ffce909a", "retrieved_docs": ["Taking the input image Xvas input, the vision encoder compresses the original image into more\ncompact patch features Zv, as represented by the following formula:\nZv=g(Xv). (1)\n4", "encoded image embeddings. For example, \u201c <s>document </s> \u201d is a text input, and \u201c <s>paragraph\n<image> Image Embedding </image> paragraph </s> \u201d is an interleaved image-text input. Table 21\nin Appendix shows some examples of input format.\nAn embedding module is used to encode both text tokens and other input modalities into vectors. Then\nthe embeddings are fed into the decoder. For input tokens, we use a lookup table to map them into\nembeddings. For the modalities of continuous signals (e.g., image, and audio), it is also feasible to\nrepresent inputs as discrete code and then regard them as \u201cforeign languages\u201d [ WBD+22,WCW+23].\nIn this work, following [ HSD+22], we employ a vision encoder as the embedding module for input\n5", "start our discussion, we specify the input to be text-image\npairs and the output to be text responses. Similar discus-\nsions can be made for other modalities, e.g., language-audio\nmodels [825], which is beyond our scope here. In essence,\nMLLMs are developed by adapting the information from\nother modalities to the text modality, so as to leverage the\nexcellent model capacities of LLMs that are learned based on\nworld text. Typically, a MLLM comprises an image encoder\nfor image encoding and a LLM for text generation, associ-\nated by a connection module that aligns vision and language\nrepresentations. During generation, the image is first split\ninto patches, and then transformed into patch embeddings\nby the image encoder and the connection module, to derive\na visual representation that can be understood by the LLM.\nSubsequently, the patch embeddings and text embeddings\nare concatenated, and fed into the MLLM, allowing the\nlanguage model to generate the response autoregressively.", "Multi-view Input Directly employing high-resolution vision encoders for fine-grained percep-\ntion is prohibitively costly and does not align with practical usage requirements. Therefore, to\nutilize low-resolution vision encoders while enabling MLLM to perceive detailed information, a\ncommon approach is to input multi-view HR images, i.e., a global view: low-resolution images\nobtained through resizing, and a local view: image patches derived from splitting. For example,\n7", "and capture temporal dynamics in videos.\nInstructBLIP (1.2B) (Dai et al., 2023) is\na vision-language instruction tuning framework\ninitialized with a pre-trained BLIP-2 (Li et al.,\n2023d)) model consisting of an image encoder,\nan LLM (FlanT5 (3B/11B) (Chung et al., 2022)\nor Vicuna (7B/13B) (Chiang et al., 2023)), and\na Query Transformer (Q-Former) to bridge the\ntwo. As shown in Figure 7, the Q-Former extracts\ninstruction-aware visual features from the output\nembeddings of the frozen image encoder, and\nFigure 6: Overall architecture of Video-LLaMA. The\nfigure is copied from Zhang et al. (2023b).\nFigure 7: Overall architecture of InstructBLIP. The\nfigure is copied from Dai et al. (2023).\nfeeds the visual features as soft prompt input\nto the frozen LLM. The authors evaluate the\nproposed InstructBLIP model on a variety of vision-\nlanguage tasks, including image classification,\nimage captioning, image question answering, and\nvisual reasoning. They use 26 publicly available"], "retrieved_docs_id": ["11ffce909a", "c8e0c72bbc", "f96143f9ba", "f8392fc0db", "67f1e98cc7"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Vid2Seq improve language model prediction?\n", "true_answer": "Vid2Seq improves language model prediction by introducing special time markings, enabling it to predict event boundaries and text descriptions seamlessly.", "source_doc": "RAG.pdf", "source_id": "535efdce62", "retrieved_docs": ["transforming MT data into ST data. UEOP [Chan et al. , 2023 ]\nintroduces a new breakthrough in end-to-end automatic\nspeech recognition by introducing external offline strate-\ngies for voice-to-text mapping. Audio embeddings and\nsemantic text embeddings generated by text-to-speech\nmethods can bias ASR through KNN-based attention fu-\nsion, effectively shortening domain adaptation time. The\nVid2Seq [Yang et al. , 2023a ]architecture enhances the lan-\nguage model by introducing special time markings, enabling\nit to seamlessly predict event boundaries and text descriptions", "iments show that it not only achieves competitive performance with state-of-the-art efficient meth-\nods but also boasts faster speeds due to its linear sequential modeling.It also excels in overcom-\ning visual illusions and spatial relationship judgments in closed-set challenging prediction bench-\nmarks and achieves performance comparable to LLaV A while using only 43% of the parameters.\nVL-Mamba[18] substitutes the Transformer-based backbone language model with the pre-trained\nMamba language model. It explores how to effectively implement the 2D vision selective scan\nmechanism for multimodal learning and the combinations of different vision encoders and pre-\ntrained Mamba language model variants.\nInference Acceleration SPD[45] proposes the speculative decoding with a language-only model\nto improve inference efficiency. By employing a language-only model as a draft model for specu-\n9", "\u2022We leverage the compression-prediction equivalence to employ compressors as generative\nmodels and visually illustrate the performance of the underlying compressor.\n\u2022We demonstrate that tokenization, which can be viewed as a pre-compression, does, in general,\nnot improve compression performance, but allows models to increase the information content\nin their context and is thus generally employed to improve prediction performance.\n2. Background\nInthissection, wereviewthenecessarybackgroundoninformationtheoryanditsrelationtolikelihood\nmaximization. To that end, we consider streams of data \ud835\udc651:\ud835\udc5b:=\ud835\udc651\ud835\udc652...\ud835\udc65\ud835\udc5b\u2208X\ud835\udc5bof length\ud835\udc5bfrom a\nfinite set of symbols X. We write \ud835\udc65\u2264\ud835\udc57=\ud835\udc65<\ud835\udc57+1:=\ud835\udc651:\ud835\udc57for\ud835\udc57\u2264\ud835\udc5band denote the empty string as \ud835\udf16. Finally,\nwe denote the concatenation of two strings \ud835\udc60and\ud835\udc5fby\ud835\udc60\ud835\udc5f.\nCoding Distributions A coding distribution \ud835\udf0cis a sequence of probability mass functions \ud835\udf0c\ud835\udc5b:X\ud835\udc5b\u21a6\u2192\n(0,1], which for all \ud835\udc5b\u2208\u2115satisfy the constraint that \ud835\udf0c\ud835\udc5b(\ud835\udc651:\ud835\udc5b)=\u00cd\n\ud835\udc66\u2208X\ud835\udf0c\ud835\udc5b+1(\ud835\udc651:\ud835\udc5b\ud835\udc66)for all\ud835\udc651:\ud835\udc5b\u2208X\ud835\udc5b,", "we know from Section 7 that the 8B model has signi\ufb01cantly less memorization capacity than 540B, if this\ndi\ufb00erence were primarily due to memorization, we would expect the delta to be smaller on 8B.\nPaLM 8B 0-shot PaLM 62B 0-shot PaLM 540B 0-shot\nLanguage Clean Full Set Clean Set Full Set Clean Set Full Set Clean Set\nPair Proportion BLEU BLEU BLEU BLEU BLEU BLEU\nEn-Fr 88.9% 21.4 21.6 25.5 25.0 38.2 38.4\nEn-De 96.5% 17.8 17.9 15.0 14.8 31.8 31.8\nEn-Ro 94.3% 7.6 7.6 15.2 15.1 24.3 24.2\nFr-En 64.0% 33.9 34.2 35.3 36.4 40.2 41.1\nDe-En 74.2% 31.7 34.9 31.5 34.8 40.4 43.8\nRo-En 75.2% 31.9 31.6 31.8 32.0 39.6 39.9\nTable 19: Performance on the \u201cclean\u201d subset for 0-shot machine translation tasks.\n9 Exploring Explanations\nIn Section 6.3, we empirically demonstrated how chain-of-thought prompting (Wei et al., 2022b) can drastically\nimprove prediction accuracy in multi-step reasoning tasks. In that case, the reasoning chain generated by the", "\ud835\udc3b(\ud835\udf0c,\u02c6\ud835\udf0c):=\ud835\udd3c\ud835\udc65\u223c\ud835\udf0c\"\ud835\udc5b\u2211\ufe01\n\ud835\udc56=1\u2212log2\u02c6\ud835\udf0c(\ud835\udc65\ud835\udc56|\ud835\udc65<\ud835\udc56)#\n. (2)\nThus, we can minimize the expected length of the encoded data stream with symbols distributed\naccording to \ud835\udf0cby minimizing the cross-entropy with respect to some \u02c6\ud835\udf0c, which is equivalent to\nlikelihood maximization (MacKay, 2003). However, Eq. (2) is exactly the same objective used to\ntrain current foundation models, i.e., the log-loss. Thus, minimizing the log-loss is equivalent to\nminimizing the compression rate of that model used as a lossless compressor with arithmetic coding,\ni.e., current language model training protocols use a maximum-compression objective.\nCompression-Based Sequence Prediction Analogous to how a predictive distribution can be used\nfor lossless compression via arithmetic coding (described above), any compressor can be employed for\nsequence prediction (Frank et al., 2000). The main idea is to define \ud835\udf0c(\ud835\udc651:\ud835\udc5b)as the coding distribution"], "retrieved_docs_id": ["535efdce62", "6bebc6e320", "2e677bfe82", "e9e7dfa6da", "c76c1cb9e8"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is an example of a method for mitigating data-related hallucinations in NLP?\n", "true_answer": "Introducing negative data, such as LRV-Instruction, is an example of a method for mitigating data-related hallucinations in NLP.", "source_doc": "hallucination.pdf", "source_id": "19a4c2c778", "retrieved_docs": ["to the data filtering strategy. This is achieved by simply modifying the Maximum Likelihood\nEstimation (MLE), enabling the model to mitigate hallucination through learning from regular\ninstruction data.\n5.3.2 Reinforcement Learning. Reinforcement learning (RL) is introduced to train MLLMs for\nmitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "policy optimization (PPO) (Schulman et al., 2017)\nmethod, a policy gradient reinforcement learning\nmethod. Steps (2) and (3) are alternated multiple\ntimes until the model performance does not\nsignificantly improve.\nOverall, InstructGPT outperforms GPT-3. For\nautomatic evaluations, InstructGPT outperforms\nGPT-3 by 10% on the TruthfulQA (Lin et al.,\n2021) dataset in terms of truthfulness and\nby 7% on the RealToxicityPrompts (Gehman\net al., 2020) in terms of toxicity. On\nNLP datasets (i.e., WSC), InstructGPT achieves\ncomparable performance to GPT-3. For human\nevaluations, regarding four different aspects,\nincluding following correct instructions, following\nexplicit constraints, fewer hallucinations, and\ngenerating appropriate responses, InstructGPT\noutperforms GPT-3 +10%, +20%, -20%, and +10%,\nrespectively.\n4.2 BLOOMZ\nBLOOMZ (176B) (Muennighoff et al., 2022) is\ninitialized with BLOOM (176B) (Scao et al.,\n2022), and then fine-tuned on the instruction", "Correct!\nDoes not exist!\n\u2705\n\"Figure 9: Example of Retrieval-Augmented GPT-4 ,\naccessed on 02/06/2023.\nstep. Dziri et al. [136] observe a positive correlation\nbetween increased diversity in response generation\nand hallucinations.\nThe reason for inducing randomness and diver-\nsity in popular decoding strategies is that gener-\nating the most likely sequence often leads to an\nunsurprising and unnatural text compared to hu-\nman communication [ 489,207,662]. Zhang et al.\n[662] phrase this challenge as a trade-off between\ndiversity and quality. While this challenge re-\nmains largely unsolved, several approaches such\nas diverse beam search [ 567] and confident decod-\ning [ 552] try reducing the induced hallucinations\nat the decoding level.\nUncertainty-Aware Beam Search [ 620]is\nbased on the observation that higher predictive un-\ncertainty corresponds to a larger chance of gener-\nating hallucinations. Therefore, the method intro-\nduces a penalty term in the beam search to penalize", "Inference (\u00a73.4)Lose Visual Attention e.g.OPERA [45], HaELM [104]\nHallucination\nMetrics and\nBenchmarks(\u00a74)Hallucination MetricsCHAIR CHAIR [90]\nPOPE POPE [69]\nLLM-based e.g.GAVIE [73], HaELM [104], HallusionBench [72]\nOthers e.g.Faith-Score [55], AMBER [103]\nHallucination BenchmarksDiscriminative Task e.g.POPE [69], RAH-Bench [16], FGHE [105]\nGenerative Task e.g.GAVIE [73], Faith-Score [55]\nHallucination\nMitigation (\u00a75)Mitigating Data-related\nHallucinations (\u00a75.1)Introducing\nNegative Datae.g.LRV-Instruction [73]\nIntroducing\nCounterfactual Datae.g.HalluciDoctor [117]\nMitigating Noises\nand Errorse.g.ReCaption [105], EOS [120]\nMitigating Model-related\nHallucinations (\u00a75.2)Scale-up Resolution e.g.LLaVA-1.5 [74], InternVL [14], HallE-Switch [123]\nVersatile\nVision Encoderse.g.VCoder [49], IVE [38]\nDedicated Module e.g.HallE-Switch [123]\nMitigating Training-related\nHallucinations (\u00a75.3)Auxiliary SupervisionVisual Supervision e.g.Chen et al. [16]\nContrastive Loss e.g.HACL [52]", "22 Bai, et al.\ncause hallucination. In order to improve the accuracy and reliability of hallucinated content, it is\ncrucial to ensure that MLLMs have access to high-quality and diverse training data. Future research\nshould focus on developing techniques for data collection, augmentation, and calibration. Firstly,\ncollecting enough data at the initial stage is crucial to address the data scarcity issue and increase\ndata diversity. Secondly, data augmentation is an effective solution to further expand the size of data.\nFinally, exploring methods for re-calibrating existing datasets is crucial. This includes eliminating\nbiases, promoting diversity and inclusivity, and mitigating other potential issues that may induce\nhallucinations.\n6.2 Cross-modal Alignment and Consistency\nThe key challenge of multimodal hallucination is the cross-modal consistency issue. Ensuring that\ngenerated content remains consistent and contextually relevant to the input modality requires"], "retrieved_docs_id": ["11d0900242", "066f3ff01c", "e0158da525", "19a4c2c778", "7ed2952b17"], "reranker_type": "colbert", "search_type": "text", "rr": 0.25, "hit": 1}, {"question": "How does Woodpecker, an early attempt on hallucination detection and correction, identify and correct hallucinations?\n", "true_answer": "Woodpecker identifies and corrects hallucinations by extracting key concepts from the generated text and validating them using visual content. It then detects and corrects any hallucinated concepts by asking questions around the extracted concepts.", "source_doc": "hallucination.pdf", "source_id": "b4dda01e19", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 21\n5.4.2 Post-hoc Correction. Post-hoc correction refers to first allowing the MLLM to generate a text\nresponse and then identifying and eliminating hallucinating content, resulting in less hallucinated\noutput. This is usually achieved by grounding on visual content [ 114], pre-trained revisior [ 137],\nand self-revision [63].\nWoodpecker [ 114] is an early attempt on hallucination detection and correction. Similar to how\na woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated\ntext. The key idea of Woodpecker is to extract key concepts from the generated text and validate\nthem using visual content. Subsequently, the hallucinated concepts can be detected and corrected\naccordingly. Specifically, it consists of five stages: 1) Key concept extraction identifies the main objects\nmentioned in the generated sentences; 2) Question formulation asks questions around the extracted", "mentioned in the generated sentences; 2) Question formulation asks questions around the extracted\nobjects; 3) Visual knowledge validation answers the formulated questions via expert models; 4)\nVisual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge\nbase; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence\nunder the guidance of the visual knowledge base. Woodpecker is a training-free method, where\neach component can be implemented using either hand-crafted rules or off-the-shelf pre-trained\nmodels.\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\nSpecifically, inspired by denoising autoencoders [ 101], which are designed to reconstruct clean data\nfrom corrupted input, LURE [ 137] employs a hallucination revisor that aims to transform potentially\nhallucinatory descriptions into accurate ones. To train such a revisor model, a dataset has been", "14 Bai, et al.\nMHaluBench [ 13]This benchmark does not aim to evaluate the MLLMs themselves. Instead, it\nis intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether\na tool can successfully detect the hallucination produced by an MLLM. Thus, the benchmark\nconsists of hallucinatory examples. Specifically, the benchmark unifies image-to-text tasks and the\ntext-to-image tasks into one evaluation suite: cross-modal consistency checking. The hallucinatory\nexamples are generated using leading MLLMs and image generation models, such as LLaVA [ 75],\nMiniGPT-4 [ 138], DALL-E2 [ 89], and DALL-E3 [ 6]. During evaluation, the benchmark can be used\nto compare different hallucination detection methods based on their performance. So far, there are\nnot many dedicated hallucination detection methods. This work serves as a basis for this direction.\nVHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual", "reduce hallucination. Visual context refers to the visual tokens that can be grounded from the\ngenerated text response. An oracle study showed that decoding from the provided optimal visual\ncontexts eliminates over 84.5% of hallucinations. Based on the insight and observation, the authors\ndesigned mechanisms to locate the fine-grained visual information to correct each generated\ntoken that might be hallucinating. This is essentially a visual content-guided decoding strategy.\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that\nutilizes a visual matching score to steer the generation of the final outputs, balancing both object\nhallucination mitigation and text generation quality.\nOthers. The work of OPEAR [ 45] makes an interesting observation that most hallucinations\nare closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,", "deploying LLMs in real-world applications. To alleviate\nthis problem, alignment tuning strategies (as discussed in\nSection 5.2) have been widely utilized in existing work [66],\nwhich rely on tuning LLMs on high-quality data or using\nhuman feedback. Moreover, the integration of external\ntools for the provision of credible information sources can\nhelp alleviate the hallucination issue [81, 602, 659]. Another\nline of research work leverages uncertainty estimation of\nLLMs to identify hallucinations [663, 664]. For instance,\nconsidering that hallucinated facts are prone to exhibit\ninconsistency across different sampled outputs, SelfCheck-\nGPT [664] detects hallucination by measuring information\ninconsistency within sampled outputs. For the evaluation\nof the hallucination problem, a set of hallucination de-\ntection tasks have been proposed, e.g., TruthfulQA [556]\nfor detecting human falsehood mimicked by models. More\nrecently, HaluEval [602] creates a large-scale LLM-generated"], "retrieved_docs_id": ["b4dda01e19", "ceeab98980", "5c89e9ef97", "31eefbd9eb", "13ba88ca8d"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "Which model outperforms GPT-4 in generating EDA tools scripts and as an engineering assistant chatbot?\n", "true_answer": "The fine-tuned LLaMA2 70B model, as demonstrated by ChipEDA (He et al., 2023), outperforms the GPT-4 model on these tasks.", "source_doc": "ChipNemo.pdf", "source_id": "e6b9ba907a", "retrieved_docs": ["processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "Bard, etc.) and open-source (Vicuna (Chiang et al., 2023),\nLLaMA2 (Touvron et al., 2023), etc.) large language mod-\nels (LLM) provide an unprecedented opportunity to help\nautomate these language-related chip design tasks. Indeed,\nearly academic research (Thakur et al., 2023; Blocklove\net al., 2023; He et al., 2023) has explored applications of\nLLMs for generating Register Transfer Level (RTL) code\nthat can perform simple tasks in small design modules as\nwell as generating scripts for EDA tools.\nWe believe that LLMs have the potential to help chip de-\nsign productivity by using generative AI to automate many\nlanguage-related chip design tasks such as code generation,\nresponses to engineering questions via a natural language\ninterface, analysis and report generation, and bug triage. In\nthis study, we focus on three specific LLM applications: an\nengineering assistant chatbot for GPU ASIC and Architec-\nture design engineers, which understands internal hardware"], "retrieved_docs_id": ["e6b9ba907a", "a6c3d05123", "c7d05c4b43", "af6e8c3fb2", "f23b3625e0"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What issue do large language models (LLMs) face that affects their reliability in knowledge-intensive tasks?\n", "true_answer": "LLMs like ChatGPT often struggle with hallucinations, knowledge updates, and data-related issues, which can affect their performance in tasks requiring access to a vast amount of knowledge, such as open-domain question answering and common-sense reasoning.", "source_doc": "RAG.pdf", "source_id": "ee184b2a82", "retrieved_docs": ["Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "overall smaller parameter size, both the retriever and gener-\nator often undergo synchronized end-to-end training or fine-\ntuning [Izacard et al. , 2022 ].\nAfter the emergence of LLM like ChatGPT, generative lan-\nguage models became predominant, showcasing impressive\nperformance across various language tasks [Baiet al. , 2022,\nOpenAI, 2023, Touvron et al. , 2023, Google, 2023 ]. How-\never, LLMs still face challenges such as hallucina-\ntions [Yaoet al. , 2023, Bang et al. , 2023 ], knowledge up-\ndates, and data-related issues. This affects the relia-\nbility of LLMs, making them struggle in certain seri-\nous task scenarios, especially in knowledge-intensive tasks\nrequiring access to a vast amount of knowledge, such\nas open-domain question answering [Chen and Yih, 2020,\nReddy et al. , 2019, Kwiatkowski et al. , 2019 ]and common-\nsense reasoning [Clark et al. , 2019, Bisk et al. , 2020 ]. Im-\nplicit knowledge within parameters may be incomplete and\ninsufficient.", "recently, HaluEval [602] creates a large-scale LLM-generated\nand human-annotated hallucinated samples to evaluate the\nability of language models to recognize hallucination in both\ntask-specific and general scenarios.\nHallucination\nLLMs are prone to generate untruthful informa-\ntion that either conflicts with the existing source\nor cannot be verified by the available source.\nEven the most powerful LLMs such as ChatGPT\nface great challenges in migrating the hallucina-\ntions of the generated texts. This issue can be\npartially alleviated by special approaches such as\nalignment tuning and tool utilization.\n\u2022Knowledge recency . As another major challenge, LLMs\nwould encounter difficulties when solving tasks that require", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "can become capable of solving various real-world tasks.\nTo investigate this problem, a promising approach is to\nstudy the capacity learning (or selection) mechanism based\non unsupervised pre-training, since the model capacity of\nLLMs strongly depends on pre-training data. In addition,\nscaling plays an important role in improving the capacity\nof LLMs [31, 55, 64], and it is very useful to conduct more\ntheoretical analysis about how the behaviors of large models\nrelate to those of small models, e.g., what behaviors of large\nmodels can be inferred from small models and what can\u2019t be\npredicted indeed. Another research direction is to explore\nmore deep analysis on model generalization for LLMs,\nsince increasing concerns have been raised about whether\nLLMs can generalize beyond the knowledge encoded by\npre-training data. Furthermore, data contamination has be-\ncome a severe issue for fairly assessing the performance of\nLLMs [738], and thus setting appropriate evaluation proto-"], "retrieved_docs_id": ["af911eac69", "ee184b2a82", "fa2581a685", "114f3dada8", "72df77630b"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "Which area of application has potential for efficient Multimodal Large Language Models?\n", "true_answer": "Efficient Multimodal Large Language Models have potential for widespread application in academia and industry, especially in edge computing scenarios.", "source_doc": "multimodal.pdf", "source_id": "ac70fcc9f2", "retrieved_docs": ["Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "cient MLLMs, research state of efficient structures and strategies, and the appli-\ncations. Finally, we discuss the limitations of current efficient MLLM research\nand promising future directions. Please refer to our GitHub repository for more\ndetails: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.\n1 Introduction\nLarge-scale pretraining, a leading approach in Artificial Intelligence(AI), has seen general-purpose\nmodels like large language and multimodal models outperform specialized deep learning models\nacross many tasks. The remarkable abilities of Large Language Models (LLM) have inspired efforts\nto merge them with other modality-based models to enhance multimodal competencies. This con-\ncept is further supported by the remarkable success of proprietary models like OpenAI\u2019s GPT-4V [1]\nand Google\u2019s Gemini[2]. As a result, Multimodal Large Language Models (MLLMs) have emerged,\nincluding the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],", "the domain of efficient MLLMs. In addition to the survey, we have established a GitHub repository\nwhere we compile the papers featured in the survey, organizing them with the same taxonomy at\nhttps://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey. We will actively maintain it and\nincorporate new research as it emerges.\n2 Architecture\nFollowing the standard MLLM framework, efficient MLLMs can be divided into three main mod-\nules: a visual encoder gtasked with receiving and processing visual inputs, a pre-trained language\nmodel that manages the received multimodal signals and performs reasoning, and a visual-language\nprojector Pwhich functions as a bridge to align the two modalities. To enhance the efficiency of the\ngeneral MLLMs, the primary optimization lies in handling high-resolution images, compressing vi-\nsion tokens, implementing efficient structures, and utilizing compact language models, among other", "FastV[46], VTW[47]\nTraining (\u00a75)Pre-Training (\u00a75.1) Idefics2[48], TinyLLaV A[23], VILA[49]\nInstruction-Tuning (\u00a75.2) LaVIN[50], HyperLLaV A[51]\nDiverse Training Steps (\u00a75.3) SPHINX-X[14], Cobra[13], TinyGPT-V[28]\nParameter Efficient\nTransfer Learning (\u00a75.4)EAS [52], MemVP [53]\nData and Benchmarks (\u00a76)Pre-Training Data (\u00a76.1)CC595k[7], LLava-1.5-PT[54],\nShareGPT4V-PT[55],\nBunny-pretrain-LAION-2M[24],\nALLaV A-Caption-4V[29], etc.\nInstrcution-Tuning Data (\u00a76.2)LLaV A\u2019s IT[7], LLaV A-1.5\u2019s IT[54],\nShareGPT4V\u2019s IT[55], Bunny-695K[24],\nLVIS-INSTRUCT-4V[56], etc.\nBenchmarks (\u00a76.3)VQAv2[57], TextVQA[58], GQA[59],\nMME[60], MMBench[61], POPE[62]\nApplication (\u00a77)Biomedical Analysis (\u00a77.1) LLaV A-Rad [63], MoE-TinyMed [64]\nDocument Understanding (\u00a77.2)TextHawk [36], TinyChart [37],\nMonkey [65], HRVDA [66]\nVideo Comprehension (\u00a77.3)mPLUG-video [67], Video-LLaV A [44],\nMA-LMM [68], LLaMA-VID [69]\nFigure 2: Organization of efficient multimodal large language models advancements."], "retrieved_docs_id": ["ac70fcc9f2", "542e5c49da", "e021f7788d", "de74717e46", "d85947fa4f"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How much does RAG improve the scores of ChipNeMo-70B-Steer, GPT-4, and LLaMA2-70b-Chat?\n", "true_answer": "RAG improves ChipNeMo-70B-Steer by 0.56, GPT-4 by 1.68, and LLaMA2-70b-Chat by 2.05.", "source_doc": "ChipNemo.pdf", "source_id": "af6e8c3fb2", "retrieved_docs": ["scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "and required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the differencein retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022ChipNeMo-70B-Steer outperforms similar sized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and", "results on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks\nwhere we check for fully accurate code. For \u201cHard\u201d bench-\nmarks in Figure 9 we check for partial correctness of the\ncode, which is evaluated by a human user on a 0-10 scale.\nChipNeMo-70B-Steer performs significantly better than off-\nthe-shelf GPT-4 and LLaMA2-70B-Chat model.\nFigure 8: EDA Script Generation Evaluation Results, Pass@5\nAs seen in Figure 8, models like GPT-4 and LLaMA2-70B-\nChat have close to zero accuracy for the Python tool where\nthe domain knowledge related to APIs of the tool are neces-\nsary. This shows the importance of DAPT. Without DAPT,\nthe model had little to no understanding of the underlying\nAPIs and performed poorly on both automatic and human\nevaluated benchmarks. Our aligned model further improved\nthe results of DAPT because our domain instructional data\nhelps guide the model to present the final script in the most\nuseful manner. An ablation study on inclusion of domain", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ntask assignnment. Participants are tasked with rating the\nmodel\u2019s performance on a 7-point Likert scale for each of\nthese three assignments. The results can be found in Fig-\nure 10. Although the GPT-4 model excels in all three tasks,\noutperforming both our ChipNeMo-70B-Steer model and\nthe LLaMA2-70B-Chat model, ChipNeMo-70B-Steer does\nexhibit enhancements compared to the off-the-shelf LLaMA\nmodel of equivalent size. We attribute the comparatively\nlower improvements in summarization tasks resulting from\nour domain-adaptation to the limited necessity for domain-\nspecific knowledge in summarization compared to other\nuse-cases.\n4. Related Works\nMany domains have a significant amount of proprietary data\nwhich can be used to train a domain-specific LLM. One ap-\nproach is to train a domain specific foundation model from\nscratch, e.g., BloombergGPT(Wu et al., 2023) for finance,\nBioMedLLM(Venigalla et al., 2022) for biomed, and Galac-", "processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving"], "retrieved_docs_id": ["af6e8c3fb2", "1ed1c2ae54", "cf9d13203d", "74fe22ec46", "e6b9ba907a"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Flare decide when to retrieve references during text generation?\n", "true_answer": "Flare decides to retrieve references based on the probability of the generated text. When the probability of a term falls below a predefined threshold, Flare's information retrieval system retrieves references and removes terms with lower probabilities.", "source_doc": "RAG.pdf", "source_id": "b844a74991", "retrieved_docs": ["they can decide to search for a relevant query to collect the\nnecessary materials, similar to the tool call of the agent.\nWebGPT [Nakano et al. , 2021 ]employs a reinforcement\nlearning framework to automatically train the GPT-3 model\nto use a search engine for text generation. It uses special to-\nkens to perform actions, including querying on a search en-\ngine, scrolling rankings, and citing references. This allows\nGPT-3 to leverage a search engine for text generation.\nFlare [Jiang et al. , 2023b ], on the other hand, automates the\ntiming of retrieval and addresses the cost of periodic docu-\nment retrieval based on the probability of the generated text.\nIt uses probability as an indicator of LLMs\u2019 confidence during\nthe generation process. When the probability of a term falls\nbelow a predefined threshold, the information retrieval sys-\ntem would retrieve references and removes terms with lower\nprobabilities. This approach is designed to handle situations", "kens (e.g., kNN-LM [Khandelwal et al. , 2019 ]), phrases (e.g.,\nNPM [Leeet al. , 2020 ], COG [Vaze et al. , 2021 ]), and docu-\nment paragraphs. Finer-grained retrieval units can often bet-\nter handle rare patterns and out-of-domain scenarios but come\nwith an increase in retrieval costs.\nAt the word level, FLARE employs an active retrieval strat-\negy, conducting retrieval only when the LM generates low-\nprobability words. The method involves generating a tempo-\nrary next sentence for retrieval of relevant documents, then\nre-generating the next sentence under the condition of the re-\ntrieved documents to predict subsequent sentences.\nAt the chunk level, RETRO uses the previous chunk to re-\ntrieve the nearest neighboring chunk and integrates this infor-\nmation with the contextual information of the previous chunk\nto guide the generation of the next chunk. RETRO achieves\nthis by retrieving the nearest neighboring block N(Ci\u22121)\nfrom the retrieval database, then fusing the contextual in-", "itized. This method, which involves querying related doc-\numents and inputting into a LLM based on context, may\nlead to efficiency issues. Adaptive retrieval methods such\nas those introduced by Flare [Jiang et al. , 2023b ]and Self-\nRAG [Asai et al. , 2023b ], optimize the RAG retrieval process,\nenabling the LLM to actively judge the timing and content of\nretrieval. This helps to improve the efficiency and relevance\nof the information retrieved.\nIn fact, the way in which LLM actively uses tools and\nmakes judgments is not originated from RAG but has been\nwidely used in the agents of large models [Yang et al. , 2023c,\nSchick et al. , 2023, Zhang, 2023 ]. The retrieval steps\nof Graph-Toolformer [Zhang, 2023 ]are roughly divided\ninto: LLMs actively use the retriever, Self-Ask and\nDSP[Khattab et al. , 2022 ]try to use few-shot prompts to trig-\nger LLM search queries. When LLMs think it is necessary,\nthey can decide to search for a relevant query to collect the", "Recite-Read [Sunet al. , 2022 ]transforms external re-\ntrieval into retrieval from model weights, initially hav-\ning LLM memorize task-relevant information and gener-\nate output for handling knowledge-intensive natural lan-\nguage processing tasks.\n\u2022Adjusting the Flow between Modules In the realm of\nadjusting the flow between modules, there is an empha-\nsis on enhancing interaction between language models\nand retrieval models. DSP [Khattab et al. , 2022 ]intro-\nduces the Demonstrate-Search-predict framework, treat-\ning the context learning system as an explicit program\nrather than a terminal task prompt to address knowledge-\nintensive tasks. ITER-RETGEN [Shao et al. , 2023 ]\nutilizes generated content to guide retrieval, itera-\ntively performing \u201cretrieval-enhanced generation\u201d and\n\u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read-\nRetrieve-Read flow. Self-RAG [Asai et al. , 2023b ]fol-\nlows the decide-retrieve-reflect-read process, introduc-", "probabilities. This approach is designed to handle situations\nwhere LLMs might need additional knowledge.\nSelf-RAG [Asai et al. , 2023b ]introduces an important in-\nnovation called Reflection tokens. These special tokens are\ngenerated to review the output and come in two types: Re-\ntrieve and Critic. The model can autonomously decide when\nto retrieve paragraphs or use a set threshold to trigger re-\ntrieval. When retrieval is needed, the generator processes\nmultiple paragraphs simultaneously, performing fragment-\nlevel beam search to obtain the best sequence. The scores for\neach subdivision are updated using Critic scores, and these\nweights can be adjusted during the inference process to cus-\ntomize the model\u2019s behavior. The Self-RAG framework also\nallows the LLM to autonomously determine whether recall\nis necessary, avoiding training additional classifiers or rely-\ning on NLI models. This enhances the model\u2019s ability to au-\ntonomously judge inputs and generate accurate answers."], "retrieved_docs_id": ["b844a74991", "92f5901d31", "8d605d7952", "dfac20a7d8", "2449b179e1"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the role of GShard in MoE?\n", "true_answer": "GShard is a module composed of a set of lightweight annotation APIs and XLA compiler extensions that offers an elegant way to express various parallel computation patterns while making minimal changes to existing model code, enabling scaling of multi-lingual neural machine translation in MoE.", "source_doc": "multimodal.pdf", "source_id": "a7d187c571", "retrieved_docs": ["model. During the inference phase, given an input, all experts are ranked, and the most relevant\nones are selected for computation. This approach considerably reduces the amount of computa-\ntion, as only a subset of experts is involved in the calculation.By distributing computational tasks\namong different experts, MoE achieves more efficient utilization of computational resources during\nboth training and inference phases. In MoE, each expert has its own set of parameters; however,\nthese parameters are shared during the training process. This parameter-sharing strategy reduces\nthe overall number of parameters in the model, consequently lowering storage and computational\ncosts. GShard [149] is a module composed of a set of lightweight annotation APIs and XLA com-\npiler extensions, which offers an elegant way to express various parallel computation patterns while\nmaking minimal changes to existing model code. It enables us to scale multi-lingual neural machine", "Switch Transformers\nMixture of Experts (MoE), in the context of modern deep learning architectures, was\nproven e\ufb00ective in Shazeer et al. (2017). That work added an MoE layer which was stacked\nbetween LSTM (Hochreiter and Schmidhuber, 1997) layers, and tokens were separately\nrouted to combinations of experts. This resulted in state-of-the-art results in language\nmodeling and machine translation benchmarks. The MoE layer was reintroduced into the\nTransformer architecture by the Mesh Tensor\ufb02ow library (Shazeer et al., 2018) where MoE\nlayers were introduced as a substitute of the FFN layers, however, there were no accom-\npanying NLP results. More recently, through advances in machine learning infrastructure,\nGShard (Lepikhin et al., 2020), which extended the XLA compiler, used the MoE Trans-\nformer to dramatically improve machine translation across 100 languages. Finally Fan et al.\n(2021) chooses a di\ufb00erent deterministic MoE strategy to split the model parameters into", "layer\u2019s execution, tokens meant to be processed by a specific expert are routed to the corresponding\nGPU for processing, and the expert\u2019s output is returned to the original token location. Note that EP\nintroduces challenges in load balancing, as it is essential to distribute the workload evenly across the\nGPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\nIn a Transformer model, the MoE layer is applied independently per token and replaces the\nfeed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU\narchitecture as the expert function Ei(x)and set K= 2. This means each token is routed to two\nSwiGLU sub-blocks with different sets of weights. Taking this all together, the output yfor an input\ntoken xis computed as:\ny=n\u22121X\ni=0Softmax (Top2 (x\u00b7Wg))i\u00b7SwiGLU i(x).\nThis formulation is similar to the GShard architecture [ 21], with the exceptions that we replace all", "FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a\nmore elaborate gating strategy for the second expert assigned to each token.\n3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair\ncomparison. We measure performance on a wide variety of tasks categorized as follow:\n\u2022Commonsense Reasoning (0-shot): Hellaswag [ 32], Winogrande [ 26], PIQA [ 3], SIQA [ 27],\nOpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\n\u2022World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]\n\u2022Reading Comprehension (0-shot): BoolQ [7], QuAC [5]\n\u2022Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4\n\u2022Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)\n\u2022Popular aggregated results: MMLU [ 16] (5-shot), BBH [ 29] (3-shot), and AGI Eval [ 34]\n(3-5-shot, English multiple-choice questions only)", "x, respectively. We can then write the output as\ny=Pn\ni=1G(x)iEi(x). Wherever G(x)i= 0,\nwe do not need to compute Ei(x), thereby saving\ncompute during inference. Lepikhin et al. [298]\nscale up an SG-MoE model to 600B parameters\nby proposing GShard , a model parallelism method\nthat extends the XLA [ 468] compiler. While SG-\nMoE selects the top- kexperts with k > 1, the\nSwitch Transformer (ST) [ 145] architecture uses\nk= 1experts, which reduces routing computation\nand communication across experts (which may be\nlocated on different accelerators). ST empirically\noutperformed a strongly tuned T5 model with up to\n7x pre-training speedups. Lewis et al. [302] notice\nthat the learned routers can result in unbalanced\nassignments across experts. To ensure balanced\nrouting, they formulate a linear assignment prob-\nlem that maximizes token-expert affinities while\nequally distributing the number of tokens across\nexperts. Yu et al. [653] propose sMLP , an MoE"], "retrieved_docs_id": ["a7d187c571", "f698765e41", "db00f64555", "481b718aab", "add767b444"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the main difference in hallucination research between LLMs and MLLMs?\n", "true_answer": "The main difference is that hallucination research in LLMs typically focuses on discrepancies between generated content and real-world facts or user instructions, while research in MLLMs mainly focuses on cross-modal inconsistency between generated text response and provided visual content.", "source_doc": "hallucination.pdf", "source_id": "ce4c90f626", "retrieved_docs": ["The problem of hallucination originates from LLMs themselves. In the NLP community, the\nhallucination problem is empirically categorized into two types [ 44]: 1) factuality hallucination\nemphasizes the discrepancy between generated content and verifiable real-world facts, typically\nmanifesting as factual inconsistency or fabrication; 2) faithfulness hallucination refers to the di-\nvergence of generated content from user instructions or the context provided by the input, as\nwell as self-consistency within generated content. In contrast to pure LLMs, research efforts of\nhallucination in MLLMs mainly focus on the discrepancy between generated text response and\nprovided visual content [69,76,137],i.e., cross-modal inconsistency. This difference suggests that\nstudies in LLMs cannot be seemingly transferred to MLLMs. Therefore, there is a growing need to\ncomprehensively survey recent advancements in MLLMs\u2019 hallucination phenomena to inspire new\nideas and foster the field\u2019s development.", "offer valuable insights that deepen understanding of the opportunities and challenges associated\nwith hallucinations in MLLMs. This exploration not only enhances our understanding of the limita-\ntions of current MLLMs but also offers essential guidance for future research and the development\nof more robust and trustworthy MLLMs.\nComparison with existing surveys. In pursuit of reliable generative AI, hallucination stands\nout as a major challenge, leading to a series of survey papers on its recent advancements. For pure\nLLMs, there are several surveys [ 44,129], describing the landscape of hallucination in LLMs. In\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "HaELM [ 104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4\nmodels to assess the quality of the MLLM response. In contrast, the work of Hallucination Evaluation\nbased on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination\ndetection. It collects a set of hallucination data generated by a wide range of MLLMs, simulates data\nusing ChatGPT, and trains an LLM based on LLaMA [ 99]. After that, the HaELM model becomes\nproficient in hallucination evaluation, leveraging reference descriptions of images as the basis of\nassessment.\nFaithScore [ 55]Considering the natural forms of interaction between humans and MLLMs,\nFaithScore aims to evaluate free-form responses to open-ended questions. Different from LLM-based\noverall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate,\nand analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence", "6.4 Establishing Standardized Benchmarks\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in as-\nsessing the degree of hallucination in MLLMs. In Table 1, it can be observed that there is a variety\nof evaluation benchmarks, but a lack of unified standards. Among them, one of the most popular\nbenchmarks might be POPE [ 69], which employs a \u2019Yes-or-No\u2019 evaluation protocol. However, this\nbinary-QA manner does not align with how humans use MLLMs. Accordingly, some benchmarks\nspecifically evaluate the hallucination of MLLMs in the (free-form) generative context. Yet, they\noften rely on external models, such as vision expert models or other LLMs, which limits their wide-\nspread application. Moving forward, future research can investigate standardized benchmarks that\nare theoretically sound and easy to use. Otherwise, research on methods to mitigate hallucinations\nmay be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\""], "retrieved_docs_id": ["ce4c90f626", "76835931c1", "23d981a684", "312439a972", "f49f3b54ce"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the issue associated with the rapid advancement of multimodal large language models?\n", "true_answer": "These models, also known as large vision-language models, have a tendency to generate hallucinations, producing seemingly plausible but factually inaccurate content.", "source_doc": "hallucination.pdf", "source_id": "da0a465b6c", "retrieved_docs": ["2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "Hallucination of Multimodal Large Language Models: A Survey 17\nobject detection and optical-character recognition models into the MLLM architecture. It also\nexplores various ways to integrate this information, including training-free infusion, LoRA [ 41]\naugmented retraining, and LoRA augmented finetuning.\n5.2.3 Dedicated Module. Following our previous discussion, the parametric knowledge embedded\nin the LLM is identified as a significant factor leading to hallucination, directing the generation to\nbe based on language knowledge instead of visual content. To address this issue, the work of [ 123]\nproposes training a dedicated \" switch \" module, termed HallE-Switch , which controls the extent of\nparametric knowledge within detailed captions. The detailed implementation is inspired by LM-\nswitch [ 33], which involves adding a control parameter \ud835\udf16serving as a \"switching value\". The switch\nmodule is trained using contrastive training data from both contextual (visual content-related) and"], "retrieved_docs_id": ["da0a465b6c", "542e5c49da", "114f3dada8", "92e73c053a", "5c6eccc2bc"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does HyDE generate a hypothetical document relevant to the query?\n", "true_answer": "HyDE generates a hypothetical document relevant to the query by establishing query vectors through the use of text indicators and using these indicators to generate a document that captures the relevant pattern, even if it may not truly exist.", "source_doc": "RAG.pdf", "source_id": "71a4057422", "retrieved_docs": ["the semantic space of the user\u2019s query and documents is very\nnecessary. This section introduces two key technologies to\nachieve this goal.\nQuery Rewrite\nThe most intuitive way to align the semantics of\nquery and document is to rewrite the query. As\nmentioned in Query2Doc [Wang et al. , 2023b ]and ITER-\nRETGEN [Shao et al. , 2023 ], the inherent capabilities of\nlarge language models are utilized to generate a pseudo-\ndocument by guiding it, and then the original query is\nmerged with this pseudo-document to form a new query.\nIn HyDE [Gao et al. , 2022 ], query vectors are established\nthrough the use of text indicators, using these indicators to\ngenerate a \u2019hypothetical\u2019 document that is relevant, yet may\nnot truly exist, it only needs to capture the relevant pattern.\nRRR [Maet al. , 2023a ]introduced a new framework that in-\nverts the order of retrieval and reading, focusing on query\nrewriting. This method generates a query using a large lan-", "different scenarios, including using query engines pro-\nvided by frameworks like LlamaIndex, employing tree\nqueries, utilizing vector queries, or employing the most\nbasic sequential querying of chunks.\u2022HyDE: This approach is grounded on the assumption\nthat the generated answers may be closer in the embed-\nding space than a direct query. Utilizing LLM, HyDE\ngenerates a hypothetical document (answer) in response\nto a query, embeds the document, and employs this em-\nbedding to retrieve real documents similar to the hypo-\nthetical one. In contrast to seeking embedding similarity\nbased on the query, this method emphasizes the embed-\nding similarity from answer to answer. However, it may\nnot consistently yield favorable results, particularly in\ninstances where the language model is unfamiliar with\nthe discussed topic, potentially leading to an increased\ngeneration of error-prone instances.\nModular RAG\nThe modular RAG structure breaks away from the traditional", "Summarization XSumPlease generate a one-sentence summary for the given document. 21.71\n42.08 [740] {document }Try your best to summarize the main content of the given\ndocument. And generate a short summary in 1 sentence for it. \\n\nSummary:23.01\nKUClosed-Book QA ARCChoose your answer to the question. {query} {options } 85.19\n92.00 [741]\nChoose a correct answer according to the given question, and output\nthe corresponding id, do not answer other content except the answer\nid.85.86\nOpen-Book QA OBQAChoose your answer to the question: {question } {choices }. You must\nonly output A, B, C, or D without any extra explanation. The answer\nis81.20\n87.20 [741]\nFollowing is a question that requires multi-step reasoning, use\nof additional common and commonsense knowledge, and rich text\ncomprehension. Choose your answer to the question: \\n Question:\nFrilled sharks and angler fish live far beneath the surface of the\nocean, which is why they are known as \\n Choices: \\n A. Deep sea", "information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-", "cross-attention scores, selecting the highest scoring input to-\nkens to effectively filter tokens. RECOMP [Xuet al. , 2023a ]\nproposes extractive and generative compressors, which gen-\nerate summaries by selecting relevant sentences or syn-\nthesizing document information to achieve multi-document\nquery focus summaries.In addition to that, a novel approach,\nPKG [Luoet al. , 2023 ], infuses knowledge into a white-box\nmodel through directive fine-tuning, and directly replaces the\nretriever module, used to directly output relevant documents\nbased on the query.\n5 Generator\nAnother core component in RAG is the generator, responsible\nfor transforming retrieved information into natural and fluent\ntext. Its design is inspired by traditional language models,\nbut in comparison to conventional generative models, RAG\u2019s\ngenerator enhances accuracy and relevance by leveraging the\nretrieved information. In RAG, the generator\u2019s input includes"], "retrieved_docs_id": ["71a4057422", "d96393bb4b", "3f3798f19a", "7fabdba415", "cd69a480bb"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a model that tailors for medical applications and significantly lowers parameter demands?\n", "true_answer": "MoE-TinyMed [64]", "source_doc": "multimodal.pdf", "source_id": "51c7c3d212", "retrieved_docs": ["paradigm in biomedicine, achieving state-of-the-art results on many applications, including medical\nquestion answering [194] and medical image classification [195]. Recently, multimodal generative\nAI has emerged as an exciting frontier in the biomedical domain, expanding the application scope\nfrom single-modality to multi-modality, such as VQA and radiology report generation.\nThe mixture of Expert Tuning has effectively enhanced the performance of general MLLMs with\nfewer parameters, yet its application in resource-limited medical settings has not been fully explored.\nMoE-TinyMed [64] is a model tailored for medical applications that significantly lower parameter\ndemands. LLaV A-Rad [63] is a state-of-the-art tool that demonstrates rapid performance on a sin-\ngle V100 GPU in private settings, making it highly applicable for real-world clinical scenarios. It\nemploys a modular approach, integrating unimodal pre-trained models and emphasizing the training", "retrieving external knowledge databases, such\nas the Disease Database and Wikipedia retrieval,\nduring doctor-patient conversations to obtain more\naccurate outputs from the model. The ChatDoctor\nsignificantly improves the model\u2019s ability to\ncomprehend patient needs and provide informed\nadvice. By equipping the model with self-directed\ninformation retrieval from reliable online and\noffline sources, the accuracy of its responses is\nsubstantially improved.\nChatGLM-Med (Haochun Wang, 2023) is\nfine-tuned on the Chinese medical instruction\ndataset based on the ChatGLM-6B (Du et al.,\n2022) model. The instruction dataset comprises\nmedically relevant question and answer pairs,\ncreated using the GPT 3.5 API and the Medical\nKnowledge Graph. This model improves the\nquestion-answering performance of ChatGLM (Du\net al., 2022) in the medical field.\n6.7 Arithmetic\nGoat (Liu and Low, 2023) is a fine-tuned\nLLaMA-7B (Touvron et al., 2023a) model based\non instructions, which aims to solve arithmetic", "Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu,\nJing Gao, Ahmed Hassan Awadallah, and Jian-\nfeng Gao. 2022. Adamix: Mixture-of-adapter for\nparameter-efficient tuning of large language models.\nArXiv , abs/2205.12410.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and\nZhang You. 2023. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain\nknowledge. arXiv preprint arXiv:2303.14070 .", "performance as traditional fine-tuning approaches. This creates a challenging trade-off between maximizing efficiency\nand maintaining high model quality.\nParameter-Efficient Fine-Tuning (PEFT) methods [ 11] have emerged as a valuable approach to facilitate the\nefficient adaptation of pre-trained language models (PLMs) for diverse downstream applications. PEFT methods\naddress this challenge by selectively fine-tuning only a small subset of additional model parameters. As a result, the\ncomputational and storage costs associated with PEFT are significantly reduced. Notably, recent advancements in PEFT\nhave demonstrated remarkable performance comparable to that achieved through full fine-tuning. This highlights the\neffectiveness of PEFT methods in striking a balance between computational efficiency and maintaining competitive\nmodel performance. By enabling efficient adaptation of PLMs without the need to fine-tune all parameters, PEFT", "including the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],\nMiniGPT-v2 [9], and MiniGPT-4[10]. These models circumvent the computational cost of train-\ning from scratch by effectively leveraging the pre-training knowledge of each modality. MLLMs\ninherit the cognitive capabilities of LLMs, showcasing numerous remarkable features such as robust\nlanguage generation and transfer learning abilities. Moreover, by establishing strong representa-\ntional connections and alignments with other modality-based models, MLLMs can process inputs\nfrom multiple modalities, significantly broadening their application scope.\nThe success of MLLMs is largely attributed to the scaling law: the performance of an AI model\nimproves as more resources, such as data, computational power, or model size, are invested into it.\nHowever, scalability comes at the cost of high resource demands, which hinders the development"], "retrieved_docs_id": ["51c7c3d212", "cbf528979b", "03748e7ce7", "e2bd360036", "7a547e4fbb"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can increasing the scale of data affect bias in a model?\n", "true_answer": "Increasing the scale of data can alleviate the effect of bias in a model, but it cannot fully resolve it due to the long-tail distribution of the real world.", "source_doc": "hallucination.pdf", "source_id": "44cf8ffcb0", "retrieved_docs": ["dataset has strong effects on the behavior of the model. Frequently appeared objects and object\nco-occurrence are two prominent types of statistical bias, as discussed in [ 69,90,137]. For example,\n\u2018person \u2019 might be one of the most frequently appearing objects in the training data. During inference,\neven if the given image does not contain a person, the model still tends to predict the presence\nof a person. On the other hand, object co-occurrence refers to the phenomenon that the model\nwill remember which two objects usually \u2018go together\u2019 [ 90]. For instance, given an image of a\nkitchen with a refrigerator, MLLMs are prone to answer \u2018 Yes\u2019 when asked about a microwave, as\nrefrigerators and microwaves frequently appear together in kitchen scenes. Bias exists in most\ndatasets. Increasing the scale of data may alleviate the effect, but cannot fully resolve it, given the\nlong-tail distribution of the real world.\n3.2 Model", "stream tasks. To further explore scaling effect, a potential\nissue is that the amount of available data for training LLMs\nis actually limited. With the ever-increasing model scale, the\npublic text data would be soon \u201cexhausted\u201d for LLMs [60].\nThus, it will be meaningful to study how scaling laws apply\nto a data-constrained regime [61], where data repetition or\naugmentation might be useful to alleviate data scarcity.\n\u2022Task-level predictability . Existing research of scaling laws\nare mostly conducted in terms of language modeling loss\n(e.g., per-token cross-entropy loss in nats [30]), while in\npractice we are more concerned about the performance of\nLLMs on actual tasks. Thus, a basic problem is that how\nthe decrease of language modeling loss translates into the\nimprovement of task performance [58]. Intuitively, a model\nwith a smaller language modeling loss tends to yield a\nbetter performance on downstream tasks, since language\nmodeling loss can be considered as a general measure of", "We use these key properties of Pythia in order to study for\nthe first time how properties like gender bias, memorization,\nand few-shot learning are affected by the precise training\ndata processed and model scale. We intend the following ex-\nperiments to be case studies demonstrating the experimental\nsetups Pythia enables, and to additionally provide directions\nfor future work.\nMitigating Gender Bias There is much work cataloging\nhow language models reflect the biases encoded in their\ntraining data. However, while some work has explored\nfinetuning\u2019s effects on bias in language models (Gira et al.,\n2022; Kirtane et al., 2022; Choenni et al., 2021), or the\nrelationship between the corpus statistics and the measured\nbias (Bordia & Bowman, 2019; Van der Wal et al., 2022b),\nresearchers have generally lacked the tools to study the role\nof the training data on the learning dynamics of bias in large\nlanguage models of different sizes. To demonstrate what is", "biases even within the studied categories.\nBroadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to re\ufb02ect stereotypes\npresent in their training data. Below we discuss our preliminary \ufb01ndings of bias along the dimensions of gender, race,\nand religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how\nthey are different in this dimension.\n6.2.1 Gender\nIn our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found\nthat occupations in general have a higher probability of being followed by a male gender identi\ufb01er than a female one\n(in other words, they are male leaning) when given a context such as \"The{occupation}was a\" (Neutral Variant).\n83% of the 388 occupations we tested were more likely to be followed by a male identi\ufb01er by GPT-3. We measured", "7 Conclusion\nWe present a rigorous study of the effect of how various optimization choices affect INT8 PTQ with\nthe goal of reconciling the recent contradictory observations regarding emergent properties in Large\nLanguage Models. We show that regularization directly impacts PTQ performance and that higher\nlevels of regularization through common techniques such as weight-decay, and gradient-clipping\nleads to lower post-training quantization degradation. We further demonstrate that the choice of\nhalf-precision training data type has a significant impact on PTQ performance \u2013 emergent features\nare significantly less pronounced when training with bf16.\nBroader Impact Our work serves as a useful counter-example to scholarship which has advanced\nthe notion that certain properties depend only on model scale (Wei et al., 2022a). Rather, our\nresults support the conclusion that optimization choices play a large role in whether emergent"], "retrieved_docs_id": ["44cf8ffcb0", "5d3c3d184a", "a72d5b292c", "8b58be435d", "b89847c44e"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the challenge faced by MLLMs in tasks requiring intricate recognition?\n", "true_answer": "MLLMs face challenges in tasks like crowd counting and OCR of small characters, which require detailed recognition.", "source_doc": "multimodal.pdf", "source_id": "8beea9b82e", "retrieved_docs": ["tion answering and image captioning. However, MLLMs face considerable challenges in tasks ne-\ncessitating intricate recognition, including crowd counting and OCR of small characters. A direct\napproach to address these challenges involves increasing the image resolution, practically, the num-\nber of visual tokens. This strategy, nonetheless, imposes a substantial computational burden on\nMLLMs, primarily due to the quadratic scaling of computational costs with the number of input to-\nkens in the Transformer architecture. Motivated by this challenge, vision token compression, aimed\nto reduce the prohibitive computation budget caused by numerous tokens, has become an essential\naspect of efficient MLLMs. We will explore this topic through several key techniques, including\nmulti-view input, token processing, multi-scale information fusion, vision expert agents and video-\nspecific methods.\nMulti-view Input Directly employing high-resolution vision encoders for fine-grained percep-", "robust evaluation of object hallucination, POPE [844] pro-\nposes a polling-based object probing approach for convert-\ning object recognition into a series of binary questions, and\nthe results indicate that current MLLMs often struggle with\nobject hallucination. Cognition tasks, on the other hand, re-\nquire MLLMs to perform reasoning based on image percep-\ntion. A common reasoning task is visual question answering\n(VQA), where models answer questions about images that\ndemand reasoning about spatial relationships [845], general\nknowledge [846], or scene text [847]. To fully explore the\ncapabilities of MLLMs, HallusionBench [848] collects 200\nsophisticated visual dependent or supplement questions, on\nwhich even the most advanced MLLMs like LLaVA-1.5 [831]\nand GPT-4V [133] fail to achieve good performance.\n\u2022Evaluation paradigms. The responses of MLLMs can\nbe evaluated either in a closed-ended or an open-ended\nmanner. Traditional multimodal tasks often rely on a closed-", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "ideas and foster the field\u2019s development.\nIn the realm of computer vision, object recognition is the core task, including sub-tasks such as\nobject classification [ 60], detection [ 27], and segmentation [ 37], etc. Similarly, studies on halluci-\nnation in MLLMs primarily focus on object hallucination. In pre-MLLM era, there is a pioneering\nwork on object hallucination in image captioning [ 90], evaluating object existence by comparing\ncaptions and image content. In MLLMs, object hallucination has been empirically categorized into\nthree categories: 1) category , which identifies nonexistent or incorrect object categories in the given\nimage; 2) attribute , which emphasizes descriptions of the objects\u2019 attributes, such as color, shape,\nmaterial, etc; and 3) relation , which assesses the relationships among objects, such as human-object\ninteractions or relative positions. Note that some literature may consider objects counting, objects", "even OCR-free NLP (directly fed with document images), (ii) perception-language\ntasks, including multimodal dialogue, image captioning, visual question answering,\nand (iii) vision tasks, such as image recognition with descriptions (specifying\nclassi\ufb01cation via text instructions). We also show that MLLMs can bene\ufb01t from\ncross-modal transfer, i.e., transfer knowledge from language to multimodal, and\nfrom multimodal to language. In addition, we introduce a dataset of Raven IQ test,\nwhich diagnoses the nonverbal reasoning capability of MLLMs.\nInput Prompt CompletionQuestion: Explain why \nthis photo is funny? \nAnswer:\nThe cat is wearing a \nmask that gives the \ncat a smile.\nQuestion: Why did the \nlittle boy cry? Answer:\nBecause his scooter \nbroke.Question: What is the \nhairstyle of the blond \ncalled? Answer:\npony tailQuestion: When will the \nmovie be released? \nAnswer:\nOn June 27\n5 + 4 = 9Question: The result \nis? Answer:What is TorchScale ?\nA library that allows"], "retrieved_docs_id": ["8beea9b82e", "736e8a6bfb", "114f3dada8", "595dbaf855", "1da9964ec4"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Mixture of Experts (MoE) decompose a large-scale model?\n", "true_answer": "MoE decomposes a large-scale model into several smaller models, each focusing on learning a specific part of the input data.", "source_doc": "multimodal.pdf", "source_id": "811840a2cd", "retrieved_docs": ["into a high-dimensional space, where task-related information can be more readily captured. In\nthis new space, each word in the text sequence is represented as a high-dimensional vector, and the\ndistances between these vectors serve to measure their similarities. Low-Rank [147] aims to decom-\npose a high-dimensional matrix into the product of two lower-dimensional matrices. Consequently,\nby calculating the inverses of these two lower-dimensional matrices, an approximate inverse of the\nattention matrix can be obtained, thereby significantly reducing computational complexity.\n4.2 Framework\nMixture of Experts The core idea behind MoE [89] is to decompose a large-scale model into sev-\neral smaller models, each of which focuses on learning a specific part of the input data. During the\ntraining process, each expert is assigned a weight that determines its importance within the overall\nmodel. During the inference phase, given an input, all experts are ranked, and the most relevant", "resentative LLMs based on prefix decoders include GLM-\n130B [93] and U-PaLM [118].\nMixture-of-Experts. For the above three types of archi-\ntectures, we can further extend them via the mixture-of-\nexperts (MoE) scaling, in which a subset of neural network\nweights for each input are sparsely activated, e.g., Switch\nTransformer [25] and GLaM [112]. The major merit is that\nMoE is a flexible way to scale up the model parameter while\nmaintaining a constant computational cost [25]. It has been\nshown that substantial performance improvement can be\nobserved by increasing either the number of experts or the\ntotal parameter size [246]. Despite the merits, training large\nMoE models may suffer from instability issues due to the\ncomplex, hard-switching nature of the routing operation.\nTo enhance the training stability of MoE-based language\nmodels, techniques such as selectively using high-precision\ntensors in the routing module or initializing the model with", "\u2022Model capacity is most critical for very large data sets. The existing literature on condi-\ntional computation deals with relatively small image recognition data sets consisting of up\nto 600,000 images. It is hard to imagine that the labels of these images provide a suf\ufb01cient\nsignal to adequately train a model with millions, let alone billions of parameters.\nIn this work, we for the \ufb01rst time address all of the above challenges and \ufb01nally realize the promise\nof conditional computation. We obtain greater than 1000x improvements in model capacity with\nonly minor losses in computational ef\ufb01ciency and signi\ufb01cantly advance the state-of-the-art results\non public language modeling and translation data sets.\n1.2 O URAPPROACH : THESPARSELY -GATED MIXTURE -OF-EXPERTS LAYER\nOur approach to conditional computation is to introduce a new type of general purpose neural net-\nwork component: a Sparsely-Gated Mixture-of-Experts Layer (MoE). The MoE consists of a num-", "computation, achieving greater than 1000x improvements in model capacity with\nonly minor losses in computational ef\ufb01ciency on modern GPU clusters. We in-\ntroduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to\nthousands of feed-forward sub-networks. A trainable gating network determines\na sparse combination of these experts to use for each example. We apply the MoE\nto the tasks of language modeling and machine translation, where model capacity\nis critical for absorbing the vast quantities of knowledge available in the training\ncorpora. We present model architectures in which a MoE with up to 137 billion\nparameters is applied convolutionally between stacked LSTM layers. On large\nlanguage modeling and machine translation benchmarks, these models achieve\nsigni\ufb01cantly better results than state-of-the-art at lower computational cost.\n1 I NTRODUCTION AND RELATED WORK\n1.1 C ONDITIONAL COMPUTATION", "Under review as a conference paper at ICLR 2017\nResults: Results are reported in Table 6. All the combinations containing at least one the two\nlosses led to very similar model quality, where having no loss was much worse. Models with higher\nvalues ofwloadhad lower loads on the most overloaded expert.\nB H IERACHICAL MIXTURE OF EXPERTS\nIf the number of experts is very large, we can reduce the branching factor by using a two-level\nhierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted com-\nbination of \u201cexperts\", each of which is itself a secondary mixture-of-experts with its own gating\nnetwork.3If the hierarchical MoE consists of agroups ofbexperts each, we denote the primary gat-\ning network by Gprimary , the secondary gating networks by (G1,G2..Ga), and the expert networks\nby(E0,0,E0,1..Ea,b). The output of the MoE is given by:\nyH=a\u2211\ni=1b\u2211\nj=1Gprimary (x)i\u00b7Gi(x)j\u00b7Ei,j(x) (12)\nOur metrics of expert utilization change to the following:"], "retrieved_docs_id": ["811840a2cd", "3c084f5868", "0a345ecf77", "1ddecce079", "5f55e9e161"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the name of the concurrent work that also uses preference-based reinforcement learning to improve the faithfulness of MLLMs?\n", "true_answer": "Silkie [68]", "source_doc": "hallucination.pdf", "source_id": "0b66cff7c9", "retrieved_docs": ["samples) and hallucinatory descriptions (negative samples). HA-DPO then trains the model using\nthese sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions. This\ngoal is achieved through direction preference optimization (DPO), which optimizes a specific loss\nfunction designed to maximize the model\u2019s preference for positive samples while minimizing its\npreference for negative samples.\nA concurrent work, Silkie [ 68], introduces a similar approach of utilizing preference-based\nreinforcement learning to enhance the faithfulness of MLLMs. Specifically, it emphasizes the\nconcept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more\nrobust MLLM, i.e., GPT-4V [ 83]. Responses are first generated by models from 12 MLLMs, and then\nassessed by GPT-4V. The constructed dataset, termed as VLFeedback, contains preferences distilled\nfrom GPT-4V and is utilized to train other MLLMs through direct preference optimization.", "18 Bai, et al.\nOptimization, 2) Reinforcement Learning from AI Feedback, 3) Reinforcement Learning from\nHuman Feedback.\nAutomatic Metric-based Optimization. Motivated by the limitation of LLMs (and MLLMs) training,\nwhich is unable to optimize at the sequence level, the MOCHa [ 5] framework is proposed to\napply reinforcement learning. This work aims to improve the accuracy and relevance of image\ncaptioning, thereby reducing hallucination. The framework introduces three metric-based objectives\nto guide the reinforcement learning process for image captioning: 1) Natural Language Inference\n(NLI) for fidelity, focusing on the accuracy of the caption in describing the image content; 2)\nBERTScore [ 127] for semantic adequacy, assessing the relevance and richness of the description;\nand 3) Kullback\u2013Leibler (KL) divergence for regularization, which constrains the model to stay\nclose to its initial policy. The framework incorporates these objectives into a multi-objective reward", "from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based\nmethods. These methods rely on the traditional preference data generation process in LLMs, where\nboth preferred and dispreferred responses may potentially be incorrect. Therefore, this work pro-\nposes the Preference Optimization in VLLM with AI-Generated Dispreferences (POVID) framework,\naiming to exclusively generate dispreferred feedback data using AI models. The dispreferred data\nis generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and\n2) provoking inherent hallucination by introducing noise into MLLMs. In the DPO optimization\nframework, the ground-truth multimodal instructions serves as the preferred answers.\nReinforcement Learning from Human Feedback (RLHF). HalDetect [ 32] first introduces the M-", "to the data filtering strategy. This is achieved by simply modifying the Maximum Likelihood\nEstimation (MLE), enabling the model to mitigate hallucination through learning from regular\ninstruction data.\n5.3.2 Reinforcement Learning. Reinforcement learning (RL) is introduced to train MLLMs for\nmitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "function for reinforcement learning. Subsequently, the proximal policy optimization reinforcement\nlearning algorithm is employed to maximize the expected reward. By promoting the creation of\naccurate, contextually appropriate, and varied descriptions, the hallucination of MLLM can be\nmitigated.\nReinforcement Learning from AI Feedback (RLAIF). HA-DPO [ 133] addresses hallucination as a\npreference selection problem by training models to prioritize accurate responses over hallucinatory\nones. To achieve this goal, HA-DPO initially constructs a high-quality dataset. Specifically, it first\nutilizes MLLMs to generate descriptions corresponding to images, then employs GPT-4 to detect\nwhether these descriptions contain hallucinations. If hallucinations are detected, the descriptions\nare rewritten. Thus, HA-DPO constructs a dataset that includes both accurate descriptions (positive\nsamples) and hallucinatory descriptions (negative samples). HA-DPO then trains the model using"], "retrieved_docs_id": ["0b66cff7c9", "9ab3536359", "4ed1c08405", "11d0900242", "f9a4bc3b58"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can the CHAIR metric be affected in the context of MLLMs?\n", "true_answer": "The CHAIR metric can be affected by the instruction designs and the length of generated captions in the context of MLLMs.", "source_doc": "hallucination.pdf", "source_id": "84a3c00c17", "retrieved_docs": ["MHaluBench [13] arXiv\u201924 Feb. MSCOCO [70] 1,860 Gen Acc/P/R/F \u2713 \u2713 \u2717 T2I\nVHTest [46] arXiv\u201924 Feb. MSCOCO [70] 1,200 Dis & Gen Acc \u2713 \u2713 \u2717 \u2713\nHal-Eavl [53] arXiv\u201924 Feb.MSCOCO [70] &\nLAION [92]10,000 Dis & GenAcc/P/R/F &\nLLM Assessment\u2713 \u2713 \u2713 Obj. Event\n(denoted as CHAIR \ud835\udc60):\nCHAIR \ud835\udc56=|{hallucinated objects }|\n|{all objects mentioned }|,\nCHAIR \ud835\udc60=|{sentences with hallucinated object }|\n|{all sentences}|.\nIn the paper of CHAIR [ 90], the range of objects is restricted to the 80 MSCOCO objects. Sentence\ntokenization and synonyms mapping are applied to determine whether a generated sentence\ncontains hallucinated objects. Ground-truth caption and object segmentations both serve as ground-\ntruth objects in the computation. In the MLLM era, this metric is still widely used for assessing the\nresponse of MLLMs.\nPOPE [ 69]. When used in MLLMs, the work of [ 69] argues that the CHAIR metric can be\naffected by the instruction designs and the length of generated captions. Therefore, it proposes a", "tokens. The issue of \u2019losing attention\u2019 would also lead to the model\u2019s output response being\nirrelevant to the visual content.\n4 HALLUCINATION METRICS AND BENCHMARKS\nIn this section, we present a comprehensive overview of existing hallucination metrics and bench-\nmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge\nMLLMs. Currently, the primary focus of these benchmarks is on evaluating the object hallucination\nof MLLM-generated content. Tab. 1 illustrates a summary of related benchmarks.\nCHAIR [ 90]. As one of the early works, the metric of CHAIR was proposed to evaluate ob-\nject hallucination in the traditional image captioning task. This is achieved by computing what\nproportion of words generated are actually in the image according to the ground truth sentences\nand object segmentations. The computation of the CHAIR metric is straightforward and easy", "affected by the instruction designs and the length of generated captions. Therefore, it proposes a\nnew evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation\n(POPE). The basic idea is to convert the evaluation of hallucination into a binary classification task\nby prompting MLLMs with simple Yes-or-Noshort questions about the probing objects ( e.g., Is there\nacarin the image?) Compared to CHAIR, POPE offers increased stability and flexibility. Based on\nthis metric design, it further proposed an evaluation benchmark, drawing 500 images from the\nMSCOCO dataset. The questions in the benchmark consist of both positive and negative questions.\nThe positive questions are formed based on the ground-truth objects, while the negative questions\nare built from sampling nonexistent objects. The benchmark is divided into three subsets according\nto different negative sampling strategy: random, popular, and adversarial. Popular and adversarial", "The Visual Dependent questions are defined as questions that do not have an affirmative answer\nwithout the visual context. This setting aims to evaluate visual commonsense knowledge and visual\nreasoning skills. The Visual Supplement questions can be answered without the visual input; the\nvisual component merely provides supplemental information or corrections. This setting is designed\nto evaluate visual reasoning ability and the balance between parametric memory (language prior)\nand image context. This division provides a new perspective for understanding and diagnosing\nMLLMs.\nCCEval [ 123]CCEval focuses on the hallucination evaluation of detailed captions. Traditional\ncaption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\nHowever, short captions often lack detail and contain less information. To address this issue, CCEval\nrandomly samples 100 images from Visual Genome to form a benchmark. In evaluation, GPT-4", "6.4 Establishing Standardized Benchmarks\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in as-\nsessing the degree of hallucination in MLLMs. In Table 1, it can be observed that there is a variety\nof evaluation benchmarks, but a lack of unified standards. Among them, one of the most popular\nbenchmarks might be POPE [ 69], which employs a \u2019Yes-or-No\u2019 evaluation protocol. However, this\nbinary-QA manner does not align with how humans use MLLMs. Accordingly, some benchmarks\nspecifically evaluate the hallucination of MLLMs in the (free-form) generative context. Yet, they\noften rely on external models, such as vision expert models or other LLMs, which limits their wide-\nspread application. Moving forward, future research can investigate standardized benchmarks that\nare theoretically sound and easy to use. Otherwise, research on methods to mitigate hallucinations\nmay be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature"], "retrieved_docs_id": ["84a3c00c17", "52c95dc6e8", "d3d3d6a133", "4e7d38fc3d", "312439a972"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "Why is parametric knowledge not updated dynamically in this model?\n", "true_answer": "The model parameters cannot be updated dynamically, making the parametric knowledge susceptible to becoming outdated over time.", "source_doc": "RAG.pdf", "source_id": "9ebdac13a1", "retrieved_docs": ["and more specific knowledge. Secondly, since the model\nparameters cannot be updated dynamically, the parametric\nknowledge is susceptible to becoming outdated over time.\nLastly, an expansion in parameters leads to increased com-arXiv:2312.10997v1  [cs.CL]  18 Dec 2023", "can be updated during inference time to reflect\nan updated state of the underlying knowledge.\nE.g., Lewis et al. [304] demonstrate that swapping\ntheir model\u2019s non-parametric memory with an up-\ndated version enabled it to answer questions about\nworld leaders who had changed between the mem-\nory collection dates. Similarly, Izacard et al. [236]\ndemonstrate that their retrieval-augmented model\ncan update its knowledge forward and backward in\ntime by swapping the index.\n2.11 Brittle Evaluations\nOne reason why the evaluation of language models\nis a challenging problem is that they have an un-\neven capabilities surface \u2014a model might be able\nto solve a benchmark problem without issues, but\na slight modification of the problem (or even a sim-\nple change of the prompt) can give the opposite\nresult [ 675,342,533] (see Section 2.7). Unlike\nhumans, we cannot easily infer that an LLM that\ncan solve one problem will have other related capa-\nbilities. This means that it is difficult to assess the", "play distinct roles. Parametric knowledge is acquired through\ntraining LLMs and stored in the neural network weights, rep-\nresenting the model\u2019s understanding and generalization of\nthe training data, forming the foundation for generated re-\nsponses. Non-parametric knowledge, on the other hand, re-\nsides in external knowledge sources such as vector databases,\nnot encoded directly into the model but treated as updatable\nsupplementary information. Non-parametric knowledge em-\npowers LLMs to access and leverage the latest or domain-\nspecific information, enhancing the accuracy and relevance\nof responses.\nPurely parameterized language models (LLMs) store their\nworld knowledge, which is acquired from vast corpora, in\nthe parameters of the model. Nevertheless, such models have\ntheir limitations. Firstly, it is difficult to retain all the knowl-\nedge from the training corpus, especially for less common\nand more specific knowledge. Secondly, since the model", "to be still at a superficial level. In addition, existing studies\nalso explore editing parameters of language models to up-\ndate intrinsic knowledge [669\u2013671]. Nevertheless, previous\nwork [672] has shown that several parameter editing meth-\nods perform not well on LLMs, though they can improve\nthe performance of small language models. Therefore, it\nis still difficult to directly amend intrinsic knowledge or\ninject specific knowledge into LLMs, which remains an\nopen research problem [672]. Recently, a useful framework\nEasyEdit [673] has been released to facilitate the research of\nknowledge editing for LLMs.\nKnowledge Recency\nThe parametric knowledge of LLMs is hard to be\nupdated in a timely manner. Augmenting LLMs\nwith external knowledge sources is a practical\napproach to tackling the issue. However, how\nto effectively update knowledge within LLMs\nremains an open research problem.\n7.1.3 Complex Reasoning\nComplex reasoning refers to the ability of understanding", "numerous shortcomings. They often fabricate\nfacts [Zhang et al. , 2023b ]and lack knowledge when\ndealing with specific domains or highly specialized\nqueries [Kandpal et al. , 2023 ]. For instance, when the infor-\nmation sought extends beyond the model\u2019s training data or\nrequires the latest data, LLM may fail to provide accurate\nanswers. This limitation poses challenges when deploying\ngenerative artificial intelligence in real-world production\nenvironments, as blindly using a black-box LLM may not\nsuffice.\nTraditionally, neural networks adapt to specific domains\nor proprietary information by fine-tuning models to param-\neterize knowledge. While this technique yields significant\nresults, it demands substantial computational resources, in-\ncurs high costs, and requires specialized technical expertise,\nmaking it less adaptable to the evolving information land-\nscape. Parametric knowledge and non-parametric knowledge\nplay distinct roles. Parametric knowledge is acquired through"], "retrieved_docs_id": ["9ebdac13a1", "e1118fca72", "fc82ce8e28", "66aa6c0fd1", "b40c0db2f1"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "Which model outperforms both GPT-4V and Med-PaLM in terms of efficiency and effectiveness?\n", "true_answer": "LLaV A-Rad", "source_doc": "multimodal.pdf", "source_id": "0da5fa4a36", "retrieved_docs": ["employs a modular approach, integrating unimodal pre-trained models and emphasizing the training\nof lightweight adapters. As a result, LLaV A-Rad outperforms larger models such as GPT-4V and\nMed-PaLM in terms of standard metrics, showcasing its superior efficiency and effectiveness.\n22", "As for the 13B model, it could not be trained with AdamW on the available 8 RTX 3090 GPUs due to\nmemory limitations. In this scenario where model parallelism is necessary for LOMO, LOMO still\noutperforms SGD in terms of throughput. This advantage is attributed to LOMO\u2019s memory-efficient\nproperties and the requirement of only two GPUs to train the model with the same settings, resulting\nin reduced communication costs and greater throughput. Furthermore, when training the 30B model,\nSGD encounters out-of-memory (OOM) issues with the 8 RTX 3090 GPUs, while LOMO performs\nwell with only 4 GPUs.\nFinally, we successfully train the 65B model using 8 RTX 3090 GPUs, achieving a throughput of\n4.93 TGS. Utilizing such a server configuration and LOMO, the training process on 1000 samples,\neach containing 512 tokens, requires approximately 3.6 hours.\n4.3 D OWNSTREAM PERFORMANCE\nTo assess the effectiveness of LOMO in fine-tuning large language models, we conduct an extensive", "independently for each answer. The MC1 accuracy results are shown in Figure 4 (left). We can see\nthat with a modified ranking approach, Dromedary significantly outperforms the powerful GPT-4\nmodel and other baselines, achieving a new state-of-the-art MC1 accuracy of 69.\nIn the generation task, models generate full-sentence answers given the question. The benchmark\nevaluates the model\u2019s performance on both questions to measure truthful models and the intersection\nof truthful and informative. As shown in Table 4 (right), Dromedary achieves higher scores than\nGPT-3 ,LLaMA ,Alpaca in both categories, while failing behind the ChatGPT -distilled Vicuna model.\n4.2.2 BIG-bench HHH Eval\nThe BIG-bench HHH Eval [ 39,3] was specifically designed to evaluate a model\u2019s performance in\nterms of helpfulness, honesty, and harmlessness (HHH). It is a Multiple-Choice (MC) task, which\ntests the models\u2019 ability to select superior answers from two reference answers10. We calculate the", "lessly incorporate them into the MLP lay-\ners, configuring the bottleneck size to 256.\n\u2022Regarding LoRA, we seamlessly integrate\nit into both the Multi-head Attention layers\nand the MLP layers with rank 32.\n4.2 Arithmetic Reasoning\nIn order to evaluate the effectiveness of adapters\non the Arithmetic Reasoning task, we conducted\na study where adapters are fine-tuned on the\nMath10K dataset and subsequently evaluated on\nsix different math reasoning datasets. As our base-\nline, we utilize the GPT-3.5 model, specifically the\ntext-Davinci-003 variant, for Zero-shot CoT ac-\ncording to Kojima et al. (2022). The results of the\nGPT-3.5 model can be found in Wang et al. (2023).\nTable 3 reports the performance of different PEFT\nmethods and the baseline. On average, the GPT-3.5\nmodel (175B) outperforms adapter-based PEFT\nLLMs in terms of accuracy. However, for sim-\npler math reasoning datasets such as MultiArith,\nAddSub, and SingleEq, adapter-based methods like", "3.7.1 Medical Question Answering and\nComprehension\nMedical question answering and comprehension\nconsists of generating multiple-choice and free-text\nresponses to medical questions.\nSinghal et al. [511] proposed using few-shot,\nCoT, and self-consistency prompting to specialize\nthe general-purpose PaLM LLM to medical ques-\ntion answering and comprehension. They demon-\nstrate a Flan-PaLM model [ 93] using a combination\nof the three prompting strategies to achieve the pre-\nvious SOTA results on the MedQA, MedMCQA,\nPubMedQA, and MMLU medical datasets. To fur-\nther align the model to the medical domain, they\nproposed Med-PaLM, which utilizes instruction\nprompt-tuning based on 40 examples from a panel\nof clinicians and task-specific human-engineered\nprompts.\nSinghal et al. [512] then extend the Med-PaLM\napproach with Med-PaLM 2 using the newer PaLM\n2 LLM as its base model. Singhal et al. [512]\nconduct further instruction-fine tuning and use a\nnew ensemble refinement (ER) prompting strategy"], "retrieved_docs_id": ["0da5fa4a36", "107af6bc23", "431dc0ad99", "ecb3ed4cb9", "13a0ba9bc1"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the only concurrent survey on the hallucination problem in Multimodal Large Language Models (MLLMs)?\n", "true_answer": "The only concurrent survey on the hallucination problem in MLLMs is a short survey on the hallucination problem of LVLMs, as mentioned in the context.", "source_doc": "hallucination.pdf", "source_id": "33d47ad8cc", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "Hallucination of Multimodal Large Language Models: A Survey 9\noverride the visual content. For example, given an image showing a red banana, which is\ncounter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as\n\"banana is yellow\" is a deep-rooted knowledge in the LLM. Such language/knowledge prior\nmakes the model overlook the visual content and response with hallucination.\n\u2022Weak alignment interface. The alignment interface plays an essential role in MLLMs, as\nit serves as the bridge between the two modalities. A weak alignment interface can easily\ncause hallucinations. One potential cause of a weak alignment interface is data, as discussed\nin earlier sections. Apart from that, the interface architecture itself and training loss design\nalso matter [ 52,77,123]. Recent work [ 52] argues that the LLaVA-like linear projection\ninterface preserves most of the information, but lacks supervision on the projected feature."], "retrieved_docs_id": ["33d47ad8cc", "114f3dada8", "72dc971633", "f49f3b54ce", "a1c28916ce"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is a technique for efficiently encoding images of varying resolutions?\n", "true_answer": "LLaVA A-UHD [35] proposes an image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding.", "source_doc": "multimodal.pdf", "source_id": "c0bdc4830f", "retrieved_docs": ["Figure 6: Comparision of Phi[86] (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-small) versus\nLlama-2 [91] family of models(7B, 13B, 34B, 70B) that were trained on the same fixed data.\nLLaV A-UHD [35] proposes an image modularization strategy that divides native-resolution im-\nages into smaller variable-sized slices for efficient and extensible encoding. Inaddition, InternLM-\nXComposer2-4KHD [90] introduces a strategy that dynamically adjusts resolution with an automatic\nlayout arrangement, which not only maintains the original aspect ratios of images but also adaptively\nalters patch layouts and counts, thereby enhancing the efficiency of image information extraction.\nBy implementing an adaptive input strategy for images of varying resolutions, a balance between\nperceptual capability and efficiency can be achieved.\nToken Processing Techniques designed to process lengthy visual token squence are critical in ef-", "Token Processing Techniques designed to process lengthy visual token squence are critical in ef-\nficient MLLMs as they address the dual challenges of preserving fine-grained details and reducing\ncomputational complexity. LLaV A-UHD [35] presents a novel approach to manage the computa-\ntional burden associated with high-resolution images. It puts forward two key components: (1) a\ncompression module that further condenses image tokens from visual encoders, significantly re-\nducing the computational load, and (2) a spatial schema to organize slice tokens for LLMs. No-\ntably, LLaV A-UHD demonstrates its efficiency by supporting 6 times larger resolution images using\nonly 94% of the inference computation compared to previous models. Furthermore, the model\ncan be efficiently trained in academic settings, completing the process within 23 hours on 8 A100\nGPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-", "VBR [Zhuet al. , 2022 ]method is used to generate images to\nguide the text generation of the language model, which has\nsignificant effects in open text generation tasks.\nIn the code field, RBPS [Nashid et al. , 2023 ]is used for\nsmall-scale learning related to code. By encoding or fre-\nquency analysis, similar code examples to the developers\u2019\ntasks are automatically retrieved. This technique has proven\nits effectiveness in test assertion generation and program re-\npair tasks. In the field of structured knowledge, methods like\nCoK [Liet al. , 2023c ]hints first retrieve facts related to the\ninput question from the knowledge graph and then add these\nfacts to the input in the form of hints. This method has per-\nformed well in knowledge graph question answering tasks.\nFor the field of audio and video, the\nGSS[Zhao et al. , 2022 ]method retrieves and concatenates\naudio clips from the spoken vocabulary bank, immediately\ntransforming MT data into ST data. UEOP [Chan et al. , 2023 ]", "on top of FlashAttention and incorporate at-\ntention sparsity patterns, encompassing key/query\ndropping and hashing-based attention. Pope et al.\n[432] implement different sharding techniques to\nefficiently spread the feedforward and attention\ncomputations across devices while optimizing for\ninter-device communication costs, enabling context\nlengths of up to 43,000 tokens using multi-query\nattention.\nWith regards to the second stream of work, a\ncommon theme to improve the computational or\nmemory complexity of the attention mechanism is\nto sparsify the attention matrix or introducing (lin-\near) approximations [543]. However, the scalabil-\nity of some efficient Attention approximations has\nbeen questioned. For example, Tay et al. [542] , Hua\net al. [220] find that the Performer attention approx-\nimation [ 85] severely underperforms the vanilla\nself-attention mechanism, especially when scaled\nup to large models.\nQuantization is a post-training technique that", "for sub-word models. These results may be confounded by\ndiffering amounts of compute and tuning used, but show\nthatMEGABYTE gives results competitive with state-of-the-\nart models trained on subwords. These results suggest that\nMEGABYTE may allow future large language models to be\ntokenization-free.\n3The only prior byte-level experiments we are aware of are\nat a smaller scale in Hutchins et al. (2022), who report results\nequivalent to test perplexities of 46.5 with a version of the Block-\nRecurrent transformer, and 49.5 with Memorizing Transformers\n(Wu et al., 2022), compared to 36.4 with our model.6. Image Modeling\n6.1. Sequence Modeling on ImageNet\nWe test MEGABYTE on variants of the autoregressive image\ngeneration task on ImageNet (Oord et al., 2016), to mea-\nsure its ability to ef\ufb01ciently use long context. We test on\nthree different resolutions of images, ranging from 64 \u00d764 to\n640\u00d7640 pixels \u2013 the latter requiring the effective modeling"], "retrieved_docs_id": ["c0bdc4830f", "986687f08e", "150d364554", "cbefd8cc37", "64b78b539c"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What model is used to evaluate all prompts in the RAGAS framework?\n", "true_answer": "The gpt-3.5-turbo-16k model, which is available through the OpenAI API, is used to evaluate all prompts in the RAGAS framework.", "source_doc": "RAG.pdf", "source_id": "ffd5c8b41e", "retrieved_docs": ["open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "ARES\nARES aims to automatically evaluate the performance of\nRAG systems in three aspects: Context Relevance, Answer\nFaithfulness, and Answer Relevance. These evaluation met-\nrics are similar to those in RAGAS. However, RAGAS, being\na newer evaluation framework based on simple handwritten\nprompts, has limited adaptability to new RAG evaluation set-\ntings, which is one of the significances of the ARES work.\nFurthermore, as demonstrated in its assessments, ARES per-\nforms significantly lower than RAGAS.\nARES reduces the cost of evaluation by using a small\namount of manually annotated data and synthetic data,\nand utilizes Predictive-Driven Reasoning (PDR) to provide\nstatistical confidence intervals, enhancing the accuracy of\nevaluation [Saad-Falcon et al. , 2023 ].\nAlgorithm Principles\n1. Generating Synthetic Dataset: ARES initially generates\nsynthetic questions and answers from documents in the\ntarget corpus using a language model to create positive\nand negative samples.", "Evaluations on API distribution. Our main metric is human preference ratings on a held out set\nof prompts from the same source as our training distribution. When using prompts from the API for\nevaluation, we only select prompts by customers we haven\u2019t included in training. However, given\nthat our training prompts are designed to be used with InstructGPT models, it\u2019s likely that they\ndisadvantage the GPT-3 baselines. Thus, we also evaluate on prompts submitted to GPT-3 models\non the API; these prompts are generally not in an \u2018instruction following\u2019 style, but are designed\nspeci\ufb01cally for GPT-3. In both cases, for each model we calculate how often its outputs are preferred\nto a baseline policy; we choose our 175B SFT model as the baseline since its performance is near the\nmiddle of the pack. Additionally, we ask labelers to judge the overall quality of each response on a\n1-7 Likert scale and collect a range of metadata for each model output (see Table 3).", "yet analyzed) are publicly available.29\nPrompt name Prompt Target\na_good_translation-source+target Given the following source text: [source sentence], a good L2 translation is: [target sentence]\ngpt3-target What is the L2 translation of the sentence: [source sentence]? [target sentence]\nversion-target if the original version says [source sentence]; then the L2 version should say: [target sentence]\nxglm-source+target L1: [source sentence] = L2: [target sentence]\nTable 5: Four prompts for the WMT\u201914 dataset (Bojar et al., 2014) for MT evaluation.\nAbove, \u201cL1\u201d and \u201cL2\u201d are replaced with language names (e.g. \u201cBengali\u201d and \u201cRussian\u201d).\n4.1.2 Infrastructure\nOur framework extends EleutherAI\u2019s Language Model Evaluation Harness (Gao et al.,\n2021) by integrating it with the promptsource (Bach et al., 2022) library described in\nSection 3.1.4. We release our Prompted Language Model Evaluation Harness as an open\nsource library for people to use. We use this framework in order to run the experiments", "Published in Transactions on Machine Learning Research (08/2023)\nDataset Selection. Gehman et al. (2020) introduced the main dataset used to evaluate toxicity in Re-\nalToxicityPrompts . Shortly thereafter, Dhamala et al. (2021) introduced BOLD, which follows a similar\nstructure as RealToxicityPrompts but uses more innocuous input prompts. Given this, we choose to\nevaluate on both datasets to understand the relationship between properties of the prompt and properties\nof the model generation. We note that other works, especially Abid et al. (2021), also demonstrate language\nmodel toxicity, but do not release a standardized dataset that has been used for measuring toxicity more\nbroadly.\nProblem Setting. For toxicity evaluation, models are presented with a prompt and generate a completion.\nIn the case of RealToxicityPrompts , these prompts are drawn from OpenWebText (Gokaslan & Cohen,\n2019), which is a collection of Internet text that replicates the training data of GPT-2 (Radford et al.,"], "retrieved_docs_id": ["ffd5c8b41e", "1b1cdfdd79", "0ca5a05595", "d47ee6e41f", "35f53f76e0"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does IBD compute a more reliable next-token probability distribution?\n", "true_answer": "IBD calculates a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model.", "source_doc": "hallucination.pdf", "source_id": "9a2cc490f3", "retrieved_docs": ["decoding probability distribution is calibrated using the reference (distorted) distribution.\nFollowing the same idea of contrastive decoding, IBD [ 139] proposes an image-biased decoding\nstrategy. Specifically, IBD involves computing a more reliable next-token probability distribution\nby contrasting the predictions of the original model with those of an image-biased model, which\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "than 25%, performance is noticeably degraded\u2014see Figure A1.10We use Gaussian smoothing with a\nwindow length of 10 steps to smooth the training curve.\nD.2. Approach 3: Parametric \ufb01tting of the loss\nIn this section, we \ufb01rst show how Equation (2)can be derived. We repeat the equation below for\nclarity,\n\u02c6\ud835\udc3f\u00b9\ud835\udc41\u0094\ud835\udc37\u00ba,\ud835\udc38\u00b8\ud835\udc34\n\ud835\udc41\ud835\udefc\u00b8\ud835\udc35\n\ud835\udc37\ud835\udefd\u0094 (5)\nbased on a decomposition of the expected risk between a function approximation term and an\noptimisation suboptimality term. We then give details on the optimisation procedure for \ufb01tting the\nparameters.\nLoss decomposition. Formally, we consider the task of predicting the next token \ud835\udc662Ybased on\nthe previous tokens in a sequence \ud835\udc652Y\ud835\udc60, with\ud835\udc60varying from 0to\ud835\udc60max\u2014the maximum sequence\nlength. We consider a distribution \ud835\udc432D\u00b9X\u0002Y\u00ba of tokens inYand their past in X. A predictor\n\ud835\udc53:X!D\u00b9Y\u00ba computes the probability of each token given the past sequence. The Bayes classi\ufb01er,\n\ud835\udc53\u2605, minimizes the cross-entropy of \ud835\udc53\u00b9\ud835\udc65\u00bawith the observed tokens \ud835\udc66, with expectation taken on the", "A Watermark for Large Language Models. Page 4 of 13.\nAlgorithm 2 Text Generation with Soft Red List\nInput: prompt, s(\u2212Np)\u00b7\u00b7\u00b7s(\u22121)\ngreen list size, \u03b3\u2208(0,1)\nhardness parameter, \u03b4 >0\nfort= 0,1,\u00b7\u00b7\u00b7do\n1. Apply the language model to prior tokens\ns(\u2212Np)\u00b7\u00b7\u00b7s(t\u22121)to get a logit vector l(t)over\nthe vocabulary.\n2. Compute a hash of token s(t\u22121),and use it to\nseed a random number generator.\n3. Using this random number generator, randomly\npartition the vocabulary into a \u201cgreen list\u201d Gof\nsize\u03b3|V|,and a \u201cred list\u201d Rof size (1\u2212\u03b3)|V|.\n4. Add\u03b4to each green list logit. Apply the soft-\nmax operator to these modified logits to get a\nprobability distribution over the vocabulary.\n\u02c6p(t)\nk=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3exp(l(t)\nk+\u03b4)\nP\ni\u2208Rexp(l(t)\ni)+P\ni\u2208Gexp(l(t)\ni+\u03b4), k\u2208G\nexp(l(t)\nk)\nP\ni\u2208Rexp(l(t)\ni)+P\ni\u2208Gexp(l(t)\ni+\u03b4), k\u2208R.\n5. Sample the next token, s(t),using the water-\nmarked distribution \u02c6p(t).\nend for\nConsider again the case in which we detect the watermark\nforz >4.Just like in the case of the hard watermark, we", "or word fragments known as \u201ctokens.\u201d Typical vocab-\nularies contain |V|= 50 ,000 tokens or more (Radford\net al., 2019; Liu et al., 2019). Consider a sequence of\nTtokens {s(t)} \u2208 VT. Entries with negative indices,\ns(\u2212Np),\u00b7\u00b7\u00b7, s(\u22121), represent a \u201cprompt\u201d of length Npand\ns(0),\u00b7\u00b7\u00b7, s(T)are tokens generated by an AI system in re-\nsponse to the prompt.\nAlanguage model (LM) for next word prediction is a func-\ntionf, often parameterized by a neural network, that accepts\nas input a sequence of known tokens s(\u2212Np),\u00b7\u00b7\u00b7, s(t\u22121),\nwhich contains a prompt and the first t\u22121tokens already\nproduced by the language model, and then outputs a vector\nof|V|logits, one for each word in the vocabulary. These\nlogits are then passed through a softmax operator to convert\nthem into a discrete probability distribution over the vocab-\nulary. The next token at position tis then sampled from this\ndistribution using either standard multinomial sampling, or\ngreedy sampling (greedy decoding) of the single most likely", "of the generated texts, mostly independent of the sampling method, but slightly dependent on the LM\nused. Sec. 5.3 experimentally shows mirostat sampling avoids both boredom and confusion traps for\na wide range of target perplexity values. Sec. 5.4 provides our own experiments with human raters\nthat demonstrate mirostat ef\ufb01cacy for \ufb02uency, coherence, and overall quality.\n1.1 R ELATED WORK\nSampling from distorted probability distribution Pure sampling from LMs often leads to in-\ncoherent text whereas greedy decoding leads to repetitions. Distorting probability distributions,\nas in top-k, top-p, or temperature sampling help improve quality of generated texts, if parameters\nare properly tuned (Holtzman et al., 2018; Fan et al., 2018; Holtzman et al., 2020). Tuning these\nmethods, however, is ad hoc and does not provide good control over the statistics of the output.\nOur method uses statistics of previously-generated tokens as input to generate the next token, by"], "retrieved_docs_id": ["9a2cc490f3", "bbded01a59", "327dc862d6", "b11f177b7b", "dd12a3c5b3"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can the adaptability of MLLMs be improved according to the context?\n", "true_answer": "The adaptability of MLLMs can be improved by incorporating a more varied set of lightweight LLMs, which can be customized to cater to a broad spectrum of requirements.", "source_doc": "multimodal.pdf", "source_id": "f10976c224", "retrieved_docs": ["can reserve the original capacities of LLMs but likely have\na weak an adaptation performance, while the latter can\nfully adapt to multimodal tasks but suffer from the loss of\noriginal capacities of LLMs. More efforts should be made to\ninvestigate how to effectively balance the two aspects, so as\nto achieving improved multimodal capacities. In addition,\nexisting MLLMs are still overly dependent on the capacities\nof LLMs, which pose the limits on many multimodal tasks\n(e.g., space positioning). It will be meaningful to explore\nimproved training approaches of language models, so that\nmultimodal information can be also utilized in this process.\n\u2022Safety and alignment. Safety and alignment has been\nwidely discussed in LLMs, which aim to regulate the behav-\niors of models by technical approaches [66]. This topic is also\nimportant to MLLMs. Even a highly advanced MLLM ( e.g.,\nGPT-4V [133]) can be susceptible to safety issues. For exam-", "general-purpose interface, i.e., language models.\nNew capabilities of MLLMs. As shown in Table 1, apart from the capabilities found in previous\nLLMs [ BMR+20,CND+22], MLLMs enable new usages and possibilities. First, we can conduct\nzero- and few-shot multimodal learning by using natural language instructions and demonstration\nexamples. Second, we observe promising signals of nonverbal reasoning by evaluating the Raven\nIQ test, which measures the \ufb02uid reasoning ability of humans. Third, MLLMs naturally support\nmulti-turn interactions for general modalities, such as multimodal dialogue.\n2 K OSMOS -1: A Multimodal Large Language Model\nAs shown in Figure 1, KOSMOS -1is a multimodal language model that can perceive general\nmodalities, follow instructions, learn in context, and generate outputs. Given the previous context, the\nmodel learns to generate texts in an auto-regressive manner. Speci\ufb01cally, the backbone of KOSMOS -1", "ods, to deploy LLMs efficiently and effectively in real-world\nrecommender systems. In addition, existing LLMs have\nlimited capacities in long context modeling, make it difficult\nto process the huge amount of user-item interaction data.\nImproved context length extension and context information\nutilization approaches should be developed to improve the\nmodeling capacities of LLMs in long interaction sequences.\n8.1.4 Multimodal Large Language Model\nIn existing literature [823, 824], multimodal models mainly\nrefer to the models that can process and integrate informa-\ntion of various modalities ( e.g., text, image, and audio) frominput, and further produce corresponding output in certain\nmodalities. In this part, we mainly focus on the multimodal\nextension of LLMs by enabling the information modeling\nof non-textual modalities, especially the vision modality,\ncalled multimodal large language models (MLLMs) [797]49. To\nstart our discussion, we specify the input to be text-image", "ing images from public sources with manually-collected\ntext instructions for perception and cognition evaluations.\nMMBench [838] transforms these instructions into multiple-\nchoice questions and introduces CircularEval to ensure\nevaluation consistency. SEED-Bench [854] further considers\ntemporal understanding tasks and enlarges the evaluation\nscale to 19K multiple-choice questions with the assistance of\nLLMs. MM-Vet [855] presents more complex tasks to assess\nthe integrated multimodal capabilities of MLLMs. It starts\nby defining six essential multimodal abilities and then cre-\nates intricate questions by combining multiple abilities. In\nsummary, the above benchmarks collectively contribute to\nthe comprehensive evaluation and improved development\nof MLLMs.\nKey Points for Improving MLLMs. To develop capable\nMLLMs, we continue to discuss three key points to improve\nthe model capacities, from the perspectives of instruction\ndata, training strategy, and safety and alignment.", "77\ntext output. To boost the performance, high-quality visual\ninstruction data is key to eliciting and enhancing the abil-\nities of MLLMs. Therefore, most studies are dedicated to\nconstructing various visual instruction datasets. As the basic\napproaches, early studies construct visual instructions by\ndistilling from GPT-4 [149] or reformulating vision-language\ntask datasets [151]. To enhance the quality of instruction\ndata, recent work further proposes improved strategies by\nincreasing the instruction diversity [834], incorporating fine-\ngrained information ( e.g., coordinate of objects) into the\ninstruction [833], or synthesizing complex visual reasoning\ninstructions [835].\nEvaluation of MLLM. After introducing the approaches to\ndeveloping MLLMs, we further discuss how to effectively\nassess the multimodal capabilities of MLLMs from the fol-\nlowing three aspects.\n\u2022Evaluation perspectives. The evaluation tasks for MLLMs\ncan be categorized into two main types: perception and"], "retrieved_docs_id": ["32fb098424", "5494ed4540", "593e38b258", "df8870e586", "722c60f298"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "What is one of the processes involved in post-retrieval processing to enhance the quality of information retrieval?\n", "true_answer": "Information compression is one of the processes involved in post-retrieval processing. It is used to optimize the relevant information retrieved by the retriever from a large document database.", "source_doc": "RAG.pdf", "source_id": "faf8e03358", "retrieved_docs": ["rely on well-recognized large language models like GPT-\n4[OpenAI, 2023 ]to leverage their robust internal knowl-\nedge for the comprehensive retrieval of document knowledge.\nHowever, inherent issues of these large models, such as con-\ntext length restrictions and vulnerability to redundant infor-\nmation, persist. To mitigate these issues, some research has\nmade efforts in post-retrieval processing. Post-retrieval pro-\ncessing refers to the process of further treating, filtering, or\noptimizing the relevant information retrieved by the retriever\nfrom a large document database. Its primary purpose is to en-\nhance the quality of retrieval results to better meet user needs\nor for subsequent tasks. It can be understood as a process of\nreprocessing the documents obtained in the retrieval phase.\nThe operations of post-retrieval processing usually involve in-\nformation compression and result rerank.\nInformation Compression\nEven though the retriever can fetch relevant information from", "put forward various methods to optimize the retrieval process.\nIn terms of specific implementation, Advanced RAG can be\nadjusted either through a pipeline or in an end-to-end manner.\nPre-Retrieval Process\n\u2022Optimizing Data Indexing\nThe purpose of optimizing data indexing is to enhance\nthe quality of indexed content. Currently, there are five\nmain strategies employed for this purpose: increasing\nthe granularity of indexed data, optimizing index struc-\ntures, adding metadata, alignment optimization, and\nmixed retrieval.\n1.Enhancing Data Granularity: The objective of\npre-index optimization is to improve text standard-\nization, consistency, and ensure factual accuracy\nand contextual richness to guarantee the perfor-\nmance of the RAG system. Text standardization in-\nvolves removing irrelevant information and special\ncharacters to enhance the efficiency of the retriever.\nIn terms of consistency, the primary task is to elim-\ninate ambiguity in entities and terms, along with", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "challenging, and the augmentation process needs to balance\nthe value of each passage appropriately. The retrieved con-\ntent may also come from different writing styles or tones, and\nthe augmentation process needs to reconcile these differences\nto ensure output consistency. Lastly, generation models may\noverly rely on augmented information, resulting in output thatmerely repeats the retrieved content, without providing new\nvalue or synthesized information.\n3.2 Advanced RAG\nAdvanced RAG has made targeted improvements to over-\ncome the deficiencies of Naive RAG. In terms of the quality\nof retrieval generation, Advanced RAG has incorporated pre-\nretrieval and post-retrieval methods. To address the indexing\nissues encountered by Naive RAG, Advanced RAG has op-\ntimized indexing through methods such as sliding window,\nfine-grained segmentation, and metadata. Concurrently, it has\nput forward various methods to optimize the retrieval process.", "depending on the needs of different tasks. If there is historical\ndialogue information, it can also be merged into the prompt\nfor multi-round dialogues.\nDrawbacks in Naive RAG\nThe Naive RAG confronts principal challenges in three ar-\neas: retrieval quality, response generation quality, and the\naugmentation process.\nRegarding retrieval quality, the issues are multifaceted.\nThe primary concern is low precision, where not all blocks\nwithin the retrieval set correlate with the query, leading to\npotential hallucination and mid-air drop issues. A secondary\nissue is low recall, which arises when not all relevant blocks\nare retrieved, thereby preventing the LLM from obtaining suf-\nficient context to synthesize an answer. Additionally, out-\ndated information presents another challenge, where data re-\ndundancy or out-of-date data can result in inaccurate retrieval\noutcomes.\nIn terms of response generation quality, the issues are\nequally diverse. Hallucination is a prominent issue where the"], "retrieved_docs_id": ["faf8e03358", "8a71abd00a", "fefa202c19", "873e6df003", "b66fd4b3d0"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the speed difference in training between Switch Transformer and Google's T5-XXL?\n", "true_answer": "The Switch Transformer trains four times faster than T5-XXL under the same computational resources.", "source_doc": "multimodal.pdf", "source_id": "45effa0e86", "retrieved_docs": ["C4 data set after 250k and 500k steps, respectively. We observe that the Switch-\nC Transformer variant is 4x faster to a \ufb01xed perplexity (with the same compute\nbudget) than the T5-XXL model, with the gap increasing as training progresses.\ndepth, number of heads, and so on, are all much smaller than the T5-XXL model. In\ncontrast, the Switch-XXL is FLOP-matched to the T5-XXL model, which allows for larger\ndimensions of the hyper-parameters, but at the expense of additional communication costs\ninduced by model-parallelism (see Section 5.5 for more details).\nSample e\ufb03ciency versus T5-XXL. In the \ufb01nal two columns of Table 9 we record\nthe negative log perplexity on the C4 corpus after 250k and 500k steps, respectively. After\n250k steps, we \ufb01nd both Switch Transformer variants to improve over the T5-XXL version\u2019s\nnegative log perplexity by over 0.061.10To contextualize the signi\ufb01cance of a gap of 0.061,\nwe note that the T5-XXL model had to train for an additional 250k steps to increase", "making minimal changes to existing model code. It enables us to scale multi-lingual neural machine\ntranslation Transformer models with sparse gated mixtures of experts to over 600 billion parameters\nusing automatic sharding. Switch Transformer [150] replaces the feedforward network (FFN) layer\nin the standard Transformer with a MoE routing layer, where each expert operates independently on\nthe tokens in the sequence. Its training speed is four times faster than Google\u2019s previously developed\nlargest model, T5-XXL, under the same computational resources. The proposed training techniques\nhave eliminated instability during the training process, demonstrating that large sparse models can\nalso be trained in a low-precision format, such as bfloat16.\nTransformer-Alternative Structures Although the Transformer is the dominant architecture in\ncurrent large-scale language models, models like RWKV [151] and Mamba [77] have emerged as", "to achieve these compression rates.\nModel Parameters FLOPS SuperGLUE (\u2191)\nT5-Base 223M 124B 74.6\nSwitch-Base 7410M 124B 81.3\nDistilled T5-Base 223M 124B (30%) 76.6\nTable 8: Distilling a \ufb01ne-tuned SuperGLUE model. We distill a Switch-Base model \ufb01ne-\ntuned on the SuperGLUE tasks into a T5-Base model. We observe that on smaller\ndata sets our large sparse model can be an e\ufb00ective teacher for distillation. We\n\ufb01nd that we again achieve 30% of the teacher\u2019s performance on a 97% compressed\nmodel.\npre-training both versions for 1M steps, we \ufb01nd that on all101 languages considered,\nSwitch Transformer increases the \ufb01nal negative log perplexity over the baseline. In Figure\n8, we present a di\ufb00erent view and now histogram the per step speed-up of using Switch\nTransformer over the mT5-Base.9We \ufb01nd a mean speed-up over mT5-Base of 5x and\nthat 91% of languages achieve at least a 4x speedup. This presents evidence that Switch\nTransformers are e\ufb00ective multi-task and multi-lingual learners.", "Table 11: Router Exploration Strategies. Quality of the Switch Transformer, measured by\nthe negative log perplexity, under di\ufb00erent randomness-strategies for selecting\nthe expert (lower is better). There is no material speed performance di\ufb00erence\nbetween the variants.\nat the scale of 10B+ parameter models, but we show in Figure 12 as few as 2 experts\nproduce compelling gains over a FLOP-matched counterpart. Even if a super computer is\nnot readily available, training Switch Transformers with 2, 4, or 8 experts (as we typically\nrecommend one expert per core) results in solid improvements over T5 dense baselines.\n30", "with equal FLOPs per example. For a \ufb01xed amount of computation and training\ntime, Switch Transformers signi\ufb01cantly outperform the dense Transformer base-\nline. Our 64 expert Switch-Base model achieves the same quality in one-seventh\nthe time of the T5-Base and continues to improve.\nFigures 5 and 6 address this question. Figure 5 measures the pre-training model quality\nas a function of time. For a \ufb01xed training duration and computational budget, Switch\nTransformers yield a substantial speed-up. In this setting, our Switch-Base 64 expert model\ntrains in one-seventh the time that it would take the T5-Base to get similar perplexity.\n3.3 Scaling Versus a Larger Dense Model\nThe above analysis shows that a computationally-matched dense model is outpaced by its\nSwitch counterpart. Figure 6 considers a di\ufb00erent scenario: what if we instead had allocated\nour resources to a larger dense model? We do so now, measuring Switch-Base against the"], "retrieved_docs_id": ["526652a233", "45effa0e86", "57628be53d", "9afbf9d40f", "85347c86b3"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "How does Efficient Vision aim to optimize visual fracture extraction strategies?\n", "true_answer": "Efficient Vision explores optimizing visual fracture extraction strategies by emphasizing methods that enhance efficiency without compromising accuracy. It also focuses on integrating high-quality visual data for effective cross-modal understanding.", "source_doc": "multimodal.pdf", "source_id": "f53fc9e54d", "retrieved_docs": ["the development of novel technologies.\n\u2022 Efficient Vision explores optimizing efficient visual fracture extraction strategies, empha-\nsizing methods that boost efficiency while maintaining accuracy. It addresses integrating\nhigh-quality visual data for effective cross-modal understanding.\n\u2022 Efficient LLMs explores these strategies of improving the computational efficiency and\nscalability of language models. It examines the trade-offs between model complexity and\nperformance while suggesting promising avenues for balancing these competing factors.\n2", "pacities. To improve the alignment performance, it is crucial\nto design effective training strategies and select appropriate\npre-training data [829, 830]. Existing work mainly employs\nthe following strategies for cross-modality alignment: (1) if\nthe number of image-text pairs is not sufficiently large ( e.g.,\nless than 1M), it is often suggested to only update the\nconnection module [831]; (2) if the training data includes\nhigh-quality text corpora [832] or image-text pairs with\nfine-grained annotations [833], fine-tuning the LLM can be\nconducted to boost the performance; (3) if the number of\nimage-text pairs is very large ( e.g., about 1B), fine-tuning\nthe vision encoder is also plausible [829, 830], but the benefit\nremains further verification.\n\u2022Visual instruction tuning. After vision-language pre-\ntraining, the second-stage training, i.e., visual instruction\ntuning, aims to improve the instruction-following and task-\nsolving abilities of MLLMs. Generally, the input of vi-", "GPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-\nproach that significantly decreases the number of visual tokens while preserving comparable model\nperformance. TinyChart [37] and TextHawk [36] focus on document-oriented tasks, with the former\nadopting the Vision Token Merging module and the latter introducing the ReSampling and ReAr-\nrangement module. These modules can enhance fine-grained visual perception and information\ncompression capabilities.\nMulti-Scale Information Fusion Utilizing multi-scale image information is indeed crucial for\nvisual feature extraction. This approach allows the model to capture both the fine-grained details\npresent in smaller scales and the broader context available in larger scales. Mini-Gemini [26] com-\nprises twin encoders, one for high-resolution images and the other for low-resolution visual em-\nbedding. It proposes Patch Info Mining, which uses low-resolution visual embeddings as queries", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "78\nmakes good visual instructions and how visual instructions\nelicit specific multimodal abilities in MLLMs.\n\u2022Model training. Different from LLMs, MLLMs are not\ntrained from scratch, but instead developed based on pre-\ntrained language and vision models. Existing work em-\nploys a typical two-stage approach for training MLLMs,\ni.e.,vision-language alignment pre-training and visual in-\nstruction tuning. In essence, existing MLLMs aim to (1) pre-\nserve the inherent capabilities and parametric knowledge\nof LLMs as possible, and meanwhile (2) effectively adapt\nto multimodal tasks by leveraging the pre-trained LLMs\nand visual encoders. To achieve the above two goals, two\ntypical training strategies are often employed for visual\ninstruction tuning, either only optimizing the connection\nmodule [151] or fine-tuning both the connector module\nand LLM component [851]. As we can see, the former\ncan reserve the original capacities of LLMs but likely have"], "retrieved_docs_id": ["f53fc9e54d", "687ba2bcbe", "8e97c297be", "4ee780b19c", "88b06a3483"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the VQAv2-IDK benchmark categorize \"I Know (IK)\" hallucination in machine-learned language models?\n", "true_answer": "The VQAv", "source_doc": "hallucination.pdf", "source_id": "d18c108916", "retrieved_docs": ["desired answer is \u2019I don\u2019t know\u2019. The concept is defined as \u2019I Know (IK)\u2019 hallucination in the work\nof [11]. Accordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of\nhallucination. VQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as\ndetermined by human annotators. In this benchmark, \u2019I Know (IK)\u2019 hallucination has been further\ncategorized into four types:\n\u2022Unanswerable: no one can know.\n\u2022Don\u2019t know: human may not know, but robot might.\n\u2022False questions: refers non-existing.\n\u2022Not sure: ambiguous to answer.\nThis benchmark opens a new track for the study of hallucination in MLLMs.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "providing the MLLM with images that are highly relevant but inconsistent with the answers,\ncausing MLLMs to suffer from hallucination. Such visual inputs are defined as \u2019spurious visual\ninputs\u2019. This benchmark reveals that most of mainstream MLLMs, including GPT-4V, suffer from\nhallucination when presented with such spurious visual inputs. This phenomenon indicates that\nan image can induce MLLMs to instinctively focus on visual content, resulting in responses that\nare predominantly based on visual information without proper reasoning and thinking.\nVQAv2-IDK [ 11]It has been widely discussed that in the binary QA scenario, MLLMs generally\nhave a bias on answering \u2019Yes-or-No, \u2019 leading to hallucination. In a more detailed question and\nanswer scenario, MLLMs generally tend to respond to the user\u2019s question plausibly, even if the\ndesired answer is \u2019I don\u2019t know\u2019. The concept is defined as \u2019I Know (IK)\u2019 hallucination in the work", "14 Bai, et al.\nMHaluBench [ 13]This benchmark does not aim to evaluate the MLLMs themselves. Instead, it\nis intentionally designed to evaluate the hallucination detection tools of MLLMs, i.e., judge whether\na tool can successfully detect the hallucination produced by an MLLM. Thus, the benchmark\nconsists of hallucinatory examples. Specifically, the benchmark unifies image-to-text tasks and the\ntext-to-image tasks into one evaluation suite: cross-modal consistency checking. The hallucinatory\nexamples are generated using leading MLLMs and image generation models, such as LLaVA [ 75],\nMiniGPT-4 [ 138], DALL-E2 [ 89], and DALL-E3 [ 6]. During evaluation, the benchmark can be used\nto compare different hallucination detection methods based on their performance. So far, there are\nnot many dedicated hallucination detection methods. This work serves as a basis for this direction.\nVHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual", "Hallucination of Multimodal Large Language Models: A Survey 11\nthere are four popular object related subtasks in its perception evaluation, including object existence,\ncount, position, color. Similar to POPE, these tasks are formulated as Yes-or-Notasks.\nCIEM [ 42]CIEM is a benchmark to evaluate hallucination of MLLMs. Unlike previous works\nutilize human annotated objects, CIEM is generated using an automatic pipeline. The pipeline takes\nthe text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows\nthat the generated data has low error rate, around 5%.\nMMHal-Bench [ 96]Comprising 96 image-question pairs, ranging in 8 question categories \u00d712\nobject topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs. The\n8 question categories cover various types of hallucination, including object attributes, counting,", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions."], "retrieved_docs_id": ["d18c108916", "842ef8fff1", "5c89e9ef97", "004e988006", "92e73c053a"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does PRCA train the adapter?\n", "true_answer": "PRCA trains the adapter through the Contextual Extraction Stage and the Reward-Driven Stage.", "source_doc": "RAG.pdf", "source_id": "af13cfcd4c", "retrieved_docs": ["igate alignment issues. PRCA [Yang et al. , 2023b ]lever-\naged reinforcement learning to train a context adapter\ndriven by LLM rewards, positioned between the re-\ntriever and generator. It optimizes the retrieved in-\nformation by maximizing rewards in the reinforcement\nlearning phase within the labeled autoregressive pol-\nicy. AAR [Yuet al. , 2023b ]proposed a universal plu-\ngin that learns LM preferences from known-source\nLLMs to assist unknown or non-co-finetuned LLMs.\nRRR [Maet al. , 2023a ]designed a module for rewriting\nqueries based on reinforcement learning to align queries\nwith documents in the corpus.\n\u2022Validation Module: In real-world scenarios, it is notalways guaranteed that the retrieved information is reli-\nable. Retrieving irrelevant data may lead to the occur-\nrence of illusions in LLM. Therefore, an additional val-\nidation module can be introduced after retrieving docu-\nments to assess the relevance between the retrieved doc-", "the retrieved documents as latent variables. Perplexity Dis-\ntillation directly trains using the perplexity of the model-\ngenerated tokens as an indicator.LOOP introduces a new loss\nfunction based on the effect of document deletion on LM\nprediction, providing an effective training strategy for better\nadapting the model to specific tasks.\nPlug in an adapter However, fine-tuning an embed-\nding model can be challenging due to factors such as\nutilizing an API to implement embedding functionality\nor insufficient local computational resources. There-\nfore, some works choose to externally attach an adapter\nfor alignment.PRCA [Yang et al. , 2023b ]trains the Adapter\nthrough the Contextual Extraction Stage and the Reward-\nDriven Stage, and optimizes the output of the re-\ntriever based on a token-based autoregressive strategy.\nTokenFiltering [Berchansky et al. , 2023 ]method calculates\ncross-attention scores, selecting the highest scoring input to-", "Figure 3: Comparison between the three paradigms of RAG\n\u2022Task Adaptable Module: Focused on trans-\nforming RAG to adapt to various downstream\ntasks, UPRISE [Cheng et al. , 2023a ] automati-\ncally retrieves prompts for given zero-shot task\ninputs from a pre-constructed data pool, en-\nhancing universality across tasks and models.\nPROMPTAGATOR [Daiet al. , 2022 ]utilizes LLM\nas a few-shot query generator and, based on the gener-\nated data, creates task-specific retrievers. Leveraging\nthe generalization capability of LLM, PROMPTAGA-\nTOR enables the creation of task-specific end-to-end\nretrievers with just a few examples.\n\u2022Alignment Module: The alignment between queries\nand texts has consistently been a critical issue influenc-\ning the effectiveness of RAG. In the era of Modular\nRAG, researchers have discovered that adding a train-\nable Adapter module to the retriever can effectively mit-\nigate alignment issues. PRCA [Yang et al. , 2023b ]lever-", "In LoRA, the rank ris a hyperparameter that\nshould be tuned for each task. Moreover, LoRA\nis astatic low-rank adapter that works only with a\nparticular size of r, which has been trained on it.4 Our Method: DyLoRA\nIn this section, we introduce our solution to get\ndynamic low-rank adapters that can be trained and\ndeployed well on a range of ranks instead of a\nsingle particular rank (with a \ufb01xed training budget).\nThis \ufb02exibility can free us from searching for the\nbest ranks by training the model multiple times.\nWithout loss of generality, we explain our so-\nlution on top of LoRA as one of the prominent\nlow-rank adapter techniques in the literature. In\neach LoRA module, we have an up-projection\n(Wup\u2208Rm\u00d7r) and a down-projection matrix\n(Wdw\u2208Rr\u00d7d). Let\u2019s assume that we would like to\ntrain the LoRA module to operate in the range of\nr\u2208Range [rmin,rmax]whererminandrmaxcan\nbe treated as new hyper-parameters. To make the\nLoRA module work in a range of ranks instead of", "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nFine-tuning method GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n\u22c6All parameters 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\nAdapter layers, d= 32 80 .52 15 .08 79 .32 60 .40 13 .84 17 .88 15 .54\nAdapter layers, d= 128 81 .51 16 .62 79 .47 63 .03 19 .83 27 .50 22 .63\nAdapter layers, d= 512 81 .54 17 .78 79 .18 64 .30 23 .45 33 .98 25 .81\nAdapter layers, d= 2048 81 .51 16 .62 79 .47 63 .03 19 .83 27 .50 22 .63\nGradual unfreezing 82.50 18 .95 79 .17 70.79 26.71 39 .02 26 .93\nTable 10: Comparison of di\ufb00erent alternative \ufb01ne-tuning methods that only update a subset\nof the model\u2019s parameters. For adapter layers, drefers to the inner dimensionality\nof the adapters.\n218steps. As such, we subdivide the \ufb01ne-tuning process into 12episodes of 218/12steps each\nand train from layers 12\u2212nto12in thenth episode. We note that Howard and Ruder\n(2018) suggested \ufb01ne-tuning an additional layer after each epoch of training. However, since"], "retrieved_docs_id": ["8d0a82337c", "af13cfcd4c", "bbfa682738", "bb7b8e9da0", "a7b1516ffb"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "What is a hardware-aware quantization method for ViTs?\n", "true_answer": "GPUSQ-ViT", "source_doc": "multimodal.pdf", "source_id": "8a087225e4", "retrieved_docs": ["improved performance for the ViT student model.\n3.4 Quantization\nViT quantization is the process of reducing the precision of numerical representations in ViT models,\ntypically transitioning from floating-point to fixed-point arithmetic [140]. This reduction in preci-\nsion aims to decrease memory usage, computational complexity, and energy consumption while\npreserving model accuracy to an acceptable level. Current research can be mainly categorized into\npost-training quantization, quantization-aware training, and hardware-aware quantization.\nPost-Training Quantization (PTQ) compresses trained ViT models by converting their param-\neters from high-precision floating-point numbers to lower-precision fixed-point numbers, such as\n8-bit integers. For example, Liu et al. [141] introduced a ranking loss method to identify opti-\nmal low-bit quantization intervals for weights and inputs, ensuring the functionality of the attention", "NN are grouped into sensitive/insensitive to quantization,\nand higher/lower bits are used for each layer. As such,\none can minimize accuracy degradation and still bene\ufb01t\nfrom reduced memory footprint and faster speed up with\nlow precision quantization. Recent work [ 267] has also\nshown that this approach is hardware-ef\ufb01cient as mixed-\nprecision is only used across operations/layers.\nC. Hardware Aware Quantization\nOne of the goals of quantization is to improve the\ninference latency. However, not all hardware provide\nthe same speed up after a certain layer/operation is\nquantized. In fact, the bene\ufb01ts from quantization is\nhardware-dependant, with many factors such as on-chip\nmemory, bandwidth, and cache hierarchy affecting the\nquantization speed up.\nIt is important to consider this fact for achieving\noptimal bene\ufb01ts through hardware-aware quantization [ 87,\n91,246,250,254,256,265,267]. In particular, the\nwork [ 246] uses a reinforcement learning agent to", "Bit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain\naccuracy throughout quantization. PackQViT [129] mitigates outlier effects during quantization.\nBiViT [128] introduces Softmax-aware Binarization to adjust the binarization process, minimizing\nerrors in binarizing softmax attention values. Xiao et al. [142] integrated a gradient regularization\nscheme to curb weight oscillation during binarization training and introduced an activation shift\nmodule to reduce information distortion in activations. Additionally, BinaryViT [130] integrates\nessential architectural elements from CNNs into a pure ViT framework, enhancing its capabilities.\nHardware-Aware Quantization optimizes the quantization process of neural network models for\nspecific hardware platforms ( e.g., GPUs [131], FPGA [132]). It adjusts precision levels and quan-\ntization strategies to maximize performance and energy efficiency during inference. For example,", "in the seminal work of Optimal Brain Damage [ 139].\nIn HAWQv2, this method was extended to mixed-\nprecision activation quantization [ 50], and was shown to\nbe more than 100x faster than RL based mixed-precision\nmethods [ 246]. Recently, in HAWQv3, an integer-only,\nhardware-aware quantization was introduced [ 267] that\nproposed a fast Integer Linear Programming method to\n\ufb01nd the optimal bit precision for a given application-\nspeci\ufb01c constraint (e.g., model size or latency). This work\nalso addressed the common question about hardware\nef\ufb01ciency of mixed-precision quantization by directly\ndeploying them on T4 GPUs, showing up to 50% speed\nup with mixed-precision (INT4/INT8) quantization as\ncompared to INT8 quantization.\nSummary (Mixed-precision Quantization). Mixed-\nprecision quantization has proved to be an effective and\nhardware-ef\ufb01cient method for low-precision quantizationof different NN models. In this approach, the layers of a", "91,246,250,254,256,265,267]. In particular, the\nwork [ 246] uses a reinforcement learning agent to\ndetermine the hardware-aware mixed-precision setting\nfor quantization, based on a look-up table of latency\nwith respect to different layers with different bitwidth.\nHowever, this approach uses simulated hardware latency.\nTo address this the recent work of [ 267] directly deploys\nquantized operations in hardware, and measures the\nactual deployment latency of each layer for different\nquantization bit precisions.\nD. Distillation-Assisted Quantization\nAn interesting line of work in quantization is to\nincorporate model distillation to boost quantization accu-\nracy [ 126,177,195,267]. Model distillation [ 3,95,150,\n177,195,207,268,270,289] is a method in which a\nlarge model with higher accuracy is used as a teacher to\nhelp the training of a compact student model. During the\ntraining of the student model, instead of using just the\nground-truth class labels, model distillation proposes to"], "retrieved_docs_id": ["354a427ccf", "3e3cb80a9a", "31efe3044d", "64d03aeb69", "a58f51092e"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "How does MARINE implement guided decoding?\n", "true_answer": "MARINE implements guided decoding by employing an additional vision encoder for object grounding and utilizing the grounded objects to guide the decoding process, using the classifier-free guidance technique.", "source_doc": "hallucination.pdf", "source_id": "9e707211bd", "retrieved_docs": ["20 Bai, et al.\nfocuses more on the image information. The image-based model is created by modifying the\nattention weight matrix structure within the original model, without altering its parameters. This\napproach emphasizes the knowledge of the image-biased model and diminishes that of the original\nmodel, which may be text-biased. Thus, it encourages the extraction of correct content while\nsuppressing hallucinations resulting from textual over-reliance.\nGuided Decoding. MARINE [ 131] proposes a training-free approach. It employs an additional\nvision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\nSpecifically, it innovatively adapts the classifier-free guidance [ 40] technique to implement guided\ndecoding, showing promising performance in emphasizing the detected objects while reducing\nhallucination in the text response.\nSimilarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that", "Contrastive Loss e.g.HACL [52]\nOthers e.g.EOS [120]\nReinforcement LearningAutomatic\nMetric-basede.g.MOCHa [5]\nRLAIF-based e.g.HA-DPO [133], POVID [136]\nRLHF-based e.g.LLaVA-RLHF [96], RLHF-V [119]\nMitigating Inference-related\nHallucinations (\u00a75.4)Generation InterventionContrastive Decoding e.g.VCD [64], IBD [139]\nGuided Decoding e.g.MARINE [131], GCD [24]\nOthers e.g.OPERA [45], Skip\u2018\\n\u2019 [36]\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\nFig. 1. The main content flow and categorization of this survey.\n(RLHF) [ 19,84,94] emerges as a notable approach for achieving alignment through reinforcement\nlearning. RLHF typically employs a preference model [ 7], trained to predict preference rankings\nbased on prompts and human-labeled responses. To better align with human preferences, RLHF opti-\nmizes the LLM to generate outputs that maximize rewards provided by the trained preference model,", "Similarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that\nCLIPScore [ 88] can effectively distinguish between hallucinated and non-hallucinated sentences\nthrough a series of studies across different models and datasets. Based on this conclusion, it further\nrecalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which\ndesigns a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that\nare less likely to be hallucinated, and 2) guided sentence generation, which generates responses\nbased on this scoring. This is implemented in a similar way to beam search but at the sentence\nlevel.\nHALC [ 15] provides a key insight that when decoding a specific token in the MLLM, identifying a\ntoken-wise optimal visual context to provide the most informative visual grounding can effectively\nreduce hallucination. Visual context refers to the visual tokens that can be grounded from the", "greater than or equal to 2K, otherwise return all the KV cache . The function compute_attention()\nwill return new KV cache and the indices for evicted entries during decoding iterations, which would\nbe the place to implement a customized eviction strategy. (If the current number of stored KV cache is\nless than 2K, the eviction will be ignored.) During each decoding iteration, the oldest one among the\nlastKtokens (if we have no less than Ktokens\u2019 KV cache stored) will be removed from the reserved\nlastKentries, one heavy hitter will be evicted for each head, and the newest token will be added to\ntheKV cache with a position in the last Kentries. This happens in the function store_cache() .\ndef generation_loop (...) :\n# Prologue\n...\n# Generate\nfor i in range ( gen_len ):\nfor j in range ( num_layers ):\nfor k in range ( num_gpu_batches ):\nload_weight (i, j+1 , k)\nload_cache (i, j, k +1)\nstore_hidden (i, j, k -1)\nload_hidden (i, j, k +1)\ncompute_layer (i, j, k)\nstore_cache (i, j, k -1)", "fuse them into a single kernel. (2) Fusing block read and atten-\ntion. We adapt the attention kernel in FasterTransformer [ 31]\nto read KV cache according to the block table and perform\nattention operations on the fly. To ensure coalesced memory\naccess, we assign a GPU warp to read each block. More-\nover, we add support for variable sequence lengths within a\nrequest batch. (3) Fused block copy. Block copy operations,\nissued by the copy-on-write mechanism, may operate on\ndiscontinuous blocks. This can lead to numerous invocations\nof small data movements if we use the cudaMemcpyAsync\nAPI. To mitigate the overhead, we implement a kernel that\nbatches the copy operations for different blocks into a single\nkernel launch.\n5.2 Supporting Various Decoding Algorithms\nvLLM implements various decoding algorithms using three\nkey methods: fork ,append , and free . The fork method\ncreates a new sequence from an existing one. The append\nmethod appends a new token to the sequence. Finally, the"], "retrieved_docs_id": ["9e707211bd", "9644813f29", "17a462daf3", "bc39048b90", "442b069e06"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does TinyLlava adjust pre-trained modules during pre-training?\n", "true_answer": "TinyLlava partially freezes pre-trained vision encoder and symmetric language model (SLM) modules to activate more parameters for learning alignment during the pre-training stage.", "source_doc": "multimodal.pdf", "source_id": "1bd741e7c9", "retrieved_docs": ["Figure 14: Training stages of efficient MLLMs.\nusing a standard cross-entropy loss function:\nmax\n\u03b8LX\ni=1logp\u03b8(xi|Xv, Xinstruct , Xa,<i), (4)\nwhere Lis the length of Xaand\u03b8denotes the trainable parameters. In order to better align different\nmodalities of knowledge and avoid catastrophic forgetting during the pre-training stage, \u03b8typically\nincludes only a learnable modality interface, i.e., a vision-language projector.\nWhich part to unfreeze? Considering that only training the connector may not well align the\nvision and text information when using SLMs, TinyLlava[23] also opt to partially freeze pre-\ntrained modules (i.e. vision encoder and SLM) to activate more parameters for learning alignment.\nVILA[49] reveals that updating the base LLM throughout the pre-training stage is essential to in-\nheriting some of the appealing LLM properties like in-context learning. ShareGPT4V[55] found\nthat unfreezing more parameters, particularly in the latter half of the vision encoder\u2019s layers, proves", "expressiveness and generalization capabilities. Adapter-based tuning introduces lightweight adapter\nmodules into the pre-trained model\u2019s architecture. These adapter modules, typically composed of\nfeed-forward neural networks with a small number of parameters, are inserted between the layers\nof the original model. During fine-tuning, only the adapter parameters are updated, while the pre-\ntrained model\u2019s parameters remain fixed. This method significantly reduces the number of trainable\nparameters, leading to faster training and inference times without compromising the model\u2019s per-\nformance. LLM-Adapters [154] presents a framework for integrating various adapters into large\nlanguage models, enabling parameter-efficient fine-tuning for diverse tasks. This framework en-\n16", "used in model pre-training [234, 235, 237, 238]. It aims to\norganize different parts of pre-training data for LLMs in\na specific order, e.g., starting with easy/general examples\nand progressively introducing more challenging/special-\nized ones. More generally, it can broadly refer to the adap-\ntive adjustment of data proportions for different sources\nduring pre-training. Existing work about data curriculum\nmainly focuses on continual pre-training, such as special-\nized coding LLMs ( e.g., CodeLLaMA [235]) or long context\nLLMs ( e.g., LongLLaMA [238]). However, it still lacks of\nmore detailed report about data curriculum for general-\npurpose LLMs ( e.g., LLaMA) in the literature. To determine\ndata curriculum, a practical approach is to monitor the de-\nvelopment of key abilities of LLMs based on specially con-\nstructed evaluation benchmarks, and then adaptively adjust\nthe data mixture during pre-training. Next, we take three\ncommon abilities as examples to introduce how the concept", "Homomorphic KDs can further classified into logit-level [114, 115], patch-level [117], module-\nlevel [116], and feature-level KDs [118]. For logit-level methods, in DeiT [114], a distillation token\nis incorporated into the self-attention module to emulate the class label inferred by the teacher model,\nfacilitating interaction between the student attention and layers, thus enabling the learning of hard\nlabels during back-propagation. TinyViT [115] applies distillation during pretraining, where logits\nfrom large teacher models are pre-stored in the hardware, enabling memory and computational ef-\nficiency when transferring knowledge to scaled-down student transformers. Patch-level techniques\nlike DeiT-Tiny [117] train a small student model to match a pre-trained teacher model on patch-level\nstructures, then optimize with a decomposed manifold matching loss for reduced computational\ncosts. Module-level methods involve segregating teacher modules from a pre-trained unified model,", "Figure 15: Comparison of different multimodal adaptation schemes for LLMs in LaVIN [50].\nWith this multimodal instruction-following sequence, IT can be performed by using the same auto-\nregressive training objective as that of the pre-training stage. A prevalent strategy involves main-\ntaining the visual encoder weights in a fixed state while continuing to update the pre-trained weights\nof both the projector and the SLM during the IT process.\nEfficient IT Current IT solutions are prohibitively expensive, requiring optimization of a large\nnumber of parameters and additional large-scale training. LaVIN [50] introduces an innovative\nand cost-effective solution for efficient instruction tuning of MLLMs. The Mixture-of-Modality\nAdaptation (MMA) in LaVIN uses lightweight modules to bridge the gap between LLMs and VL\ntasks. This also facilitates the joint optimization of vision and language models. The actual cost of"], "retrieved_docs_id": ["1bd741e7c9", "004ffc5dd9", "55df9c8a81", "534dcc9fda", "cde7cc5eff"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "What is the focus of the survey presented in the paper?\n", "true_answer": "The focus of the survey presented in the paper is the latest developments regarding hallucinations in multimodal large language models (MLLMs).", "source_doc": "hallucination.pdf", "source_id": "e1b4ac9436", "retrieved_docs": ["(ii) the token generation phase, which leverages and updates the KV cache to generate new tokens\nincrementally. Each generation step relies on the previously generated tokens. The primary focus of\nthis paper is to enhance the efficiency of the KV cache in attention during the token generation phase,\nthereby accelerating LLM inference.\n3", "demonstrations in a reasonable order.\nA comprehensive review of ICL has been presented in\nthe survey paper [50], and we suggest the readers refer-\nring to it for a more general, detailed discussion on this\ntopic. Compared with this survey, we specially focus on the\ndiscussion of applying ICL to LLMs in two major aspects,\ni.e.,demonstration design and the underlying mechanism\nof ICL. Also, ICL has a close connection with instruction\ntuning (discussed in Section 5.1) in that both utilize nat-\nural language to format the task or instances. However,\ninstruction tuning needs to fine-tune LLMs for adaptation,\nwhile ICL only prompts LLMs for utilization. Furthermore,\ninstruction tuning can enhance the ICL ability of LLMs to\nperform target tasks, especially in the zero-shot setting (only\nusing task descriptions) [69].\n6.2.2 Demonstration Design\nSeveral studies have shown that the effectiveness of ICL\nis highly affected by the design of demonstrations [432,", "Organization of this survey. In this paper, we present a comprehensive survey of the latest\ndevelopments regarding hallucinations in MLLMs. The survey is organized as follows: We begin by\nproviding sufficient context and defining concepts related to LLMs, MLLMs, hallucination, etc. Next,\nwe delve into an in-depth analysis of the factors contributing to hallucinations in MLLMs. Following\nthis, we present a set of metrics and benchmarks employed for evaluating hallucinations in MLLMs.\nWe then elaborate on a range of approaches designed to mitigate hallucinations in MLLMs. Finally,\nwe delve into the challenges and open questions that frame the current limitations and future\nprospects of this field, offering insights and delineating potential pathways for forthcoming research.\n2 DEFINITIONS\n2.1 Large Language Models\nBefore moving to multimodal large language models, it is essential to introduce the concept of large", "once [38,39,56] such as Longformer [ 5] and BigBird [ 70], segment-based recurrence such as Transformer-\nXL [19] and Compressive Transformer [ 49]. Please refer to a recent survey [ 58] for more details. In this paper,\nwe mainly explore within the scope of approximating dense or full attention matrices.\nExisting combination of Sparse and Low-rank Attention: Our focus on the classical and well-\nde\ufb01ned problem of matrix approximation, as opposed to simply designing an e\ufb03cient model that performs\nwell on downstream tasks (e.g., Longformer, Luna, Long-short transformer, etc.) a\ufb00ords us several advantages:\n(i) Easier understanding and theoretical analysis (Section 3, 4). We see that Scatterbrain yields an unbiased\nestimate of the attention matrix, and we can also understand how its variance changes. (ii) Clear-cut\nevaluation based on approximation error, as well as the ability to directly replace a full attention layer with", "Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest"], "retrieved_docs_id": ["ac87edd9a9", "e00fea5766", "e1b4ac9436", "6ac90ae1de", "33d47ad8cc"], "reranker_type": "colbert", "search_type": "text", "rr": 0.3333333333333333, "hit": 1}, {"question": "How does D-Abstractor maintain the local context in visual feature abstraction?\n", "true_answer": "D-Abstractor, or Deformable attention-based Abstractor, maintains the local context through a 2-D coordinate-based sampling process, using reference points and sampling offsets.", "source_doc": "multimodal.pdf", "source_id": "3a3d9edb48", "retrieved_docs": ["additional LResNet blocks, which facilitate the abstraction of visual features to any squared num-\nber of visual tokens. Conversely, D-Abstractor, or Deformable attention-based Abstractor utilizes\ndeformable attention, which maintains the local context through a 2-D coordinate-based sampling\nprocess, using reference points and sampling offsets.\n6", "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "frames for short video understanding.\nTo address the computational challenges associated with processing long videos due to the excessive\nnumber of visual tokens, several approaches have been developed. mPLUG-video [67] is designed\nfor video understanding tasks and begins with a TimeSformer-based video encoder to extract fea-\ntures from sparsely sampled video frames effectively, followed by a visual abstractor module to\nreduce sequence length. Video-LLaV A [44] excels in various video understanding tasks by unify-\ning visual representations of images and videos into a single language feature space before projec-\ntion. This approach enables effective learning of multi-modal interactions with LanguageBind [93].\nLLaMA-VID [69] addresses this issue by representing each frame with two distinct tokens, namely\ncontext token and content token. The context token encodes the overall image context based on user", "which can pose a significant computational challenge within the context window of LLMs. Ely-\nsium [92] provides a trade-off between performance and visual token consumption, where T-Selector\nis introduced as a visual token compression network to enable LLMs to distinguish individual frames\nwhile reducing visual token use. VideoLLaV A [44], building upon LanguageBind [93], unifies vi-\nsual representation into the language feature space to advance foundational LLMs towards a unified\nlanguage-vision LLM without incurring a large computational burden.\n2.5 Efficient Structures\nEfficient structures primarily explore three directions: Mixture-of-Experts, Mamba and Inference\nAcceleration.\nMixture of Experts MoE enhances model capacity by modulating the total count of model pa-\nrameters while maintaining the activated parameters unchanged, hence, not significantly compro-\nmising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework", "GPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-\nproach that significantly decreases the number of visual tokens while preserving comparable model\nperformance. TinyChart [37] and TextHawk [36] focus on document-oriented tasks, with the former\nadopting the Vision Token Merging module and the latter introducing the ReSampling and ReAr-\nrangement module. These modules can enhance fine-grained visual perception and information\ncompression capabilities.\nMulti-Scale Information Fusion Utilizing multi-scale image information is indeed crucial for\nvisual feature extraction. This approach allows the model to capture both the fine-grained details\npresent in smaller scales and the broader context available in larger scales. Mini-Gemini [26] com-\nprises twin encoders, one for high-resolution images and the other for low-resolution visual em-\nbedding. It proposes Patch Info Mining, which uses low-resolution visual embeddings as queries"], "retrieved_docs_id": ["3a3d9edb48", "3238be52f9", "c16b4c1887", "ffe176eb03", "8e97c297be"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does Mamba Cobra [13] incorporate the efficient Mamba [77] language model into the vision modality?\n", "true_answer": "Mamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modality by exploring different modal fusion schemes to develop an effective multi-modal Mamba.", "source_doc": "multimodal.pdf", "source_id": "5510d4cc4e", "retrieved_docs": ["mising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework\nthat effectively increases the number of parameters without compromising computational efficiency.\nFurthermore, it introduces MoE-Tuning, a three-stage training strategy designed to adapt MoE [89]\nto MLLMs and prevent model degradation caused by sparsity. MM1[30] designs two variants of\nMoE models. The first is a 3B-MoE model that employs 64 experts and substitutes a dense layer\nwith a sparse one every two layers. The second is a 7B-MoE model that utilizes 32 experts and\nsubstitutes a dense layer with a sparse one every four layers.\nMamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modal-\nity and explores different modal fusion schemes to develop an effective multi-modal Mamba. Exper-\niments show that it not only achieves competitive performance with state-of-the-art efficient meth-", "iments show that it not only achieves competitive performance with state-of-the-art efficient meth-\nods but also boasts faster speeds due to its linear sequential modeling.It also excels in overcom-\ning visual illusions and spatial relationship judgments in closed-set challenging prediction bench-\nmarks and achieves performance comparable to LLaV A while using only 43% of the parameters.\nVL-Mamba[18] substitutes the Transformer-based backbone language model with the pre-trained\nMamba language model. It explores how to effectively implement the 2D vision selective scan\nmechanism for multimodal learning and the combinations of different vision encoders and pre-\ntrained Mamba language model variants.\nInference Acceleration SPD[45] proposes the speculative decoding with a language-only model\nto improve inference efficiency. By employing a language-only model as a draft model for specu-\n9", "sion tokens, implementing efficient structures, and utilizing compact language models, among other\nstrategies. A diagram of the architecture is illustrated in Figure. 3. Table. 1 surveys a summary of\nthe efficient MLLMs, which outlines the base LLM, the vision encoder, image resolution, and the\nprojector used to connect vision and language. These efficient MLLMs include: MobileVLM [20],\nLLaV A-Phi [21], Imp-v1 [22], TinyLLaV A [23], Bunny [24], Gemini Nano-2 [2], MobileVLM-\nv2 [17], MoE-LLaV A-3.6B [25], Cobra [13], Mini-Gemini [26], Vary-toy [27], TinyGPT-V [28],\nSPHINX-Tiny [14], ALLaV A [29], MM1-3B [30], LLaV A-Gemma [31], Mipha-3B [32], VL-\nMamba[18], MiniCPM-V2.0 [70], DeepSeek-VL [34], KarmaVLM [71], moondream2 [72]. In\nthis section, we sequentially present a comprehensive overview of these three modules, along with\nother efficient components.\n2.1 Vision Encoder\nTaking the input image Xvas input, the vision encoder compresses the original image into more", "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "Efficient MLLMArchitecture (\u00a72)Vision Encoder (\u00a72.1)ViTamin [11], BRA VE[12],\nCobra[13], SPHINX-X[14]\nVision-Language Projector (\u00a72.2)QFormer [15], Perceiver Resampler[16],\nLDPv2[17], VSS[18], C/D-Abstractor[19],\nMEQ-Former[12]\nSmall Language Models (\u00a72.3)MobileVLM [20], LLaV A-Phi [21],\nImp-v1 [22], TinyLLaV A [23],\nBunny [24], Gemini Nano-2 [2],\nMobileVLM-v2 [17], MoE-LLaV A [25],\nCobra [13], Mini-Gemini [26],\nVary-toy [27], TinyGPT-V [28],\nSPHINX-Tiny [14], ALLaV A [29],\nMM1 [30], LLaV A-Gemma [31],\nMipha [32], VL-Mamba [18]\nMiniCPM-V 2.0 [33], DeepSeek-VL [34]\nVision Token Compression (\u00a72.4)Mini-Gemini [26], LLaV A-UHD [35],\nTextHawk [36], TinyChart [37], P2G [38],\nIXC2-4KHD [39], SPHINX-X[14], S2[40]\nLLaV A-PruMerge[41], MADTP[42],\nMoV A[43], Video-LLaV A[44]\nEfficient Structures (\u00a72.5)SPD [45], MoE-LLaV A [25],\nMM1 [30], Cobra [13], VL-Mamba [18],\nFastV[46], VTW[47]\nTraining (\u00a75)Pre-Training (\u00a75.1) Idefics2[48], TinyLLaV A[23], VILA[49]"], "retrieved_docs_id": ["5510d4cc4e", "6bebc6e320", "b24e6a172f", "3238be52f9", "93d03b64f9"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How does the RETRO system retrieve and integrate information for chunk-level language generation?\n", "true_answer": "RETRO retrieves the nearest neighboring chunk (N(Ci\u22121)) from the retrieval database and integrates this information with the contextual information of the previous chunk to guide the generation of the next chunk.", "source_doc": "RAG.pdf", "source_id": "92f5901d31", "retrieved_docs": ["kens (e.g., kNN-LM [Khandelwal et al. , 2019 ]), phrases (e.g.,\nNPM [Leeet al. , 2020 ], COG [Vaze et al. , 2021 ]), and docu-\nment paragraphs. Finer-grained retrieval units can often bet-\nter handle rare patterns and out-of-domain scenarios but come\nwith an increase in retrieval costs.\nAt the word level, FLARE employs an active retrieval strat-\negy, conducting retrieval only when the LM generates low-\nprobability words. The method involves generating a tempo-\nrary next sentence for retrieval of relevant documents, then\nre-generating the next sentence under the condition of the re-\ntrieved documents to predict subsequent sentences.\nAt the chunk level, RETRO uses the previous chunk to re-\ntrieve the nearest neighboring chunk and integrates this infor-\nmation with the contextual information of the previous chunk\nto guide the generation of the next chunk. RETRO achieves\nthis by retrieving the nearest neighboring block N(Ci\u22121)\nfrom the retrieval database, then fusing the contextual in-", "mentation for pre-training a self-regressive language model,\nenabling large-scale pre-training from scratch by retrieving\nfrom a massive set of labeled data and significantly reducing\nmodel parameters. RETRO shares the backbone structure\nwith GPT models and introduces an additional RETRO\nencoder to encode features of neighboring entities retrieved\nfrom an external knowledge base. Additionally, RETRO\nincorporates block-wise cross-attention layers in its decoder\ntransformer structure to effectively integrate retrieval infor-\nmation from the RETRO encoder. RETRO achieves lower\nperplexity than standard GPT models. Moreover, it provides\nflexibility in updating knowledge stored in the language\nmodels by updating the retrieval database without the need\nfor retraining the language models [Petroni et al. , 2019 ].\nAtla[Izacard et al. , 2022 ]employs a similar approach, in-\ncorporating a retrieval mechanism using the T5 architecture", "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "Recite-Read [Sunet al. , 2022 ]transforms external re-\ntrieval into retrieval from model weights, initially hav-\ning LLM memorize task-relevant information and gener-\nate output for handling knowledge-intensive natural lan-\nguage processing tasks.\n\u2022Adjusting the Flow between Modules In the realm of\nadjusting the flow between modules, there is an empha-\nsis on enhancing interaction between language models\nand retrieval models. DSP [Khattab et al. , 2022 ]intro-\nduces the Demonstrate-Search-predict framework, treat-\ning the context learning system as an explicit program\nrather than a terminal task prompt to address knowledge-\nintensive tasks. ITER-RETGEN [Shao et al. , 2023 ]\nutilizes generated content to guide retrieval, itera-\ntively performing \u201cretrieval-enhanced generation\u201d and\n\u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read-\nRetrieve-Read flow. Self-RAG [Asai et al. , 2023b ]fol-\nlows the decide-retrieve-reflect-read process, introduc-", "(up to millions of samples), filtering, and clustering\nof candidate solutions generated by AlphaCode to\nselect the final submissions.\nHowever, whilst these existing code-generation\nLLMs have achieved impressive results, a criti-\ncal current constraint in applying LLMs to code\ngeneration is the inability to fit the full code base\nand dependencies within the context window. To\ndeal with this constraint, a few frameworks have\nbeen proposed to retrieve relevant information or\nabstract the relevant information into an API defi-\nnition.\nLong-Range Dependencies [ 660,504]\nLong-range dependencies across a code\nrepository usually cannot be regarded be-\ncause of limited context lengths (Sec. 2.6).\nZhang et al. [660] introduce RepoCoder, a\nretrieval-based framework for repository-level code\ncompletion that allows an LLM to consider the\nbroader context of the repository. A multi-step\nretrieval-augmented generation approach is taken,\nwhere the initial code generated is then used to re-"], "retrieved_docs_id": ["92f5901d31", "422e1adde8", "6291d3f5de", "dfac20a7d8", "5c21f0d3d2"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can cross-modal alignment be improved in MLLMs training?\n", "true_answer": "Cross-modal alignment in MLLMs training can be improved by designing more advanced architectures, introducing additional learning objectives, or incorporating diverse supervision signals.", "source_doc": "hallucination.pdf", "source_id": "83c3718d9d", "retrieved_docs": ["generated content remains consistent and contextually relevant to the input modality requires\nsophisticated techniques for capturing and modeling cross-modal relationships. The direction of\ncross-modal alignment encompasses both MLLMs training and hallucination evaluation. Regarding\ntraining, future research should explore methods for aligning representations between different\nmodalities. Achieving this goal may involve designing more advanced architectures, introducing\nadditional learning objectives [ 52], or incorporating diverse supervision signals [ 16]. Regarding\nevaluation, cross-modal consistency checking has been a long-standing topic, ranging from multi-\nmodal understanding [ 66,88] to text-to-image generation [ 13,17]. Drawing on proven experiences\nfrom these domains to improve the assessment of MLLM hallucination, or unifying them into an\noverall framework, may be promising research directions.\n6.3 Advancements in Model Architecture", "22 Bai, et al.\ncause hallucination. In order to improve the accuracy and reliability of hallucinated content, it is\ncrucial to ensure that MLLMs have access to high-quality and diverse training data. Future research\nshould focus on developing techniques for data collection, augmentation, and calibration. Firstly,\ncollecting enough data at the initial stage is crucial to address the data scarcity issue and increase\ndata diversity. Secondly, data augmentation is an effective solution to further expand the size of data.\nFinally, exploring methods for re-calibrating existing datasets is crucial. This includes eliminating\nbiases, promoting diversity and inclusivity, and mitigating other potential issues that may induce\nhallucinations.\n6.2 Cross-modal Alignment and Consistency\nThe key challenge of multimodal hallucination is the cross-modal consistency issue. Ensuring that\ngenerated content remains consistent and contextually relevant to the input modality requires", "Category HallucinationAttribute HallucinationRelation Hallucination\nFig. 3. Three types of typical hallucination.\nPre-training. Given that models from each modality are pre-trained on their respective data, the\nobjective of this pre-training phase is to achieve cross-modal feature alignment. During training,\nboth the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface\nbeing trained. Similar to traditional VLMs training, as exemplified by CLIP [ 88], web-scale image-\ntext pairs [ 92] are utilized for training. Given that the final output is at the LLM side, the most\nwidely used loss function in this stage is the text generation loss, typically cross-entropy loss, which\naligns with the pre-training of LLMs. Certain studies (e.g., [ 22,66]) explore the incorporation of\ncontrastive loss and image-text matching loss to further enhance alignment. After training, the\ninterface module maps the visual features into the input embedding space of the LLM.", "Data quality relevant to hallucinations can be further categorized into the following three facets.\n\u2022Noisy data. As mentioned in the definition section, training MLLMs involves two stages. The\npre-training stage employs image-text pairs crawled from the web, which contain inaccurate,\nmisaligned, or corrupted data samples. The noisy data would limit the cross-modal feature\nalignment [ 117,120], which serves as the foundation of MLLMs. As for the instruction tuning\ndata, prevalent methods, such as LLaVA [ 75], utilize the advanced GPT-4 [ 82] model to\ngenerate instructions. However, ChatGPT is a language model that cannot interpret visual\ncontent, leading to the risk of noisy data. Moreover, language models themselves suffer\nfrom the issue of hallucination [ 44], further increasing the risk. LLaVA-1.5 [ 74] adds human\nannotated QA data into instruction following and shows improved results, revealing the\neffect of noisy data.", "can reserve the original capacities of LLMs but likely have\na weak an adaptation performance, while the latter can\nfully adapt to multimodal tasks but suffer from the loss of\noriginal capacities of LLMs. More efforts should be made to\ninvestigate how to effectively balance the two aspects, so as\nto achieving improved multimodal capacities. In addition,\nexisting MLLMs are still overly dependent on the capacities\nof LLMs, which pose the limits on many multimodal tasks\n(e.g., space positioning). It will be meaningful to explore\nimproved training approaches of language models, so that\nmultimodal information can be also utilized in this process.\n\u2022Safety and alignment. Safety and alignment has been\nwidely discussed in LLMs, which aim to regulate the behav-\niors of models by technical approaches [66]. This topic is also\nimportant to MLLMs. Even a highly advanced MLLM ( e.g.,\nGPT-4V [133]) can be susceptible to safety issues. For exam-"], "retrieved_docs_id": ["83c3718d9d", "7ed2952b17", "0be0058571", "dcdb797076", "32fb098424"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}, {"question": "How can a token-wise optimal visual context reduce hallucination in MLLMs?\n", "true_answer": "By providing the most informative visual grounding when decoding a specific token in the MLLM, it can effectively reduce hallucination.\n\nHere, MLLMs refer to Multimodal Language Learning Models. The factoid answer is derived from the context, specifically the key insight provided by HALC [15].", "source_doc": "hallucination.pdf", "source_id": "17a462daf3", "retrieved_docs": ["reduce hallucination. Visual context refers to the visual tokens that can be grounded from the\ngenerated text response. An oracle study showed that decoding from the provided optimal visual\ncontexts eliminates over 84.5% of hallucinations. Based on the insight and observation, the authors\ndesigned mechanisms to locate the fine-grained visual information to correct each generated\ntoken that might be hallucinating. This is essentially a visual content-guided decoding strategy.\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that\nutilizes a visual matching score to steer the generation of the final outputs, balancing both object\nhallucination mitigation and text generation quality.\nOthers. The work of OPEAR [ 45] makes an interesting observation that most hallucinations\nare closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,", "Similarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that\nCLIPScore [ 88] can effectively distinguish between hallucinated and non-hallucinated sentences\nthrough a series of studies across different models and datasets. Based on this conclusion, it further\nrecalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which\ndesigns a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that\nare less likely to be hallucinated, and 2) guided sentence generation, which generates responses\nbased on this scoring. This is implemented in a similar way to beam search but at the sentence\nlevel.\nHALC [ 15] provides a key insight that when decoding a specific token in the MLLM, identifying a\ntoken-wise optimal visual context to provide the most informative visual grounding can effectively\nreduce hallucination. Visual context refers to the visual tokens that can be grounded from the", "supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [ 57],\nguiding MLLMs to focus on highly-related image content. With the additional supervision from\nthe mask prediction loss, MLLMs are encouraged to extract features that can better represent these\ncrucial instances, thus generating more accurate responses and mitigating vision hallucination. The\nintuitive idea of supervising MLLMs with grounding shows promising performance in mitigating\nhallucination.\nAnother line of work analyzes the training loss from the perspective of embedding space distri-\nbution. As introduced earlier, popular MLLMs typically project the encoded vision features into the\ninput space of a specific LLM. A recent work, HACL [ 52], argues that an ideal projection should\nblend the distribution of visual and textual embeddings. However, despite visual projection, a sig-\nnificant modality gap exists between textual and visual tokens, suggesting that the current learned", "MLLMs. Based on the detection result, the hallucinated content can be eliminated. Secondly, this\nwork observes that long-tail distribution and object co-occurrence in the training data are two\nprimary factors of hallucination. Thus, a counterfactual visual instruction generation strategy is\nproposed to expand the dataset. Using the proposed methods, the instruction tuning data can be\nbalanced and experience reduced hallucination. MLLMs trained on the calibrated dataset are shown\nto be less prone to hallucination.\nReCaption [ 105]This work proposes a framework called ReCaption to rewrite the text captions\nof existing image-text pairs in datasets. The framework comprises two steps: 1) keyword extraction,\nwhich extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which\nemploys an LLM to generate sentences based on the extracted keywords. Ultimately, the framework", "6.4 Establishing Standardized Benchmarks\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in as-\nsessing the degree of hallucination in MLLMs. In Table 1, it can be observed that there is a variety\nof evaluation benchmarks, but a lack of unified standards. Among them, one of the most popular\nbenchmarks might be POPE [ 69], which employs a \u2019Yes-or-No\u2019 evaluation protocol. However, this\nbinary-QA manner does not align with how humans use MLLMs. Accordingly, some benchmarks\nspecifically evaluate the hallucination of MLLMs in the (free-form) generative context. Yet, they\noften rely on external models, such as vision expert models or other LLMs, which limits their wide-\nspread application. Moving forward, future research can investigate standardized benchmarks that\nare theoretically sound and easy to use. Otherwise, research on methods to mitigate hallucinations\nmay be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature"], "retrieved_docs_id": ["31eefbd9eb", "17a462daf3", "c505f06d1a", "294848c460", "312439a972"], "reranker_type": "colbert", "search_type": "text", "rr": 0.5, "hit": 1}, {"question": "In what month and year was the preprint with arXiv ID 2404.1893v1 published in the field of computer vision?\n", "true_answer": "The preprint was published in the field of computer vision in April 2024.", "source_doc": "hallucination.pdf", "source_id": "35a7709274", "retrieved_docs": ["international conference on computer vision , pages 10012\u201310022, 2021.\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 , 2017.\n[40] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0\nregularization. arXiv preprint arXiv:1712.01312 , 2017.\n[41] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Re-\ncovering neural network quantization error through weight factorization. In International Conference on\nMachine Learning , pages 4486\u20134495. PMLR, 2019.\n[42] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843 , 2016.\n[43] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through\nweight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 1325\u20131334, 2019.", "Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 .\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning \ufb01lters for ef\ufb01cient\nconvnets. arXiv: Computer Vision and Pattern Recognition .\nYuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. 2017. Learning from noisy\nlabels with distillation. In Proceedings of the IEEE International Conference on Computer Vision , pages 1910\u2013\n1918.\nChenxing Li, Lei Zhu, Shuang Xu, Peng Gao, and Bo Xu. 2018. Compression of acoustic model via knowledge\ndistillation and pruning. In 2018 24th International Conference on Pattern Recognition (ICPR) , pages 2785\u2013\n2790. IEEE.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365 .", "quantization. arXiv preprint arXiv:1511.06488 ,\n2015.\n[230] Christian Szegedy, Vincent Vanhoucke, Sergey\nIoffe, Jon Shlens, and Zbigniew Wojna. Rethinking\nthe Inception architecture for computer vision. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition , pages 2818\u20132826,\n2016.\n[231] Shyam A Tailor, Javier Fernandez-Marques, and\nNicholas D Lane. Degree-quant: Quantization-\naware training for graph neural networks. Inter-\nnational Conference on Learning Representations ,\n2021.\n[232] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay\nVasudevan, Mark Sandler, Andrew Howard, and\nQuoc V Le. Mnasnet: Platform-aware neural\narchitecture search for mobile. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 2820\u20132828, 2019.\n[233] Mingxing Tan and Quoc V Le. Ef\ufb01cientNet:\nRethinking model scaling for convolutional neural\nnetworks. arXiv preprint arXiv:1905.11946 , 2019.\n[234] Wei Tang, Gang Hua, and Liang Wang. How to", "plications. arXiv preprint arXiv:1704.04861 , 2017.\nGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional\nnetworks. In Conference on Computer Vision and Pattern Recognition , 2017.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on Machine Learning , pp. 448\u2013456, 2015.\nKevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture for object\nrecognition? In 2009 IEEE 12th International Conference on Computer Vision , 2009.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference\non Learning Representations , 2015.\nG\u00a8unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural net-\nworks. arXiv preprint arXiv:1706.02515 , 2017.", "and Keutzer, K. Zeroq: A novel zero shot quantization\nframework. arXiv preprint arXiv:2001.00281 , 2020.\nChen, L.-C., Zhu, Y ., Papandreou, G., Schroff, F., and Adam,\nH. Encoder-decoder with atrous separable convolution forsemantic image segmentation. The European Conference\non Computer Vision (ECCV) , 2018.\nChoi, J., Wang, Z., Venkataramani, S., Chuang, P. I., Srini-\nvasan, V ., and Gopalakrishnan, K. PACT: parameterized\nclipping activation for quantized neural networks. arXiv\npreprint arxiv:805.06085 , 2018.\nChoukroun, Y ., Kravchik, E., and Kisilev, P. Low-bit quan-\ntization of neural networks for ef\ufb01cient inference. Inter-\nnational Conference on Computer Vision (ICCV) , 2019.\nDong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and\nKeutzer, K. HAWQ: hessian aware quantization of neural\nnetworks with mixed-precision. International Conference\non Computer Vision (ICCV) , 2019.\nEsser, S. K., McKinstry, J. L., Bablani, D., Appuswamy,\nR., and Modha, D. S. Learned step size quantization."], "retrieved_docs_id": ["833afe5301", "669b769e36", "d326b855a0", "8844c39736", "c2203992cc"], "reranker_type": "colbert", "search_type": "text", "rr": 0.0, "hit": 0}, {"question": "What is the optimization goal of the information condenser training process in RECOMP [Xuet al., 2023a]?\n", "true_answer": "The optimization goal is to minimize the contrastive loss, which involves maximizing the similarity between a data point and its positive sample while minimizing the similarity between the data point and negative samples.", "source_doc": "RAG.pdf", "source_id": "ba4f3a6fe9", "retrieved_docs": ["input document. The objective of the training process is to\nminimize the discrepancy between Cextracted and the actual\ncontext Ctruth as much as possible. The loss function they\nadopted is as follows:\nminL (\u03b8) =\u22121\nNNX\ni=1C(i)\ntruthlog(f.(S(i)\ninput;\u03b8)) (3)\nwhere f.is the information extractor and \u03b8is the parameter\nof the extractor. RECOMP [Xuet al. , 2023a ]similarly trains\nan information condenser by leveraging contrastive learning.\nFor each training data point, there exists one positive sample\nand five negative samples. The encoder is trained using con-\ntrastive loss [Karpukhin et al. , 2020 ]during this process.The\nspecific optimization goals are as follows:\n\u2212logesim(xi,pi)\nsim(xi, pi) +P\nnj\u2208Niesim(xi,pi)(4)", "information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-", "Where \u03b6,\u03beare learnable linear projection layers.z is the av-\nerage representations of the graph from Encoder,h is the mean\nof decoder representations. z\u2032,h\u2032represent the corresponding\nnegative samples respectively. In the given text, \u2019h\u201d and\n\u2019z\u201d represent negative samples. By introducing a contrastive\nlearning objective, the model can learn to generate diverse\nand reasonable replies better, rather than just the one seen in\nthe training data. This helps to mitigate the risk of overfitting\nand improves the model\u2019s generalization ability in real-world\nscenarios.\nWhen dealing with retrieval tasks that involve structured\ndata, the work of SANTA [Liet al. , 2023d ]utilized a three-\nstage training process to fully understand the structural and\nsemantic information. Specifically, in the training phase\nof the retriever, contrastive learning was adopted, with the\nmain goal of optimizing the embedding representations of the\nqueries and documents. The specific optimization objectives", "cross-attention scores, selecting the highest scoring input to-\nkens to effectively filter tokens. RECOMP [Xuet al. , 2023a ]\nproposes extractive and generative compressors, which gen-\nerate summaries by selecting relevant sentences or syn-\nthesizing document information to achieve multi-document\nquery focus summaries.In addition to that, a novel approach,\nPKG [Luoet al. , 2023 ], infuses knowledge into a white-box\nmodel through directive fine-tuning, and directly replaces the\nretriever module, used to directly output relevant documents\nbased on the query.\n5 Generator\nAnother core component in RAG is the generator, responsible\nfor transforming retrieved information into natural and fluent\ntext. Its design is inspired by traditional language models,\nbut in comparison to conventional generative models, RAG\u2019s\ngenerator enhances accuracy and relevance by leveraging the\nretrieved information. In RAG, the generator\u2019s input includes", "83\nels still can\u2019t well process the information in the context\nwindow [299]. To address this issue, specific architecture\nadaptations or algorithms might be needed to enhance the\nmodeling and utilization of long context information. An-\nother worrying concern is that existing work mostly focuses\non training LLMs with decoder-only Transformers. Despite\nthe effectiveness, it severely limits the more wide, diverse\nexplorations on alternative model architectures.\nModel Training. For pre-training, it is essential to establish\na data-centric infrastructure and training procedure for LLM\noptimization, which can effectively support a systematic\nprocess of data collection, data cleaning, data mixture, and\ndata curriculum. Furthermore, it also calls for more flexible\nmechanisms of hardware support or resource schedule, so\nas to better organize and utilize the resources in a computing\ncluster. In practice, it is very challenging to pre-train capable"], "retrieved_docs_id": ["ba4f3a6fe9", "7fabdba415", "29b4a935b2", "cd69a480bb", "e168d0108d"], "reranker_type": "colbert", "search_type": "text", "rr": 1.0, "hit": 1}]