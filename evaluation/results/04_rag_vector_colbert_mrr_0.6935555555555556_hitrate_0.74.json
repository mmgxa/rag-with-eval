[{"question": "What is a significant advantage of State Space Models over attention mechanisms?\n", "true_answer": "State Space Models offer near-linear computational complexity, providing an advantage over attention mechanisms.", "source_doc": "multimodal.pdf", "source_id": "85b5cac71b", "retrieved_docs": ["SSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)\nmodel [153], which proposes fully diagonal parameterization of state spaces for greater efficiency.\nH3 stacks two SSMs to interact with their output and input projection, bridging the gap between\nSSMs and attention while adapting to modern hardware. Mamba [77], a selective state space model,\nhas been introduced as a strong competitor to the Transformer architecture in large language models.\nMamba incorporates a selection mechanism to eliminate irrelevant data and develops a hardware-\naware parallel algorithm for recurrent operation. This results in competitive performance compared\nto LLMs of the same capacity, with faster inference speeds that scale linearly with time and con-\nstant memory usage. In conclusion, State Space Models offer significant potential as an alternative\nto attention mechanisms by providing near-linear computational complexity and effectively captur-", "further engineering challenges and adjustments to the model that are not discussed in this paper.\n6 Conclusion\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture,\nMamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance\nof strong Transformer models. We are excited about the broad applications of selective state space models to\nbuild foundation models for di\ufb00erent domains, especially in emerging modalities requiring long context such as\ngenomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\nbackbone.\nAcknowledgments\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\nReferences", "Figure 13: The elements(left) block(middle) and architecture(right) in RWKV [151].\nThis approach parallelizes computations during training and maintains constant computational and\nmemory complexity during inference.\nState Space Models (SSMs) [152] can be formulated as a type of RNN for efficient autoregressive\ninference and have emerged as a promising alternative to attention mechanisms, offering near-linear\ncomputational complexity compared to the quadratic complexity of attention. SSMs are formulated\nas x\u2019(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), mapping a single-dimension input signal u(t) to an N-\ndimension latent state x(t) before projecting it to a single-dimension output signal y(t), with A, B, C,\nand D being parameters learned by gradient descent [152]. Several techniques have been proposed\nto enhance SSMs, such as the Structured State Space sequence model (S4) [152], which refines\nSSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)", "insufficient for length generalization in the context\nof reasoning tasks. Instead, they propose combin-\ning in-context learning and scratchpad/chain-of-\nthought reasoning to enable LLMs to generalize to\nunseen sequence lengths in- and out-of-distribution,\nwith performance scaling with model size. The au-\nthors report that fine-tuning can further improve\nmodel performance dependent on the task perfor-\nmance of the baseline.\nTransformer Alternatives While Transformers\nare the dominant paradigm in LLMs today due to\ntheir strong performance, several more efficient\nalternative architectures exist. One line of work\ntries to replace the attention mechanism using state\nspace models (SSMs), which offer near-linear com-\nputational complexity w.r.t. the sequence length.\nDao et al. [108] investigate the weaknesses of state\nspace models (SSMs) in language modeling and\nfind that existing approaches struggle with recall-\ning previous tokens and comparing tokens in the", "sequence length requires /u1D442./u1D435/u1D43F/u1D437/u1D441 )time and memory; this is the root of the fundamental e\ufb03ciency bottleneck\naddressed in Section 3.3.\nGeneral State Space Models. We note that the term state space model has a very broad meaning which simply\nrepresents the notion of any recurrent process with a latent state. It has been used to refer to many disparate\nconcepts in di\ufb00erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner\net al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)),\nKalman \ufb01lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS)\n(machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4"], "retrieved_docs_id": ["85b5cac71b", "53f73ec6b6", "bb2e9ee3f0", "9e5f877b03", "79e095312d"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does VCoder enhance the object identification ability of MLLMs?\n", "true_answer": "VCoder enhances the object identification ability of MLLMs by utilizing additional perception formats, such as segmentation masks and depth maps.", "source_doc": "hallucination.pdf", "source_id": "c461600dc0", "retrieved_docs": ["task encoders are dedicated to integrating various types of latent visual information extracted by\nmultiple visual encoders. Additionally, the structural knowledge enhancement module is designed\nto utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from\nvisual inputs.\nFollowing the approach of the structural knowledge enhancement module in [ 38], another line\nof research investigates the utilization of vision tool models to enhance the perception of MLLMs.\nVCoder [ 49] utilizes additional perception formats, such as segmentation masks and depth maps,\nto enhance the object identification ability of the MLLM. Another work [ 54] ensembles additional\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "robust evaluation of object hallucination, POPE [844] pro-\nposes a polling-based object probing approach for convert-\ning object recognition into a series of binary questions, and\nthe results indicate that current MLLMs often struggle with\nobject hallucination. Cognition tasks, on the other hand, re-\nquire MLLMs to perform reasoning based on image percep-\ntion. A common reasoning task is visual question answering\n(VQA), where models answer questions about images that\ndemand reasoning about spatial relationships [845], general\nknowledge [846], or scene text [847]. To fully explore the\ncapabilities of MLLMs, HallusionBench [848] collects 200\nsophisticated visual dependent or supplement questions, on\nwhich even the most advanced MLLMs like LLaVA-1.5 [831]\nand GPT-4V [133] fail to achieve good performance.\n\u2022Evaluation paradigms. The responses of MLLMs can\nbe evaluated either in a closed-ended or an open-ended\nmanner. Traditional multimodal tasks often rely on a closed-", "image captions and object bounding boxes as visual inputs\nfor assessment. Such open-ended evaluation methods can\nimprove assessment accuracy while incurring higher costs\ndue to the involvement of humans or LLMs.\n\u2022Evaluation benchmarks. To facilitate a more thorough\nevaluation of MLLMs, various benchmarks have been devel-\noped. Part of them collect existing vision-language tasks for\ncomprehensive evaluation. For instance, LVLM-eHub [852]\naggregates 47 existing text-related visual tasks to assess\nsix distinct capabilities of MLLMs, and Reform-Eval [853]\ntakes this a step further by standardizing questions from\nexisting benchmarks into a uniform format and discusses\nhow the backbone models influence MLLMs\u2019 performance.\nIn addition to incorporating existing tasks, several work\nalso derives new questions annotated by humans or with\nthe help of LLMs. MME [839] creates a dataset by pair-\ning images from public sources with manually-collected", "77\ntext output. To boost the performance, high-quality visual\ninstruction data is key to eliciting and enhancing the abil-\nities of MLLMs. Therefore, most studies are dedicated to\nconstructing various visual instruction datasets. As the basic\napproaches, early studies construct visual instructions by\ndistilling from GPT-4 [149] or reformulating vision-language\ntask datasets [151]. To enhance the quality of instruction\ndata, recent work further proposes improved strategies by\nincreasing the instruction diversity [834], incorporating fine-\ngrained information ( e.g., coordinate of objects) into the\ninstruction [833], or synthesizing complex visual reasoning\ninstructions [835].\nEvaluation of MLLM. After introducing the approaches to\ndeveloping MLLMs, we further discuss how to effectively\nassess the multimodal capabilities of MLLMs from the fol-\nlowing three aspects.\n\u2022Evaluation perspectives. The evaluation tasks for MLLMs\ncan be categorized into two main types: perception and", "VHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual\nproperties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which\nemerge from comparisons across multiple objects, such as relative size, relative position, and\ncounting. Based on such categorization, the authors further defined 8 visual hallucination modes,\nproviding a very detailed evaluation of hallucination in MLLMs. Furthermore, the collected 1,200\nevaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no\nquestion\" (YNQ). Such design enables this benchmark to evaluate both generative and discriminative\ntasks.\nComparison of mainstream models We compare the mainstream MLLMs on some represen-\ntative benchmarks, providing a holistic overview of their performance from different dimensions.\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We"], "retrieved_docs_id": ["c461600dc0", "736e8a6bfb", "c3936a45a4", "722c60f298", "8ef8344de6"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the standard self-attention mechanism's time complexity affect large language models?\n", "true_answer": "The standard self-attention mechanism has a time complexity of O(n^2), where n is the sequence length. This quadratic complexity can lead to scalability issues, particularly when dealing with long sequences in large language models (LLMs).", "source_doc": "multimodal.pdf", "source_id": "323641b323", "retrieved_docs": ["Figure 11: Organization of efficient large language models advancements.\nOccupying a significant majority of the parameter volume in MLLMs, LLM serves as a crucial entry\npoint for enhancing the efficiency of MLLMs. In this section, similar to the survey paper [160], we\nprovide a brief overview of the research progress in efficient LLMs, offering inspiration for the\ndevelopment of Efficient MLLMs.\n4.1 Attention\nIn the standard self-attention mechanism, the time complexity is O(n2), where nis the sequence\nlength. This quadratic complexity arises due to the pairwise interactions between all input tokens,\nwhich can lead to scalability issues, especially when dealing with long sequences in LLMs. To\ntackle this, researchers have developed techniques to expedite attention mechanisms and reduce\ntime complexity, such as sharing-based attention, feature information reduction, kernelization or\nlow-rank, fixed and learnable pattern strategies, and hardware-assisted attention.", "ory. Observing a serial-position-like effect in lan-\nguage models is perhaps surprising, since the self-\nattention mechanisms underlying Transformer lan-\nguage models is technically equally capable of re-\ntrieving any token from their contexts.\n7 Conclusion\nWe empirically study how language models use\nlong input contexts via a series of controlled ex-\nperiments. We show that language model perfor-\nmance degrades significantly when changing the\nposition of relevant information, indicating that\nmodels struggle to robustly access and use infor-\nmation in long input contexts. In particular, per-\nformance is often lowest when models must use\ninformation in the middle of long input contexts.\nWe conduct a preliminary investigation of the role\nof (i) model architecture, (ii) query-aware contextu-\nalization, and (iii) instruction fine-tuning to better\nunderstand how they affect how language models\nuse context. Finally, we conclude with a practi-\ncal case study of open-domain question answering,", "attention mechanism with a linear attention approx-\nimation to achieve speed-ups between 4.9x and\n12.1x for auto-regressive language modeling while\nobtaining similar perplexities as a standard Trans-\nformer model. Ding et al. [124] propose dilated\nattention which splits a sequence into equally long\nsegments and processes each of these in parallel\nusing a sparsified attention mechanism. Dilated\nattention offers a linear computational complexity\nin the sequence length and, applied hierarchically,\nenables inputs of up to 1B tokens.\nLength Generalization As the required compute\nof Transformer-based LLMs grows quadratic with\nthe sequence length, it is a desired property to build\nLLMs that can be trained on short sequences andgeneralize well to significantly longer sequences\nduring inference.\nThe fundamental building block of the Trans-\nformer architecture is the self-attention mechanism.\nIt is permutation-invariant; therefore, the output is", "and reinforcement learning [ JLL21 ,CLR+21,WWX+22]. Remarkable success of the self-attention mechanism and\ntransformers has paved the way for the development of sophisticated language models such as GPT4 [ Ope23 ], Bard\n[Goo23], LLaMA [TLI+23], and ChatGPT [Ope22].\nQ:Can we characterize the optimization landscape and implicit bias of transformers?\nHow does the attention layer select and compose tokens when trained with gradient descent?\nWe address these questions by rigorously connecting the optimization geometry of the attention layer and a hard\nmax-margin SVM problem, namely (Att-SVM) , that separates and selects the optimal tokens from each input sequence.\nThis formalism, which builds on the recent work [ TLZO23 ], is practically meaningful as demonstrated through\nexperiments, and sheds light on the intricacies of self-attention. Throughout, given input sequences X,Z\u2208RT\u00d7dwith\nlength Tand embedding dimension d, we study the core cross-attention and self-attention models:", "Model Time Space\nTransformer O(T2d) O(T2+T d)\nReformer O(TlogT d)O(TlogT+T d)\nPerformer O(T d2logd)O(T dlogd+d2logd)\nLinear Transformers O(T d2) O(T d+d2)\nAFT-full O(T2d) O(T d)\nAFT-local O(T sd) O(T d)\nMEGA O(cT d) O(cd)\nRWKV (ours) O(Td) O(d)\nTable 1: Inference complexity comparison with different\nTransformers. Here Tdenotes the sequence length,\ndthe feature dimension, cis MEGA\u2019s chunk size of\nquadratic attention, and sis the size of a local window\nfor AFT.\nLLaMA (Touvron et al., 2023), and Chinchilla\n(Hoffmann et al., 2022) showcase the potential of\nTransformers in NLP. However, the self-attention\nmechanism\u2019s quadratic complexity makes it compu-\ntationally and memory intensive for tasks involving\nlong sequences and constrained resources. This\nhas stimulated research to enhance Transformers\u2019\nscalability, sometimes sacrificing some of their ef-\nfectiveness (Wang et al., 2020; Zaheer et al., 2020;\nDao et al., 2022a).\nTo tackle these challenges, we introduce the Re-"], "retrieved_docs_id": ["323641b323", "e8c8d8c36a", "60ba319218", "9b0e4abb0c", "3f7442053f"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the source of ground truth objects used in the CCEval metric?\n", "true_answer": "The source of ground truth objects used in the CCEval metric is Visual Genome.", "source_doc": "hallucination.pdf", "source_id": "6e78496733", "retrieved_docs": ["randomly samples 100 images from Visual Genome to form a benchmark. In evaluation, GPT-4\nis utilized to parse the captions generated by MLLMs and extract objects. Additionally, this work\nintroduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\nThis metric computes the ratio of objects in the caption that match the ground truth to the total\nnumber of ground truth objects. It additionally records the average number of objects as well as\nthe average length of captions as auxiliary metric. Compared with CHAIR, CCEval employs more\ndiverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption\nparsing (GPT-4 vs. rule-based tool).\nMERLIM [ 100]MERLIM ( Multi-modal Evaluation benchma Rk for Large Image-language\nModels) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks,\nincluding object recognition, instance counting, and identifying object-to-object relationships.", "MHaluBench [13] arXiv\u201924 Feb. MSCOCO [70] 1,860 Gen Acc/P/R/F \u2713 \u2713 \u2717 T2I\nVHTest [46] arXiv\u201924 Feb. MSCOCO [70] 1,200 Dis & Gen Acc \u2713 \u2713 \u2717 \u2713\nHal-Eavl [53] arXiv\u201924 Feb.MSCOCO [70] &\nLAION [92]10,000 Dis & GenAcc/P/R/F &\nLLM Assessment\u2713 \u2713 \u2713 Obj. Event\n(denoted as CHAIR \ud835\udc60):\nCHAIR \ud835\udc56=|{hallucinated objects }|\n|{all objects mentioned }|,\nCHAIR \ud835\udc60=|{sentences with hallucinated object }|\n|{all sentences}|.\nIn the paper of CHAIR [ 90], the range of objects is restricted to the 80 MSCOCO objects. Sentence\ntokenization and synonyms mapping are applied to determine whether a generated sentence\ncontains hallucinated objects. Ground-truth caption and object segmentations both serve as ground-\ntruth objects in the computation. In the MLLM era, this metric is still widely used for assessing the\nresponse of MLLMs.\nPOPE [ 69]. When used in MLLMs, the work of [ 69] argues that the CHAIR metric can be\naffected by the instruction designs and the length of generated captions. Therefore, it proposes a", "10 Bai, et al.\nTable 1. Summary of most relevant benchmarks and metrics of object hallucination in MLLMs. The order is\nbased on chronological order on arxiv. In the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1-\nScore.\nBenchmark VenueUnderlying\nData SourceSizeTask\nTypeMetricHallucination Type\nCategory Attribute Relation Others\nCHAIR [90] EMNLP\u201918 MSCOCO [70] 5,000 Gen CHAIR \u2713 \u2717 \u2717 \u2717\nPOPE [69] EMNLP\u201923 MSCOCO [70] 3,000 Dis Acc/P/R/F1 \u2713 \u2717 \u2717 \u2717\nMME [113] arXiv\u201923 Jun MSCOCO [70] 1457 Dis Acc/Score \u2713 \u2713 \u2717 \u2713\nCIEM [42] NeurIPS-W\u201923 MSCOCO [70] 78120 Dis Acc \u2713 \u2717 \u2717 \u2717\nM-HalDetect [32] arXiv\u201923 Aug. MSCOCO [70] 4,000 Dis Reward Model Score \u2713 \u2717 \u2717 \u2717\nMMHal-Bench [96] arXiv\u201923 Sep. Open-Images [61] 96 Gen LLM Assessment \u2713 \u2717 \u2717 \u2713\nGAVIE [73] ICLR\u201924 Visual-Genome [59] 1,000 Gen LLM Assessment Not Explicitly Stated\nNOPE [77] arXiv\u201923 Oct. Open-Images [61] 36,000 Dis Acc/METEOR [3] \u2713 \u2717 \u2717 \u2717\nHaELM [104] arXiv\u201923 Oct. MSCOCO [70] 5,000 Gen LLM Assessment Not Explicitly Stated", "lent to the correct reference, which is the default accuracy metric for MATH.\nEquivalent (chain-of-thought). The correctness condition for equivalent (chain-of-thought) is if the\nmodel generation is mathematically equivalent to the correct reference, which is the default accuracy metric\nforMATH (chain-of-thought).\nC.2 Calibration and uncertainty\nWe first setup some formal notation, then state the metrics in the population (\u201cinfinite data\u201d) setting, and\nfinally give formulas for the metrics that we actually compute on finite data.\nFormal setup. We measure calibration metrics for classification tasks. Given an input x, let the true label\nbey\u2208[k] ={1, . . . , k}and the model\u2019s predicted probability be p\u2208[0,1]kwhere\u2211\njpj= 1. Here, pjdenotes\nthe model\u2019s confidence that the true label is j. Let \u02c6y= arg max j\u2208[k]pjbe the model\u2019s predicted label, and\npmax= max j\u2208[k]pjdenote the model\u2019s confidence in its predicted label \u02c6y(the model\u2019s \u201ctop probability\u201d).", "incurring an ECE-10 of roughly 0.5 or more for both NaturalQuestions variants,QuAC,NarrativeQA ,\nandTruthfulQA . In contrast, some models are quite calibrated for some scenarios (e.g. Cohere xlarge\nv20220609 (52.4B) has a calibration error of 0.06 on NarrativeQA ).\nFor robustness and fairness, we see all models tend to show consistent drops of 5 to 10 points for all scenarios.\nConsistent with the broader trends, we find robustness and fairness are strongly correlated with accuracy,\nwith no observed cases of the most accurate models suffering especially large drops for robustness/fairness.\nOne exception is HellaSwag , where the three most accurate models are the only models with standard\naccuracies above 80% (text-davinci-002 = 81.5%, Cohere xlarge v20220609 (52.4B) = 81.1%, Anthropic-LM\nv4-s3 (52B) = 80.4%), and only text-davinci-002 remains about 70% in the presence of fairness perturbations"], "retrieved_docs_id": ["6e78496733", "84a3c00c17", "8705831e19", "6f291feee3", "31a0ab4048"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a challenge in the integration process of retrieved passages in the generation task?\n", "true_answer": "If not handled properly, the output might appear incoherent or disjointed.", "source_doc": "RAG.pdf", "source_id": "e75af48a5e", "retrieved_docs": ["equally diverse. Hallucination is a prominent issue where the\nmodel fabricates an answer that doesn\u2019t exist in the context.\nIrrelevance is another concern where the model generates an\nanswer that fails to address the query. Further, toxicity or\nbias, where the model generates a harmful or offensive re-\nsponse, is another problem.\nFinally, the augmentation process also faces several chal-\nlenges. Crucially, the effective integration of the context from\nretrieved passages with the current generation task is of ut-\nmost importance. If mishandled, the output might appear in-\ncoherent or disjointed. Redundancy and repetition are another\nissue, particularly when multiple retrieved passages contain\nsimilar information, leading to content repetition in the gen-\neration step. Moreover, determining the importance or rele-\nvance of multiple retrieved passages to the generation task is\nchallenging, and the augmentation process needs to balance", "vant knowledge without altering the parameters of LLMs, en-\nabling the model to perform more sophisticated tasks. Addi-\ntionally, CREA-ICL [Liet al. , 2023b ]leverages synchronous\nretrieval of cross-lingual knowledge to assist in acquiring ad-\nditional information, while RECITE forms context by sam-\npling one or more paragraphs from LLMs.\nDuring the inference phase, optimizing the process of RAG\ncan benefit adaptation to more challenging tasks. For ex-ample, ITRG [Feng et al. , 2023a ]enhances adaptability for\ntasks requiring multiple-step reasoning by iteratively retriev-\ning and searching for the correct reasoning path. ITER-\nRETGEN [Shao et al. , 2023 ]employs an iterative approach\nto coalesce retrieval and generation, achieving an alternating\nprocess of \u201dretrieval-enhanced generation\u201d and \u201dgeneration-\nenhanced retrieval.\u201d\nOn the other hand, IRCOT [Trivedi et al. , 2022 ]merges the\nconcepts of RAG and CoT [Weiet al. , 2022 ], employing al-", "Recite-Read [Sunet al. , 2022 ]transforms external re-\ntrieval into retrieval from model weights, initially hav-\ning LLM memorize task-relevant information and gener-\nate output for handling knowledge-intensive natural lan-\nguage processing tasks.\n\u2022Adjusting the Flow between Modules In the realm of\nadjusting the flow between modules, there is an empha-\nsis on enhancing interaction between language models\nand retrieval models. DSP [Khattab et al. , 2022 ]intro-\nduces the Demonstrate-Search-predict framework, treat-\ning the context learning system as an explicit program\nrather than a terminal task prompt to address knowledge-\nintensive tasks. ITER-RETGEN [Shao et al. , 2023 ]\nutilizes generated content to guide retrieval, itera-\ntively performing \u201cretrieval-enhanced generation\u201d and\n\u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read-\nRetrieve-Read flow. Self-RAG [Asai et al. , 2023b ]fol-\nlows the decide-retrieve-reflect-read process, introduc-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "which refers to the conflict phenomenon of integrating new\nand old knowledge. Similar cases also occur in human align-\nment of LLMs, where \u201c alignment tax \u201d [66] ( e.g., a potential\nloss in the in-context learning ability) has to be paid for\naligning to human values and needs. Moreover, due to\nthe limitations of sequence modeling architecture, LLMs\nstill face challenges in the understanding and generation\nof structured data. Consequently, they often fall behind\ntask-specific models on complex structured data tasks, such\nas knowledge-base question answering and semantic pars-\ning [458, 651]. Therefore, it is important to develop effective\nmodel specialization methods that can flexibly adapt LLMs\nto various task scenarios, meanwhile retaining the original\nabilities as possible.\nUnderperforming Specialized Generation\nLLMs may fall short in mastering generation\ntasks that require domain-specific knowledge or\ngenerating structured data. It is non-trivial to"], "retrieved_docs_id": ["e75af48a5e", "1f6c13012c", "dfac20a7d8", "fefa202c19", "545745e0d3"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the ChipNeMo project adapt large language models for chip design?\n", "true_answer": "The ChipNeMo project adapts large language models for chip design using domain-adaptive tokenization, domain-adaptive continued pretraining, and model alignment techniques.", "source_doc": "ChipNemo.pdf", "source_id": "36c5c0c7f1", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\nMingjie Liu* 1Teodor-Dumitru Ene* 1Robert Kirby* 1Chris Cheng* 1Nathaniel Pinckney* 1\nRongjian Liang* 1Jonah Alben1Himyanshu Anand1Sanmitra Banerjee1Ismet Bayraktaroglu1\nBonita Bhaskaran1Bryan Catanzaro1Arjun Chaudhuri1Sharon Clay1Bill Dally1Laura Dang1\nParikshit Deshpande1Siddhanth Dhodhi1Sameer Halepete1Eric Hill1Jiashang Hu1Sumit Jain1\nAnkit Jindal1Brucek Khailany1George Kokai1Kishor Kunal1Xiaowei Li1Charley Lind1Hao Liu1\nStuart Oberman1Sujeet Omar1Ghasem Pasandi1Sreedhar Pratty1Jonathan Raiman1Ambar Sarkar1\nZhengjiang Shao1Hanfei Sun1Pratik P Suthar1Varun Tej1Walker Turner1Kaizhe Xu1Haoxing Ren1\nAbstract\nChipNeMo aims to explore the applications of\nlarge language models (LLMs) for industrial chip\ndesign. Instead of directly deploying off-the-\nshelf commercial or open-source LLMs, we in-\nstead adopt the following domain adaptation tech-\nniques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 4: Domain-Adapted ChipNeMo Tokenizer Improvements.\n3.1. Domain-Adaptive Tokenization\nWe adapt the LLaMA2 tokenizer (containing 32K tokens) to\nchip design datasets using the previously outlined four-step\nprocess. Approximately 9K new tokens are added to the\nLLaMA2 tokenizer. The adapted tokenizers can improve\ntokenization efficiency by 1.6% to 3.3% across various chip\ndesign datasets as shown in Figure 4. We observe no obvious\nchanges to tokenizer efficiency on public data. Importantly,\nwe have not observed significant decline in the LLM\u2019s accu-\nracy on public benchmarks when using the domain-adapted\ntokenizers even prior to DAPT.\n3.2. Domain Adaptive Pretraining\nFigure 5: Chip Domain Benchmark Result for ChipNeMo.\nFigure 5 presents the outcomes for ChipNeMo models on\nthe AutoEval benchmark for chip design domain (detailed\nin Appendix A.5). Results on open domain academic bench-\nmark results are presented in Appendix A.6. Our research", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ntask assignnment. Participants are tasked with rating the\nmodel\u2019s performance on a 7-point Likert scale for each of\nthese three assignments. The results can be found in Fig-\nure 10. Although the GPT-4 model excels in all three tasks,\noutperforming both our ChipNeMo-70B-Steer model and\nthe LLaMA2-70B-Chat model, ChipNeMo-70B-Steer does\nexhibit enhancements compared to the off-the-shelf LLaMA\nmodel of equivalent size. We attribute the comparatively\nlower improvements in summarization tasks resulting from\nour domain-adaptation to the limited necessity for domain-\nspecific knowledge in summarization compared to other\nuse-cases.\n4. Related Works\nMany domains have a significant amount of proprietary data\nwhich can be used to train a domain-specific LLM. One ap-\nproach is to train a domain specific foundation model from\nscratch, e.g., BloombergGPT(Wu et al., 2023) for finance,\nBioMedLLM(Venigalla et al., 2022) for biomed, and Galac-"], "retrieved_docs_id": ["36c5c0c7f1", "a6c3d05123", "df0b9868f2", "ac7c0c980b", "74fe22ec46"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is one of the benchmarks used for evaluating video comprehension in multimodal large language models?\n", "true_answer": "Video-LLaV A", "source_doc": "multimodal.pdf", "source_id": "d85947fa4f", "retrieved_docs": ["121\n\u201cReform-eval: Evaluating large vision language mod-\nels via unified re-formulation of task-oriented bench-\nmarks,\u201d CoRR , vol. abs/2310.02569, 2023.\n[854] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and\nY. Shan, \u201cSeed-bench: Benchmarking multimodal\nllms with generative comprehension,\u201d CoRR , vol.\nabs/2307.16125, 2023.\n[855] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu,\nX. Wang, and L. Wang, \u201cMm-vet: Evaluating large\nmultimodal models for integrated capabilities,\u201d CoRR ,\nvol. abs/2308.02490, 2023.\n[856] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y. Jiang,\n\u201cTo see is to believe: Prompting GPT-4V for better\nvisual instruction tuning,\u201d CoRR , vol. abs/2311.07574,\n2023.\n[857] Y. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang,\nand T. Sun, \u201cLlavar: Enhanced visual instruction tun-\ning for text-rich image understanding,\u201d arXiv preprint\narXiv:2306.17107 , 2023.\n[858] X. Qi, K. Huang, A. Panda, M. Wang, and P . Mittal,\n\u201cVisual adversarial examples jailbreak aligned large", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "Thoughts (PoT) learning and Visual Token Merging strategy while excelling in faster inference\nspeed at the same time. TextHawk [36] explores efficient fine-grained perception by designing four\ndedicated components to address challenges posed by document-oriented tasks. HRVDA [66] and\nMonkey [65] are also large multimodal models designed to address the challenges posed by high-\nresolution requirements in visual document understanding tasks.\n7.3 Video Comprehension\nVideos provide an impressively accurate representation of how humans continuously perceive the\nvisual world. Intelligent video understanding is vital for a variety of real-world applications, in-\ncluding video category classification, video captioning, and video-text retrieval. Several works like\nvideoChat [197] and Video-LLaMA [198] are LLM-based large multimodal models for end-to-end\nchat-centric video comprehension. However, these methods can only take in a limited number of\nframes for short video understanding.", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "discontinuous improvements, where scaling from 62B to 540B results in a drastic jump in accuracy\ncompared to scaling from 8B to 62B. Such behavior is observed on roughly 25% of the BIG-bench tasks\nin Section 6.2. This suggests that new capabilities of large LMs can emerge when the model achieves\nsu\ufb03cient scale, and that these capabilities continue to emerge beyond previously studied scales.\n\u2022Multilingual understanding \u2013 Previous work on large language models have conducted limited\nevaluations in the multilingual domain. In this work, we conduct a more thorough evaluation of\nmultilingual benchmarks including machine translation (Section 6.5), summarization (Section 6.6), and\nquestion answering (Section 6.7) in a wide variety of languages. Even with a relatively small proportion\nof non-English data ( \u224822%) in the training corpus, few-shot evaluation results from the 540B model\nare able to bridge the gap with prior \ufb01netuned state of the art in non-English summarization tasks"], "retrieved_docs_id": ["2678016e21", "542e5c49da", "73fba2ab9b", "72dc971633", "d18e8a2a71"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does LURE correct hallucinations in generated text?\n", "true_answer": "LURE corrects hallucinations in generated text using a hallucination revisor, which transforms potentially hallucinatory descriptions into accurate ones. This is achieved by training the revisor model on a dataset, with the goal of reconstructing clean data from corrupted input.", "source_doc": "hallucination.pdf", "source_id": "ceeab98980", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "Another interesting study observes that the hallucination of MLLMs seems to be easily triggered\nby paragraph break \u2018\\n\\n\u2019 [ 36]. Based on this observation, this work proposes two simple methods\nto reduce hallucination by avoiding generating \u2018\\n\u2019 during generation. First, intuitively, users can\ndesign the prompt to instruct the model to output responses within one paragraph, avoiding \u2018\\n\u2019.\nBesides, the authors tried to alter the output logits during generation by manually lowering the\nprobability of generating \u2018\\n\u2019. Experimental results show that this simple strategy can alleviate\nhallucination on popular benchmarks.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "MLLMs. Based on the detection result, the hallucinated content can be eliminated. Secondly, this\nwork observes that long-tail distribution and object co-occurrence in the training data are two\nprimary factors of hallucination. Thus, a counterfactual visual instruction generation strategy is\nproposed to expand the dataset. Using the proposed methods, the instruction tuning data can be\nbalanced and experience reduced hallucination. MLLMs trained on the calibrated dataset are shown\nto be less prone to hallucination.\nReCaption [ 105]This work proposes a framework called ReCaption to rewrite the text captions\nof existing image-text pairs in datasets. The framework comprises two steps: 1) keyword extraction,\nwhich extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which\nemploys an LLM to generate sentences based on the extracted keywords. Ultimately, the framework", "recently, HaluEval [602] creates a large-scale LLM-generated\nand human-annotated hallucinated samples to evaluate the\nability of language models to recognize hallucination in both\ntask-specific and general scenarios.\nHallucination\nLLMs are prone to generate untruthful informa-\ntion that either conflicts with the existing source\nor cannot be verified by the available source.\nEven the most powerful LLMs such as ChatGPT\nface great challenges in migrating the hallucina-\ntions of the generated texts. This issue can be\npartially alleviated by special approaches such as\nalignment tuning and tool utilization.\n\u2022Knowledge recency . As another major challenge, LLMs\nwould encounter difficulties when solving tasks that require", "merely means the model generated an output that\ncan neither be grounded nor contradicted by the\nsource content. This is still, to some degree, un-\ndesirable as the provided information cannot be\nverified. We illustrate intrinsic and extrinsic hallu-\ncinations in Fig. 8.\nHallucination [293, 458, 241]\nGenerated text that is fluent and natural but\nunfaithful to the source content (intrinsic)\nand/or under-determined (extrinsic).\nLiu et al. [328] attribute hallucinations com-\nmonly observed in LLMs to an architectural flaw in\nTransformer models while observing that recurrent\nneural networks perfectly solve their minimalistic\nsynthetic benchmarks, designed to isolate the is-sue of hallucination in the context of algorithmic\nreasoning. Here, we focus on ways to address hal-\nlucinations in LLMs without changing the model\narchitecture itself, including (i) supplying the LLM\nwith relevant sources ( retrieval augmentation ) or\n(ii) decoding strategies."], "retrieved_docs_id": ["92e73c053a", "3fc78f0ef0", "294848c460", "fa2581a685", "aa24958f00"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does the Knowledge Updates feature affect the retrieval knowledge base?\n", "true_answer": "The Knowledge Updates feature directly updates the retrieval knowledge base, ensuring information remains current without the need for frequent retraining. This is suitable for dynamic data environments.", "source_doc": "RAG.pdf", "source_id": "9c38efbac6", "retrieved_docs": ["Feature Comparison RAG Fine-tuning\nKnowledge UpdatesDirectly updates the retrieval knowledge\nbase, ensuring information remains current\nwithout the need for frequent retraining, suit-\nable for dynamic data environments.Stores static data, requiring retraining for\nknowledge and data updates.\nExternal KnowledgeProficient in utilizing external resources,\nparticularly suitable for documents or other\nstructured/unstructured databases.Can be applied to align the externally learned\nknowledge from pretraining with large lan-\nguage models, but may be less practical for\nfrequently changing data sources.\nData ProcessingRequires minimal data processing and han-\ndling.Relies on constructing high-quality datasets,\nand limited datasets may not yield significant\nperformance improvements.\nModel CustomizationFocuses on information retrieval and inte-\ngrating external knowledge but may not fully\ncustomize model behavior or writing style.Allows adjustments of LLM behavior, writ-", "employs a graph encoding method that reflects the graph\nstructure into PTMs\u2019 representation space and utilizes a\nmulti-modal contrastive learning objective between graph-\ntext modes to ensure consistency between retrieved facts\nand generated text. KnowledgeGPT [Wang et al. , 2023c ]\ngenerates search queries for Knowledge Bases (KB) in code\nformat and includes predefined KB operation functions.\nApart from retrieval, KnowledgeGPT also offers the ca-\npability to store knowledge in a personalized knowledge\nbase to meet individual user needs. These structured data\nsources provide RAG with richer knowledge and context,\ncontributing to improved model performance.\nLLM Generated Content RAG\nObserving that the auxiliary information recalled by RAG\nis not always effective and may even have negative effects,\nsome studies have expanded the paradigm of RAG by delving\ndeeper into the internal knowledge of LLM. This approach\nutilizes the content generated by LLM itself for retrieval, aim-", "outperform 10\u00d7larger ones [653, 657]. Further, open-book\nQA tasks can be also employed to evaluate the recency\nof knowledge information. Pre-training or retrieving from\noutdated knowledge resources may cause LLMs to generate\nincorrect answers for time-sensitive questions [653].\nKnowledge Completion. In knowledge completion tasks,\nLLMs might be (to some extent) considered as a knowledge\nbase [576], which can be leveraged to complete or predict the\nmissing parts of knowledge units ( e.g., knowledge triples).\nSuch tasks can probe and evaluate how much and what kind\nofknowledge LLMs have learned from the pre-training\ndata. Existing knowledge completion tasks can be roughly\ndivided into knowledge graph completion tasks ( e.g.,FB15k-\n237 [572] and WN18RR [574]) and fact completion tasks ( e.g.,\nWikiFact [571]), which aim to complete the triples from a\nknowledge graph and incomplete sentences about specific\nfacts, respectively. Empirical studies have revealed that it", "63\nthe latest knowledge beyond the training data. To tackle\nthis issue, a straightforward approach is to regularly update\nLLMs with new data. However, it is very costly to fine-tune\nLLMs, and also likely to cause the catastrophic forgetting\nissue when incrementally training LLMs. Therefore, it is\nnecessary to develop efficient and effective approaches that\ncan integrate new knowledge into existing LLMs, making\nthem up-to-date. Existing studies have explored how to\nutilize the external knowledge source ( e.g., search engine)\nto complement LLMs, which can be either jointly optimized\nwith LLMs [653] or used as a plug-and-play module [659].\nFor instance, ChatGPT utilizes a retrieval plugin to access\nup-to-date information sources [665]. By incorporating the\nextracted relevant information into the context [666\u2013668],\nLLMs can acquire new factual knowledge and perform\nbetter on relevant tasks. However, such an approach seems\nto be still at a superficial level. In addition, existing studies", "Retrieval-Augmented LLM. Due to the huge amount of\nfact records in a KG, existing work typically adopts a\nretrieval model to first obtain a relatively small subgraph\nfrom KG, and then leverages it to enhance LLMs by en-\nriching the relevant knowledge. Before the advent of LLMs,\nthe retrieved subgraphs are often supplemented into train-\ning data, injecting knowledge information into PLMs via\nparameter learning [863\u2013865]. In contrast, to leverage the\nretrieved knowledge, LLMs mainly incorporate it as part of\nthe prompt, without parameter update. To implement this\napproach, there are two main technical problems, i.e.,how\nto retrieve relevant knowledge from KGs and how to make\nbetter use of the structured data by LLMs. For the first issue\n(i.e.,retrieving relevant knowledge), a typical approach is\nto train a small language model ( e.g., RoBERTa) to iden-\ntify question-related fact triples [866]. To further improve\nthe retrieval performance, several studies also propose an"], "retrieved_docs_id": ["9c38efbac6", "4c3ac6cb2e", "e798d8fddc", "b08dd9fa3b", "d0140a8a43"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How is the \"Faithfulness Score\" calculated in the given context?\n", "true_answer": "The \"Faithfulness Score\" is calculated by comparing the number of statements supported by the context to the total number of statements, using a large language model (LLM) to break down the context into individual statements and verify their consistency with the original context.", "source_doc": "RAG.pdf", "source_id": "716582522f", "retrieved_docs": ["into individual statements using an LLM and verify\nwhether each statement is consistent with the context.\nUltimately, a \u201dFaithfulness Score\u201d is calculated by com-\nparing the number of supported statements to the total\nnumber of statements.\n2. Assessing Answer Relevance: Generate potential ques-\ntions using an LLM and calculate the similarity between\nthese questions and the original question. The Answer\nRelevance Score is derived by calculating the average\nsimilarity of all generated questions to the original ques-\ntion.\n3. Assessing Context Relevance: Extract sentences directly\nrelevant to the question using an LLM, and use the ratio\nof these sentences to the total number of sentences in the\ncontext as the Context Relevance Score.", "from what appears in the document being summarized. Consequently, it is important to measure and\nimprove the faithfulness of these systems since unfaithful systems may be harmful by potentially spreading\nmisinformation, including dangerous, yet hard to detect errors, when deployed in real-world settings. We\nevaluate the LMs using recently proposed reference-free evaluation metrics that have been shown to get high\ncorrelations with human scores for faithfulness (Laban et al., 2022; Fabbri et al., 2022). We note recent\nwork has shown that some reference-free evaluation metrics may be mostly relying on spurious correlations\n(Durmus et al., 2022).\nDatasets. There is a growing collection of summarization datasets, including datasets that capture finer-\ngrained and more specific summarization functions (e.g. summarizing multiple documents or conditional\non a user query). Bommasani & Cardie (2020) show that there is significant diversity in summarization", "open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "evaluation metrics. Additionally, the latest evalu-\nation frameworks like RAGAS [Eset al. , 2023 ]and\nARES [Saad-Falcon et al. , 2023 ]also involve RAG eval-\nuation metrics. Summarizing these works, three core metrics\nare primarily focused on: Faithfulness of the answer, Answer\nRelevance, and Context Relevance.\n1.Faithfulness\nThis metric emphasizes that the answers generated by\nthe model must remain true to the given context, ensur-\ning that the answers are consistent with the context infor-\nmation and do not deviate or contradict it. This aspect of\nevaluation is vital for addressing illusions in large mod-\nels.\n2.Answer Relevance\nThis metric stresses that the generated answers need to\nbe directly related to the posed question.\n3.Context Relevance\nThis metric demands that the retrieved contextual infor-\nmation be as accurate and targeted as possible, avoid-\ning irrelevant content. After all, processing long texts\nis costly for LLMs, and too much irrelevant information", "for wedging. Values are mean scores and values in parentheses are standard deviations of scores. Reiteration\nvalues are in the range from 1 to 5, while wedging values are between -1 to 1, except for Hostility, which is\nrated from 0 to 2.\nResults and discussion. Table 8 displays the results of our human evaluation for disinformation. We\nfind that for the reiteration scenario, all models received average quality scores above 3, indicating that\nthey generated text that tended to support the given thesis statements. When it came to style, there was\ngreater variation with text-davinci-002, Anthropic-LM v4-s3 (52B), OPT (175B), and even davinci (175B)\nreceiving scores above 4.0, but OPT (66B) and GLM (130B), received much lower scores. text-davinci-002\nand Anthropic-LM v4-s3 (52B) generate text that supports the given thesis statements, and looks like real\nheadlines. text-davinci-002 significantly outperforms Anthropic-LM v4-s3 (52B) ( p= 0.028), GLM (130B)"], "retrieved_docs_id": ["716582522f", "e99622852f", "ffd5c8b41e", "57b75e5528", "897b2d91c3"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does RAG's generator improve the accuracy and relevance of the generated text?\n", "true_answer": "RAG's generator enhances accuracy and relevance by leveraging the retrieved information, in contrast to conventional generative models.", "source_doc": "RAG.pdf", "source_id": "cd69a480bb", "retrieved_docs": ["retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "quently, it utilizes this retrieved information to generate re-\nsponses or text, thereby enhancing the quality of predictions.\nThe RAG method allows developers to avoid the need for\nretraining the entire large model for each specific task. In-\nstead, they can attach a knowledge base, providing additional\ninformation input to the model and improving the accuracy\nof its responses. RAG methods are particularly well-suited\nfor knowledge-intensive tasks. In summary, the RAG system\nconsists of two key stages:1. Utilizing encoding models to retrieve relevant docu-\nments based on questions, such as BM25, DPR, Col-\nBERT, and similar approaches [Robertson et al. , 2009,\nKarpukhin et al. , 2020, Khattab and Zaharia, 2020 ].\n2. Generation Phase: Using the retrieved context as a con-\ndition, the system generates text.\n2.2 RAG vs Fine-tuning\nIn the optimization of Large Language Models (LLMs), in\naddition to RAG, another important optimization technique\nis fine-tuning.", "information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-", "cross-attention scores, selecting the highest scoring input to-\nkens to effectively filter tokens. RECOMP [Xuet al. , 2023a ]\nproposes extractive and generative compressors, which gen-\nerate summaries by selecting relevant sentences or syn-\nthesizing document information to achieve multi-document\nquery focus summaries.In addition to that, a novel approach,\nPKG [Luoet al. , 2023 ], infuses knowledge into a white-box\nmodel through directive fine-tuning, and directly replaces the\nretriever module, used to directly output relevant documents\nbased on the query.\n5 Generator\nAnother core component in RAG is the generator, responsible\nfor transforming retrieved information into natural and fluent\ntext. Its design is inspired by traditional language models,\nbut in comparison to conventional generative models, RAG\u2019s\ngenerator enhances accuracy and relevance by leveraging the\nretrieved information. In RAG, the generator\u2019s input includes", "ules and offers more flexibility.\nIn the subsequent chapters, we further analyze three key\nparts of RAG in detail. Chapter 4 introduces the retriever of\nRAG, how to process corpora to obtain better semantic repre-\nsentations, how to mitigate the semantic gap between Query\nand documents, and how to adjust the retriever to fit the gen-\nerator. Chapter 5 explains how the generator obtains better\ngeneration results by post-processing retrieved documents,\navoiding the \u201dLost in the middle\u201d issue, as well as methods to\nadjust the generator to fit the retriever. Subsequently, in Chap-\nter 6, we review the current retrieval enhancement methods\nfrom the aspects of the retrieval stage, retrieval data sources,\nand retrieval process.\nChapter 7 explains how to evaluate current RAG methods,\nincluding evaluation, key indicators, and current evaluation\nframeworks Finally, we provided an outlook on the poten-\ntial future research directions for RAG. As a method that"], "retrieved_docs_id": ["fefa202c19", "80558327ad", "7fabdba415", "cd69a480bb", "8cdb0d5ff2"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.25, "hit": 1}, {"question": "What is the purpose of the source with the given context?\n", "true_answer": "The purpose of the source is to serve as a continually updated source of information, promoting ongoing growth in a certain field by providing brief overviews of key contributions.", "source_doc": "multimodal.pdf", "source_id": "6e2ea7a4ef", "retrieved_docs": ["source library for people to use. We use this framework in order to run the experiments\nand aggregate results.\n29.github.com/bigscience-workshop/promptsource/tree/eval-hackathon\n25", "data[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-\ndundancy and noise are common issues. Instead of di-\nrectly retrieving from a data source, the Extra Gener-\nation Module leverages LLM to generate the required\ncontext [Yuet al. , 2022 ]. Content generated by LLM is\nmore likely to contain relevant information compared to\ndirect retrieval.", "contexts. Each item contains:\n\u2022An ambiguous context, in which the correct answer should be \u201cunknown,\u201d or a disambiguated context, in which\nthe correct answer is one of the two people mentioned in the context\n\u2022A negative question that explicitly reinforces a social bias, or a non-negative question that implicitly reinforces a\nsocial bias\nThe following example set includes 4 possible context & question inputs, and tests the social bias that girls are bad at\nmath (Casad et al., 2017), along with how different possible model responses are coded. The designation of \u201cbiased\nresponse\u201d means that the answer reinforces the social bias in that particular context & question pair, and \u201canti-biased\nresponse\u201d means that the answer contradicts it.\n\u2022Ambiguous context : After the \ufb01rst day of middle school math, Nancy and Donald stayed after class to talk to\nthe teacher.\n\u2022Disambiguated context : [Ambiguous context] + Donald thought the class was too easy and asked to be moved", "Context\u2192Organisms require energy in order to do what?\nCorrect Answer \u2192mature and develop.\nIncorrect Answer \u2192rest soundly.\nIncorrect Answer \u2192absorb light.\nIncorrect Answer \u2192take in nutrients.\nFigure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional\nprobability of each answer as described in 2.\nContext\u2192Making a cake: Several cake pops are shown on a display. A woman and girl\nare shown making the cake pops in a kitchen. They\nCorrect Answer \u2192bake them, then frost and decorate.\nIncorrect Answer \u2192taste them as they place them on plates.\nIncorrect Answer \u2192put the frosting on the cake as they pan it.\nIncorrect Answer \u2192come out and begin decorating the cake as well.\nFigure G.9: Formatted dataset example for HellaSwag\nContext\u2192anli 3: anli 3: We shut the loophole which has American workers actually\nsubsidizing the loss of their own job. They just passed an expansion of\nthat loophole in the last few days: $43 billion of giveaways, including", "used to transform the original records ( e.g., knowledge\ntriples) into sequences [458] due to the simplicity. Further,\nthe programming language ( e.g., executable code) has also\nbeen utilized to formulate the structured data, which can\nalso support using external tools ( e.g., program executor) to\nproduce the precise results [459, 460].\n\u2022Contextual information. In addition to the task descrip-\ntion and input data, contextual or background information"], "retrieved_docs_id": ["61c5de7e37", "3ed835a82b", "1ce8a97cfb", "879c865355", "20be91c676"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is the task of the vision-language projector in this context?\n", "true_answer": "The task of the vision-language projector is to map the visual patch embeddings into the text feature space.", "source_doc": "multimodal.pdf", "source_id": "6ac775b4ef", "retrieved_docs": ["vision and language models. It commences with a convolutional stem, succeeded by Mobile Con-\nvolution Blocks in the first and second stages, and Transformer Blocks in the third stage. Remark-\nably, ViTamin-XL, with a modest count of 436M parameters, attains an 82.9% ImageNet zero-shot\naccuracy. This outperforms the 82.0% accuracy achieved by EV A-E [80], which operates with a pa-\nrameter count ten times larger, at 4.4B. Simply replacing LLaV A\u2019s image encoder with ViTamin-L\ncan establish new standards in various MLLM performance metrics.\n2.2 Vision-Language Projector\nThe task of the vision-language projector is to map the visual patch embeddings Zvinto the text\nfeature space:\nHv=P(Zv), (2)\nwhere Hvdenotes the projected visual embeddings. The aligned visual features are used as prompts\nand inputted into the language model along with the text embeddings. Vision-language projector\n5", "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "Perception-language tasks\nCOCO Caption [LMB+14] Image captioning CIDEr, etc. \u0013 \u0013\nFlicker30k [YLHH14] Image captioning CIDEr, etc. \u0013 \u0013\nVQAv2 [GKSS+17] Visual question answering VQA acc. \u0013 \u0013\nVizWiz [GLS+18] Visual question answering VQA acc. \u0013 \u0013\nWebSRC [CZC+21] Web page question answering F1 score \u0013\nVision tasks\nImageNet [DDS+09] Zero-shot image classi\ufb01cation Top-1 acc. \u0013\nCUB [WBW+11] Zero-shot image classi\ufb01cation with descriptions Accuracy \u0013\nTable 1: We evaluate the capabilities of KOSMOS -1on language, perception-language, and vision\ntasks under both zero- and few-shot learning settings.\n1 Introduction: From LLMs to MLLMs\nLarge language models (LLMs) have successfully served as a general-purpose interface across various\nnatural language tasks [ BMR+20]. The LLM-based interface can be adapted to a task as long as we\nare able to transform the input and output into texts. For example, the input of the summarization task", "Figure 4: BRA VE [12] concatenates features from K different Vision Encoders in a sequence-wise\nmanner. These concatenated features are then reduced by the MEQ-Former.\navoids the high cost of training an end-to-end multimodal model from scratch and effectively lever-\nages the capabilities of pre-trained language and vision models.\nMLP-based As outlined in [7, 54], the vision-language projector is typically realized using a\nstraightforward, learnable Linear Projector or a Multi-Layer Perceptron (MLP), i.e., several linear\nprojectors interleaved with non-linear activation functions, as illustrated in Table.1.\nAttention-based BLIP2 [15] introduces Q-Former, a lightweight transformer, which employs a\nset of learnable query vectors to extract visual features from a frozen vision model. Perceiver\nResampler, proposed by Flamingo[16], contemplates the use of learnable latent queries as Q in\ncross-attention, while image features are unfolded and concatenated with Q to serve as K and V in", "4 Evaluation\nMLLMs can handle both language tasks and perception-intensive tasks. We evaluate KOSMOS -1on\nvarious types of tasks as follows:\n\u2022 Language tasks\n\u2013Language understanding\n\u2013Language generation\n\u2013OCR-free text classi\ufb01cation\n\u2022 Cross-modal transfer\n\u2013Commonsense reasoning\n\u2022 Nonverbal reasoning\n\u2013IQ Test (Raven\u2019s Progressive Matrices)\n\u2022 Perception-language tasks\n\u2013Image captioning\n\u2013Visual question answering\n\u2013Web page question answering\n\u2022 Vision tasks\n\u2013Zero-shot image classi\ufb01cation\n\u2013Zero-shot image classi\ufb01cation with descriptions\n4.1 Perception-Language Tasks\nWe evaluate the perception-language capability of KOSMOS -1under vision-language settings. Specif-\nically, we conduct zero-shot and few-shot experiments on two widely used tasks, including image\ncaptioning and visual question answering. Image captioning involves generating a natural language\ndescription of an image, while visual question answering aims to answer a natural language question\nwith respect to an image."], "retrieved_docs_id": ["6ac775b4ef", "3238be52f9", "4c8f867655", "1fea51e26c", "92a6c47296"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How many GPU hours are needed to train MiniGPT-v2?\n", "true_answer": "Over 800 GPU hours are needed to train MiniGPT-v2, based on NVIDIA A100 GPUs.", "source_doc": "multimodal.pdf", "source_id": "2f6f7fb082", "retrieved_docs": ["However, scalability comes at the cost of high resource demands, which hinders the development\nand deployment of large models. For example, the training of MiniGPT-v2 necessitates a total of\nover 800 GPU hours, as calculated based on NVIDIA A100 GPUs [9]. This imposes a substantial\nexpense that is difficult for researchers outside of major enterprises to bear. Aside from training,\n1* Equal contribution.\n2Yizhang Jin is an intern in Tencent, and Jian Li is the project leader.\nPreprint. Under review.arXiv:2405.10739v1  [cs.CV]  17 May 2024", "Pythia: A Suite for Analyzing Large Language Models\nD. Training Hardware and GPU hours\nWe additionally report the number of accelerators used to train each Pythia model size, alongside counts of total GPU-hours\nrequired for training our models at the throughputs that we achieve.\nModel Size GPU Count Total GPU hours required\n70 M 32 510\n160 M 32 1,030\n410 M 32 2,540\n1.0 B 64 4,830\n1.4 B 64 7,120\n2.8 B 64 14,240\n6.9 B 128 33,500\n12 B 256 72,300\nTotal 136,070\nTable 5. Model sizes in the Pythia suite, number of GPUs used during training, and the total number of GPU hours, calculated via\n(iteration time (s) \u00d7number of iterations \u00d7number of GPUs \u00f73600 s/hour). All GPUs are A100s with 40GB of memory.\nHere \u201ctotal\u201d refers to training one model of each size in our suite. For this paper, we trained two models of each size (one on\nthe Pile and one on the Pile deduplicated) and had to retrain both model suites an additional time as discussed in Appendix B.", "modeling) reaches the target 72.0%, and the wall-clock run-time is measured. We train with FP16 precision\nusing Apex AMP (with O2 optimization level).\nWe compare our results with the reported training speed from Nvidia that was submitted to MLPerf 1.1\n(Table 1).\nWe use the same train / validation data split provided by MLPerf 1.1 reference implementation. In\nparticular, we evaluate on the same 10000 validation examples as the baseline from Nvidia.\nWe train the model on 8 \u0002A100-80GB GPUs. Each training run takes between 16 and 19 minutes, and we\naverage the results of 10 runs.\nE.2 GPT-2\nWe use the standard implementations of GPT-2 [ 67] from Huggingface transformers library and from\nNvidia\u2019s Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.\nWe use an e\ufb00ective batch size of 512, and use gradient accumulation to \ufb01t into available GPU memory.\nWe use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and", "split it into two parts:\nnctx=nQ+nR (1)Model size 7B 13B 33B\nGSM8k RFT\nSFT FLOPs 2.4\u00d710184.3\u00d710181.1\u00d71019\nSFT GPI hrs 6.1 12.1 37.4\nCode Alpaca\nSFT FLOPs 4.7\u00d710177.8\u00d710172.0\u00d71018\nSFT GPI hrs 1.2 2.5 8.2\nShareGPT\nSFT FLOPs 2.2\u00d710183.9\u00d710189.7\u00d71019\nSFT GPI hrs 5.4 10.9 34.0\nTable 3: The statistics of FLOPs and GPU hours re-\nquired for SFT. For 33B, we use DeepSpeed ZeRO3\n(Rasley et al., 2020) for distributed training. All the\nGPU hours are based on NVIDIA A100 80GB GPU.\nNote we use non-embedding parameters to compute\nFLOPs in our experiments.\nCtrain\u22486NnctxNs (2)\nwhere nQ, nRdenotes the length of question\nand generated answers respectively. N,Nsdenotes\nthe non-embedding parameters and the numbers of\nsamples.\nTherefore, We estimate the SFT FLOPs follow-\ning (Kaplan et al., 2020) and GPU times in Table 3.\nE Validation Experiments in More SFT\nAbilities\nTo validate the generalization of our conclusions,\nwe selected representative datasets to evaluate", "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model con\ufb01gurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity\u2193 Accuracy\u2191\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the con\ufb01gurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is"], "retrieved_docs_id": ["2f6f7fb082", "0a2bdce1fe", "530e7971d2", "756c61629f", "cd60ee97f9"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the primary optimization for enhancing the efficiency of efficient MLLMs?\n", "true_answer": "The primary optimization for enhancing the efficiency of efficient MLLMs includes handling high-resolution images, compressing vision tokens, implementing efficient structures, and utilizing compact language models.", "source_doc": "multimodal.pdf", "source_id": "de74717e46", "retrieved_docs": ["and preserving user privacy.\nIn light of these challenges, there has been growing attention on the study of efficient MLLMs.\nThe primary objective of these endeavors is to decrease the resource consumption of MLLMs\nand broaden their applicability while minimizing performance degradation. Research on efficient\nMLLMs began with replacing large language models with lightweight counterparts and performing\ntypical visual instruction tuning. Subsequent studies further enhanced capabilities and expanded\nuse cases in the following ways: (1) lighter architectures were introduced with an emphasis on ef-\nficiency, aiming to reduce the number of parameters or computational complexity[25, 13, 18]; (2)\nmore specialized components were developed, focusing on efficiency optimizations tailored to ad-\nvanced architectures or imbuing specific properties, such as locality[19, 17, 12]; and (3) support\nfor resource-sensitive tasks was provided, with some works employing visual token compression", "the domain of efficient MLLMs. In addition to the survey, we have established a GitHub repository\nwhere we compile the papers featured in the survey, organizing them with the same taxonomy at\nhttps://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey. We will actively maintain it and\nincorporate new research as it emerges.\n2 Architecture\nFollowing the standard MLLM framework, efficient MLLMs can be divided into three main mod-\nules: a visual encoder gtasked with receiving and processing visual inputs, a pre-trained language\nmodel that manages the received multimodal signals and performs reasoning, and a visual-language\nprojector Pwhich functions as a bridge to align the two modalities. To enhance the efficiency of the\ngeneral MLLMs, the primary optimization lies in handling high-resolution images, compressing vi-\nsion tokens, implementing efficient structures, and utilizing compact language models, among other", "Figure 11: Organization of efficient large language models advancements.\nOccupying a significant majority of the parameter volume in MLLMs, LLM serves as a crucial entry\npoint for enhancing the efficiency of MLLMs. In this section, similar to the survey paper [160], we\nprovide a brief overview of the research progress in efficient LLMs, offering inspiration for the\ndevelopment of Efficient MLLMs.\n4.1 Attention\nIn the standard self-attention mechanism, the time complexity is O(n2), where nis the sequence\nlength. This quadratic complexity arises due to the pairwise interactions between all input tokens,\nwhich can lead to scalability issues, especially when dealing with long sequences in LLMs. To\ntackle this, researchers have developed techniques to expedite attention mechanisms and reduce\ntime complexity, such as sharing-based attention, feature information reduction, kernelization or\nlow-rank, fixed and learnable pattern strategies, and hardware-assisted attention.", "a richer diversity of input modalities, and augmenting their generative capacities, we can\nsignificantly bolster their multifunctionality and widen their applicability.\n\u2022 There are two principal pathways to fortify efficient MLLM models. Firstly, the incorpora-\ntion of a more varied set of lightweight LLMs can render the design of MLLMs more adapt-\nable, facilitating their customization to cater to a broad spectrum of requirements. Sec-\nondly, leveraging high-quality instruction tuning datasets can empower efficient MLLMs\nto better comprehend and implement a vast array of instructions, thereby amplifying their\nzero-shot learning capabilities.\n\u2022 The development of embodied agents capable of deployment on edge devices represents a\ncrucial application prospect for efficient MLLMs. An agent possessing specialized knowl-\nedge and the capability to interact with the real world has far-reaching implications, poten-", "beneficial when learning larger and more diverse datasets, showing the choice of training recipe is\nclosely related to the quality of the data.\nMulti-stage pre-training To maximize compute efficiency, Idefics2 [48] decomposes the pre-\ntraining in two stages. In the first stage, it limits the max image resolution to 384 pixels and use\na large global batch size. In the second stage, PDF documents are introduced to increase image\nresolution to a maximum of 980 pixels for the text to be legible.\n5.2 Instruction-Tuning\nInstruction-tuning (IT) is a crucial aspect of efficient MLLMs, which aims to fine-tune the models on\nspecific tasks by leveraging task-specific instructions. This approach is built upon the concept that\nMLLMs can understand and follow instructions provided in natural language, thereby enhancing\ntheir performance on the target task. The benefits of IT in efficient MLLMs are manifold. Firstly,"], "retrieved_docs_id": ["04b6ebc53f", "de74717e46", "323641b323", "f10976c224", "8bb6ba6dfa"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "How is a foundation model adapted to a specific domain using DAPT?\n", "true_answer": "DAPT, or Domain-Adaptive Pre-Training, adapts a foundation model to a specific domain by continued pretraining with in-domain data. In this case, the domain-specific pre-training dataset is constructed from a collection of proprietary hardware-related code and natural language datasets.", "source_doc": "ChipNemo.pdf", "source_id": "926168a67f", "retrieved_docs": ["propose to combine the following techniques: Domain-\nAdaptive Pre-Training (DAPT) (Gururangan et al., 2020) of\nfoundation models with domain-adapted tokenizers, model\nalignment using general and domain-specific instructions,\nand retrieval-augmented generation (RAG) (Lewis et al.,\n2021b) with a trained domain-adapted retrieval model.\nAs shown in Figure 1, our approach is to start with a base\nfoundational model and apply DAPT followed by model\nalignment. DAPT, also known as continued pretraining with\nin-domain data, has been shown to be effective in areas such\nas biomedical and computer science publications, news, and\nreviews. In our case, we construct our domain-specific pre-\ntraining dataset from a collection of proprietary hardware-\nrelated code (e.g. software, RTL, verification testbenches,\netc.) and natural language datasets (e.g. hardware specifi-\ncations, documentation, etc.). We clean up and preprocess\nthe raw dataset, then continued-pretrain a foundation model", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat.", "models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ntask assignnment. Participants are tasked with rating the\nmodel\u2019s performance on a 7-point Likert scale for each of\nthese three assignments. The results can be found in Fig-\nure 10. Although the GPT-4 model excels in all three tasks,\noutperforming both our ChipNeMo-70B-Steer model and\nthe LLaMA2-70B-Chat model, ChipNeMo-70B-Steer does\nexhibit enhancements compared to the off-the-shelf LLaMA\nmodel of equivalent size. We attribute the comparatively\nlower improvements in summarization tasks resulting from\nour domain-adaptation to the limited necessity for domain-\nspecific knowledge in summarization compared to other\nuse-cases.\n4. Related Works\nMany domains have a significant amount of proprietary data\nwhich can be used to train a domain-specific LLM. One ap-\nproach is to train a domain specific foundation model from\nscratch, e.g., BloombergGPT(Wu et al., 2023) for finance,\nBioMedLLM(Venigalla et al., 2022) for biomed, and Galac-", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting"], "retrieved_docs_id": ["926168a67f", "273b593026", "7eb44773ae", "74fe22ec46", "a5a7c4ceb0"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the DSP framework enhance the generation quality in the RAG method?\n", "true_answer": "The DSP framework enhances the generation quality in the RAG method by passing natural language text between a frozen Language Model (LM) and a Retrieval Model (RM), providing the model with more informative context.", "source_doc": "RAG.pdf", "source_id": "0264588829", "retrieved_docs": ["complement each other, enhancing the model\u2019s capabilities at\ndifferent levels. In certain situations, combining these two\ntechniques can achieve optimal model performance. The en-\ntire process of optimizing with RAG and fine-tuning may re-\nquire multiple iterations to achieve satisfactory results.\nExisting research has demonstrated significant ad-\nvantages of Retrieval-Augmented Generation (RAG)\ncompared to other methods for optimizing large lan-\nguage models [Shuster et al. , 2021, Yasunaga et al. , 2022,\nWang et al. , 2023c, Borgeaud et al. , 2022 ]:\n\u2022 RAG improves accuracy by associating answers with ex-\nternal knowledge, reducing hallucination issues in lan-\nguage models and making generated responses more ac-\ncurate and reliable.\n\u2022 The use of retrieval techniques allows the identifica-\ntion of the latest information. Compared to traditionallanguage models relying solely on training data, RAG\nmaintains the timeliness and accuracy of responses.", "ules and offers more flexibility.\nIn the subsequent chapters, we further analyze three key\nparts of RAG in detail. Chapter 4 introduces the retriever of\nRAG, how to process corpora to obtain better semantic repre-\nsentations, how to mitigate the semantic gap between Query\nand documents, and how to adjust the retriever to fit the gen-\nerator. Chapter 5 explains how the generator obtains better\ngeneration results by post-processing retrieved documents,\navoiding the \u201dLost in the middle\u201d issue, as well as methods to\nadjust the generator to fit the retriever. Subsequently, in Chap-\nter 6, we review the current retrieval enhancement methods\nfrom the aspects of the retrieval stage, retrieval data sources,\nand retrieval process.\nChapter 7 explains how to evaluate current RAG methods,\nincluding evaluation, key indicators, and current evaluation\nframeworks Finally, we provided an outlook on the poten-\ntial future research directions for RAG. As a method that", "Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "challenging, and the augmentation process needs to balance\nthe value of each passage appropriately. The retrieved con-\ntent may also come from different writing styles or tones, and\nthe augmentation process needs to reconcile these differences\nto ensure output consistency. Lastly, generation models may\noverly rely on augmented information, resulting in output thatmerely repeats the retrieved content, without providing new\nvalue or synthesized information.\n3.2 Advanced RAG\nAdvanced RAG has made targeted improvements to over-\ncome the deficiencies of Naive RAG. In terms of the quality\nof retrieval generation, Advanced RAG has incorporated pre-\nretrieval and post-retrieval methods. To address the indexing\nissues encountered by Naive RAG, Advanced RAG has op-\ntimized indexing through methods such as sliding window,\nfine-grained segmentation, and metadata. Concurrently, it has\nput forward various methods to optimize the retrieval process."], "retrieved_docs_id": ["72cb2b4f23", "8cdb0d5ff2", "af911eac69", "4fffd3dc2b", "873e6df003"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does the Funnel-Transformer model address the issue of computational efficiency in attention mechanisms?\n", "true_answer": "The Funnel-Transformer model addresses the issue of computational efficiency in attention mechanisms by progressively reducing the sequence size of hidden representations in self-attention models.", "source_doc": "multimodal.pdf", "source_id": "3045b9cbb1", "retrieved_docs": ["UniNet [102] introduced context-aware down-sampling modules improving information accommo-\ndation by transformer and MLP operators.\nOptimization of Attention Mechanisms Methods focus on reducing computational complexity\nby introducing adaptive attention, learning sparse attention patterns, and dynamically adjusting at-\ntention mechanisms. Fayyaz et al. [135] implemented adaptive attention by scoring and adaptively\nsampling significant tokens. PatchMerger [103] extracted global information among regional to-\nkens and exchanged local self-attention with information among regional tokens via self-attention.\nDynamicViT [104] proposed an attention masking strategy to differentiably prune tokens by block-\ning interactions with other tokens. Additionally, Sepvit [105] conducted local-global information\ninteraction within and across windows using depthwise separable self-attention. These methods\ncollectively optimize attention mechanisms, enhancing computational efficiency and performance.", "presented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling ef\ufb01ciency. Future research should be\nwary of this hyperparameter to design large transformer\nmodels that balance model speed and model accuracy.\nD.2. Strong Scaling\nOur model parallelism is primarily designed to enable train-\ning models larger than what can \ufb01t in the memory of a", "to conventional models like MLPs and CNNs, self-attention models employ global interactions to capture feature\nrepresentations, resulting in exceptional empirical performance.\nDespite their achievements, the mechanisms and learning processes of attention layers remain enigmatic. Recent\ninvestigations [ EGKZ22 ,SEO+22,ENM22 ,BV22 ,DCL21 ] have concentrated on specific aspects such as sparse\nfunction representation, convex relaxations, and expressive power. Expressivity discussions concerning hard-attention\n[Hah20 ] or attention-only architectures [ DCL21 ] are connected to our findings when h(\u00b7)is linear. In fact, our work\nreveals how linear hresults in attention\u2019s optimization dynamics to collapse on a single token whereas nonlinear h\nprovably requires attention to select and compose multiple tokens. This supports the benefits of the MLP layer for\nexpressivity of transformers. There is also a growing body of research aimed at a theoretical comprehension of in-context", "scale, reaching similar perplexity and downstream performance with a smaller computational budget (Section\n4.2) andwithout hybridization of attention.\nNarrowing the capabilities gap The design of Hyenais motivated by a quality gap between standard\ndense attention and alternative subquadratic operators, which we identify by focusing on reasoning tasks cor-\nrelatedwithlanguagemodelingperformanceatscale. Weextendthesuiteofbasicmechanisticinterpretability\nbenchmarks ( induction andrecall) with additional tasks that probe how quickly model performance degrades\n1Self-attention can be expressed as y=A(k, q)vwhere Ais the attention matrix conditioned by linear projections k, qof the\ninput and multiplied by v, another projection.\n2", "where W\u2113,i\nK,W\u2113,i\nQ,W\u2113,i\nV,W\u2113,i\nO\u2208Rd\u00d7d\u2032are attention parameters, and W\u2113\nA,W\u2113\nB\u2208Rd\u00d7d\u2032are the\nfeedforward parameters, and \u03c3is a continuously twice-differentiable activation. Suppose that the\nattention parameters are diagonal matrices: i.e., W\u2113,i\nK=diag(w\u2113,i\nK)\u2208Rd\u00d7d, and similarly for the\nW\u2113,i\nQ,W\u2113,i\nV,W\u2113,i\nOmatrices. Then by the definition of the attention layer (1), the final output of\nthe transformer ZLonly depends on the attention parameters through the elementwise products\nw\u2113,i\nK\u2299w\u2113,i\nQandw\u2113,i\nV\u2299w\u2113,i\nO. In other words, we can write\nZL=h(X;u\u2299v),\nfor vectors u= [w\u2113,i\nK,w\u2113,i\nV](\u2113,i)\u2208[L]\u00d7[H]\u2208R2dHLandv= [w\u2113,i\nQ,w\u2113,i\nO](\u2113,i)\u2208[L]\u00d7[H]\u2208R2dHL, and some\nsmooth model h. Thus, if only the attention layers are trained, the diagonal transformer fits under\nDefinition 3.1.\n4 Incremental learning in networks with diagonal weights\nWe prove that if the initialization scale \u03b1is small, then learning proceeds in incremental stages, where"], "retrieved_docs_id": ["e7939ae097", "7796a06fa5", "bfb96ebe0c", "50409052be", "448716c5a7"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is the architecture of the C-Abstractor in the Hybrid Structure Honeybee project?\n", "true_answer": "The C-Abstractor, or Conventional Abstractor, in the Hybrid Structure Honeybee project employs a convolutional architecture consisting of LResNet blocks, followed by adaptive average pooling and additional LResNet blocks for abstracting visual features.", "source_doc": "multimodal.pdf", "source_id": "3238be52f9", "retrieved_docs": ["original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "further engineering challenges and adjustments to the model that are not discussed in this paper.\n6 Conclusion\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture,\nMamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance\nof strong Transformer models. We are excited about the broad applications of selective state space models to\nbuild foundation models for di\ufb00erent domains, especially in emerging modalities requiring long context such as\ngenomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\nbackbone.\nAcknowledgments\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\nReferences", "P. Cramer. Alphafold2 and the future of structural biology. Nature structural & molecular biology , 28(9):\n704\u2013705, 2021.\nE. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition workshops , pages 702\u2013703, 2020.\nT. Dao, A. Gu, M. Eichhorn, A. Rudra, and C. R\u00e9. Learning fast algorithms for linear transforms using\nbutter\ufb02y factorizations. In International conference on machine learning , pages 1517\u20131527. PMLR, 2019.\nT. Dao, B. Chen, N. S. Sohoni, A. Desai, M. Poli, J. Grogan, A. Liu, A. Rao, A. Rudra, and C. R\u00e9. Monarch:\nExpressive structured matrices for e\ufb03cient and accurate training. In International Conference on Machine\nLearning , pages 4690\u20134721. PMLR, 2022a.\nT. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-e\ufb03cient exact attention\nwith io-awareness. arXiv preprint arXiv:2205.14135 , 2022b.", "Butterfly  \nFlat Butterfly  + + +\n++\nFlat Block Butterfly  \nBlock Butterfly  \n+ + ++\n+Figure 3: Visualization of Flat, Block, and Flat Block butter\ufb02y.\nsparsity structure. However, there are three technical challenges. We highlight them here along with our\napproaches to address them:\n1. Slow speed: butter\ufb02y matrices are not friendly to modern hardware as their sparsity patterns are not\nblock-aligned, thus are slow. We introduce a variant of butter\ufb02y matrices, block butter\ufb02y , which operate\nat the block level, yielding a block-aligned sparsity pattern.\n2. Di\ufb03culty of parallelization: the sequential nature of butter\ufb02y matrices as products of many factors makes\nit hard to parallelize the multiplication. We propose another class of matrices, \ufb02at butter\ufb02y matrices, that\nare the \ufb01rst-order approximation of butter\ufb02y with residual connections. Flat butter\ufb02y turns the product\nof factors into a sum, facilitating parallelization.", "the former, to avoid communication in layernorm.\nThe CollectiveEinsum loops are the overwhelming major-\nity of the inference latency, so we invested considerable\neffort to maximize their performance. First, we used the\nunderlying \u201casync CollectivePermute\u201d APIs of Wang et al.\n(2023) to develop a suite of variants of the CollectiveEinsum\nconcept, to optimize for different scenarios: latency versus\nthroughput, different numbers of torus axes, fusing with dif-\nferent input/output collectives. Second, we explicitly match\nup communication collectives with the matrix multiplies\nthat they should be fused with, to maximize the potential for\noverlap. Through such optimizations, we achieved about\n1.4 times better performance than the simpler compiler-\npartitioned-and-scheduled implementation that we started\nwith. Some of the weight-gathered layouts would exhaust\nmemory without these optimizations.\nWe also included the following low-level optimizations:"], "retrieved_docs_id": ["3238be52f9", "53f73ec6b6", "e3e1c7914c", "86e4683867", "0cfc5d9a38"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does OpenAI's embeddings-ada-02 handle context compared to full-size language models like GPT-4?\n", "true_answer": "OpenAI's embeddings-ada-02 is more sophisticated than static embedding models and can capture a certain level of context, but it may not be as sensitive to context as the latest full-size language models like GPT-4.", "source_doc": "RAG.pdf", "source_id": "5b18d3e068", "retrieved_docs": ["as much context as possible to ensure \u201chealthy\u201d out-\ncomes.Built upon the principles of large language mod-\nels like GPT, OpenAI\u2019s embeddings-ada-02 is more so-\nphisticated than static embedding models, capturing a\ncertain level of context. While it excels in contextual\nunderstanding, it may not exhibit the same sensitivity to\ncontext as the latest full-size language models like GPT-\n4.\nPost-Retrieval Process\nAfter retrieving valuable context from the database, merg-\ning it with the query for input into LLM poses challenges.\nPresenting all relevant documents to the LLM at once may\nexceed the context window limit. Concatenating numerous\ndocuments to form a lengthy retrieval prompt is ineffective,\nintroducing noise and hindering the LLM\u2019s focus on crucial\ninformation. Additional processing of the retrieved content is\nnecessary to address these issues.\n\u2022ReRank: Re-ranking to relocate the most relevant in-\nformation to the edges of the prompt is a straightfor-", "4.2.4 Long Context Modeling\nIn real applications, there is an increasing demand for long\ncontext modeling capacities of LLMs, such as PDF pro-\ncessing and story writing [286]. Many closed-source LLMs\nprovide professional support for long text processing. For\ninstance, OpenAI releases GPT-4 Turbo with a 128K context\nwindow, and Anthropic releases Claude 2.1 with a 200K\ncontext window. To enhance the long context modeling\nabilities, there are generally two feasible directions, namely\nscaling position embeddings and adapting context window.\nNext, we introduce the two parts in detail.\nScaling Position Embeddings. Transformer-based LLMs\ncan learn effective position embeddings within the maxi-\nmum training length. Thus, when adapting LLMs to lan-\nguage tasks beyond the maximum training length, it is\nnecessary to scale to larger position indices. Some specific\nposition embeddings have been shown to possess a certain\ndegree of ability to generalize to text beyond the training", "et al., 2023), which extends the LLaMA-13B (Tou-\nvron et al., 2023a) context window from 2048 to\n16384 tokens by using condensed rotary positional\nembeddings before fine-tuning with 16384-token\nsequences.\nClosed models. We use the OpenAI API to ex-\nperiment with GPT-3.5-Turbo and GPT-3.5-Turbo", "2023; Rubin and Berant, 2023, inter alia ) have\nresulted in language models with larger context\nwindows (e.g., 4096, 32K, and even 100K tokens),\nbut it remains unclear how these extended-context\nlanguage models make use of their input contexts\nwhen performing downstream tasks.\nWe empirically investigate this question via\ncontrolled experiments with a variety of state-of-\nthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in set-\ntings that require accessing and using information\nwithin an input context. In particular, our experi-\nments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input con-\ntexts, then their performance should be minimally\naffected by the position of the relevant information\nin the input context.", "13\nlanguage models, which have led to the emergence of a va-\nriety of popular models, including LLaVA [149], MiniGPT-\n4 [150], InstructBLIP [151], and PandaGPT [152]. The re-\nlease of LLaMA has greatly advanced the research progress\nof LLMs. To summarize the research work conducted on\nLLaMA, we present a brief evolutionary graph in Figure 5.\nPublic API of LLMs . Instead of directly using the model\ncopies, APIs provide a more convenient way for common\nusers to use LLMs, without the need of running the model\nlocally. As a representative interface for using LLMs, the\nAPIs for the GPT-series models [46, 55, 66, 105] have\nbeen widely used for both academia and industry19.\nOpenAI has provided seven major interfaces to the models\nin GPT-3 series: ada,babbage ,curie ,davinci (the\nmost powerful version in GPT-3 series), text-ada-001 ,\ntext-babbage-001 , and text-curie-001 . Among\nthem, the first four interfaces can be further fine-\ntuned on the host server of OpenAI. In particular,"], "retrieved_docs_id": ["5b18d3e068", "c0cb2f2bb5", "b64fb4982f", "af5472ed0b", "57e2547eaf"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does Retrieval-Augmented Generation (RAG) improve the performance of large language models?\n", "true_answer": "RAG improves the performance of large language models by retrieving relevant information from external knowledge bases before answering questions, which enhances answer accuracy, reduces model hallucination, and is particularly beneficial for knowledge-intensive tasks.", "source_doc": "RAG.pdf", "source_id": "af911eac69", "retrieved_docs": ["Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "complement each other, enhancing the model\u2019s capabilities at\ndifferent levels. In certain situations, combining these two\ntechniques can achieve optimal model performance. The en-\ntire process of optimizing with RAG and fine-tuning may re-\nquire multiple iterations to achieve satisfactory results.\nExisting research has demonstrated significant ad-\nvantages of Retrieval-Augmented Generation (RAG)\ncompared to other methods for optimizing large lan-\nguage models [Shuster et al. , 2021, Yasunaga et al. , 2022,\nWang et al. , 2023c, Borgeaud et al. , 2022 ]:\n\u2022 RAG improves accuracy by associating answers with ex-\nternal knowledge, reducing hallucination issues in lan-\nguage models and making generated responses more ac-\ncurate and reliable.\n\u2022 The use of retrieval techniques allows the identifica-\ntion of the latest information. Compared to traditionallanguage models relying solely on training data, RAG\nmaintains the timeliness and accuracy of responses.", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "erating document-level context and the pre-training objective\nof causal language modeling, allowing for better utilization\nof world knowledge stored in the model parameters.\nSelfmem [Cheng et al. , 2023b ]iteratively uses a retrieval-\nenhanced generator to create an unbounded memory pool. A\nmemory selector is employed to choose an output as the mem-\nory for subsequent generations. This output serves as the dual\nproblem to the original question. By combining the originaland dual problems, a retrieval-enhanced generative model can\nleverage its own output to enhance itself.\nThese diverse approaches showcase innovative strategies in\nRAG retrieval enhancement, aiming to elevate model perfor-\nmance and effectiveness.\n6.3 Augmentation Process\nMost RAG research typically only performs a single retrieval\nand generation process. However, single retrievals may con-\ntain redundant information, leading to a \u201dlost in the mid-\ndle\u201d phenomenon [Liuet al. , 2023 ]. This redundant informa-", "putational expenses for both training and inference. To ad-\ndress the limitations of purely parameterized models, lan-\nguage models can adopt a semi-parameterized approach by\nintegrating a non-parameterized corpus database with pa-\nrameterized models. This approach is known as Retrieval-\nAugmented Generation (RAG).\nThe term Retrieval-Augmented Generation (RAG) was\nfirst introduced by [Lewis et al. , 2020 ]. It combines a pre-\ntrained retriever with a pre-trained seq2seq model (generator)\nand undergoes end-to-end fine-tuning to capture knowledge\nin a more interpretable and modular way. Before the advent\nof large models, RAG primarily focused on direct optimiza-\ntion of end-to-end models. Dense retrievals on the retrieval\nside, such as the use of vector-based Dense Passage Retrieval\n(DPR) [Karpukhin et al. , 2020 ], and training smaller models\non the generation side are common practices. Due to the\noverall smaller parameter size, both the retriever and gener-"], "retrieved_docs_id": ["af911eac69", "72cb2b4f23", "fefa202c19", "6958fee9ba", "33aae5a21d"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does MoE-LLaV increase model capacity without significantly compromising inference speed?\n", "true_answer": "MoE-LLaV increases model capacity by modulating the total count of model parameters while keeping the activated parameters unchanged, which does not significantly affect the inference speed.", "source_doc": "multimodal.pdf", "source_id": "ffe176eb03", "retrieved_docs": ["mising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework\nthat effectively increases the number of parameters without compromising computational efficiency.\nFurthermore, it introduces MoE-Tuning, a three-stage training strategy designed to adapt MoE [89]\nto MLLMs and prevent model degradation caused by sparsity. MM1[30] designs two variants of\nMoE models. The first is a 3B-MoE model that employs 64 experts and substitutes a dense layer\nwith a sparse one every two layers. The second is a 7B-MoE model that utilizes 32 experts and\nsubstitutes a dense layer with a sparse one every four layers.\nMamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modal-\nity and explores different modal fusion schemes to develop an effective multi-modal Mamba. Exper-\niments show that it not only achieves competitive performance with state-of-the-art efficient meth-", "contexts , it substantially increases the inference time. For\na 500B+ model with multihead attention, the attention KVcache grows large: for batch size 512 and context length\n2048, the KV cache totals 3TB, which is 3 times the size of\nthe model\u2019s parameters. The on-chip memory needs to load\nthis KV cache from off-chip memory once for every token\ngenerated during which the computational core of the chip\nis essentially idle.\nIf an applications requires of\ufb02ine inference and latency is not\na concern, the primary goal is to maximize per-chip through-\nput (i.e., minimize total cost per token). It is most ef\ufb01cient\nto increase the batch size because larger batches typically\nresult in better MFU, but certain partitioning strategies that\nare not ef\ufb01cient for small batch sizes become ef\ufb01cient as the\nbatch size grows larger.\n2.2 Inference Setup\nWe brie\ufb02y introduce the inference setup and notation. We\nconsider a Transformer model with nparams parameters laid", "the issue of severe GPU memory fragmentation, it does not\naddress the challenge of deploying models on PCs where the\nentire model cannot \ufb01t within the available GPU memory.\n10 Conclusion\nPowerInfer is a fast inference system optimized for LLMs\nthat exploits the locality property in LLM inference. It utilizes\nadaptive predictors and neuron-aware operators for neuron\nactivation and computational sparsity. PowerInfer achieves\nup to 11.69\u00d7faster LLM inference compared to systems like\nllama.cpp, without compromising accuracy.\n11 Acknowledgments\nWe express our sincere gratitude to to Rong Chen and Yubin\nXia for their comprehensive and constructive feedback, which\ngreatly enhanced the quality of this paper. We also thank\nYifan Tan and Shuyi Lin for their valuable contributions to\nthe experimental setups.\nReferences\n[1]Abien Fred Agarap. Deep learning using recti\ufb01ed linear units\n(relu). arXiv preprint arXiv:1803.08375 , 2018.\n[2]Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,", "Efficient LLM Inference on CPUs\nHaihao Shen Hanwen Chang Bo Dong Yu Luo Hengyu Meng\n{haihao.shen, hanwen.chang, bo1.dong, yu.luo, hengyu.meng}@intel.com\nAbstract\nLarge language models (LLMs) have demonstrated remarkable performance and\ntremendous potential across a wide range of tasks. However, deploying these\nmodels has been challenging due to the astronomical amount of model parameters,\nwhich requires a demand for large memory capacity and high memory bandwidth.\nIn this paper, we propose an effective approach that can make the deployment of\nLLMs more efficiently. We support an automatic INT4 weight-only quantization\nflow and design a special LLM runtime with highly-optimized kernels to accelerate\nthe LLM inference on CPUs. We demonstrate the general applicability of our\napproach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase\nthe extreme inference efficiency on CPUs. The code is publicly available at:\nhttps://github.com/intel/intel-extension-for-transformers.", "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly\nproportional to the inference compute cost, but does not consider the memory costs and hardware\nutilization. The memory costs for serving Mixtral are proportional to its sparse parameter count,\n47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer\nintroduces additional overhead due to the routing mechanism and due to the increased memory loads\nwhen running more than one expert per device. They are more suitable for batched workloads where\none can reach a good degree of arithmetic intensity.\nComparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B\ncompared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the\ntwo other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller"], "retrieved_docs_id": ["5510d4cc4e", "4cd6b5b85d", "5a695f8061", "053d87b5a3", "a0c6f9916d"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does the Memory Module in RAG find relevant memories?\n", "true_answer": "The Memory Module in RAG finds relevant memories by leveraging the memory capabilities of the LLM itself and finding memories most similar to the current input. It iteratively employs a retrieval-enhanced generator to create an unbounded memory pool.", "source_doc": "RAG.pdf", "source_id": "9067222c76", "retrieved_docs": ["RAG, the search module, tailored to specific sce-\nnarios, incorporates direct searches on (additional)\ncorpora in the process using LLM-generated code,\nquery languages (e.g., SQL, Cypher), or other cus-\ntom tools. The data sources for searching can include\nsearch engines, text data, tabular data, or knowledge\ngraphs [Wang et al. , 2023c ].\n\u2022Memory Module: Leveraging the memory capabili-\nties of LLM itself to guide retrieval, the principle in-\nvolves finding memories most similar to the current in-\nput. Self-mem [Cheng et al. , 2023b ]iteratively employs\na retrieval-enhanced generator to create an unbounded\nmemory pool, combining the \u201coriginal question\u201d and\n\u201cdual question.\u201d A retrieval-enhanced generative model\ncan use its own outputs to enhance itself, making the\ntext closer to the data distribution in the reasoning pro-\ncess, with the model\u2019s own outputs rather than training\ndata[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "Modular RAG\nThe modular RAG structure breaks away from the traditional\nNaive RAG framework of indexing, retrieval, and genera-\ntion, offering greater diversity and flexibility in the over-\nall process. On one hand, it integrates various methods to\nexpand functional modules, such as incorporating a search\nmodule in similarity retrieval and applying a fine-tuning ap-\nproach in the retriever [Linet al. , 2023 ]. Additionally, spe-\ncific problems have led to the emergence of restructured\nRAG modules [Yuet al. , 2022 ]and iterative approaches like\n[Shao et al. , 2023 ]. The modular RAG paradigm is becom-\ning the mainstream in the RAG domain, allowing for ei-\nther a serialized pipeline or an end-to-end training approach\nacross multiple modules.The comparison between three RAG\nparadigms is illustrated in Fig 3.\nNew Modules\n\u2022Search Module: Diverging from the similarity re-\ntrieval between queries and corpora in Naive/Advanced\nRAG, the search module, tailored to specific sce-", "lows the decide-retrieve-reflect-read process, introduc-\ning a module for active judgment. This adaptive and\ndiverse approach allows for the dynamic organization of\nmodules within the Modular RAG framework.\n4 Retriever\nIn the context of RAG, the \u201dR\u201d stands for retrieval, serving\nthe role in the RAG pipeline of retrieving the top-k relevant\ndocuments from a vast knowledge base. However, crafting\na high-quality retriever is a non-trivial task. In this chapter,\nwe organize our discussions around three key questions: 1)\nHow to acquire accurate semantic representations? 2) How\nto match the semantic spaces of queries and documents? 3)\nHow to align the output of the retriever with the preferences\nof the Large Language Model ?\n4.1 How to acquire accurate semantic\nrepresentations?\nIn RAG, semantic space is the multidimensional space where\nquery and Document are mapped. When we perform re-\ntrieval, it is measured within the semantic space. If the se-", "external knowledge, alleviates hallucination issues, identifies\ntimely information via retrieval technology, and enhances re-\nsponse accuracy. Additionally, by citing sources, RAG in-\ncreases transparency and user trust in model outputs. RAG\ncan also be customized based on specific domains by index-\ning relevant text corpora. RAG\u2019s development and charac-\nteristics are summarized into three paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG, each with its models,\nmethods, and shortcomings. Naive RAG primarily involves\nthe \u2019retrieval-reading\u2019 process. Advanced RAG uses more\nrefined data processing, optimizes the knowledge base in-\ndexing, and introduces multiple or iterative retrievals. As\nexploration deepens, RAG integrates other techniques like\nfine-tuning, leading to the emergence of the Modular RAG\nparadigm, which enriches the RAG process with new mod-\nules and offers more flexibility.\nIn the subsequent chapters, we further analyze three key"], "retrieved_docs_id": ["9067222c76", "fefa202c19", "1d479682a6", "8fe8499442", "123a2dcc44"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does REPLUG improve the initial retrieval model?\n", "true_answer": "REPLUG improves the initial retrieval model by obtaining feedback from the language model through supervised signals.", "source_doc": "RAG.pdf", "source_id": "662eb558d5", "retrieved_docs": ["domain question-answering tasks. Concerning retriever fine-\ntuning, REPlUG [Shiet al. , 2023 ]treats the language model\n(LM) as a black box and enhances it through an adjustable re-\ntrieval model. By obtaining feedback from the black-box lan-\nguage model through supervised signals, REPLUG improves\nthe initial retrieval model. UPRISE [Cheng et al. , 2023a ], on\nthe other hand, fine-tunes retrievers by creating a lightweight\nand versatile retriever through fine-tuning on diverse task\nsets. This retriever can automatically provide retrieval\nprompts for zero-shot tasks, showcasing its universality and\nimproved performance across tasks and models.\nSimultaneously, methods for fine-tuning generators in-\nclude Self-Mem [Cheng et al. , 2023b ], which fine-tunes the\ngenerator through a memory pool of examples, and\nSelf-RAG [Asai et al. , 2023b ], which satisfies active re-\ntrieval needs by generating reflection tokens. The RA-\nDIT[Linet al. , 2023 ]method fine-tunes both the generator", "complement each other, enhancing the model\u2019s capabilities at\ndifferent levels. In certain situations, combining these two\ntechniques can achieve optimal model performance. The en-\ntire process of optimizing with RAG and fine-tuning may re-\nquire multiple iterations to achieve satisfactory results.\nExisting research has demonstrated significant ad-\nvantages of Retrieval-Augmented Generation (RAG)\ncompared to other methods for optimizing large lan-\nguage models [Shuster et al. , 2021, Yasunaga et al. , 2022,\nWang et al. , 2023c, Borgeaud et al. , 2022 ]:\n\u2022 RAG improves accuracy by associating answers with ex-\nternal knowledge, reducing hallucination issues in lan-\nguage models and making generated responses more ac-\ncurate and reliable.\n\u2022 The use of retrieval techniques allows the identifica-\ntion of the latest information. Compared to traditionallanguage models relying solely on training data, RAG\nmaintains the timeliness and accuracy of responses.", "erating document-level context and the pre-training objective\nof causal language modeling, allowing for better utilization\nof world knowledge stored in the model parameters.\nSelfmem [Cheng et al. , 2023b ]iteratively uses a retrieval-\nenhanced generator to create an unbounded memory pool. A\nmemory selector is employed to choose an output as the mem-\nory for subsequent generations. This output serves as the dual\nproblem to the original question. By combining the originaland dual problems, a retrieval-enhanced generative model can\nleverage its own output to enhance itself.\nThese diverse approaches showcase innovative strategies in\nRAG retrieval enhancement, aiming to elevate model perfor-\nmance and effectiveness.\n6.3 Augmentation Process\nMost RAG research typically only performs a single retrieval\nand generation process. However, single retrievals may con-\ntain redundant information, leading to a \u201dlost in the mid-\ndle\u201d phenomenon [Liuet al. , 2023 ]. This redundant informa-", "Recite-Read [Sunet al. , 2022 ]transforms external re-\ntrieval into retrieval from model weights, initially hav-\ning LLM memorize task-relevant information and gener-\nate output for handling knowledge-intensive natural lan-\nguage processing tasks.\n\u2022Adjusting the Flow between Modules In the realm of\nadjusting the flow between modules, there is an empha-\nsis on enhancing interaction between language models\nand retrieval models. DSP [Khattab et al. , 2022 ]intro-\nduces the Demonstrate-Search-predict framework, treat-\ning the context learning system as an explicit program\nrather than a terminal task prompt to address knowledge-\nintensive tasks. ITER-RETGEN [Shao et al. , 2023 ]\nutilizes generated content to guide retrieval, itera-\ntively performing \u201cretrieval-enhanced generation\u201d and\n\u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read-\nRetrieve-Read flow. Self-RAG [Asai et al. , 2023b ]fol-\nlows the decide-retrieve-reflect-read process, introduc-", "phase to capture key semantic meanings. In the later\nstages of this process, larger blocks with more contex-\ntual information are provided to the language model\n(LM). This two-step retrieval method helps strike a bal-\nance between efficiency and contextually rich responses.\n\u2022StepBack-prompt: Integrated into the RAG process,\nthe StepBack-prompt approach [Zheng et al. , 2023 ]en-\ncourages LLM to step back from specific instances and\nengage in reasoning about the underlying general con-\ncepts or principles. Experimental findings indicate a sig-\nnificant performance improvement in various challeng-\ning, inference-intensive tasks with the incorporation of\nbackward prompts, showcasing its natural adaptability\nto RAG. The retrieval-enhancing steps can be applied in\nboth the generation of answers to backward prompts and\nthe final question-answering process.\n\u2022Subqueries: Various query strategies can be employed in\ndifferent scenarios, including using query engines pro-"], "retrieved_docs_id": ["662eb558d5", "72cb2b4f23", "6958fee9ba", "dfac20a7d8", "ad03b3dcc5"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the main factor contributing to the success of multimodal large language models (MLLMs)?\n", "true_answer": "The success of MLLMs is largely attributed to the scaling law, which states that the performance of an AI model improves as more resources, such as data, computational power, or model size, are invested into it.", "source_doc": "multimodal.pdf", "source_id": "7a547e4fbb", "retrieved_docs": ["2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "is a document and the output is its summary. So we can feed the input document into the language\nmodel and then produce the generated summary.\nDespite the successful applications in natural language processing, it is still struggling to natively use\nLLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal\nperception is a necessity to achieve arti\ufb01cial general intelligence, in terms of knowledge acquisition\nand grounding to the real world. More importantly, unlocking multimodal input [ TMC+21,HSD+22,\nWBD+22,ADL+22,AHR+22,LLSH23 ] greatly widens the applications of language models to\nmore high-value areas, such as multimodal machine learning, document intelligence, and robotics.\nIn this work, we introduce KOSMOS -1, a Multimodal Large Language Model (MLLM) that can\nperceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e.,", "cient MLLMs, research state of efficient structures and strategies, and the appli-\ncations. Finally, we discuss the limitations of current efficient MLLM research\nand promising future directions. Please refer to our GitHub repository for more\ndetails: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.\n1 Introduction\nLarge-scale pretraining, a leading approach in Artificial Intelligence(AI), has seen general-purpose\nmodels like large language and multimodal models outperform specialized deep learning models\nacross many tasks. The remarkable abilities of Large Language Models (LLM) have inspired efforts\nto merge them with other modality-based models to enhance multimodal competencies. This con-\ncept is further supported by the remarkable success of proprietary models like OpenAI\u2019s GPT-4V [1]\nand Google\u2019s Gemini[2]. As a result, Multimodal Large Language Models (MLLMs) have emerged,\nincluding the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],", "Vision Audition\n31 8 \u2026 70 2\nA B C\nD E F\nFigure 1: KOSMOS -1is a multimodal large language model (MLLM) that is capable of perceiving\nmultimodal input, following instructions, and performing in-context learning for not only language\ntasks but also multimodal tasks. In this work, we align vision with large language models (LLMs),\nadvancing the trend of going from LLMs to MLLMs.\n\u2217Equal contribution. \u2020Corresponding author.arXiv:2302.14045v2  [cs.CL]  1 Mar 2023"], "retrieved_docs_id": ["da0a465b6c", "ac70fcc9f2", "74bb21ad4f", "e021f7788d", "f0ea146bbd"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is the number of MLLMs (Multimodal Large Language Models) evaluated in Table 4?\n", "true_answer": "22 MLLMs were evaluated in Table 4.", "source_doc": "multimodal.pdf", "source_id": "de63235613", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 15\nTable 3. Comparison of mainstream MLLMs on discriminative benchmarks. The numbers come from the\noriginal papers of these benchmarks.\nModelLLM\nSizeMME\nExistence\nScore\u2191MME\nCount\nScore\u2191MME\nPosition\nScore\u2191MME\nColor\nScore\u2191POPE\nRandom\nF1-Score\u2191POPE\nRandom\nF1-Score\u2191POPE\nAdversarial\nF1-Score\u2191RAH-Bench\nF1 Score\u2191AMBER\nDis.\nF1-Score\u2191AMBER\nScore\u2191Hal-Eval\nIn-domain\nEvent. F1\u2191Hal-Eval\nOut-of-domain\nEvent. F1\u2191\nmPLUG-Owl [111] 7B 120.00 50.00 50.00 55.00 68.06 66.79 66.82 69.3 31.2 54.1 47 46.6\nImageBind-LLM [34] 7B 128.33 60.00 46.67 73.33 - - - - - - - -\nInstructBLIP [22] (7B) 7B - - - - - - - 89.1 82.6 86.2 66.2 66.6\nInstructBLIP [22] (13B) 13B 185.00 143.33 66.67 153.33 89.29 83.45 78.45 84.7 - - - -\nVisualGLM-6B [25] 6B 85.00 50.00 48.33 55.00 - - - - - - - -\nMultimodal-GPT [28] 7B 61.67 55.00 58.33 68.33 66.68 66.67 66.67 - - - - -\nPandaGPT [95] 7B 70.00 50.00 50.00 50.00 - - - - - - - -", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "general-purpose interface, i.e., language models.\nNew capabilities of MLLMs. As shown in Table 1, apart from the capabilities found in previous\nLLMs [ BMR+20,CND+22], MLLMs enable new usages and possibilities. First, we can conduct\nzero- and few-shot multimodal learning by using natural language instructions and demonstration\nexamples. Second, we observe promising signals of nonverbal reasoning by evaluating the Raven\nIQ test, which measures the \ufb02uid reasoning ability of humans. Third, MLLMs naturally support\nmulti-turn interactions for general modalities, such as multimodal dialogue.\n2 K OSMOS -1: A Multimodal Large Language Model\nAs shown in Figure 1, KOSMOS -1is a multimodal language model that can perceive general\nmodalities, follow instructions, learn in context, and generate outputs. Given the previous context, the\nmodel learns to generate texts in an auto-regressive manner. Speci\ufb01cally, the backbone of KOSMOS -1"], "retrieved_docs_id": ["90bbefc8ec", "ac70fcc9f2", "f49f3b54ce", "da0a465b6c", "5494ed4540"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does the Extra Generation Module generate required context according to Wang et al. (2022a)?\n", "true_answer": "The Extra Generation Module generates required context by using a large language model (LLM) to produce the content. This approach is more likely to contain relevant information compared to direct retrieval.", "source_doc": "RAG.pdf", "source_id": "3ed835a82b", "retrieved_docs": ["data[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-\ndundancy and noise are common issues. Instead of di-\nrectly retrieving from a data source, the Extra Gener-\nation Module leverages LLM to generate the required\ncontext [Yuet al. , 2022 ]. Content generated by LLM is\nmore likely to contain relevant information compared to\ndirect retrieval.", "(up to millions of samples), filtering, and clustering\nof candidate solutions generated by AlphaCode to\nselect the final submissions.\nHowever, whilst these existing code-generation\nLLMs have achieved impressive results, a criti-\ncal current constraint in applying LLMs to code\ngeneration is the inability to fit the full code base\nand dependencies within the context window. To\ndeal with this constraint, a few frameworks have\nbeen proposed to retrieve relevant information or\nabstract the relevant information into an API defi-\nnition.\nLong-Range Dependencies [ 660,504]\nLong-range dependencies across a code\nrepository usually cannot be regarded be-\ncause of limited context lengths (Sec. 2.6).\nZhang et al. [660] introduce RepoCoder, a\nretrieval-based framework for repository-level code\ncompletion that allows an LLM to consider the\nbroader context of the repository. A multi-step\nretrieval-augmented generation approach is taken,\nwhere the initial code generated is then used to re-", "Due to the decomposition in Eq. 1, the LLM can only sam-\nple and generate new tokens one by one, and the generation\nprocess of each new token depends on all the previous tokens\nin that sequence, specifically their key and value vectors. In\nthis sequential generation process, the key and value vectors\nof existing tokens are often cached for generating future\ntokens, known as KV cache . Note that the KV cache of one\ntoken depends on all its previous tokens. This means that the\nKV cache of the same token appearing at different positions\nin a sequence will be different.\nGiven a request prompt, the generation computation in\nthe LLM service can be decomposed into two phases:The prompt phase takes the whole user prompt (\ud835\udc651,...,\ud835\udc65\ud835\udc5b)\nas input and computes the probability of the first new to-\nken\ud835\udc43(\ud835\udc65\ud835\udc5b+1|\ud835\udc651,...,\ud835\udc65\ud835\udc5b). During this process, also gener-\nates the key vectors \ud835\udc581,...,\ud835\udc58\ud835\udc5band value vectors \ud835\udc631,...,\ud835\udc63\ud835\udc5b.\nSince prompt tokens \ud835\udc651,...,\ud835\udc65\ud835\udc5bare all known, the computa-", "to 2000 tokens for these models. To evaluate with closed source models, we collect another set of generations\nwith 2000 context and generation length.\nWhile collecting generations, we append a system prompt prior to the prompt for evaluation. The system\nprompt for each model is shown in Table 31. Since ChatGPT, PaLM, and Falcon do not provide a system\nprompt, we use the same system prompt as Llama 2-Chat model. Generations from different models on an\nexample prompt can be seen in Table 34.\nModel System Prompt\nLlama 2-Chat , ChatGPT,\nPaLM-chat, FalconYou are a helpful, respectful and honest assistant. Always answer as helpfully\nas possible, while being safe. Your answers should not include any harmful,\nunethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that\nyour responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why", "to 2000 tokens for these models. To evaluate with closed source models, we collect another set of generations\nwith 2000 context and generation length.\nWhile collecting generations, we append a system prompt prior to the prompt for evaluation. The system\nprompt for each model is shown in Table 31. Since ChatGPT, PaLM, and Falcon do not provide a system\nprompt, we use the same system prompt as Llama 2-Chat model. Generations from different models on an\nexample prompt can be seen in Table 34.\nModel System Prompt\nLlama 2-Chat , ChatGPT,\nPaLM-chat, FalconYou are a helpful, respectful and honest assistant. Always answer as helpfully\nas possible, while being safe. Your answers should not include any harmful,\nunethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that\nyour responses are socially unbiased and positive in nature.\nIf a question does not make any sense, or is not factually coherent, explain why"], "retrieved_docs_id": ["3ed835a82b", "5c21f0d3d2", "219a7ba352", "f3de8d4f04", "f3de8d4f04"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the approach of augmented pre-training perform in knowledge-intensive tasks compared to standard GPT models?\n", "true_answer": "The approach of augmented pre-training outperforms standard GPT models in handling knowledge-intensive tasks, particularly in open-domain question answering.", "source_doc": "RAG.pdf", "source_id": "7411eec79c", "retrieved_docs": ["Figure 4: Taxonomy of RAG\u2019s Core Components\nextension of RETRO, increased the model\u2019s parameter scale.\nStudies have found consistent improvements in text genera-\ntion quality, factual accuracy, low toxicity, and downstream\ntask accuracy, particularly in knowledge-intensive tasks such\nas open-domain question answering. These research findings\nhighlight the promising direction of pretraining autoregres-\nsive language models in conjunction with retrieval for future\nfoundational models.\nIn summary, the advantages and limitations of augmented\npre-training are evident. On the positive side, this approach\noffers a more powerful foundational model, outperforming\nstandard GPT models in perplexity, text generation quality,\nand downstream task performance. Moreover, it achieves\nhigher efficiency by utilizing fewer parameters compared to\npurely pre-trained models. It particularly excels in handling\nknowledge-intensive tasks, allowing the creation of domain-", "\ufb01ndings potentially help explain the widespread success of\npre-training techniques for down-stream NLP tasks as we\nshow that, in the limit, one of these pre-training techniques\nbegins to learn to perform tasks directly without the need\nfor supervised adaption or modi\ufb01cation.\nOn reading comprehension the performance of GPT-2 is\ncompetitive with supervised baselines in a zero-shot setting.\nHowever, on other tasks such as summarization, while it\nis qualitatively performing the task, its performance is still\nonly rudimentary according to quantitative metrics. While\nsuggestive as a research result, in terms of practical applica-\ntions, the zero-shot performance of GPT-2 is still far from\nuse-able.\nWe have studied the zero-shot performance of WebText\nLMs on many canonical NLP tasks, but there are many addi-\ntional tasks that could be evaluated. There are undoubtedly\nmany practical tasks where the performance of GPT-2 is\nstill no better than random. Even on common tasks that we", "spike [56] and mixed precision training [78]. More recently,\nGPT-4 [46] proposes to develop special infrastructure and\noptimization methods that reliably predict the performance\nof large models with much smaller models.\n\u2022Ability eliciting . After being pre-trained on large-scale\ncorpora, LLMs are endowed with potential abilities as\ngeneral-purpose task solvers. These abilities might not be\nexplicitly exhibited when LLMs perform some specific tasks.\nAs the technical approach, it is useful to design suitable task\ninstructions or specific in-context learning strategies to elicit\nsuch abilities. For instance, chain-of-thought prompting has\nbeen shown to be useful to solve complex reasoning tasks\nby including intermediate reasoning steps. Furthermore,\nwe can perform instruction tuning on LLMs with task\ndescriptions expressed in natural language, for improving\nthe generalizability of LLMs on unseen tasks. These eliciting\ntechniques mainly correspond to the emergent abilities of", "as many steps. Unsurprisingly, we \ufb01nd that pre-training provides signi\ufb01cant gains across\nalmost all benchmarks. The only exception is WMT English to French, which is a large\nenough data set that gains from pre-training tend to be marginal. We include this task in\nour experiments to test the behavior of transfer learning in the high-resource regime. Since\nwe perform early stopping by selecting the best-performing checkpoint, the large disparity\nbetween our baseline and \u201cno pre-training\u201d emphasize how much pre-training improves\nperformance on tasks with limited data. While we do not explicitly measure improvements\nin data e\ufb03ciency in this paper, we emphasize that this is one of the primary bene\ufb01ts of the\ntransfer learning paradigm.\n14", "tasks we \ufb01nd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap\nbetween zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models\nare more pro\ufb01cient meta-learners.\nFinally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and\nbroader societal impacts, and attempt a preliminary analysis of GPT-3\u2019s characteristics in this regard.\nThe remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training\nGPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings.\nSection 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3.\nSection 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.\n2 Approach"], "retrieved_docs_id": ["7411eec79c", "076cc924c5", "0c478460b3", "afe7417e5f", "48e99e85eb"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the SPHINX-X project create image captions with fine-grained correspondence to texts?\n", "true_answer": "The SPHINX-X project creates image captions with fine-grained correspondence to texts by using GPT-4V to generate captions from marked images and domain-specific guidelines.", "source_doc": "multimodal.pdf", "source_id": "7d5705c52b", "retrieved_docs": ["regional details and object relationships insight. During the training process, SPHINX-X utilizes the\nunaltered images rather than the marked ones. ALLaV A[29] propose to distill a caption and a QA\npair for an image within a single session. Specifically, it prompts GPT-4V with an image, and ask it\nto first generate a fine-grained caption then a VQA pair.\nAdditionally, excluding multimodal instructional data, conversations solely based on language be-\ntween users and assistants can significantly contribute to enhancing a model\u2019s conversational exper-\ntise and responsiveness to directives.For example, VILA\u2019s[49] research demonstrates that integrating\ntext-only instructional data with image-text data during the fine-tuning process not only mitigates the\ndecline in performance for text-only tasks but also enhances the accuracy of MLLM-related tasks.\nDataset Name Type I \u2192O Source Method #.Instance Representative Publications", "simultaneously saving all text annotations along with their respective bounding boxes. Ultimately,\nthese elements are converted into a unified question-answering format.\nWhile multi-task datasets provide an abundant source of data, they may not always be suitable\nfor complex real-world situations, such as engaging in multi-turn conversations. To address this\nchallenge, some research has explored the use of self-instruction by leveraging LLMs to gener-\nate text-based or multimodal instruction-following data from a limited number of hand-annotated\nsamples. SPHINX-X[14] assembles a rich multi-domain dataset with fine-grained correspondence\nbetween images and texts.It gathers images from diverse sources and then employs annotations to\napply various markers onto the original images. By prompting GPT-4V with these marked images\nand tailored domain-specific guidelines, the system generates captions that offer an image overview,", "15M English image-caption pairs and consists of two datasets: CC3M and CC12M, which are also\ncollected from internet webpages using a Flume pipeline. For Conceptual Captions, we discard pairs\nwhose captions contain special tags such as \u201c<PERSON>\u201d.\nB.1.3 Interleaved Data\nWe collect a large corpus of 2 billion web pages from the snapshots of common crawls. To ensure\nquality and relevance, we apply several \ufb01ltering criteria. First, we discard any pages that are not\nwritten in English. Second, we discard any pages that do not have images interspersed in the text.\nThird, we discard any images that have a resolution lower than 64 by 64 pixels or that are single-\ncolored. Fourth, we discard any text that is not meaningful or coherent, such as spam or gibberish.\nWe use some heuristics to identify and remove gibberish text containing emoji symbols, hashtags,\nand URL links. After applying these \ufb01lters, we end up with about 71 million documents for training.\nB.2 Data Format", "[SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande:\nAn adversarial winograd schema challenge at scale. In AAAI , pages 8732\u20138740, 2020.\n[SBV+22]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\net al. Laion-5b: An open large-scale dataset for training next generation image-text\nmodels. arXiv preprint arXiv:2210.08402 , 2022.\n[SDGS18] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions:\nA cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers , pages\n2556\u20132565. Association for Computational Linguistics, 2018.\n[SDP+22]Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,", "[43] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,\nRichard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation\nwith visual attention. In ICML , 2015.\n[44] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329 , 2014.\n[45] Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn\u00edk, and J\u00fcrgen Schmidhuber. Recurrent\nhighway networks. In ICML , 2017.\n11"], "retrieved_docs_id": ["54fa378ef2", "7d5705c52b", "5234b23b8d", "264229819e", "7071beac18"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "What is one ability of the RGB model in augmented generation that deals with erroneous information?\n", "true_answer": "Counterfactual Robustness is the ability of the RGB model in augmented generation that deals with erroneous information by identifying and handling it when receiving instructions about potential risks in retrieved information.", "source_doc": "RAG.pdf", "source_id": "070aa6c4f4", "retrieved_docs": ["augmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness\nThis capability measures the model\u2019s efficiency in han-\ndling noisy documents, which are those related to the\nquestion but do not contain useful information.\n2.Negative Rejection\nWhen documents retrieved by the model lack the knowl-\nedge required to answer a question, the model should\ncorrectly refuse to respond. In the test setting for neg-\native rejection, external documents contain only noise.\nIdeally, the LLM should issue a \u201dlack of information\u201d or\nsimilar refusal signal.\n3.Information Integration\nThis ability assesses whether the model can integrate\ninformation from multiple documents to answer more\ncomplex questions.4.Counterfactual Robustness\nThis test aims to evaluate whether the model can iden-\ntify and deal with known erroneous information in doc-\numents when receiving instructions about potential risks\nin retrieved information. Counterfactual robustness tests", "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "equally diverse. Hallucination is a prominent issue where the\nmodel fabricates an answer that doesn\u2019t exist in the context.\nIrrelevance is another concern where the model generates an\nanswer that fails to address the query. Further, toxicity or\nbias, where the model generates a harmful or offensive re-\nsponse, is another problem.\nFinally, the augmentation process also faces several chal-\nlenges. Crucially, the effective integration of the context from\nretrieved passages with the current generation task is of ut-\nmost importance. If mishandled, the output might appear in-\ncoherent or disjointed. Redundancy and repetition are another\nissue, particularly when multiple retrieved passages contain\nsimilar information, leading to content repetition in the gen-\neration step. Moreover, determining the importance or rele-\nvance of multiple retrieved passages to the generation task is\nchallenging, and the augmentation process needs to balance", "feng Gao, and Furu Wei. Visually-augmented language modeling. In International\nConference on Learning Representations , 2023.\n21", "scores both serve as signals that model generations need to be edited. Further, we make explicit that our\nannotators are not disinformation experts (they are crowd-workers on Amazon Mechanical Turk), and this\nmight overestimate the true utility of the model generations\u2014trained annotators might be able to find more\nfine-grained issues with the generations. On the other hand, disinformation actors may be able to elicit\nstrong performance through fine-tuning or more sophisticated prompting, i.e. we should expect further op-\ntimization and encouraging modeling of more adversarial/worst-case behavior in future work as a result.\nAnd we do not currently evaluate all language models (e.g. PaLM, Gopher), including especially models\nthat are designed specifically by malicious actors for disinformation generation. Overall, we conservatively\nrecommend our results be interpreted as a lower bound of the currentdisinformation risk posed by language\nmodels.\n9 Related work and discussion"], "retrieved_docs_id": ["070aa6c4f4", "6291d3f5de", "e75af48a5e", "344b0d30d9", "ac8db2d950"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the perplexity score of the Mini-Gemini [26] Gemma-2B model?\n", "true_answer": "The perplexity score of the Mini-Gemini [26] Gemma-2B model is 56.2.", "source_doc": "multimodal.pdf", "source_id": "0ad4077d27", "retrieved_docs": ["Cobra [13] Mamba-2.8B 75.9 58.5 - 46.0 52.0 - - - - - - 88.0 - -\nMini-Gemini [26] Gemma-2B - - - 56.2 - 31.7/29.1 29.4 1341.0 312.0 59.8 - - - 31.1\nVary-toy [27] Qwen-1.8B - - - - - - - - - - - - - 29.0\nTinyGPT-V [28] Phi-2 (2.7B) - 33.6 - - 24.8 - - - - - - - - -\nSPHINX-Tiny [14] TinyLlama-1.1B - - - 57.8 - - 26.4 1261.2 242.1 56.6 17.1/- 82.2 52.3 23.8\nALLaV A-Longer [29] Phi-2 (2.7B) - 50.0 - 50.3 - 33.2/- - 1564.6\u202064.6 - - 71.7 35.5\nMM1-3B [30] MM1-3B 82.5 - 76.1 72.9 - 38.6/35.7 32.6 1469.4 303.1 70.8 63.9/69.4 87.6 76.8 42.2\nLLaV A-Gemma [31] Gemma-2b-it 71.4 58.7 - - - - - 1133.0 307.0 - - 85.3 - 19.1\nMipha-3B [32] Phi-2 (2.7B) 81.3\u221763.9\u221770.9 56.6 47.5 - - 1488.9 295.0 69.7 - 86.7 - 32.1\nVL-Mamba [18] Mamba-2.8B 76.6 56.2 65.4 48.9 - - - 1369.6 - 57.0 - 84.4 - 32.6\nMiniCPM-V 2.0[33] MiniCPM-2.4B - - - 74.1 - 38.2/- 38.7 1808.6\u202069.6 - - - -\nDeepSeek-VL [34] DeepSeek-LLM-1B - - - - - 32.2/- 31.1 - - 64.6 -/66.7 87.6 - 36.8", "model has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the LAMBADA and WikiText103\ndatasets in Table 3. For more details on evaluation method-\nology, see Appendix E. We observe the trend that increasing\nmodel size also leads to lower perplexity on WikiText103\nand higher cloze accuracy on LAMBADA. Our 8.3B model\nachieves state of the art perplexity on the WikiText103 test\nset at a properly adjusted perplexity of 10.81. At 66.51%\naccuracy, the 8.3B model similarly surpasses prior cloze\naccuracy results on the LAMBADA task. We have included\nsamples generated from the 8.3 billion parameters model", "Gemini Pro [2] - 71.2 - - 74.6 - 47.9/\u2013 45.2 - 436.79 73.6 \u2013/70.7 - - 64.3\nGemini Ultra [2] - 77.8 - - 82.3 - 59.4/\u2013 53.0 - - - - - - -\nGPT4V [1] - 77.2 - - 78.0 - 56.8/55.7 49.9 - 517.14 75.8 67.3/69.1 - - 67.6\nMobileVLM [20] MobileLLaMA (2.7B) - 59.0\u221761.0 47.5 - - - 1288.9 - 59.6 - 84.9 - -\nLLaV A-Phi [21] Phi-2 (2.7B) 71.4\u2217- 68.4 48.6 35.9 - - 1335.1 - 59.8 - 85.0 - 28.9\nImp-v1 [22] Phi-2 (2.7B) 79.5 - 70.0 59.4 - - - 1434.0 - 66.5 - 88.0 - 33.1\nTinyLLaV A [23] Phi-2 (2.7B) 79.9\u221762.0\u221769.1 59.1 - - - 1464.9 - 66.9 - 86.4 75.8 32.0\nBunny [24] Phi-2 (2.7B) 79.8 62.5 70.9 - - 38.2/33.0 - 1488.8 289.3 68.6 62.5/- 86.8 - -\nGemini Nano-2 [2] - 67.5 - - 65.9 - 32.6/- 30.6 - - - - - - -\nMobileVLM-v2 [17] MobileLLaMA(2.7B) - 61.1 70.0 57.5 - - - 1440.5 - - - 84.7 - -\nMoE-LLaV A [25] Phi-2 (2.7B) 79.9\u221762.6\u221770.3 57.0 43.7 - - 1431.3 - 68.0 - 85.7 - 35.9\nCobra [13] Mamba-2.8B 75.9 58.5 - 46.0 52.0 - - - - - - 88.0 - -", "In Fig. A.2 (Left), we present the perplexity results of the 3-bit quantized LLaMA-7B model on the\nC4 benchmarks, with varying percentages of sensitive values extracted as the sparse matrix, ranging\nfrom 0% to 0.2%. The plot demonstrates that the perplexity gain diminishes as the sparsity level of\nthe sensitive values exceeds 0.05%. Therefore, we maintain a fixed sparsity level of 0.05% for the\nsensitive values throughout all experiments.\nFurthermore, in Figure A.2 (Right), we compare the performance when the sensitive values are\nnot removed as the sparse matrix (only outlier values are removed) to the case where 0.05% of\n17", "Table A.4: Perplexity comparison of LLaMA-30B and 65B models quantized into 4 and 3 bits using\ndifferent methods including RTN, GPTQ, AWQ and SpQR on C4 and WikiText-2. We compare the\nperformance of GPTQ, AWQ, and SqueezeLLM (SQLLM) in groups based on similar model sizes.\nIn the first group, we compare dense-only SqueezeLLM with non-grouped GPTQ. In the subsequent\ngroups, we compare SqueezeLLM with different levels of sparsity to GPTQ and AWQ with different\ngroup sizes.\nLLaMA-30B 3-bit 4-bit\nMethodAvg. Bits PPL (\u2193) Avg. Bits PPL (\u2193)\n(comp. rate) C4 Wiki (comp. rate) C4 Wiki\nBaseline 16 5.98 4.10 16 5.98 4.10\nRTN 3 (5.33) 28.53 14.89 4 (4.00) 6.33 4.54\nGPTQ 3 (5.33) 7.31 5.76 4 (4.00) 6.20 4.43\nSpQR - - - 3.89 (4.11) 6.08 4.25\nSQLLM 3.02 (5.31) 6.37 4.66 4.03 (3.97) 6.06 4.22\nGPTQ (g128) 3.25 (4.92) 6.47 4.83 4.25 (3.77) 6.07 4.24\nAWQ (g128) 3.25 (4.92) 6.38 4.63 4.25 (3.77) 6.05 4.21\nSQLLM (0.45%) 3.25 (4.92) 6.23 4.44 4.25 (3.77) 6.04 4.18LLaMA-65B 3-bit 4-bit"], "retrieved_docs_id": ["0ad4077d27", "16600b04e0", "b8cafcd1b9", "bd90ff9f76", "50e3b20ce2"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does ITRG enhance adaptability for tasks requiring multiple-step reasoning?\n", "true_answer": "ITRG enhances adaptability for tasks requiring multiple-step reasoning by iteratively retrieving and searching for the correct reasoning path.", "source_doc": "RAG.pdf", "source_id": "1f6c13012c", "retrieved_docs": ["vant knowledge without altering the parameters of LLMs, en-\nabling the model to perform more sophisticated tasks. Addi-\ntionally, CREA-ICL [Liet al. , 2023b ]leverages synchronous\nretrieval of cross-lingual knowledge to assist in acquiring ad-\nditional information, while RECITE forms context by sam-\npling one or more paragraphs from LLMs.\nDuring the inference phase, optimizing the process of RAG\ncan benefit adaptation to more challenging tasks. For ex-ample, ITRG [Feng et al. , 2023a ]enhances adaptability for\ntasks requiring multiple-step reasoning by iteratively retriev-\ning and searching for the correct reasoning path. ITER-\nRETGEN [Shao et al. , 2023 ]employs an iterative approach\nto coalesce retrieval and generation, achieving an alternating\nprocess of \u201dretrieval-enhanced generation\u201d and \u201dgeneration-\nenhanced retrieval.\u201d\nOn the other hand, IRCOT [Trivedi et al. , 2022 ]merges the\nconcepts of RAG and CoT [Weiet al. , 2022 ], employing al-", "model capability [869].\nSynergy-Augmented LLM. To solve complex tasks ( e.g.,\nmulti-hop question answering [656]), it often requires LLMs\nto query a KG multiple times, following a systematic solu-\ntion plan. We call such a multi-turn interaction approach to\nenhancing LLM synergy-augmented LLM . To better synergize\nthe LLM and KG in a complementary manner, recent studies\npropose to decompose the complex task into multiple sub-\ngoals and iteratively solve each one by leveraging the nec-\nessary knowledge from KG [458, 870, 871]. In this process,\nthe LLM can be regarded as an autonomous agent (detailed\nin Section 8.1.6), which automatically generates the plan\nand executes it through interaction with the KG environ-\nment [870]. Specially, the mainstream approaches typically\nstart by enumerating the candidates using the available\nknowledge information at the current step, and then retrieve\nthe most appropriate candidates for the next step according", "refine the reasoning processes, Self-Refine [685] elicits feed-\nback from LLMs on self-generated solutions, enabling the\niterative refinement of solutions based on the feedback.\nMoreover, several studies improve the consistency in the\nreasoning chain of LLMs through the integration of process-\nbased supervision during training [688, 689]. As a promis-\ning solution, recent approaches reformulate the complex\nreasoning tasks into code generation tasks, where the strict\nexecution of the generated code ensures the consistency\nbetween the reasoning process and the outcome. Also,\nit has been revealed that there might exist inconsistency\nbetween tasks with similar inputs, where small changesin the task description may cause the model to produce\ndifferent results [49, 592]. To mitigate this problem, self-\nconsistency [436] adopts the ensemble of multiple reasoning\npaths to enhance the decoding process of LLMs.\nReasoning Inconsistency\nLLMs may generate the correct answer following", "adapters into LLMs, empowering researchers to im-\nplement adapter-based PEFT methods for a wide\nrange of tasks. To evaluate different PEFT meth-\nods on downstream tasks, we construct two high-\nquality fine-tuning datasets to enhance PEFT per-\nformance on math reasoning and commonsense rea-\nsoning tasks. By utilizing the LLM-Adapter toolkit\nand the constructed fine-tuning datasets, we con-\nduct a comprehensive empirical study and find the\nanswer of research questions on the optimal place-\nment and configuration of different PEFT methods,\nthe impact of adapter architectures, and the influ-\nence of ID and OOD scenarios. We hope this work", "composition problem in the SFT stage is crucial\nfor further enhancing the capabilities of LLMs in a\ncomprehensive manner.\nIn essence, the tasks of reasoning, coding, and\naligning human intentions are of different charac-\nteristics. Reasoning and coding tasks require ad-\nhoc abilities of complex and detailed logic in de-\ncomposing task instructions and dealing with non-\nlinguistic and symbolic features (Chen et al., 2021;\nHuang and Chang, 2023), whereas aligning human\nintentions requires versatility and understanding\nobscure intentions expressed in human instructions\n(Lu et al., 2023). Given the fundamental difference\namong the tasks, multi-task learning with compos-\nite data fine-tuning for small-scaled pre-trained lan-arXiv:2310.05492v3  [cs.CL]  19 Jan 2024"], "retrieved_docs_id": ["1f6c13012c", "ebd6447842", "c938be55de", "3d9464fa95", "fe2ed22eb8"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does Llama-2 optimize memory bandwidth during autoregressive decoding?\n", "true_answer": "Llama-2 optimizes memory bandwidth during autoregressive decoding by using a technique called grouped-query attention (GQA), which is a Sharing-based Attention mechanism that partitions all query heads into several groups, with each group\u2019s query heads sharing a common key-value head.", "source_doc": "multimodal.pdf", "source_id": "7d67b0debb", "retrieved_docs": ["be accommodated. Moreover, inefficient memory manage-\nment can further decrease the batch size, as shown in Fig. 2.\nAdditionally, given the current trends, the GPU\u2019s computa-\ntion speed grows faster than the memory capacity [ 17]. For\nexample, from NVIDIA A100 to H100, The FLOPS increases\nby more than 2x, but the GPU memory stays at 80GB max-\nimum. Therefore, we believe the memory will become an\nincreasingly significant bottleneck.\nComplex decoding algorithms. LLM services offer a range\nof decoding algorithms for users to select from, each with\nvarying implications for memory management complexity.\nFor example, when users request multiple random samples\nfrom a single input prompt, a typical use case in program\nsuggestion [ 18], the KV cache of the prompt part, which\naccounts for 12% of the total KV cache memory in our ex-\nperiment (\u00a76.3), can be shared to minimize memory usage.\nOn the other hand, the KV cache during the autoregressive", "memory sharing at the granularity of a block, across the\ndifferent sequences associated with the same request or even\nacross the different requests.\nIn this work, we build vLLM , a high-throughput distributed\nLLM serving engine on top of PagedAttention that achieves\nnear-zero waste in KV cache memory. vLLM uses block-level\nmemory management and preemptive request scheduling\nthat are co-designed with PagedAttention. vLLM supports\npopular LLMs such as GPT [ 5], OPT [ 62], and LLaMA [ 52]\nwith varying sizes, including the ones exceeding the memory\ncapacity of a single GPU. Our evaluations on various models\nand workloads show that vLLM improves the LLM serving\nthroughput by 2-4 \u00d7compared to the state-of-the-art sys-\ntems [ 31,60], without affecting the model accuracy at all. The\nimprovements are more pronounced with longer sequences,\nlarger models, and more complex decoding algorithms (\u00a74.3).\nIn summary, we make the following contributions:", "Latency Profiling. We measure the latency and peak memory usage for generating 128 and 1024\ntokens on an A6000 machine using the Torch CUDA profiler. As an official implementation of\nGPTQ (in particular, the grouped version) is not available, we implement an optimized kernel for\nsingle-batch inference based on the most active open-source codebase ( GPTQ-For-LLaMA).\n5.2 M AINRESULTS\nTable 1 shows quantization results for LLaMA along with comparison with RTN, GPTQ and AWQ.\nThe models are grouped based on their average bitwidth (i.e., model size) for a better comparison\nof size-perplexity trade-offs. See Fig. 5 for a visual illustration. Below we use LLaMA-7B as the\nmain example for the discussions for the impact of dense-only and Dense-and-Sparse quantization,\nand subsequently discuss how these trends extend to larger models. We provide the full evaluation\nresult on all LLaMA models in Tab. A.4.\nDense-only Quantization. In Tab. 1 (Top), we compare dense-only SqueezeLLM with 0% sparsity", "COPA, OPT-30B MathQA, OPT-30B RTE, OPT-66BOpenBookQA, OPT-66BXSUM, LLaMA-7B XSUM, LLaMA-13B XSUM, LLaMA-30B XSUM, GPT-NeoX-20B\nCNN/Daily Mail, LLaMA-7B CNN/Daily Mail, GPT-NeoX-20B OpenBookQA, OPT-30B\nCOPA, OPT-66B\nFigure 4: Comparsion results between the baseline model with full cache, our H2O, and the \"Local\" strategy\nthat utilizes the most recent KVembeddings.\n5 Empirical Evaluation\nIn this section, our goal is to demonstrate that H2O, a remarkably simple KV cache eviction policy is\ncapable of enhancing end-to-end throughput and reducing latency in wall-clock while maintaining\ngeneration quality across a broad spectrum of domains and tasks.\n\u2022In Section 5.1, we show that H2Ocan reduce the memory footprint of KV cache by up to 5\u00d7without\naccuracy degradation on a wide range of model architectures (OPT, LLaMA, GPT-NeoX), sizes\n(from 6.7B to 175B) and evaluation benchmarks (HELM and lm-eval-harness). More importantly,", "Table 3: Latency (s) and peak memory usage (GB) of 3-bit LLaMA when generating 128 tokens\non an A6000 GPU. The table compares the FP16 baseline, non-grouped and grouped GPTQ with\nactivation ordering, and SqueezeLLM with different sparsity levels. For comparison, we include\nbitwidth and perpelxity on the C4 benchmark.\nMethodBit 7B 13B 30B 65B\nwidth PPL (C4) Lat (s) Mem (G) PPL (C4) Lat (s) Mem (G) PPL (C4) Lat (s) Mem (G) PPL (C4) Lat (s) Mem (G)\nBaseline 16 7.08 3.2 12.7 6.61 5.6 24.6 5.98 OOM OOM 5.62 OOM OOM\nGPTQ 3 9.55 1.4 2.9 8.22 2.1 5.3 7.31 4.0 12.3 6.70 6.7 24.0\nSqueezeLLM 3.02 7.75 1.5 2.9 7.08 2.4 5.4 6.37 4.0 12.5 5.99 7.6 24.5\nGPTQ (g128) 3.25 7.89 13.7 3.0 7.12 24.2 5.6 6.47 61.9 12.9 6.01 117.8 25.1\nSqueezeLLM (0.45%) 3.24 7.56 1.7 3.1 6.92 2.5 5.8 6.23 4.4 14.7 5.84 8.8 28.0\n5.4 H ARDWARE DEPLOYMENT AND PROFILING\nWhile grouping with permutation is an effective way to confine the quantization range, our Dense-"], "retrieved_docs_id": ["75b52830d5", "ac4fac2f5f", "9fcf8cca06", "e67e687aed", "d0c43919fa"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What percentage of the overall cost does DAPT account for in ChipNeMo training?\n", "true_answer": "DAPT accounts for less than 1.5% of the overall cost in ChipNeMo training.", "source_doc": "ChipNemo.pdf", "source_id": "d9ae12f819", "retrieved_docs": ["large adapter exhibiting a slight improvement.\n3.4. Training Cost\nAll models have undergone training using 128 A100 GPUs.\nWe estimate the costs associated with domain adaptive pre-\ntraining for ChipNeMo as illustrated in Table 1. It is worth\nnoting that DAPT accounts for less than 1.5% of the overall\n5", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat.", "models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting"], "retrieved_docs_id": ["d9ae12f819", "273b593026", "7eb44773ae", "df0b9868f2", "a5a7c4ceb0"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does a purely parameterized language model acquire and store world knowledge?\n", "true_answer": "A purely parameterized language model acquires world knowledge from vast corpora and stores it in the parameters of the model.", "source_doc": "RAG.pdf", "source_id": "fc82ce8e28", "retrieved_docs": ["play distinct roles. Parametric knowledge is acquired through\ntraining LLMs and stored in the neural network weights, rep-\nresenting the model\u2019s understanding and generalization of\nthe training data, forming the foundation for generated re-\nsponses. Non-parametric knowledge, on the other hand, re-\nsides in external knowledge sources such as vector databases,\nnot encoded directly into the model but treated as updatable\nsupplementary information. Non-parametric knowledge em-\npowers LLMs to access and leverage the latest or domain-\nspecific information, enhancing the accuracy and relevance\nof responses.\nPurely parameterized language models (LLMs) store their\nworld knowledge, which is acquired from vast corpora, in\nthe parameters of the model. Nevertheless, such models have\ntheir limitations. Firstly, it is difficult to retain all the knowl-\nedge from the training corpus, especially for less common\nand more specific knowledge. Secondly, since the model", "Language Models are Unsupervised Multitask Learners\nAlec Radford*1Jeffrey Wu*1Rewon Child1David Luan1Dario Amodei**1Ilya Sutskever**1\nAbstract\nNatural language processing tasks, such as ques-\ntion answering, machine translation, reading com-\nprehension, and summarization, are typically\napproached with supervised learning on task-\nspeci\ufb01c datasets. We demonstrate that language\nmodels begin to learn these tasks without any ex-\nplicit supervision when trained on a new dataset\nof millions of webpages called WebText. When\nconditioned on a document plus questions, the an-\nswers generated by the language model reach 55\nF1 on the CoQA dataset - matching or exceeding\nthe performance of 3 out of 4 baseline systems\nwithout using the 127,000+ training examples.\nThe capacity of the language model is essential\nto the success of zero-shot task transfer and in-\ncreasing it improves performance in a log-linear\nfashion across tasks. Our largest model, GPT-2,\nis a 1.5B parameter Transformer that achieves", "provide enough information about whether the produced answer would contradict.\nRetrieval-augmented language model pre-training\n(REALM) [ 186] inserts retrieved documents\ninto the pre-training examples. While Guu et al.\n[186] designed REALM for extractive tasks\nsuch as question-answering, Lewis et al. [304]\npropose retrieval-augmented generation (RAG), a\nlanguage generation framework using retrievers\nfor knowledge-intensive tasks that humans could\nnot solve without access to an external knowledge\nsource. Yogatama et al. [646] propose the adaptive\nSemiparametric Language Models architecture,\nwhich incorporates the current local context, a\nshort-term memory that caches earlier-computed\nhidden states, and a long-term memory based on a\nkey-value store of (hidden-state, output) tuples. To\nequip a retrieval-augmented LLM with few-shot\nabilities that were before only emergent in LLMs\nwith many more parameters, Izacard et al. [236]\npropose a KL-divergence loss term for retrieval", "capacities. Considering the ever-growing interest in Chat-\nGPT and GPT models, we add a special discussion about the\ntechnical evolution of the GPT-series models, to briefly sum-\nmarize the progress how they have been developed in the\npast years. Meanwhile, we drew a schematic diagram de-\npicting the technological evolution of the GPT-series models\nin Figure 4. The basic principle underlying GPT models is\nto compress the world knowledge into the decoder-only\nTransformer model by language modeling, such that it can\nrecover (or memorize) the semantics of world knowledge\nand serve as a general-purpose task solver. Two key points\nto the success are (I) training decoder-only Transformer\nlanguage models that can accurately predict the next word\nand (II) scaling up the size of language models . Overall, the\nresearch of OpenAI on LLMs can be roughly divided into\nthe following stages13.\nEarly Explorations . According to one interview with Ilya", "Recent works, such as StreamLLM [ 52] and LM-Infinite [ 53], have shown promising potential in\nenabling Language Models (LLMs) to handle input of infinite length. They achieve this by only\nretaining the initial tokens and a limited local context. However, this approach may pose challenges\nfor certain tasks where vital information lies within the middle of the input and would be lost using\nthis strategy. We investigate it through two specific types of tasks:\n25"], "retrieved_docs_id": ["fc82ce8e28", "5981cc3258", "f7770d2394", "114995c415", "84d203b8b5"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How do pre-trained language models primarily function?\n", "true_answer": "Pre-trained language models primarily function as completion machines.", "source_doc": "hallucination.pdf", "source_id": "f524021191", "retrieved_docs": ["During this stage, language models engage in autoregressive prediction, wherein they predict the\nsubsequent token in a sequence. By undergoing self-supervised training on vast textual datasets,\nthese models develop an understanding of language syntax, gain access to world knowledge, and\nenhance their reasoning capabilities. This pre-training process establishes a solid groundwork for\nthe models to undertake subsequent fine-tuning tasks effectively.\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge\nand skills, it\u2019s important to acknowledge that its primary focus is on optimizing for completion.\nConsequently, pre-trained LLMs essentially function as completion machines, which may create\na misalignment between the objective of predicting the next word within LLMs and the user\u2019s\nobjective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-", "since it performs better on long sequences.\n4.2.3 Pre-training Tasks\nPre-training plays a key role that encodes general knowl-\nedge from large-scale corpus into the massive model param-\neters. For training LLMs, there are two commonly used pre-\ntraining tasks, namely language modeling and denoising\nautoencoding.\nLanguage Modeling. The language modeling task (LM) is\nthe most commonly used objective to pre-train decoder-only\nLLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of\ntokens x={x1, . . . , x n}, the LM task aims to autoregres-\nsively predict the target tokens xibased on the preceding\ntokens x<iin a sequence. A general training objective is to\nmaximize the following likelihood:\nLLM(x) =nX\ni=1logP(xi|x<i). (6)\nSince most language tasks can be cast as the prediction\nproblem based on the input, these decoder-only LLMs might\nbe potentially advantageous to implicitly learn how to ac-\ncomplish these tasks in a unified LM way. Some studies", "objective. Early works explored the use of the technique in image classi\ufb01cation [ 20,49,63] and\nregression tasks [ 3]. Subsequent research [ 15] demonstrated that pre-training acts as a regularization\nscheme, enabling better generalization in deep neural networks. In recent work, the method has\nbeen used to help train deep neural networks on various tasks like image classi\ufb01cation [ 69], speech\nrecognition [68], entity disambiguation [17] and machine translation [48].\nThe closest line of work to ours involves pre-training a neural network using a language modeling\nobjective and then \ufb01ne-tuning it on a target task with supervision. Dai et al. [ 13] and Howard and\nRuder [ 21] follow this method to improve text classi\ufb01cation. However, although the pre-training\nphase helps capture some linguistic information, their usage of LSTM models restricts their prediction\nability to a short range. In contrast, our choice of transformer networks allows us to capture longer-", "to achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robust-\nness and mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.\n(2022) that \ufb01netuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant\nimprovement in performance as we were scaling.", "they are adopted as a new standard starting point for single-task \ufb01netuning.\n5 Related Work\nLarge Language Models As the foundation of instruction tuning, the practice of pretraining one general-\npurpose language representation that is useful for multiple downstream tasks has a long tradition that goes\nback at least Mikolov et al. (2013) and Dai and Le (2015). In 2018, Peters et al. (2018) and Devlin et al. (2019)\ncemented the paradigm of pretraining a large model on a large unsupervised corpus, and the \ufb01eld of NLP\nquickly converged to using these models which substantially outperform the prior art of non-pretrained\ntask-speci\ufb01c LSTM models on all tasks. However, the dominate way to access that high-quality syntactic\nand semantic knowledge encoded in pretrained models was not to prompt them with instructions, but to\ntrain an additional task-speci\ufb01c linear layer that maps the model activations into numerical class labels. A"], "retrieved_docs_id": ["f524021191", "dbc4b3182e", "b33337bfbf", "97eaa889af", "8848e08dc0"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How did using a larger learning rate affect the model's performance in the ablation studies?\n", "true_answer": "Using a larger learning rate led to substantial degradations across all domain-specific and academic benchmarks, except on coding.", "source_doc": "ChipNemo.pdf", "source_id": "49056b4ebb", "retrieved_docs": ["prohibitive costs of training enough models.\nAn easy yet expensive fix is to run ablations\nby varying one factor at a time, e.g., keeping\nmost hyper-parameters fixed except the model\nsize [ 44] or context lengths [ 557]. A cheaper po-\ntential remedy can be zero-shot hyper-parameter\ntransfer from smaller models to larger ones [ 608,\n633]. Yang et al. [633] find that when using the \u00b5P\nnetwork parameterization scheme, one can transfer\nthe effect of changing hyper-parameters such as the\nlearning rate across varying model depths, batch\nsizes, sequence lengths, and training times, which\nthey verify empirically up to a 6.7B model. How-\never, it has yet to be verified if such transferability\nstill holds for other varying factors; and if so, re-\nsearchers could afford to conduct more ablation\nexperiments via smaller models.\nIf additional experiments are prohibitively ex-\npensive, another recommendation is to report eval-\nuation results beyond aggregated performance mea-", "ablation studies to say this conclusively. However, Du et al. (2021) did perform ablation studies on the same\ntraining corpus, and show that the improvement in few-shot learning from careful data \ufb01ltering is extremely\nsigni\ufb01cant.\nSimilarly, we did not perform ablation studies to tease out the e\ufb00ects of (1) vs (2), due to the high training\ncost of performing such a study at full scale. In other words, a critical open scaling question is: \u201cHow would\na 62B parameter model trained for 7T tokens compare to our 540B parameter model trained for 780B tokens?\nWhat about a 120B model for 3.6T tokens? 240B for 1.8T tokens?\u201d It is clear that such a model would have\nroughly the same total training cost as PaLM 540B. However, if downstream task performance were to be\ncomparable, the smaller model would certainly be preferable, as the inference cost is proportional to its size.\nVery recently, Ho\ufb00mann et al. (2022) was published to explore this exact question. There, the authors train", "(i.e. the sample efficiency of adaptation) influences performance, we vary the maximum number of examples\nacross n\u2208{0,1,2,4,8,16}. In Figure 32, we plot model performance as a fraction of the average number\nof in-context examples provided (which may be fewer than the maximum stated above if they do not fit\ninside the context window). To explore the results further, including the model generations, see https:\n//crfm.stanford.edu/helm/v0.1.0/?group=ablation_in_context .\nWe find that all models show clear improvement from n= 0ton= 1, sometimes having 0% accuracy in the\nzero-shot setting, with the consistent exception of CNN/DailyMail where zero-shot accuracy is better for\nalmost all models. We posit that models may not effectively understand the appropriate length distribution\nand the poor reference summaries may comparatively mislead the model in the one-shot setting compared to\nthe zero-shot setting. However, for larger numbers of in-context examples, we do not see consistent benefits", "for models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\nsize are shown in Table 9.\nB.3 Ablations\nWhile it is general practice in the literature to only train on the response in instruction following\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\n23", "3.3. Training Ablation Studies\nFor our ablation studies, we conducted multiple rounds of\ndomain adaptive pre-training. We provide brief summaries\nand refer to the Appendix A.6 for details.\nThe differences between training with the augmented tok-\nenizer and the original tokenizer appeared to be negligible.\nWe thus primarily attribute the accuracy degradation on\nopen-domain academic benchmarks to domain data. More-\nover, the removal of the public dataset only slightly re-\ngressed on most tasks including academic benchmarks.\nIn our exploration, we experimented with employing a larger\nlearning rate, as in CodeLLaMA (Rozi `ere et al., 2023). We\nobserved large spikes in training loss at the initial training\nsteps. Although this approach eventually led to improved\ntraining and validation loss, we noted substantial degrada-\ntions across all domain-specific and academic benchmarks,\nexcept on coding. We hypothesize that a smaller learning"], "retrieved_docs_id": ["758ce381e6", "23ab72b1b7", "30958146c4", "355ddc67cc", "49056b4ebb"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.2, "hit": 1}, {"question": "Which Microsoft language model, with less than 3 billion parameters, can match the performance of models 25 times larger?\n", "true_answer": "Phi-2, as trained on special data recipes, can match the performance of models 25 times larger trained on regular data.", "source_doc": "multimodal.pdf", "source_id": "26327c579e", "retrieved_docs": ["its selection is closely related to the lightweight nature of MLLM. In comparison to conventional\nMLLMs with parameter sizes ranging from 7 billion to tens of billions[87, 88], efficient MLLMs\ntypically employ language models with less than 3 billion parameters, such as phi2-2.7B[74] by\nMicrosoft and Gemma-2B[78] by Google. Phi-2 trained on special data recipes can match the per-\nformance of models 25 times larger trained on regular data. Phi-3-mini [86] can be easily deployed\nlocally on a modern phone and achieves a quality that seems on-par with models such as Mixtral\n8x7B [89] and GPT-3.5. In addition to utilizing pre-trained models, MobileVLM[20] downscales\nLLaMA[87] and trains from scratch using open-source datasets. The specific model scaling is illus-\ntrated in the Table.1 and Table.4.\n2.4 Vision Token Compression\nInitial research has underscored the potential of MLLMs across various tasks, including visual ques-", "samples generated from the 8.3 billion parameters model\nin the Appendix C. Recently researchers from Microsoft in\ncollaboration with NVIDIA trained a 17 billion parameter\nGPT-2 model called Turing-NLG (Microsoft, 2020) using\nMegatron and showed that the accuracies further improve\nas they scale the model, highlighting the value of larger\nmodels.\nTo ensure we do not train on any data found in our test sets,\nwe calculate the percentage of test set 8-grams that also\nappear in our training set as done in previous work (Rad-\nford et al., 2019). The WikiText103 test set has at most\nFigure 6. Validation set perplexity. All language models are trained\nfor 300k iterations. Larger language models converge notice-\nably faster and converge to lower validation perplexities than their\nsmaller counterparts.\nTable 4. Model con\ufb01gurations used for BERT.\nParameter Layers Hidden Attention Total\nCount Size Heads GPUs\n336M 24 1024 16 128\n1.3B 24 2048 32 256\n3.9B 48 2560 40 512", "language models, both in order to focus on in-context learning performance and to reduce the complexity of our large\nmodel implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3\u2019s\nperformance on downstream tasks, especially in the \ufb01ne-tuning setting, and combining GPT-3\u2019s scale with these\nalgorithmic techniques is a promising direction for future work.\n8 Conclusion\nWe presented a 175 billion parameter language model which shows strong performance on many NLP tasks and\nbenchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of\n40", "Recently a series of Large Language Models (LLMs) have been introduced (Brown et al., 2020; Lieber\net al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022), with the largest dense\nlanguage models now having over 500 billion parameters. These large autoregressive transformers\n(Vaswani et al., 2017) have demonstrated impressive performance on many tasks using a variety of\nevaluation protocols such as zero-shot, few-shot, and \ufb01ne-tuning.\nThe compute and energy cost for training large language models is substantial (Rae et al., 2021;\nThoppilan et al., 2022) and rises with increasing model size. In practice, the allocated training\ncompute budget is often known in advance: how many accelerators are available and for how long\nwe want to use them. Since it is typically only feasible to train these large models once, accurately\nestimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021).", "consists of up to 11 billion parameters, and GPT-3 (Brown et al., 2020) comprises up to 175 billion\nparameters. Their extreme sizes bring challenges in deploying the models to practical applications\ndue to memory and computational requirements.\n\u2217Published as a conference paper in ICML 2023.\n\u2020Li, Yu, Zhang, Liang and Zhao are a ffiliated with Georgia Tech. He and Chen are a ffiliated with Microsoft Azure.\nCorrespondence to yixiaoli@gatech.edu ,yyu429@gatech.edu andtourzhao@gatech.edu .\n**Equal contributions\n1arXiv:2306.11222v2  [cs.LG]  26 Jun 2023"], "retrieved_docs_id": ["26327c579e", "873274797e", "2271efc3ce", "12fa1dd10e", "f688611ef4"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does Hardware-Aware Quantization optimize the performance of neural network models on specific hardware platforms?\n", "true_answer": "Hardware-Aware Quantization optimizes the performance of neural network models on specific hardware platforms by adjusting precision levels and quantization strategies to maximize performance and energy efficiency during inference.", "source_doc": "multimodal.pdf", "source_id": "31efe3044d", "retrieved_docs": ["NN are grouped into sensitive/insensitive to quantization,\nand higher/lower bits are used for each layer. As such,\none can minimize accuracy degradation and still bene\ufb01t\nfrom reduced memory footprint and faster speed up with\nlow precision quantization. Recent work [ 267] has also\nshown that this approach is hardware-ef\ufb01cient as mixed-\nprecision is only used across operations/layers.\nC. Hardware Aware Quantization\nOne of the goals of quantization is to improve the\ninference latency. However, not all hardware provide\nthe same speed up after a certain layer/operation is\nquantized. In fact, the bene\ufb01ts from quantization is\nhardware-dependant, with many factors such as on-chip\nmemory, bandwidth, and cache hierarchy affecting the\nquantization speed up.\nIt is important to consider this fact for achieving\noptimal bene\ufb01ts through hardware-aware quantization [ 87,\n91,246,250,254,256,265,267]. In particular, the\nwork [ 246] uses a reinforcement learning agent to", "*Equal contribution1Qualcomm AI Research, an initia-\ntive of Qualcomm Technologies, Inc.. Correspondence to:\nMarkus Nagel <markusn@qti.qualcomm.com >, Rana Ali Am-\njad<ramjad@qti.qualcomm.com >, Tijmen Blankevoort <tij-\nmen@qti.qualcomm.com >.\nProceedings of the 37thInternational Conference on Machine\nLearning , Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).cations, and even dedicated low-power hardware.\nOne effective way to optimize neural networks for infer-\nence is neural network quantization (Krishnamoorthi, 2018;\nGuo, 2018). In quantization, neural network weights and\nactivations are kept in a low-bit representation for both\nmemory transfer and calculations in order to reduce power\nconsumption and inference time. The process of quantizing\na network generally introduces noise, which results in a loss\nof performance. Various prior works adapt the quantization\nprocedure to minimize the loss in performance while going\nas low as possible in the number of bits used.", "we have shown with theoretical bounds on many layers in neural networks that quantization is almost\nalways provably better than pruning. Our hypothesis is that quantized layers are more accurate than\npruned ones, as shown in the theoretical and PTQ setting, and fine-tuning a network is still highly\ndependent on that. This is in line with fine-tuning results, in which for many networks trained under\nthe same conditions, quantization always has higher performance than pruning.\nThe conclusion is clear: Quantization generally outperforms pruning for neural networks. Taking\ninto account the unfavorable hardware implications for pruning described, it could be argued that the\nconclusion holds even stronger. Based on this research, we recommend quantizing neural networks\nwhen efficiency is required before pruning is explored.\n9", "and even sometimes in data centers, has become\nan important problem due to high latency and pro-\nhibitively large memory footprint and energy con-\nsumption.\nOne effective method to tackle this problem is\nneural network quantization. Quantization reduces\nmemory consumption by using low-bit precision\nfor weight and activation tensors. Is also reduces\ninference time, and improves energy ef\ufb01ciency by\nemploying low-bit \ufb01xed-point arithmetic instead of\n\ufb02oating-point arithmetic (Horowitz, 2014).\nQuantization, however, is not free. It introduces\nadditional noise in the network that can lead to\na drop in the model\u2019s performance. While prior\nwork has demonstrated the feasibility of integer-\nonly inference for computer vision models (Lin\net al., 2016; Jacob et al., 2018; Krishnamoorthi,\n2018; Zhang et al., 2018; Choukroun et al., 2019;\nDong et al., 2019; Esser et al., 2019; Nagel et al.,\n2019, 2020), there is relatively little work done\non quantizing NLP models (Wang et al., 2018b;", "stricted, and it has come to the forefront in recent years due\nto the remarkable performance of Neural Network models\nin computer vision, natural language processing, and re-\nlated areas. Moving from \ufb02oating-point representations to\nlow-precision \ufb01xed integer values represented in four bits\nor less holds the potential to reduce the memory footprint\nand latency by a factor of 16x; and, in fact, reductions of\n4x to 8x are often realized in practice in these applications.\nThus, it is not surprising that quantization has emerged\nrecently as an important and very active sub-area of\nresearch in the ef\ufb01cient implementation of computations\nassociated with Neural Networks. In this article, we survey\napproaches to the problem of quantizing the numerical\nvalues in deep Neural Network computations, covering the\nadvantages/disadvantages of current methods. With this\nsurvey and its organization, we hope to have presented a\nuseful snapshot of the current research in quantization"], "retrieved_docs_id": ["3e3cb80a9a", "7542ef2a73", "81e3649a4c", "464540d8f3", "aedafa4000"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "Which large language models are mentioned as being explored for application in EDA algorithms and the chip design process?\n", "true_answer": "The large language models mentioned are ChatGPT, Bard, Vicuna, and those discussed in the papers by Khailany et al. (2020), Ren & Fojtik (2021), and Roy et al. (2021).", "source_doc": "ChipNemo.pdf", "source_id": "0e1c7b711e", "retrieved_docs": ["customization for enhancing the effectiveness of\nlarge language models in specialized applications.\n1. Introduction\nOver the last few decades, Electronic Design Automation\n(EDA) algorithms and tools have provided huge gains in\nchip design productivity. Coupled with the exponential\nincreases in transistor densities provided by Moore\u2019s law,\nEDA has enabled the development of feature-rich complex\nSoC designs with billions of transistors. More recently, re-\n*Equal contribution1NVIDIA.searchers have been exploring ways to apply AI to EDA al-\ngorithms and the chip design process to further improve chip\ndesign productivity (Khailany et al., 2020; Ren & Fojtik,\n2021; Roy et al., 2021). However, many time-consuming\nchip design tasks that involve interfacing with natural lan-\nguages or programming languages still have not been auto-\nmated. The latest advancements in commercial (ChatGPT,\nBard, etc.) and open-source (Vicuna (Chiang et al., 2023),", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "Bard, etc.) and open-source (Vicuna (Chiang et al., 2023),\nLLaMA2 (Touvron et al., 2023), etc.) large language mod-\nels (LLM) provide an unprecedented opportunity to help\nautomate these language-related chip design tasks. Indeed,\nearly academic research (Thakur et al., 2023; Blocklove\net al., 2023; He et al., 2023) has explored applications of\nLLMs for generating Register Transfer Level (RTL) code\nthat can perform simple tasks in small design modules as\nwell as generating scripts for EDA tools.\nWe believe that LLMs have the potential to help chip de-\nsign productivity by using generative AI to automate many\nlanguage-related chip design tasks such as code generation,\nresponses to engineering questions via a natural language\ninterface, analysis and report generation, and bug triage. In\nthis study, we focus on three specific LLM applications: an\nengineering assistant chatbot for GPU ASIC and Architec-\nture design engineers, which understands internal hardware", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nMingjie Liu* 1Teodor-Dumitru Ene* 1Robert Kirby* 1Chris Cheng* 1Nathaniel Pinckney* 1\nRongjian Liang* 1Jonah Alben1Himyanshu Anand1Sanmitra Banerjee1Ismet Bayraktaroglu1\nBonita Bhaskaran1Bryan Catanzaro1Arjun Chaudhuri1Sharon Clay1Bill Dally1Laura Dang1\nParikshit Deshpande1Siddhanth Dhodhi1Sameer Halepete1Eric Hill1Jiashang Hu1Sumit Jain1\nAnkit Jindal1Brucek Khailany1George Kokai1Kishor Kunal1Xiaowei Li1Charley Lind1Hao Liu1\nStuart Oberman1Sujeet Omar1Ghasem Pasandi1Sreedhar Pratty1Jonathan Raiman1Ambar Sarkar1\nZhengjiang Shao1Hanfei Sun1Pratik P Suthar1Varun Tej1Walker Turner1Kaizhe Xu1Haoxing Ren1\nAbstract\nChipNeMo aims to explore the applications of\nlarge language models (LLMs) for industrial chip\ndesign. Instead of directly deploying off-the-\nshelf commercial or open-source LLMs, we in-\nstead adopt the following domain adaptation tech-\nniques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndifferent VLSI objects. Because these are hard to evaluate\nin an automated way (with current model performance), we\nhad human engineers judge the correctness between 0-10.\nWe evaluate the model on two tools, one is a fully in-house\nPython based tool and the other is a Tcl based EDA tool\nwith limited public data. The size of these benchmarks are\ndescribed in Table 2. Work is ongoing to both increase the\nsize and scope for these benchmarks to allow us to further\nassess and improve these models.\nEvaluation Benchmark Name Size\nPython Tool - Automatic (Easy) 146\nPython Tool - Automatic (Medium) 28\nPython Tool - Human (Hard) 25\nTcl Tool - Automatic (Easy) 708\nTcl Tool - Automatic (Medium) 27\nTcl Tool - Human (Hard) 25\nTable 2: EDA Script Generation Evaluation Benchmarks\nThe comparative performance of our models on these eval-\nuations are shown in Figures 8 and 9. Figure 8 shows the\nresults on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks"], "retrieved_docs_id": ["0e1c7b711e", "a6c3d05123", "f23b3625e0", "36c5c0c7f1", "76983d04e5"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How did UniNet [102] improve information accommodation by transformer and MLP operators?\n", "true_answer": "UniNet [102] introduced context-aware down-sampling modules to improve information accommodation by transformer and MLP operators.", "source_doc": "multimodal.pdf", "source_id": "e7939ae097", "retrieved_docs": ["UniNet [102] introduced context-aware down-sampling modules improving information accommo-\ndation by transformer and MLP operators.\nOptimization of Attention Mechanisms Methods focus on reducing computational complexity\nby introducing adaptive attention, learning sparse attention patterns, and dynamically adjusting at-\ntention mechanisms. Fayyaz et al. [135] implemented adaptive attention by scoring and adaptively\nsampling significant tokens. PatchMerger [103] extracted global information among regional to-\nkens and exchanged local self-attention with information among regional tokens via self-attention.\nDynamicViT [104] proposed an attention masking strategy to differentiably prune tokens by block-\ning interactions with other tokens. Additionally, Sepvit [105] conducted local-global information\ninteraction within and across windows using depthwise separable self-attention. These methods\ncollectively optimize attention mechanisms, enhancing computational efficiency and performance.", "nomenon does not extend to transformer encoders\ntrained on the MLM objective. We conjecture that\nthe causal attention mechanism, which limits atten-\ntion in one direction of the sequence, is responsible\nfor implicitly imbuing the transformer with posi-\ntional information.\n5https://twitter.com/BlancheMinerva/status/\n1394089508723900422", "performance on long texts than peer models, we have not come up with a faithful explanation.\nOur proposed RoFormer is built upon the Transformer-based infrastructure, which requires hardware resources for\npre-training purpose.\n5 Conclusions\nIn this work, we proposed a new position embedding method that incorporates explicit relative position dependency in\nself-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative\nposition can be naturally formulated using vector production in self-attention, with absolution position information\nbeing encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of\nthe proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark\ndatasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show", "Transformer Language Models without Positional Encodings\nStill Learn Positional Information\nAdi Haviv\u03c4Ori Ram\u03c4O\ufb01r Press\u03c9Peter Izsak\u03b9Omer Levy\u03c4\u00b5\n\u03c4Tel Aviv University\u03c9University of Washington\u03b9Intel Labs\u00b5Meta AI\n{adi.haviv, ori.ram, levyomer}@cs.tau.ac.il ,ofirp@cs.washington.edu, peter.izsak@intel.com\nAbstract\nCausal transformer language models (LMs),\nsuch as GPT-3, typically require some form of\npositional encoding, such as positional embed-\ndings. However, we show that LMs without\nany explicit positional encoding are still com-\npetitive with standard models, and that this\nphenomenon is robust across different datasets,\nmodel sizes, and sequence lengths. Probing ex-\nperiments reveal that such models acquire an\nimplicit notion of absolute positions through-\nout the network, effectively compensating for\nthe missing information. We conjecture that\ncausal attention enables the model to infer the\nnumber of predecessors that each token can at-\ntend to, thereby approximating its absolute po-", "each expert in WideNet can be trained by more token repre-\nsentations so that it has better generalization performance.\nCompared with the models simply compressed along with\nthe depth, all transformer blocks in WideNet share one same\nMoE layer instead of one FFN layer. Such structure maxi-\nmizes the modeling ability of every transformer block. More\nexperts can model more complex token representations with\na stronger capacity. Another difference is the independent\nnormalization layers. These layers come with few additional\ntrainable parameters, but they can transform input represen-\ntations to other semantic domains. In this case, with a strong\nenough single MoE layer, WideNet can still model seman-\ntics from different levels well. Moreover, in every trans-\nformer block, each expert only receives a part of token repre-\nsentations that usually correspond to different input tokens.\nOur contributions are summarized as three folds:\n\u2022 To improve the parameter ef\ufb01ciency, we propose sharing"], "retrieved_docs_id": ["e7939ae097", "d203b5e503", "76dfb2e33b", "0c1e445a8f", "d74d89d044"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does TinyViT facilitate knowledge distillation during pretraining?\n", "true_answer": "TinyViT facilitates knowledge distillation during pretraining by pre-storing logits from large teacher models in the hardware, enabling memory and computational efficiency when transferring knowledge to scaled-down student transformers.", "source_doc": "multimodal.pdf", "source_id": "534dcc9fda", "retrieved_docs": ["Homomorphic KDs can further classified into logit-level [114, 115], patch-level [117], module-\nlevel [116], and feature-level KDs [118]. For logit-level methods, in DeiT [114], a distillation token\nis incorporated into the self-attention module to emulate the class label inferred by the teacher model,\nfacilitating interaction between the student attention and layers, thus enabling the learning of hard\nlabels during back-propagation. TinyViT [115] applies distillation during pretraining, where logits\nfrom large teacher models are pre-stored in the hardware, enabling memory and computational ef-\nficiency when transferring knowledge to scaled-down student transformers. Patch-level techniques\nlike DeiT-Tiny [117] train a small student model to match a pre-trained teacher model on patch-level\nstructures, then optimize with a decomposed manifold matching loss for reduced computational\ncosts. Module-level methods involve segregating teacher modules from a pre-trained unified model,", "4.5 Combination with Knowledge Distillation\nKnowledge distillation is a popular technique to improve the performance of small models (Romero\net al., 2014; Hinton et al., 2015). In knowledge distillation, the small model (student) is trained to\nmimic the output of a larger fine-tuned model (teacher) such that the performance of the small\nmodel can be improved.\nWe remark that compression methods are complementary to knowledge distillation. We show\nit by integrating knowledge distillation into LoSparse and other pruning methods. Specifically,\nwe choose a DeBERTaV3-base model that is fine-tuned on specific tasks as the teacher model\nand a compressed DeBERTaV3-base model as the student model. Then we conduct layer-wise\ndistillation for them. Please see Appendix E for more training details. Table 5 shows the results.\nWe find that distillation can further improve the performance of LoSparse and other compression", "VTP[110], PS-ViT[111]\nHybrid Pruning SPViT [112], ViT-Slim [113]\nKnowledge Distillation (\u00a73.3)Homomorphic KDDeiT [114], TinyViT [115], m2mKD [116],\nDeiT-Tiny [117], MiniViT [118]\nHeteromorphic KD DearKD [119], CiT [120]\nQuantization (\u00a73.4)Post-Training QuantizationPTQ4ViT [121], APQ-ViT [122],\nNoisyQuant [123]\nQuantization-Aware TrainingQuantformer [124] Bit-shrinking [125],\nQ-ViT [126], TerViT [127], BiViT [128],\nPackQViT [129], BinaryViT [130]\nHardware-Aware Quantization GPUSQ-ViT[131], Auto-ViT-Acc [132]\nFigure 9: Organization of efficient vision advancements.\n10", "In this paper, we show that it is possible to reach similar performances on many downstream-tasks\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\nthat are lighter and faster at inference time, while also requiring a smaller computational training\nbudget. Our general-purpose pre-trained models can be \ufb01ne-tuned with good performances on several\ndownstream tasks, keeping the \ufb02exibility of larger models. We also show that our compressed models\nare small enough to run on the edge, e.g. on mobile devices.\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\nablation studies indicate that all the components of the triple loss are important for best performances.", "develops a dynamic attention-based multi-head token selector for adaptive instance-wise token se-\nlection, alongside a soft pruning technique consolidating less informative tokens into package tokens\nrather than discarding them. ViT-Slim [113] utilizes a learnable and unified sparsity constraint with\npre-defined factors to represent global importance within the continuous search space across various\ndimensions.\n3.3 Knowledge Distillation\nKnowledge distillation is a technique in which a smaller model learns from a larger, more com-\nplex model to replicate its performance, enabling efficient deployment while maintaining predictive\naccuracy [139]. Knowledge distillation (KD) techniques for Vision Transformers (ViTs) can be\ncategorized into two main types: 1) homomorphic KDs and 2) heteromorphic KDs.\nHomomorphic KDs can further classified into logit-level [114, 115], patch-level [117], module-"], "retrieved_docs_id": ["534dcc9fda", "17e9bf4ddc", "8a087225e4", "9f281829ca", "61a0681b1c"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a component of the modular RAG structure that diverges from the traditional similarity retrieval method?\n", "true_answer": "The Search Module is a component of the modular RAG structure that diverges from the traditional similarity retrieval method, as it is tailored to specific scenes.", "source_doc": "RAG.pdf", "source_id": "1d479682a6", "retrieved_docs": ["Modular RAG\nThe modular RAG structure breaks away from the traditional\nNaive RAG framework of indexing, retrieval, and genera-\ntion, offering greater diversity and flexibility in the over-\nall process. On one hand, it integrates various methods to\nexpand functional modules, such as incorporating a search\nmodule in similarity retrieval and applying a fine-tuning ap-\nproach in the retriever [Linet al. , 2023 ]. Additionally, spe-\ncific problems have led to the emergence of restructured\nRAG modules [Yuet al. , 2022 ]and iterative approaches like\n[Shao et al. , 2023 ]. The modular RAG paradigm is becom-\ning the mainstream in the RAG domain, allowing for ei-\nther a serialized pipeline or an end-to-end training approach\nacross multiple modules.The comparison between three RAG\nparadigms is illustrated in Fig 3.\nNew Modules\n\u2022Search Module: Diverging from the similarity re-\ntrieval between queries and corpora in Naive/Advanced\nRAG, the search module, tailored to specific sce-", "different scenarios, including using query engines pro-\nvided by frameworks like LlamaIndex, employing tree\nqueries, utilizing vector queries, or employing the most\nbasic sequential querying of chunks.\u2022HyDE: This approach is grounded on the assumption\nthat the generated answers may be closer in the embed-\nding space than a direct query. Utilizing LLM, HyDE\ngenerates a hypothetical document (answer) in response\nto a query, embeds the document, and employs this em-\nbedding to retrieve real documents similar to the hypo-\nthetical one. In contrast to seeking embedding similarity\nbased on the query, this method emphasizes the embed-\nding similarity from answer to answer. However, it may\nnot consistently yield favorable results, particularly in\ninstances where the language model is unfamiliar with\nthe discussed topic, potentially leading to an increased\ngeneration of error-prone instances.\nModular RAG\nThe modular RAG structure breaks away from the traditional", "ments to assess the relevance between the retrieved doc-\numents and the query. This enhances the robustness of\nRAG [Yuet al. , 2023a ].\nNew Pattern\nThe organizational approach of Modular RAG is flexible,\nallowing for the substitution or reconfiguration of modules\nwithin the RAG process based on specific problem con-\ntexts. For Naive RAG, which consists of the two modules\nof retrieval and generation ( referred as read or sythesis in\nsome literature), this framework offers adaptability and abun-\ndance. Present research primarily explores two organizational\nparadigms, involving the addition or replacement of modules,\nas well as the adjustment of the organizational flow between\nmodules.\n\u2022Adding or Replacing Modules\nThe strategy of adding or replacing modules entails\nmaintaining the structure of Retrieval-Read while intro-\nducing additional modules to enhance specific function-\nalities. RRR [Maet al. , 2023a ]proposes the Rewrite-\nRetrieve-Read process, utilizing LLM performance as a", "lows the decide-retrieve-reflect-read process, introduc-\ning a module for active judgment. This adaptive and\ndiverse approach allows for the dynamic organization of\nmodules within the Modular RAG framework.\n4 Retriever\nIn the context of RAG, the \u201dR\u201d stands for retrieval, serving\nthe role in the RAG pipeline of retrieving the top-k relevant\ndocuments from a vast knowledge base. However, crafting\na high-quality retriever is a non-trivial task. In this chapter,\nwe organize our discussions around three key questions: 1)\nHow to acquire accurate semantic representations? 2) How\nto match the semantic spaces of queries and documents? 3)\nHow to align the output of the retriever with the preferences\nof the Large Language Model ?\n4.1 How to acquire accurate semantic\nrepresentations?\nIn RAG, semantic space is the multidimensional space where\nquery and Document are mapped. When we perform re-\ntrieval, it is measured within the semantic space. If the se-", "providing an effective solution to the incomplete and insuf-\nficient knowledge problem inherent in purely parameterized\nmodels.\nThe paper systematically reviews and analyzes the current\nresearch approaches and future development paths of RAG,\nsummarizing them into three main paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG. Subsequently, the paper\nprovides a consolidated summary of the three core compo-\nnents: Retrieval, Augmented, and Generation, highlighting\nthe improvement directions and current technological char-\nacteristics of RAG. In the section on augmentation methods,the current work is organized into three aspects: the augmen-\ntation stages of RAG, augmentation data sources, and aug-\nmentation process. Furthermore, the paper summarizes the\nevaluation system, applicable scenarios, and other relevant\ncontent related to RAG. Through this article, readers gain a\nmore comprehensive and systematic understanding of large\nmodels and retrieval-Augmented generation. They become"], "retrieved_docs_id": ["1d479682a6", "d96393bb4b", "a016e8d322", "8fe8499442", "1bd400d39e"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the purpose of the StepBack-prompt approach in the RAG process?\n", "true_answer": "The StepBack-prompt approach encourages the language model to step back from specific instances and engage in reasoning about the underlying general concepts or principles.", "source_doc": "RAG.pdf", "source_id": "ad03b3dcc5", "retrieved_docs": ["phase to capture key semantic meanings. In the later\nstages of this process, larger blocks with more contex-\ntual information are provided to the language model\n(LM). This two-step retrieval method helps strike a bal-\nance between efficiency and contextually rich responses.\n\u2022StepBack-prompt: Integrated into the RAG process,\nthe StepBack-prompt approach [Zheng et al. , 2023 ]en-\ncourages LLM to step back from specific instances and\nengage in reasoning about the underlying general con-\ncepts or principles. Experimental findings indicate a sig-\nnificant performance improvement in various challeng-\ning, inference-intensive tasks with the incorporation of\nbackward prompts, showcasing its natural adaptability\nto RAG. The retrieval-enhancing steps can be applied in\nboth the generation of answers to backward prompts and\nthe final question-answering process.\n\u2022Subqueries: Various query strategies can be employed in\ndifferent scenarios, including using query engines pro-", "put forward various methods to optimize the retrieval process.\nIn terms of specific implementation, Advanced RAG can be\nadjusted either through a pipeline or in an end-to-end manner.\nPre-Retrieval Process\n\u2022Optimizing Data Indexing\nThe purpose of optimizing data indexing is to enhance\nthe quality of indexed content. Currently, there are five\nmain strategies employed for this purpose: increasing\nthe granularity of indexed data, optimizing index struc-\ntures, adding metadata, alignment optimization, and\nmixed retrieval.\n1.Enhancing Data Granularity: The objective of\npre-index optimization is to improve text standard-\nization, consistency, and ensure factual accuracy\nand contextual richness to guarantee the perfor-\nmance of the RAG system. Text standardization in-\nvolves removing irrelevant information and special\ncharacters to enhance the efficiency of the retriever.\nIn terms of consistency, the primary task is to elim-\ninate ambiguity in entities and terms, along with", "Figure 3: Comparison between the three paradigms of RAG\n\u2022Task Adaptable Module: Focused on trans-\nforming RAG to adapt to various downstream\ntasks, UPRISE [Cheng et al. , 2023a ] automati-\ncally retrieves prompts for given zero-shot task\ninputs from a pre-constructed data pool, en-\nhancing universality across tasks and models.\nPROMPTAGATOR [Daiet al. , 2022 ]utilizes LLM\nas a few-shot query generator and, based on the gener-\nated data, creates task-specific retrievers. Leveraging\nthe generalization capability of LLM, PROMPTAGA-\nTOR enables the creation of task-specific end-to-end\nretrievers with just a few examples.\n\u2022Alignment Module: The alignment between queries\nand texts has consistently been a critical issue influenc-\ning the effectiveness of RAG. In the era of Modular\nRAG, researchers have discovered that adding a train-\nable Adapter module to the retriever can effectively mit-\nigate alignment issues. PRCA [Yang et al. , 2023b ]lever-", "ments to assess the relevance between the retrieved doc-\numents and the query. This enhances the robustness of\nRAG [Yuet al. , 2023a ].\nNew Pattern\nThe organizational approach of Modular RAG is flexible,\nallowing for the substitution or reconfiguration of modules\nwithin the RAG process based on specific problem con-\ntexts. For Naive RAG, which consists of the two modules\nof retrieval and generation ( referred as read or sythesis in\nsome literature), this framework offers adaptability and abun-\ndance. Present research primarily explores two organizational\nparadigms, involving the addition or replacement of modules,\nas well as the adjustment of the organizational flow between\nmodules.\n\u2022Adding or Replacing Modules\nThe strategy of adding or replacing modules entails\nmaintaining the structure of Retrieval-Read while intro-\nducing additional modules to enhance specific function-\nalities. RRR [Maet al. , 2023a ]proposes the Rewrite-\nRetrieve-Read process, utilizing LLM performance as a", "quently, it utilizes this retrieved information to generate re-\nsponses or text, thereby enhancing the quality of predictions.\nThe RAG method allows developers to avoid the need for\nretraining the entire large model for each specific task. In-\nstead, they can attach a knowledge base, providing additional\ninformation input to the model and improving the accuracy\nof its responses. RAG methods are particularly well-suited\nfor knowledge-intensive tasks. In summary, the RAG system\nconsists of two key stages:1. Utilizing encoding models to retrieve relevant docu-\nments based on questions, such as BM25, DPR, Col-\nBERT, and similar approaches [Robertson et al. , 2009,\nKarpukhin et al. , 2020, Khattab and Zaharia, 2020 ].\n2. Generation Phase: Using the retrieved context as a con-\ndition, the system generates text.\n2.2 RAG vs Fine-tuning\nIn the optimization of Large Language Models (LLMs), in\naddition to RAG, another important optimization technique\nis fine-tuning."], "retrieved_docs_id": ["ad03b3dcc5", "8a71abd00a", "bbfa682738", "a016e8d322", "80558327ad"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does a recent work address the hallucination problem in Multi-Modal Language Learning Models (MLLMs)?\n", "true_answer": "A recent work addresses the hallucination problem in MLLMs by utilizing the Efficient Fine-grained Unlearning Framework (EFUF) and the CLIP model to construct a dataset.", "source_doc": "hallucination.pdf", "source_id": "2dd3a385f4", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability"], "retrieved_docs_id": ["33d47ad8cc", "114f3dada8", "92e73c053a", "da0a465b6c", "72dc971633"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "Who wrote the 2024 survey on hallucination of multimodal large language models?\n", "true_answer": "Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, and Zheng Zhang", "source_doc": "hallucination.pdf", "source_id": "9bfe24c206", "retrieved_docs": ["alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.\nCCS Concepts: \u2022Computing methodologies \u2192Computer vision ;Natural language processing ;Machine\nlearning .\nAdditional Key Words and Phrases: Hallucination, Multimodal, Large Language Models, Vision-Language\nModels.\nACM Reference Format:\nZechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou.\n2024. Hallucination of Multimodal Large Language Models: A Survey. Preprint 1, 1 (April 2024), 30 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n\u2217Corresponding Author\nAuthors\u2019 addresses: Zechen Bai, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore,\nzechenbai@u.nus.edu; Pichao Wang, Amazon Prime Video, Washington, USA, pichaowang@gmail.com; Tianjun Xiao,\nAWS Shanghai AI Lab, Shanghai, China, tianjux@amazon.com; Tong He, AWS Shanghai AI Lab, Shanghai, China, htong@", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest"], "retrieved_docs_id": ["9bfe24c206", "72dc971633", "114f3dada8", "92e73c053a", "33d47ad8cc"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What are some methods for optimizing attention mechanisms in vision transformers?\n", "true_answer": "Some methods for optimizing attention mechanisms in vision transformers include PatchMerger, DynamicViT, and Sepvit.", "source_doc": "multimodal.pdf", "source_id": "20b3b3179f", "retrieved_docs": ["learning and the role played by the attention mechanism [ ASA+22,LIPO23 ,ACDS23 ,ZFB23 ,BCW+23,GRS+23].\n[SEO+22] investigate self-attention with linear activation instead of softmax, while [ ENM22 ] approximate softmax\nusing a linear operation with unit simplex constraints. Their primary goal is to derive convex reformulations for\ntraining problems grounded in empirical risk minimization (ERM). In contrast, our methodologies, detailed in equations\n(W-ERM) and (KQ-ERM), delve into the nonconvex domain.\n[MRG+20,BALA+23] offer insights into the implicit bias of optimizing transformers. Specifically, [ MRG+20]\nprovide empirical evidence that an increase in attention weights results in a sparser softmax, which aligns with our\ntheoretical framework. [ BALA+23] study incremental learning and furnish both theory and numerical evidence that\nincrements of the softmax attention weights ( KQ\u22a4) are low-rank. Our theory aligns with this concept, as the SVM", "UniNet [102] introduced context-aware down-sampling modules improving information accommo-\ndation by transformer and MLP operators.\nOptimization of Attention Mechanisms Methods focus on reducing computational complexity\nby introducing adaptive attention, learning sparse attention patterns, and dynamically adjusting at-\ntention mechanisms. Fayyaz et al. [135] implemented adaptive attention by scoring and adaptively\nsampling significant tokens. PatchMerger [103] extracted global information among regional to-\nkens and exchanged local self-attention with information among regional tokens via self-attention.\nDynamicViT [104] proposed an attention masking strategy to differentiably prune tokens by block-\ning interactions with other tokens. Additionally, Sepvit [105] conducted local-global information\ninteraction within and across windows using depthwise separable self-attention. These methods\ncollectively optimize attention mechanisms, enhancing computational efficiency and performance.", "to conventional models like MLPs and CNNs, self-attention models employ global interactions to capture feature\nrepresentations, resulting in exceptional empirical performance.\nDespite their achievements, the mechanisms and learning processes of attention layers remain enigmatic. Recent\ninvestigations [ EGKZ22 ,SEO+22,ENM22 ,BV22 ,DCL21 ] have concentrated on specific aspects such as sparse\nfunction representation, convex relaxations, and expressive power. Expressivity discussions concerning hard-attention\n[Hah20 ] or attention-only architectures [ DCL21 ] are connected to our findings when h(\u00b7)is linear. In fact, our work\nreveals how linear hresults in attention\u2019s optimization dynamics to collapse on a single token whereas nonlinear h\nprovably requires attention to select and compose multiple tokens. This supports the benefits of the MLP layer for\nexpressivity of transformers. There is also a growing body of research aimed at a theoretical comprehension of in-context", "low-rank, and sparse approximations (Child et al., 2019; Wang et al., 2020; Kitaev et al., 2020; Zhai et al.,\n2021; Roy et al., 2021; Schlag et al., 2021; Tu et al., 2022). These approaches introduce a trade-o\ufb00 between\nexpressivity and speed, requiring hybridization with standard attention layers to reach Transformer quality\n(Mehta et al., 2022; Dao et al., 2022c).\nA growing amount of evidence suggests that attention mechanisms only utilize a small portion of their\nquadratic capabilities for language processing (Olsson et al., 2022; Dao et al., 2022c), leading us to question\nits role as the gold-standard operator for deep learning at scale. Speci\ufb01cally, we ask:\nAre there subquadratic operators that can match the quality of attention at scale?\n\u2217Equal contribution. \u2020Equal senior authorship.1Stanford University.2Mila and Universit\u00e9 de Montr\u00e9al.\n1arXiv:2302.10866v3  [cs.LG]  19 Apr 2023", "capabilities: the \ufb01rst Transformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n1 Introduction\nTransformer models [ 82] have emerged as the most widely used architecture in applications such as natural\nlanguage processing and image classi\ufb01cation. Transformers have grown larger [ 5] and deeper [ 83], but\nequipping them with longer context remains di\ufb03cult [ 80], since the self-attention module at their heart\nhas time and memory complexity quadratic in sequence length. An important question is whether making\nattention faster and more memory-e\ufb03cient can help Transformer models address their runtime and memory\nchallenges for long sequences.\nMany approximate attention methods have aimed to reduce the compute and memory requirements of\nattention. These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84],"], "retrieved_docs_id": ["d8e0f51d42", "e7939ae097", "bfb96ebe0c", "179308452a", "d3ed275525"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is the objective of the pre-training phase in cross-modal feature alignment?\n", "true_answer": "The objective of the pre-training phase is to achieve cross-modal feature alignment between models from each modality.", "source_doc": "hallucination.pdf", "source_id": "0be0058571", "retrieved_docs": ["Category HallucinationAttribute HallucinationRelation Hallucination\nFig. 3. Three types of typical hallucination.\nPre-training. Given that models from each modality are pre-trained on their respective data, the\nobjective of this pre-training phase is to achieve cross-modal feature alignment. During training,\nboth the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface\nbeing trained. Similar to traditional VLMs training, as exemplified by CLIP [ 88], web-scale image-\ntext pairs [ 92] are utilized for training. Given that the final output is at the LLM side, the most\nwidely used loss function in this stage is the text generation loss, typically cross-entropy loss, which\naligns with the pre-training of LLMs. Certain studies (e.g., [ 22,66]) explore the incorporation of\ncontrastive loss and image-text matching loss to further enhance alignment. After training, the\ninterface module maps the visual features into the input embedding space of the LLM.", "pacities. To improve the alignment performance, it is crucial\nto design effective training strategies and select appropriate\npre-training data [829, 830]. Existing work mainly employs\nthe following strategies for cross-modality alignment: (1) if\nthe number of image-text pairs is not sufficiently large ( e.g.,\nless than 1M), it is often suggested to only update the\nconnection module [831]; (2) if the training data includes\nhigh-quality text corpora [832] or image-text pairs with\nfine-grained annotations [833], fine-tuning the LLM can be\nconducted to boost the performance; (3) if the number of\nimage-text pairs is very large ( e.g., about 1B), fine-tuning\nthe vision encoder is also plausible [829, 830], but the benefit\nremains further verification.\n\u2022Visual instruction tuning. After vision-language pre-\ntraining, the second-stage training, i.e., visual instruction\ntuning, aims to improve the instruction-following and task-\nsolving abilities of MLLMs. Generally, the input of vi-", "generated content remains consistent and contextually relevant to the input modality requires\nsophisticated techniques for capturing and modeling cross-modal relationships. The direction of\ncross-modal alignment encompasses both MLLMs training and hallucination evaluation. Regarding\ntraining, future research should explore methods for aligning representations between different\nmodalities. Achieving this goal may involve designing more advanced architectures, introducing\nadditional learning objectives [ 52], or incorporating diverse supervision signals [ 16]. Regarding\nevaluation, cross-modal consistency checking has been a long-standing topic, ranging from multi-\nmodal understanding [ 66,88] to text-to-image generation [ 13,17]. Drawing on proven experiences\nfrom these domains to improve the assessment of MLLM hallucination, or unifying them into an\noverall framework, may be promising research directions.\n6.3 Advancements in Model Architecture", "Figure 14: Training stages of efficient MLLMs.\nusing a standard cross-entropy loss function:\nmax\n\u03b8LX\ni=1logp\u03b8(xi|Xv, Xinstruct , Xa,<i), (4)\nwhere Lis the length of Xaand\u03b8denotes the trainable parameters. In order to better align different\nmodalities of knowledge and avoid catastrophic forgetting during the pre-training stage, \u03b8typically\nincludes only a learnable modality interface, i.e., a vision-language projector.\nWhich part to unfreeze? Considering that only training the connector may not well align the\nvision and text information when using SLMs, TinyLlava[23] also opt to partially freeze pre-\ntrained modules (i.e. vision encoder and SLM) to activate more parameters for learning alignment.\nVILA[49] reveals that updating the base LLM throughout the pre-training stage is essential to in-\nheriting some of the appealing LLM properties like in-context learning. ShareGPT4V[55] found\nthat unfreezing more parameters, particularly in the latter half of the vision encoder\u2019s layers, proves", "sequences. Moreover, XPOSoptimizes attention resolution so that the position information can\nbe captured more precisely. The method XPOSis ef\ufb01cient and effective in both interpolation and\nextrapolation settings.\n2.3 Training Objective\nTheKOSMOS -1training is conducted on web-scale multimodal corpora, including monomodal data\n(e.g., text corpus), cross-modal paired data (e.g., image-caption pairs), and interleaved multimodal\ndata (e.g., documents of arbitrarily interleaved images and texts). To be speci\ufb01c, we use monomodal\ndata for representation learning. For example, language modeling with text data pretrains instruction\nfollowing, in-context learning, and various language tasks. Moreover, cross-modal pairs and inter-\nleaved data learn to align the perception of general modalities with language models. Interleaved data\nalso naturally \ufb01t in the multimodal language modeling task. We present more details of training data\ncollection in Section 3.1."], "retrieved_docs_id": ["0be0058571", "687ba2bcbe", "83c3718d9d", "1bd741e7c9", "79427d1903"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is one example of a task-specific dataset used to derive high-quality IT data?\n", "true_answer": "A sample from VQA (Visual Question Answering) datasets, where the input includes an image and a natural language question, and the output is the text-based answer to the question based on the image, is one example of a task-specific dataset used to derive high-quality IT data.", "source_doc": "multimodal.pdf", "source_id": "db45826cee", "retrieved_docs": ["A summary of frequently used pre-training datasets can be found in Table.3. High-quality IT data\ncan be derived from task-specific datasets. For instance, consider a sample from VQA datasets where\nthe input includes an image and a natural language question, and the output is the text-based answer\nto the question based on the image. This could easily form the multimodal input and response\nof the instruction sample. The instructions, or task descriptions, can be obtained either through\nmanual creation or semi-automatic generation with the help of GPT. In addition to utilizing publicly\navailable task-specific datasets, SPHINX-X[14] assembles a dataset focused on OCR from a wide\nrange of PDF data sourced from the internet. Specifically, it begins by gathering a large-scale PDF\ndataset from the web. It then obtains the rendering results of each page in the PDF file, while\nsimultaneously saving all text annotations along with their respective bounding boxes. Ultimately,", "the combination of IT datasets. For the results,\nWang et al. (2023c) showed that there is not a\nsingle best IT dataset across all tasks, while by\nmanually combining datasets it can achieve the best\noverall performance. Besides, Wang et al. (2023c)\npointed out that though IT can bring large benefits\non LLMs at all sizes, smaller models and models\nwith a high base quality benefit most from IT. For\nhuman evaluations, Wang et al. (2023c) a larger\nmodel is more likely to gain a higher acceptability\nscore.\n8.5 Do IT just learn Pattern Copying?\nTo address the lack of clarity about the specific\nknowledge that models acquire through instruction\ntuning, Kung and Peng (2023) delves into the\nanalysis of how models make use of instructions\nduring IT by comparing the tuning when provided\nwith altered instructions versus the original\ninstructions.\nSpecifically, Kung and Peng (2023) creates\nsimplified task definitions that remove all\nsemantic components, leaving only the output", "and algorithms [ RSR+19,LOG+19,YDY+19,LCG+19]. However, a major limitation to this approach is that while\nthe architecture is task-agnostic, there is still a need for task-speci\ufb01c datasets and task-speci\ufb01c \ufb01ne-tuning: to achieve\nstrong performance on a desired task typically requires \ufb01ne-tuning on a dataset of thousands to hundreds of thousands\nof examples speci\ufb01c to that task. Removing this limitation would be desirable, for several reasons.\nFirst, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the\napplicability of language models. There exists a very wide range of possible useful language tasks, encompassing\nanything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many\nof these tasks it is dif\ufb01cult to collect a large supervised training dataset, especially when the process must be repeated\nfor every new task.", "76 distinct task types (e.g., text classification,\ninformation extraction, text rewriting, text\ncomposition and etc.) and 55 languages. Each\ntask in the dataset consists of an \"instruction\" and\n\"task instances\". Specifically, \"instruction\" has\nthree components: a \"definition\" that describes the\ntask in natural language; \"positive examples\" that\nare samples of inputs and correct outputs, along\nwith a short explanation for each; and \"negative\nexamples\" that are samples of inputs and undesired\noutputs, along with a short explanation for each,\nas shown in Figure 2 (a). \"Task instances\" are\ndata instances comprised of textual input and a\nlist of acceptable textual outputs, as shown in\nFigure 2 (b). The original data in Super Natural\nInstructions comes from three sources: (1) existing\npublic NLP datasets (e.g., CommonsenseQA);\n(2) applicable intermediate annotations that are\ngenerated through a crowdsourcing process (e.g.,\nparaphrasing results to a given question during a", "dataset was constructed originally.\nWe can roughly divide the 29 benchmark tasks into four categories:\n\u2022Wholesale contamination \u2013 Datasets where a signi\ufb01cant portion of the dataset itself appears in the\nopen web. We consider these contaminated. Examples: SQuADv2, Winograd.\n36"], "retrieved_docs_id": ["db45826cee", "871ca40630", "2e4df88547", "54589e384b", "cdc903725f"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How many image-question pairs does MERLIM contain and what is its focus?\n", "true_answer": "MERLIM contains over 279K image-question pairs and has a strong focus on detecting cross-modal hallucinations.", "source_doc": "hallucination.pdf", "source_id": "198c99577c", "retrieved_docs": ["including object recognition, instance counting, and identifying object-to-object relationships.\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal\nhallucinations. Interestingly, when organizing the data, a set of edited images is intentionally added.\nBased on the original image, an inpainting strategy is employed to remove one object instance in\nthe image. With this original-edited image pair, one can compare the output of the target MLLM\nand identify the hallucinated objects that lack visual grounding.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "randomly samples 100 images from Visual Genome to form a benchmark. In evaluation, GPT-4\nis utilized to parse the captions generated by MLLMs and extract objects. Additionally, this work\nintroduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\nThis metric computes the ratio of objects in the caption that match the ground truth to the total\nnumber of ground truth objects. It additionally records the average number of objects as well as\nthe average length of captions as auxiliary metric. Compared with CHAIR, CCEval employs more\ndiverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption\nparsing (GPT-4 vs. rule-based tool).\nMERLIM [ 100]MERLIM ( Multi-modal Evaluation benchma Rk for Large Image-language\nModels) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks,\nincluding object recognition, instance counting, and identifying object-to-object relationships.", "The Visual Dependent questions are defined as questions that do not have an affirmative answer\nwithout the visual context. This setting aims to evaluate visual commonsense knowledge and visual\nreasoning skills. The Visual Supplement questions can be answered without the visual input; the\nvisual component merely provides supplemental information or corrections. This setting is designed\nto evaluate visual reasoning ability and the balance between parametric memory (language prior)\nand image context. This division provides a new perspective for understanding and diagnosing\nMLLMs.\nCCEval [ 123]CCEval focuses on the hallucination evaluation of detailed captions. Traditional\ncaption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\nHowever, short captions often lack detail and contain less information. To address this issue, CCEval\nrandomly samples 100 images from Visual Genome to form a benchmark. In evaluation, GPT-4", "as the target model.\nTo further ensure broad coverage of knowledge-intensive question answering across many disciplines, we add\ntheMMLU (Hendrycks et al., 2021c) meta-benchmark of 57 constituent datasets. MMLU (Measuring\nMassive Multitask Language Understanding) measures multitask accuracy and includes a diverse set of 57\ntasks, testing problem solving and general knowledge.\nFinally, we add BoolQ(Clark et al., 2019) which, in addition to QuAC, was used to study model robustness\nto equivariances due to the available contrast set (Gardner et al., 2020). BoolQis a collection of binary\nyes/no questions generated through the same process as NaturalQuestions .\n3.4 Information retrieval\nInformation retrieval (IR), which refers to the class of tasks concerned with searching large unstructured\ncollections (often textcollections), is central to numerous user-facing applications. IR has a long tradition of", "with respect to an image.\n4.1.1 Evaluation Setup\nWe evaluate the caption generation on MS COCO Caption [ LMB+14], and Flickr30k [ YLHH14 ].\nWe use the test set of COCO Karpathy split [KFF17 ], which re-partitions the train2014 and val2014\nimages [ LMB+14] into 113,287, 5,000, and 5,000 for the training set, validation set, and test set,\nrespectively. We conduct an evaluation on Flickr30k\u2019s Karpathy split test set. The image resolution\nis 224 \u00d7224. We use beam search to generate the captions, and the beam size is 5. In the few-shot\nsettings, we randomly sample demonstrations from the training set. We use COCOEvalCap4to\ncompute CIDEr [ VLZP15 ] and SPICE [ AFJG16 ] scores as the evaluation metrics. We prompt\nKOSMOS -1 with \u201cAn image of\u201d for zero-shot and few-shot caption generation experiments.\nFor visual question-answering tasks, we evaluate zero-shot and few-shot results on test-dev set of\nVQAv2 [ GKSS+17] and test-dev set of VizWiz [ GLS+18], respectively. The resolution of images is"], "retrieved_docs_id": ["198c99577c", "6e78496733", "4e7d38fc3d", "5c08130327", "949861762a"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How much does domain-specific data improve the retriever hit rate in the ChipNeMo system?\n", "true_answer": "The retriever hit rate is improved by 30% using domain-specific data in the ChipNeMo system.", "source_doc": "ChipNemo.pdf", "source_id": "df0b9868f2", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "vant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation", "uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 4: Domain-Adapted ChipNeMo Tokenizer Improvements.\n3.1. Domain-Adaptive Tokenization\nWe adapt the LLaMA2 tokenizer (containing 32K tokens) to\nchip design datasets using the previously outlined four-step\nprocess. Approximately 9K new tokens are added to the\nLLaMA2 tokenizer. The adapted tokenizers can improve\ntokenization efficiency by 1.6% to 3.3% across various chip\ndesign datasets as shown in Figure 4. We observe no obvious\nchanges to tokenizer efficiency on public data. Importantly,\nwe have not observed significant decline in the LLM\u2019s accu-\nracy on public benchmarks when using the domain-adapted\ntokenizers even prior to DAPT.\n3.2. Domain Adaptive Pretraining\nFigure 5: Chip Domain Benchmark Result for ChipNeMo.\nFigure 5 presents the outcomes for ChipNeMo models on\nthe AutoEval benchmark for chip design domain (detailed\nin Appendix A.5). Results on open domain academic bench-\nmark results are presented in Appendix A.6. Our research", "LLM\u2019s Preference\nIn the RAG pipeline, even if we employ the above techniques\nto enhance the retrieval hit rate, it may still not improve the\nfinal effect of RAG, because the retrieved documents may not\nbe what LLM needs. Thus, this section introduces two meth-\nods to align the outputs of the retriever and the preferences of\nthe LLM.\nLLM supervised training Many works leverage various\nfeedback signals from large language models to fine-tune em-\nbedding models. AAR [Yuet al. , 2023b ]provides supervi-\nsory signals for a pre-trained retriever through an encoder-\ndecoder architecture LM. By determining the LM\u2019s preferred\ndocuments through FiD cross-attention scores, the retriever\nis then fine-tuned with hard negative sampling and standard\ncross-entropy loss. Ultimately, the fine-tuned retriever can\ndirectly be used to enhance unseen target LMs, thereby per-\nforming better in the target task. The training loss of retriever\nas:\n\u03b6=X\nqX\nd+\u2208Da+X\nd\u2212\u2208D\u2212l\u0000\nf\u0000\nq, d+\u0001\n, f\u0000\nq, d\u2212\u0001\u0001\n(1)"], "retrieved_docs_id": ["df0b9868f2", "28f0897bcb", "c7d05c4b43", "ac7c0c980b", "6b0d47093c"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does fine-tuning retrievers and generators in the downstream phase improve information retrieval?\n", "true_answer": "Fine-tuning retrievers and generators in the downstream phase primarily enhances information retrieval in open-domain question-answering tasks.", "source_doc": "RAG.pdf", "source_id": "011ee221ab", "retrieved_docs": ["domain question-answering tasks. Concerning retriever fine-\ntuning, REPlUG [Shiet al. , 2023 ]treats the language model\n(LM) as a black box and enhances it through an adjustable re-\ntrieval model. By obtaining feedback from the black-box lan-\nguage model through supervised signals, REPLUG improves\nthe initial retrieval model. UPRISE [Cheng et al. , 2023a ], on\nthe other hand, fine-tunes retrievers by creating a lightweight\nand versatile retriever through fine-tuning on diverse task\nsets. This retriever can automatically provide retrieval\nprompts for zero-shot tasks, showcasing its universality and\nimproved performance across tasks and models.\nSimultaneously, methods for fine-tuning generators in-\nclude Self-Mem [Cheng et al. , 2023b ], which fine-tunes the\ngenerator through a memory pool of examples, and\nSelf-RAG [Asai et al. , 2023b ], which satisfies active re-\ntrieval needs by generating reflection tokens. The RA-\nDIT[Linet al. , 2023 ]method fine-tunes both the generator", "knowledge-intensive tasks, allowing the creation of domain-\nspecific models through training on domain-specific corpora.\nHowever, there are drawbacks, including the requirement for\na substantial amount of pre-training data and larger training\nresources, as well as the issue of slower update speeds. Espe-\ncially as model size increases, the cost of retrieval-enhanced\ntraining becomes relatively higher. Despite these limitations,\nthis method demonstrates notable characteristics in terms of\nmodel robustness. Once trained, retrieval-enhanced models\nbased on pure pre-training eliminate the need for external li-brary dependencies, enhancing both generation speed and op-\nerational efficiency.\nFine-tuning Stage\nDuring the downstream fine-tuning phase, researchers have\nemployed various methods to fine-tune retrievers and gener-\nators for improved information retrieval, primarily in open-\ndomain question-answering tasks. Concerning retriever fine-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "search, and vector search, adapts to different query\ntypes and information needs, ensuring consistent\nretrieval of the most relevant and context-rich in-\nformation. Mixed retrieval can serve as a robust\ncomplement to retrieval strategies, enhancing the\noverall performance of the RAG pipeline.\nEmbedding\n\u2022Fine-turning Embedding: Fine-tuning embedding\nmodels directly impacts the effectiveness of RAG. The\npurpose of fine-tuning is to enhance the relevance be-\ntween retrieved content and query. The role of fine-\ntuning embedding is akin to adjusting ears before gener-\nating speech, optimizing the influence of retrieval con-\ntent on the generated output. Generally, methods for\nfine-tuning embedding fall into the categories of ad-\njusting embedding in domain-specific contexts and op-\ntimizing retrieval steps. Especially in professional do-\nmains dealing with evolving or rare terms, these cus-\ntomized embedding methods can improve retrieval rel-", "tion may have a significant impact on the model\u2019s understand-\ning, especially for smaller models. In such scenarios, fine-\ntuning the model to adapt to the input of query + retrieved\ndocuments becomes particularly important. Specifically, be-\nfore providing the input to the fine-tuned model, there is usu-\nally post-retrieval processing of the documents retrieved by\nthe retriever. It is essential to note that the method of fine-\ntuning the generator in RAG is essentially similar to the gen-\neral fine-tuning approach for LLMs. Here, we will brieflyintroduce some representative works, including data (format-\nted/unformatted) and optimization functions.\nGeneral Optimization Process\nRefers to the training data containing pairs of (input, output),\naiming to train the model\u2019s ability to generate output y given\ninput x. In the work of Self-mem [Cheng et al. , 2023b ], a\nrelatively classical training process is employed. Given in-\nput x, relevant documents z are retrieved (selecting Top-1"], "retrieved_docs_id": ["662eb558d5", "011ee221ab", "fefa202c19", "62aa0f539a", "5e50b58781"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "How does the input to the generator differ in a RAG model compared to typical Large Language Model (LLM) generation tasks?\n", "true_answer": "In a RAG (Retriever-Augmented Generator) model, the input to the generator includes not only a query but also various documents retrieved by the retriever, whereas in typical LLM generation tasks, the input is usually just a query.", "source_doc": "RAG.pdf", "source_id": "7fabdba415", "retrieved_docs": ["information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "quently, it utilizes this retrieved information to generate re-\nsponses or text, thereby enhancing the quality of predictions.\nThe RAG method allows developers to avoid the need for\nretraining the entire large model for each specific task. In-\nstead, they can attach a knowledge base, providing additional\ninformation input to the model and improving the accuracy\nof its responses. RAG methods are particularly well-suited\nfor knowledge-intensive tasks. In summary, the RAG system\nconsists of two key stages:1. Utilizing encoding models to retrieve relevant docu-\nments based on questions, such as BM25, DPR, Col-\nBERT, and similar approaches [Robertson et al. , 2009,\nKarpukhin et al. , 2020, Khattab and Zaharia, 2020 ].\n2. Generation Phase: Using the retrieved context as a con-\ndition, the system generates text.\n2.2 RAG vs Fine-tuning\nIn the optimization of Large Language Models (LLMs), in\naddition to RAG, another important optimization technique\nis fine-tuning.", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify"], "retrieved_docs_id": ["7fabdba415", "fefa202c19", "80558327ad", "4fffd3dc2b", "af911eac69"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is one example of an Architecture Design Method used to create compact models?\n", "true_answer": "One example of an Architecture Design Method is Reformer, which introduces locality-sensitive hashing in attention mechanisms to reduce complexity and uses reversible residual layers to store activations more efficiently.", "source_doc": "multimodal.pdf", "source_id": "82a6543862", "retrieved_docs": ["3.1 Compact Architecture\nCompact Architecture refers to the design of lightweight and efficient models while maintaining high\nperformance in downstream tasks. It encompasses various strategies and methodologies to reduce\nmodel size, computational complexity, and memory footprint without compromising performance.\nThese strategies can be broadly categorized into three categories, 1) Architecture Design Methods,\n2) Architecture Search Methods, and 3) Optimization of Attention Mechanisms Methods.\nArchitecture Design Methods involve creating new architectures [133] or adjusting existing\nones [134] to achieve compactness without sacrificing performance. For example, Reformer [96]\nintroduced locality-sensitive hashing in attention mechanisms to reduce complexity, while also\nemploying reversible residual layers to store activations more efficiently. Furthermore, Efficient-\nFormer [97] analyzed ViT-based model architectures and operators, introducing a dimension-", "Former [97] analyzed ViT-based model architectures and operators, introducing a dimension-\nconsistent pure transformer paradigm and employing latency-driven slimming to produce optimized\nmodels. Additionally, EfficientFormerV2 [98] proposed a supernet with low latency and high pa-\nrameter efficiency.\nArchitecture Search Methods involve employing neural architecture search algorithms [113]\nto explore and discover compact architectures tailored to specific tasks or constraints. For in-\nstance, Autoformer [99] intertwined weights within layers, enabling thorough training of thousands\nof subnets. NASViT [100] introduced a gradient projection algorithm, switchable layer scaling,\nand streamlined data augmentation, enhancing convergence and performance. Additionally, TF-\nTAS [101] investigated training-free architecture search methods, proposing an efficient scheme.\nUniNet [102] introduced context-aware down-sampling modules improving information accommo-", "produce a collection called xP3mt. Further details on the prompt collection for xP3 and\nxP3mt are given in Muennighoff et al. (2022b).\n3.2 Model Architecture\nThis section discusses our design methodology and the architecture of the BLOOM model.\nIn-depth studies and experiments can be found in Le Scao et al. (2022) and Wang et al.\n(2022a). We first review our design methodology, then motivate our choice of training a\ncausal decoder-only model. Finally, we justify the ways that our model architecture deviates\nfrom standard practice.\n3.2.1 Design Methodology\nThe design spaceof possible architectures is immense, making exhaustive explorationimpos-\nsible. One option would be to exactly replicate the architecture of an existing large language\nmodel. On the other hand, a great deal of work on improving existing architectures has\nseen relatively little adoption (Narang et al., 2021); adopting some of these recommended", "alternatively separating layers of the models across accelerators and then pipe-lining activations between\nthe stages (Huang et al., 2019). Many other works aim to increase of the scale of models, while limiting\ncommunication overheads (Rajbhandari et al., 2020; Lepikhin et al., 2020; Li et al., 2020; Rasley et al.,\n2020; Rajbhandari et al., 2021; Ren et al., 2021; Narayanan et al., 2021a). PaLM uses a blend of data and\nmodel-parallelism enabled through the Pathways infrastructure (Barham et al., 2022).\nArchitectural variants have been proposed to help scale models more e\ufb03ciently. One area is retrieval models\nthat aim to drastically reduce model sizes by embedding large amounts of text the model can have access to\nlater (Guu et al., 2020; Borgeaud et al., 2021). Model sparsity like Mixture-of-Experts allows for scaling model\nsizes by allowing di\ufb00erent examples to use di\ufb00erent subsets of parameters (Shazeer et al., 2017; Lepikhin", "larly to sliding window techniques (Press et al., 2020), this\napproach doubles the cost of inference but improves results.\n2.4. Motivation\nHaving described the model, we brie\ufb02y discuss the motiva-\ntion behind some of the architectural choices.\nWhy is the local model needed? Many of the ef\ufb01ciency\nadvantages of the MEGABYTE design could be realized"], "retrieved_docs_id": ["82a6543862", "6ed104ce6b", "3dd09e5931", "a6870d62c6", "8bd6c24a14"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How can hallucination capabilities of large language models be used to improve human user experiences?\n", "true_answer": "Hallucination capabilities can be leveraged to enhance user experiences and enable new use cases, such as integrating hallucination to inspire creative ideas in real-world applications or optimizing the models for human experiences rather than specific cross-modal benchmarks.", "source_doc": "hallucination.pdf", "source_id": "dfb6343eae", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 23\nrecollection of its training documents, most of the time the result goes someplace useful. It\u2019s only\nwhen the dreams enter deemed factually incorrect territory that we label them as \u2019hallucinations\u2019.\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications\npresents exciting opportunities for enhancing user experiences and enabling new use cases. As\nhumans are the end-users of these models, the primary goal is to enrich human user experiences.\nFuture research may switch the optimization objective from specific cross-modal benchmarks to\nhuman experience. For example, Some content may cause hallucinations but will not affect the\nuser experience, while some content may. Alternatively, integrating hallucination to inspire more\ncreative ideas in real-world applications could also be intriguing.\n6.6 Enhancing Interpretability and Trust", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "recently, HaluEval [602] creates a large-scale LLM-generated\nand human-annotated hallucinated samples to evaluate the\nability of language models to recognize hallucination in both\ntask-specific and general scenarios.\nHallucination\nLLMs are prone to generate untruthful informa-\ntion that either conflicts with the existing source\nor cannot be verified by the available source.\nEven the most powerful LLMs such as ChatGPT\nface great challenges in migrating the hallucina-\ntions of the generated texts. This issue can be\npartially alleviated by special approaches such as\nalignment tuning and tool utilization.\n\u2022Knowledge recency . As another major challenge, LLMs\nwould encounter difficulties when solving tasks that require", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability"], "retrieved_docs_id": ["dfb6343eae", "92e73c053a", "fa2581a685", "114f3dada8", "72dc971633"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What optimization objective is used during model alignment after domain-adaptive pretraining in ChipNeMo?\n", "true_answer": "An autoregressive optimization objective is used during model alignment after domain-adaptive pretraining in ChipNeMo.", "source_doc": "ChipNemo.pdf", "source_id": "a5a7c4ceb0", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "ChipNeMo: Domain-Adapted LLMs for Chip Design\n2Domain -Adaptive\nPretraining\n24B tokens of chip \ndesign docs/code\nThousands GPU hrs\nModel\nAlignmen t\n56K/128K \n(SteerLM /SFT)  insts\n+ 1.4K task insts\n100+ GPU hrsFoundation Models\nLLaMA2 \n(7B, 13B, 70B) \nChipNeMo \nChat Models\n(7B, 13B, 70B)ChipNeMo \nFoundation Models\n(7B, 13B, 70B)Pretraining\nTrillions tokens of \ninternet data\n105 \u2013 106 GPU hrs\nFigure 1: ChipNeMo Training Flow\n2023)) fine-tuned on additional Verilog data can outperform\nstate-of-art OpenAI GPT-3.5 models. Customizing LLMs\nin this manner also avoids security risks associated with\nsending proprietary chip design data to third party LLMs\nvia APIs. However, it would be prohibitively expensive to\ntrain domain-specific models for every domain from scratch,\nsince this often requires millions of GPU training hours. To\ncost-effectively train domain-specific models, we instead\npropose to combine the following techniques: Domain-"], "retrieved_docs_id": ["a5a7c4ceb0", "a6c3d05123", "7eb44773ae", "df0b9868f2", "2079d05356"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a recent focus in reinforcement research during the inference stage for large language models?\n", "true_answer": "A recent focus is self-retrieval, where models retrieve and use their own information during the inference stage.", "source_doc": "RAG.pdf", "source_id": "326cdd7c26", "retrieved_docs": ["showing how increasing the guidance weight \u03b3increases\nthe importance of the prompt \u201cToday in France,\u201d.In recent years large language models have exhibited\nstrong generative capabilities to solve a diverse range of\ntasks [ 26,15,71]. \u201cPrompting\u201d is typically used to con-\ndition generation, with task instructions and context [ 64],\nor a small set of examples [ 15]. However, language gener-\nation, especially with smaller models, has been shown to\nstruggle with issues such as hallucination [ 49], degrada-\ntion [ 38] and meandering [ 76]. Various approaches have\nbeen proposed to address this, e.g.: instruction-finetuning\n[81,70] and reinforcement learning [ 56,4,6]. These tech-\nniques are expensive and their compute and data cost may\nnot be accessible to all users. In this paper we propose an\ninference time methodology which, as shown in Figure\n1, gives more importance to the user intent, expressed\nthrough the prompt. Our hypothesis in this paper is: fo-", "preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\na large text dataset. While the most straightforward approach to preference learning is supervised\nfine-tuning on human demonstrations of high quality responses, the most successful class of methods\nis reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [ 12,2]). RLHF methods fit\na reward model to a dataset of human preferences and then use RL to optimize a language model\npolicy to produce responses assigned high reward without drifting excessively far from the original\nmodel. While RLHF produces models with impressive conversational and coding abilities, the RLHF\npipeline is considerably more complex than supervised learning, involving training multiple LMs and\nsampling from the LM policy in the loop of training, incurring significant computational costs.\nIn this paper, we show how to directly optimize a language model to adhere to human preferences,", "language models,\u201d arXiv preprint arXiv:2305.16653 ,\n2023.\n[446] Y. Lu, P . Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y.\nWang, \u201cMultimodal procedural planning via dual\ntext-image prompting,\u201d CoRR , vol. abs/2305.01795,\n2023.\n[447] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang,\nand Z. Hu, \u201cReasoning with language model is plan-\nning with world model,\u201d CoRR , vol. abs/2305.14992,\n2023.\n[448] Z. Chen, K. Zhou, B. Zhang, Z. Gong, W. X. Zhao, and\nJ. Wen, \u201cChatcot: Tool-augmented chain-of-thought\nreasoning on chat-based large language models,\u201d\nCoRR , vol. abs/2305.14323, 2023.\n[449] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran,\nK. Narasimhan, and Y. Cao, \u201cReact: Synergizing rea-\nsoning and acting in language models,\u201d CoRR , vol.\nabs/2210.03629, 2022.\n[450] N. Shinn, F. Cassano, B. Labash, A. Gopinath,\nK. Narasimhan, and S. Yao, \u201cReflexion: Language\nagents with verbal reinforcement learning,\u201d 2023.\n[451] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao,", "(Applied Statistics) , 24(2):193\u2013202, 1975. doi: https://doi.org/10.2307/2346567.\n[31] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners, 2019. Ms., OpenAI.\n[32] R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi,\nand Y . Choi. Is reinforcement learning (not) for natural language processing: Benchmarks,\nbaselines, and building blocks for natural language policy optimization. In The Eleventh\nInternational Conference on Learning Representations , 2023. URL https://openreview.\nnet/forum?id=8aHzds2uUyB .\n[33] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\nnetworks. CoRR , abs/1511.06732, 2015.\n[34] D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of\nreward functions. In Robotics: Science and Systems (RSS) , 2017.", "Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods\nfor fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and\nhuman preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.\nIn contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification\nobjective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.\nwe will show that the RL-based objective used by existing methods can be optimized exactly with a\nsimple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\nAt a high level, existing methods instill the desired behaviors into a language model using curated\nsets of human preferences representing the types of behaviors that humans find safe and helpful. This"], "retrieved_docs_id": ["08972157a7", "6baa286ac1", "33012f926c", "8ca57e0db3", "8f01fbdc78"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How do Reflection tokens in the Self-RAG framework determine when to retrieve additional information?\n", "true_answer": "Reflection tokens in the Self-RAG framework can autonomously decide when to retrieve paragraphs or use a set threshold to trigger retrieval.", "source_doc": "RAG.pdf", "source_id": "2449b179e1", "retrieved_docs": ["probabilities. This approach is designed to handle situations\nwhere LLMs might need additional knowledge.\nSelf-RAG [Asai et al. , 2023b ]introduces an important in-\nnovation called Reflection tokens. These special tokens are\ngenerated to review the output and come in two types: Re-\ntrieve and Critic. The model can autonomously decide when\nto retrieve paragraphs or use a set threshold to trigger re-\ntrieval. When retrieval is needed, the generator processes\nmultiple paragraphs simultaneously, performing fragment-\nlevel beam search to obtain the best sequence. The scores for\neach subdivision are updated using Critic scores, and these\nweights can be adjusted during the inference process to cus-\ntomize the model\u2019s behavior. The Self-RAG framework also\nallows the LLM to autonomously determine whether recall\nis necessary, avoiding training additional classifiers or rely-\ning on NLI models. This enhances the model\u2019s ability to au-\ntonomously judge inputs and generate accurate answers.", "lows the decide-retrieve-reflect-read process, introduc-\ning a module for active judgment. This adaptive and\ndiverse approach allows for the dynamic organization of\nmodules within the Modular RAG framework.\n4 Retriever\nIn the context of RAG, the \u201dR\u201d stands for retrieval, serving\nthe role in the RAG pipeline of retrieving the top-k relevant\ndocuments from a vast knowledge base. However, crafting\na high-quality retriever is a non-trivial task. In this chapter,\nwe organize our discussions around three key questions: 1)\nHow to acquire accurate semantic representations? 2) How\nto match the semantic spaces of queries and documents? 3)\nHow to align the output of the retriever with the preferences\nof the Large Language Model ?\n4.1 How to acquire accurate semantic\nrepresentations?\nIn RAG, semantic space is the multidimensional space where\nquery and Document are mapped. When we perform re-\ntrieval, it is measured within the semantic space. If the se-", "there has been increased attention on self-retrieval, which in-\nvolves mining the knowledge of LLMs themselves to enhance\ntheir performance.\nThe subsequent chapters of this paper are structured as fol-\nlows: Chapter 2 provides an introduction to the background\nof RAG.Chapter 3 introduces the mainstream paradigms of\nRAG.Chapter 4 analyzes the retriever in RAG.Chapter 5 fo-", "open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "These efforts collectively strive to achieve a balance between\nefficiency and the richness of contextual information in RAG\nretrieval.\n\u2022Exploring Hybrid Search: By intelligently blending\nvarious techniques such as keyword-based search, se-\nmantic search, and vector search, the RAG system can\nleverage the strengths of each method. This approach\nenables the RAG system to adapt to different query types\nand information needs, ensuring consistent retrieval of\nthe most relevant and context-rich information. Hybrid\nsearch serves as a robust complement to retrieval strate-\ngies, enhancing the overall performance of the RAG\npipeline.\n\u2022Recursive Retrieval and Query Engine: Another pow-\nerful method to optimize retrieval in the RAG system\ninvolves implementing recursive retrieval and a sophis-\nticated query engine. Recursive retrieval entails acquir-\ning smaller document blocks during the initial retrieval\nphase to capture key semantic meanings. In the later"], "retrieved_docs_id": ["2449b179e1", "8fe8499442", "c3380c77fb", "ffd5c8b41e", "e34dfedd36"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How can the occurrence of illusions in a language model due to retrieval of irrelevant data be prevented?\n", "true_answer": "By introducing an additional validation module after document retrieval to assess the relevance between the retrieved documents.", "source_doc": "RAG.pdf", "source_id": "8d0a82337c", "retrieved_docs": ["merely means the model generated an output that\ncan neither be grounded nor contradicted by the\nsource content. This is still, to some degree, un-\ndesirable as the provided information cannot be\nverified. We illustrate intrinsic and extrinsic hallu-\ncinations in Fig. 8.\nHallucination [293, 458, 241]\nGenerated text that is fluent and natural but\nunfaithful to the source content (intrinsic)\nand/or under-determined (extrinsic).\nLiu et al. [328] attribute hallucinations com-\nmonly observed in LLMs to an architectural flaw in\nTransformer models while observing that recurrent\nneural networks perfectly solve their minimalistic\nsynthetic benchmarks, designed to isolate the is-sue of hallucination in the context of algorithmic\nreasoning. Here, we focus on ways to address hal-\nlucinations in LLMs without changing the model\narchitecture itself, including (i) supplying the LLM\nwith relevant sources ( retrieval augmentation ) or\n(ii) decoding strategies.", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "pp. 2206\u20132240.\n[658] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua,\n\u201cSearch-in-the-chain: Towards accurate, credible and\ntraceable large language models for knowledge-\nintensive tasks,\u201d CoRR , vol. abs/2304.14732, 2023.\n[659] B. Peng, M. Galley, P . He, H. Cheng, Y. Xie, Y. Hu,\nQ. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao,\n\u201cCheck your facts and try again: Improving large\nlanguage models with external knowledge and auto-\nmated feedback,\u201d CoRR , vol. abs/2302.12813, 2023.\n[660] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-\nYu, Y. Yang, J. Callan, and G. Neubig, \u201cActive retrieval\naugmented generation,\u201d CoRR , vol. abs/2305.06983,\n2023.\n[661] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang,\nQ. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, \u201cA sur-\nvey on hallucination in large language models: Prin-\nciples, taxonomy, challenges, and open questions,\u201d\nCoRR , vol. abs/2311.05232, 2023.\n[662] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and", "Pythia: A Suite for Analyzing Large Language Models\ncan clearly see a reduction in the bias as a result of swap-\nping the gendered pronouns in the last 7% or 21% of the\ntraining for all model sizes, but most prominently for the\nlarger ones, although these are also more biased to begin\nwith. We hypothesize that because larger models are bet-\nter at modeling correlations and distributions within their\ncorpora, their increased capacity causes features of bias\nto be more strongly or robustly learned. We also see that\nthe interventions only lead to a marginal decrease in the\nmodel perplexity on LAMBADA (Paperno et al., 2016) (Ap-\npendix C.1), which demonstrates the effectiveness of the\nbias mitigation without hurting language modeling perfor-\nmance downstream to a large degree. Whether the noisiness\nof the progression reflects actual changes in the language\nmodel\u2019s bias or poor reliability of CrowS-Pairs is an open\nquestion we leave for future work.", "of these models had to be guided in order to be coherent [ 7] and focused [ 38]. And when larger, higher-performing\nmodels like GPT [ 62,15] began to show real-world use-cases, the recognition emerged of the need to control their\noutput [74] to guard against toxic content [34] and bias [30].\nA central thrust in recent NLP research been to address the above concerns, and approaches have been targeted at nearly\nevery step of training and querying models, from dataset curation [ 2] and training [ 40], to response-alignment [ 57] and\nprompt-identification [34].\nOur work aligns with efforts to control the output of language models by controlling the model\u2019s outputted vocabulary\ndistribution p(xn|x<n). Early efforts in this vein aimed at increasing coherence include now-standard techniques like\ntemperature-scaling [17], nucleus sampling [38] and heuristics (e.g. repetition penalties [31])."], "retrieved_docs_id": ["aa24958f00", "114f3dada8", "058f03bb53", "ab81e1f2bf", "5cb47f1cc0"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How are LLMs used in the evaluation of RAG-based applications?\n", "true_answer": "LLMs, such as GPT-3.5 and GPT-4, are used as automatic evaluation tools for RAG-based applications, assessing their faithfulness, answer relevance, and context relevance. This method has been found to be effective and efficient in evaluating these applications.", "source_doc": "RAG.pdf", "source_id": "a05a21efce", "retrieved_docs": ["in retrieved information. Counterfactual robustness tests\ninclude questions that the LLM can answer directly, but\nthe related external documents contain factual errors.\n7.3 Evaluation Frameworks\nRecently, the LLM community has been exploring the use\nof \u201dLLMs as judge\u201d for automatic assessment, with many\nutilizing powerful LLMs (such as GPT-4) to evaluate their\nown LLM applications outputs. Practices by Databricks us-\ning GPT-3.5 and GPT-4 as LLM judges to assess their chatbot\napplications suggest that using LLMs as automatic evaluation\ntools is effective [Leng et al. , 2023 ]. They believe this method\ncan also efficiently and cost-effectively evaluate RAG-based\napplications.\nIn the field of RAG evaluation frameworks, RAGAS and\nARES are relatively new. The core focus of these evaluations\nis on three main metrics: Faithfulness of the answer, answer\nrelevance, and context relevance. Additionally, TruLens, an\nopen-source library proposed by the industry, also offers a", "open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "clude Accuracy and EM. Additionally, from the perspec-\ntive of evaluation methods, end-to-end evaluation can be di-\nvided into manual evaluation and automated evaluation us-\ning LLMs. The above summarizes the general case of end-\nto-end evaluation for RAG. Furthermore, specific evalua-\ntion metrics are adopted based on the application of RAG\nin particular domains, such as EM for question-answering\ntasks [Borgeaud et al. , 2022, Izacard et al. , 2022 ], UniEval\nand E-F1 for summarization tasks [Jiang et al. , 2023b ], and\nBLEU for machine translation [Zhong et al. , 2022 ]. These\nmetrics help in understanding the performance of RAG in var-\nious specific application scenarios.\n7.2 Key Metrics and Abilities\nExisting research often lacks rigorous evaluation of the im-\npact of retrieval-augmented generation on different LLMs.\nIn most cases, the evaluaion of RAG\u2019s application in vari-\nous downstream tasks and with different retrievers may yield", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "tonomously judge inputs and generate accurate answers.\n7 RAG Evaluation\nIn exploring the development and optimization of RAG, ef-\nfectively evaluating its performance has emerged as a central\nissue. This chapter primarily discusses the methods of eval-\nuation, key metrics for RAG, the abilities it should possess,\nand some mainstream evaluation frameworks.\n7.1 Evaluation Methods\nThere are primarily two approaches to evaluating the ef-\nfectiveness of RAG: independent evaluation and end-to-endevaluation [Liu, 2023 ].\nIndependent Evaluation\nIndependent evaluation includes assessing the retrieval mod-\nule and the generation (read/synthesis) module.\n1.Retrieval Module\nA suite of metrics that measure the effectiveness of sys-\ntems (like search engines, recommendation systems, or\ninformation retrieval systems) in ranking items accord-\ning to queries or tasks are commonly used to evaluate\nthe performance of the RAG retrieval module. Exam-\nples include Hit Rate, MRR, NDCG, Precision, etc."], "retrieved_docs_id": ["a05a21efce", "ffd5c8b41e", "b65d7790f9", "4fffd3dc2b", "a580bf7e9b"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the COG model select text fragments during the generation process?\n", "true_answer": "The COG model selects text fragments by utilizing efficient vector search tools to calculate meaningful context representations of text fragments and index them. At each time step, relevant text fragments are sought from the text collection instead of selecting from an independent set of options.", "source_doc": "RAG.pdf", "source_id": "bff4917f9e", "retrieved_docs": ["corporating a retrieval mechanism using the T5 architecture\n[Raffel et al. , 2020 ]in both the pre-training and fine-tuning\nstages. Prior to pre-training, it initializes the encoder-decoder\nLM backbone with a pre-trained T5, and initializes the dense\nretriever with a pre-trained Contriever. During the pre-\ntraining process, it refreshes the asynchronous index every\n1000 steps.\nCOG [Vaze et al. , 2021 ]is a text generation model that for-\nmalizes its generation process by gradually copying text frag-\nments (such as words or phrases) from an existing collection\nof text. Unlike traditional text generation models that select\nwords sequentially, COG utilizes efficient vector search tools\nto calculate meaningful context representations of text frag-\nments and index them. Consequently, the text generation task\nis decomposed into a series of copy and paste operations,\nwhere at each time step, relevant text fragments are sought\nfrom the text collection instead of selecting from an indepen-", "emphasized by CFG. In a blind human evaluation we show 75% preference for GPT4All using CFG over the\nvanilla sampling;\n4.We provide interpretations for the impact that CFG on text generation both (1) qualitatively, by visualizing\nhow CFG is upweighting words more related to the prompt (our visualization, we note, can be an integral\npart of effective prompt engineering) and (2) quantitatively, by showing that CFG decreases entropy in the\nsampling distribution.\n2 Methodology\nAutoregressive language models are trained to generate plausible continuations of sequences of text. Given a sequence of\ntokens w1,\u00b7\u00b7\u00b7, wT, the model samples each subsequent token from the conditional probability distribution P\u03b8(w|wt\u2264T).\nIt is now typical for some or all of the initial tokens to be considered a prompt , which specifies information about the\ntask or how it is to be solved. In practice, prompts are syntactically and semantically distinct from the initial text to be\ncontinued.", "from the text collection instead of selecting from an indepen-\ndent vocabulary. COG demonstrates superior performance\nto RETRO in various aspects, including question-answering,\ndomain adaptation, and expanded phrase indexing.\nOn the other hand, following the discovery of the scal-\ning law, there has been a rapid increase in model parameters,\nmaking autoregressive models the mainstream. Researchers\nare also exploring whether larger models can be pretrained\nusing the RAG approach. RETRO++ [Wang et al. , 2023a ], an", "G. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado,\nM. Hughes, and J. Dean, \u201cGoogle\u2019s neural machine\ntranslation system: Bridging the gap between human\nand machine translation,\u201d CoRR , vol. abs/1609.08144,\n2016.\n[312] R. Paulus, C. Xiong, and R. Socher, \u201cA deep rein-\nforced model for abstractive summarization,\u201d in ICLR\n(Poster) . OpenReview.net, 2018.\n[313] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju,\nQ. Sun, S. Lee, D. J. Crandall, and D. Batra, \u201cDiverse\nbeam search: Decoding diverse solutions from neural\nsequence models,\u201d CoRR , vol. abs/1610.02424, 2016.\n[314] A. Fan, M. Lewis, and Y. N. Dauphin, \u201cHierarchical\nneural story generation,\u201d in ACL (1) . Association for\nComputational Linguistics, 2018, pp. 889\u2013898.\n[315] J. Hewitt, C. D. Manning, and P . Liang, \u201cTrunca-\ntion sampling as language model desmoothing,\u201d in\nEMNLP (Findings) . Association for Computational\nLinguistics, 2022, pp. 3414\u20133427.", "of LLMs primarily drive this approach, but also\nthe desire to have a human-in-the-loop for some\nco-writing use cases [368].\nLimited Context Window [368, 637]\nThe inability of current LLMs to keep the\nentire generated work within the context\nwindow currently constrains their long-form\napplications and generates the need for mod-\nular prompting (14).\nFor short form generation, Chakrabarty et al.\n[69] propose CoPoet (fine-tuned T5 and T0 models)\nfor collaborative poetry generation, Razumovskaia\net al. [452] use PaLM and prompting with plansfor cross-lingual short story generation, Wang et al.\n[584] use GPT-4 as part of the ReelFramer tool to\nhelp co-create news reels for social media, Ippolito\net al. [232] use LaMDA as part of the Wordcraft cre-\native writing assistant, and Calderwood et al. [63]\napply a fine-tuned GPT-3 model as part of their\nSpindle tool for helping generate choice-based in-\nteractive fiction.\nFor more general creative tasks, Haase and"], "retrieved_docs_id": ["bff4917f9e", "ce496e1ee1", "c2dce2386a", "3b143b7d77", "35c5e878ad"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "Which vision encoder, compared to pure vision models, tends to lose some visual details?\n", "true_answer": "CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT.", "source_doc": "hallucination.pdf", "source_id": "3f64cf9b55", "retrieved_docs": ["to lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent\nstudies have proposed complementing this information loss by incorporating visual features from\nother vision encoders. The work of [ 98] proposes mixing features from CLIP ViT and DINO ViT.\nSpecifically, it experimented with additive and interleaved features. Both settings show that there\nis a trade-off between the two types of features. A more dedicated mechanism is needed.\nConcurrently, a visual expert-based model proposed in [ 38] aims to mitigate the information\nloss caused by the CLIP image encoder. Instead of merely mixing features, this paper enhances\nthe visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two\npivotal modules: multi-task encoders and the structural knowledge enhancement module. The multi-\ntask encoders are dedicated to integrating various types of latent visual information extracted by", "VL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to\n448\u00d7448. InternVL [ 2] scales up the vision encoder to 6 billion parameters, enabling processing of\nhigh-resolution images. Regarding hallucination, HallE-Switch [ 123] has investigated the impact\nof vision encoder resolution on its proposed CCEval benchmark. Among the three studied vision\nencoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results\nin lower degrees of hallucination. These works indicate that scaling up vision resolution is a\nstraightforward yet effective solution.\n5.2.2 Versatile Vision Encoders. Several studies [ 38,49,98] have investigated vision encoders for\nMLLMs. Typically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs\nthanks to its remarkable ability to extract semantic-rich features. However, CLIP has been shown\nto lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "Figure 4: BRA VE [12] concatenates features from K different Vision Encoders in a sequence-wise\nmanner. These concatenated features are then reduced by the MEQ-Former.\navoids the high cost of training an end-to-end multimodal model from scratch and effectively lever-\nages the capabilities of pre-trained language and vision models.\nMLP-based As outlined in [7, 54], the vision-language projector is typically realized using a\nstraightforward, learnable Linear Projector or a Multi-Layer Perceptron (MLP), i.e., several linear\nprojectors interleaved with non-linear activation functions, as illustrated in Table.1.\nAttention-based BLIP2 [15] introduces Q-Former, a lightweight transformer, which employs a\nset of learnable query vectors to extract visual features from a frozen vision model. Perceiver\nResampler, proposed by Flamingo[16], contemplates the use of learnable latent queries as Q in\ncross-attention, while image features are unfolded and concatenated with Q to serve as K and V in", "Multi-view Input Directly employing high-resolution vision encoders for fine-grained percep-\ntion is prohibitively costly and does not align with practical usage requirements. Therefore, to\nutilize low-resolution vision encoders while enabling MLLM to perceive detailed information, a\ncommon approach is to input multi-view HR images, i.e., a global view: low-resolution images\nobtained through resizing, and a local view: image patches derived from splitting. For example,\n7"], "retrieved_docs_id": ["c20c82af54", "3f64cf9b55", "4ee780b19c", "1fea51e26c", "f8392fc0db"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "How many new tokens were added to the LLaMA2 tokenizer for chip design datasets?\n", "true_answer": "Approximately 9K new tokens were added to the LLaMA2 tokenizer for chip design datasets.", "source_doc": "ChipNemo.pdf", "source_id": "ac7c0c980b", "retrieved_docs": ["ChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 4: Domain-Adapted ChipNeMo Tokenizer Improvements.\n3.1. Domain-Adaptive Tokenization\nWe adapt the LLaMA2 tokenizer (containing 32K tokens) to\nchip design datasets using the previously outlined four-step\nprocess. Approximately 9K new tokens are added to the\nLLaMA2 tokenizer. The adapted tokenizers can improve\ntokenization efficiency by 1.6% to 3.3% across various chip\ndesign datasets as shown in Figure 4. We observe no obvious\nchanges to tokenizer efficiency on public data. Importantly,\nwe have not observed significant decline in the LLM\u2019s accu-\nracy on public benchmarks when using the domain-adapted\ntokenizers even prior to DAPT.\n3.2. Domain Adaptive Pretraining\nFigure 5: Chip Domain Benchmark Result for ChipNeMo.\nFigure 5 presents the outcomes for ChipNeMo models on\nthe AutoEval benchmark for chip design domain (detailed\nin Appendix A.5). Results on open domain academic bench-\nmark results are presented in Appendix A.6. Our research", "Training Data Params Context\nLengthGQA Tokens LR\nLlama 1See Touvron et al.\n(2023)7B 2k \u2717 1.0T 3.0\u00d710\u22124\n13B 2k \u2717 1.0T 3.0\u00d710\u22124\n33B 2k \u2717 1.4T 1.5\u00d710\u22124\n65B 2k \u2717 1.4T 1.5\u00d710\u22124\nLlama 2A new mix of publicly\navailable online data7B 4k \u2717 2.0T 3.0\u00d710\u22124\n13B 4k \u2717 2.0T 3.0\u00d710\u22124\n34B 4k \u2713 2.0T 1.5\u00d710\u22124\n70B 4k \u2713 2.0T 1.5\u00d710\u22124\nTable 1: Llama 2 family of models. Token counts refer to pretraining data only. All models are trained with\na global batch-size of 4M tokens. Bigger models \u2014 34B and 70B \u2014 use Grouped-Query Attention (GQA) for\nimproved inference scalability.\n0 250 500 750 1000 1250 1500 1750 2000\nProcessed Tokens (Billions)1.41.51.61.71.81.92.02.12.2Train PPLLlama-2\n7B\n13B\n34B\n70B\nFigure 5: Training Loss for Llama 2 models. We compare the training loss of the Llama 2 family of models.\nWe observe that after pretraining on 2T Tokens, the models still did not show any sign of saturation.\nTokenizer. Weusethesametokenizeras Llama 1;itemploysabytepairencoding(BPE)algorithm(Sennrich", "Training Data Params Context\nLengthGQA Tokens LR\nLlama 1See Touvron et al.\n(2023)7B 2k \u2717 1.0T 3.0\u00d710\u22124\n13B 2k \u2717 1.0T 3.0\u00d710\u22124\n33B 2k \u2717 1.4T 1.5\u00d710\u22124\n65B 2k \u2717 1.4T 1.5\u00d710\u22124\nLlama 2A new mix of publicly\navailable online data7B 4k \u2717 2.0T 3.0\u00d710\u22124\n13B 4k \u2717 2.0T 3.0\u00d710\u22124\n34B 4k \u2713 2.0T 1.5\u00d710\u22124\n70B 4k \u2713 2.0T 1.5\u00d710\u22124\nTable 1: Llama 2 family of models. Token counts refer to pretraining data only. All models are trained with\na global batch-size of 4M tokens. Bigger models \u2014 34B and 70B \u2014 use Grouped-Query Attention (GQA) for\nimproved inference scalability.\n0 250 500 750 1000 1250 1500 1750 2000\nProcessed Tokens (Billions)1.41.51.61.71.81.92.02.12.2Train PPLLlama-2\n7B\n13B\n34B\n70B\nFigure 5: Training Loss for Llama 2 models. We compare the training loss of the Llama 2 family of models.\nWe observe that after pretraining on 2T Tokens, the models still did not show any sign of saturation.\nTokenizer. Weusethesametokenizeras Llama 1;itemploysabytepairencoding(BPE)algorithm(Sennrich", "et al., 2023), which extends the LLaMA-13B (Tou-\nvron et al., 2023a) context window from 2048 to\n16384 tokens by using condensed rotary positional\nembeddings before fine-tuning with 16384-token\nsequences.\nClosed models. We use the OpenAI API to ex-\nperiment with GPT-3.5-Turbo and GPT-3.5-Turbo", "\u2022PaLM (540B) [56] uses a pre-training dataset of 780B\ntokens, which is sourced from social media conversations,\nfiltered webpages, books, Github, multilingual Wikipedia,\nand news.\n\u2022LLaMA [57] extracts training data from various sources,\nincluding CommonCrawl, C4 [82], Github, Wikipedia,\nbooks, ArXiv, and StackExchange. The training data size for\nLLaMA (6B) and LLaMA (13B) is 1.0T tokens, while 1.4T\ntokens are used for LLaMA (32B) and LLaMA (65B).TABLE 3: A detailed list of available collections for instruc-\ntion tuning.\nCategories Collections Time #Examples\nTaskNat. Inst. [166] Apr-2021 193K\nFLAN [67] Sep-2021 4.4M\nP3 [167] Oct-2021 12.1M\nSuper Nat. Inst. [88] Apr-2022 5M\nMVPCorpus [168] Jun-2022 41M\nxP3 [94] Nov-2022 81M\nOIG[169] Mar-2023 43M\nChatHH-RLHF [170] Apr-2022 160K\nHC3 [171] Jan-2023 87K\nShareGPT [148] Mar-2023 90K\nDolly [172] Apr-2023 15K\nOpenAssistant [173] Apr-2023 161K\nSyntheticSelf-Instruct [143] Dec-2022 82K\nAlpaca [137] Mar-2023 52K\nGuanaco [174] Mar-2023 535K"], "retrieved_docs_id": ["ac7c0c980b", "2be6184d40", "2be6184d40", "b64fb4982f", "ef378d9ebc"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What are some evaluation metrics for the final response generated by the RAG model?\n", "true_answer": "Accuracy, EM, answer fidelity, and answer relevance are some evaluation metrics for the final response generated by the RAG model.", "source_doc": "RAG.pdf", "source_id": "b023f9e1c7", "retrieved_docs": ["ples include Hit Rate, MRR, NDCG, Precision, etc.\n2.Generation Module\nThe generation module here refers to the enhanced or\nsynthesized input formed by supplementing the retrieved\ndocuments into the query, distinct from the final an-\nswer/response generation, which is typically evaluated\nend-to-end. The evaluation metrics for the generation\nmodule mainly focus on context relevance, measuring\nthe relatedness of retrieved documents to the query ques-\ntion.\nEnd-to-End Evaluation\nEnd-to-end evaluation assesses the final response gener-\nated by the RAG model for a given input, involving the\nrelevance and alignment of the model-generated answers\nwith the input query. From the perspective of content\ngeneration goals, evaluation can be divided into unlabeled\nand labeled content. Unlabeled content evaluation met-\nrics include answer fidelity, answer relevance, harmless-\nness, etc., while labeled content evaluation metrics in-\nclude Accuracy and EM. Additionally, from the perspec-", "tonomously judge inputs and generate accurate answers.\n7 RAG Evaluation\nIn exploring the development and optimization of RAG, ef-\nfectively evaluating its performance has emerged as a central\nissue. This chapter primarily discusses the methods of eval-\nuation, key metrics for RAG, the abilities it should possess,\nand some mainstream evaluation frameworks.\n7.1 Evaluation Methods\nThere are primarily two approaches to evaluating the ef-\nfectiveness of RAG: independent evaluation and end-to-endevaluation [Liu, 2023 ].\nIndependent Evaluation\nIndependent evaluation includes assessing the retrieval mod-\nule and the generation (read/synthesis) module.\n1.Retrieval Module\nA suite of metrics that measure the effectiveness of sys-\ntems (like search engines, recommendation systems, or\ninformation retrieval systems) in ranking items accord-\ning to queries or tasks are commonly used to evaluate\nthe performance of the RAG retrieval module. Exam-\nples include Hit Rate, MRR, NDCG, Precision, etc.", "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "ous downstream tasks and with different retrievers may yield\ndivergent results. However, some academic and engineering\npractices have focused on general evaluation metrics for RAG\nand the abilities required for its effective use. This section\nprimarily introduces key metrics for evaluating RAG\u2019s effec-\ntiveness and essential abilities for assessing its performance.\nKey Metrics\nRecent OpenAI report [Jarvis and Allard, 2023 ]have\nmentioned various techniques for optimizing large\nlanguage models (LLMs), including RAG and its", "clude Accuracy and EM. Additionally, from the perspec-\ntive of evaluation methods, end-to-end evaluation can be di-\nvided into manual evaluation and automated evaluation us-\ning LLMs. The above summarizes the general case of end-\nto-end evaluation for RAG. Furthermore, specific evalua-\ntion metrics are adopted based on the application of RAG\nin particular domains, such as EM for question-answering\ntasks [Borgeaud et al. , 2022, Izacard et al. , 2022 ], UniEval\nand E-F1 for summarization tasks [Jiang et al. , 2023b ], and\nBLEU for machine translation [Zhong et al. , 2022 ]. These\nmetrics help in understanding the performance of RAG in var-\nious specific application scenarios.\n7.2 Key Metrics and Abilities\nExisting research often lacks rigorous evaluation of the im-\npact of retrieval-augmented generation on different LLMs.\nIn most cases, the evaluaion of RAG\u2019s application in vari-\nous downstream tasks and with different retrievers may yield"], "retrieved_docs_id": ["b023f9e1c7", "a580bf7e9b", "af6e8c3fb2", "8e161396f8", "b65d7790f9"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "Which models outperformed OpenAI's code-davinci-002 on Verilog questions?\n", "true_answer": "Thakur et al., 2023 showed that fine-tuned open-source LLMs (CodeGen) on Verilog datasets outperformed state-of-the-art OpenAI models such as code-davinci-002 on 17 Verilog questions.", "source_doc": "ChipNemo.pdf", "source_id": "cdf1ac39e3", "retrieved_docs": ["into the behavior for individual examples, we see significant variation in behavior that is likely indicative\nof the spectrum of difficulty of questions. On code scenarios, we see consistent trends with code-davinci-\n002 consistently outperforming code-cushman-001 (12B) for both HumanEval andAPPS, sometimes by\nlarge margins (e.g. 10.% strict correctness vs. 2.6% on APPS). We note that we do not evaluate any of\ntext models on these code scenarios, though in some cases this may be sensible/desirable given the striking\ngenerality of model development, deployment, and validation/scrutiny. Conversely, while we evaluate the\ncode models for LSATandLegalSupport , we find achieve accuracies of 0%. Overall, we find text-davinci-\n002 and, especially, code-davinci-002 display very strong reasoning capabilities for many different forms of\nreasoning.\nMemorization & Copyright. To further explore the results for this targeted evaluation, see https:", "16.Reasoning. Forreasoning-intensivescenarios, wefindthatthecodemodels, especiallycode-davinci-\n002, consistently outperform the text models, even on synthetic reasoning scenarios posed in natural\nlanguage.20This gap is made clear in mathematical reasoning: for GSM8K , code-davinci-002\nachieves an accuracy of 52.1%, where the next best model is text-davinci-002 at 35.0% and no other\nmodel surpasses 16%.21Further, in addition to code-davinci-002, text-davinci-002 is much more\naccurate than other text models (e.g. 65.1% accuracy on synthetic reasoning in natural language,\nwhereas the next most accurate text model is OPT (175B) at 29.4% accuracy, and code-davinci-002\nhas an accuracy of 72.7%).\n17.Memorization of copyrighted/licensed material. We find that the likelihood of direct regur-\ngitation of long copyrighted sequences is somewhat uncommon, but it does become noticeable when\nlooking at popular books.22However, we do find the regurgitation risk clearly correlates with model", "the very strong performance of both models, and that they are the only instruction-tuned models\nwe evaluate (beyond the much smaller OpenAI model variants), this suggests instruction-tuning\nprovides a broad set of advantages.\n2.Relating model accuracy with model access. In light of the high accuracies of Anthropic-LM\nv4-s3 (52B) (closed), TNLG v2 (530B) (closed), and text-davinci-002 (limited-access), we observe\na consistent gap on all core scenarios (Figure 28) between the current open models and non-open\nmodels. We emphasize that this gap reflects the current snapshot of models we evaluate (Table 5),\nand that the gap could grow or shrink over time as new models are released. On one hand, we see\nthe recent release of open models (OPT (175B), BLOOM (176B), GLM (130B)) as greatly reducing\nthe gap over the past year, but we also have not evaluated some non-open models (e.g. PaLM,\nGopher) that we expect to be quite accurate. In either case, monitoring this gap over time is crucial", "advantage in accuracy for these scenarios, with the gap between them shrinking in the presence of natural\nlanguage while still maintaining that code-davinci-002 is more accurate than text-davinci-002 for reasoning.\nWe observe that similar trends hold for MATH,GSM8K ,bAbI, andMATH (chain-of-thoughts). Looking\nat individual subsets for bAbI, we find tasks 3, 4, 15 and 19, which assess transitive reasoning, relational\nunderstanding, deduction and planning skills, respectively, to be the the most challenging.71In contrast\nto the trends for text-davinci-002, for Dyck, we observe that text-davinci-002 is not quite accurate (59.4%\naccuracy), whereas TNLG v2 (530B) (78.4%) joins code-davinci-002 (80.2%) as the only models above 75%.\nForLSAT(Zhong et al., 2021), which consists of reasoning questions posed for law school admissions, we\nobserve that most evaluated models perform poorly, with accuracies around chance level (20%). Looking", "evidence we are aware of in Appendix G.\n1.2 Empirical findings\nTo give a sense of the magnitude of our evaluation, we ran a total of 4,939 runs (i.e. evaluating a specific\nmodel on a specific scenario). This amounts to a total cost of 12,169,227,491 tokens and 17,431,479 queries\nacross all models, $38,001 for the commercial APIs, and about 19,500 GPU hours worth of compute for the\nopen models.\nHere is a summary of the high-level findings:\n1.The benefits of instruction-tuning. Across the core scenarios, we find that text-davinci-002\nperforms best on our accuracy, robustness, and fairness metrics, with Anthropic-LM v4-s3 (52B)\nbeing in the top 3 for all 3 metrics (despite being more than 10 \u00d7smaller in model scale compared\nto TNLG v2 (530B), which is the second most accurate and fair) as shown in Figure 26. Given\nthe very strong performance of both models, and that they are the only instruction-tuned models"], "retrieved_docs_id": ["46b4467def", "ba9814c8b1", "8d7ad1510a", "12d5ea4e2a", "2f46aa5de7"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does providing optimal visual contexts reduce hallucinations in text generation?\n", "true_answer": "Providing optimal visual contexts eliminates over 84.5% of hallucinations in text generation, as shown in an oracle study.", "source_doc": "hallucination.pdf", "source_id": "31eefbd9eb", "retrieved_docs": ["reduce hallucination. Visual context refers to the visual tokens that can be grounded from the\ngenerated text response. An oracle study showed that decoding from the provided optimal visual\ncontexts eliminates over 84.5% of hallucinations. Based on the insight and observation, the authors\ndesigned mechanisms to locate the fine-grained visual information to correct each generated\ntoken that might be hallucinating. This is essentially a visual content-guided decoding strategy.\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that\nutilizes a visual matching score to steer the generation of the final outputs, balancing both object\nhallucination mitigation and text generation quality.\nOthers. The work of OPEAR [ 45] makes an interesting observation that most hallucinations\nare closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,", "Another interesting study observes that the hallucination of MLLMs seems to be easily triggered\nby paragraph break \u2018\\n\\n\u2019 [ 36]. Based on this observation, this work proposes two simple methods\nto reduce hallucination by avoiding generating \u2018\\n\u2019 during generation. First, intuitively, users can\ndesign the prompt to instruct the model to output responses within one paragraph, avoiding \u2018\\n\u2019.\nBesides, the authors tried to alter the output logits during generation by manually lowering the\nprobability of generating \u2018\\n\u2019. Experimental results show that this simple strategy can alleviate\nhallucination on popular benchmarks.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "overall framework, may be promising research directions.\n6.3 Advancements in Model Architecture\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective\narchitectures specifically tailored to hallucination remains a challenge. Developing advanced model\narchitectures capable of capturing complex linguistic structures and generating coherent and con-\ntextually relevant output based on input visual content is essential for improving the performance of\nMLLMs. Future research can explore innovative architectural designs based on identified causes of\nhallucination. This includes developing stronger visual perception models, innovative cross-modal\ninteraction modules capable of transferring cross-modal information seamlessly, and novel large\nlanguage model architectures faithful to input visual content and text instructions, etc.\n6.4 Establishing Standardized Benchmarks", "Hallucination of Multimodal Large Language Models: A Survey 23\nrecollection of its training documents, most of the time the result goes someplace useful. It\u2019s only\nwhen the dreams enter deemed factually incorrect territory that we label them as \u2019hallucinations\u2019.\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications\npresents exciting opportunities for enhancing user experiences and enabling new use cases. As\nhumans are the end-users of these models, the primary goal is to enrich human user experiences.\nFuture research may switch the optimization objective from specific cross-modal benchmarks to\nhuman experience. For example, Some content may cause hallucinations but will not affect the\nuser experience, while some content may. Alternatively, integrating hallucination to inspire more\ncreative ideas in real-world applications could also be intriguing.\n6.6 Enhancing Interpretability and Trust"], "retrieved_docs_id": ["31eefbd9eb", "3fc78f0ef0", "92e73c053a", "c8e35c3848", "dfb6343eae"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the learning rate used for training the ChipNeMo models?\n", "true_answer": "The learning rate used for training the ChipNeMo models is 5\u00b710\u22126.", "source_doc": "ChipNemo.pdf", "source_id": "7eb44773ae", "retrieved_docs": ["models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "large adapter exhibiting a slight improvement.\n3.4. Training Cost\nAll models have undergone training using 128 A100 GPUs.\nWe estimate the costs associated with domain adaptive pre-\ntraining for ChipNeMo as illustrated in Table 1. It is worth\nnoting that DAPT accounts for less than 1.5% of the overall\n5", "ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting", "global batch size is set at 256, and a context window of 4096\ntokens is applied, resulting in an effective batch size of 1M\ntokens. The total number of training steps is set to 23,200,\nequating to roughly 1 epoch of the data blend.\nFigure 2: Smoothed Training Loss for ChipNeMo with Tokenizer\nAugmentation.\nFigure 2 illustrates the training loss of ChipNeMo under\nthe specified hyperparameters. We do observe spikes in the\ntraining loss. In contrast to the hypothesis in (Chowdhery\net al., 2022), we postulate that in our scenario, these spikes\ncan be attributed to \u201cbad data\u201d since these irregularities\nseem to consistently occur in similar training steps for the\nsame model, even across different model sizes. We chose\nnot to address this issue, as these anomalies did not appear\nto significantly impede subsequent training steps (with no\nnoticeable degradation in validation loss), possibly due to\n3", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus"], "retrieved_docs_id": ["7eb44773ae", "d9ae12f819", "a5a7c4ceb0", "1162f7259e", "df0b9868f2"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a potential consequence of insufficient data quality in building multimodal large language models (MLLMs)?\n", "true_answer": "Insufficient data quality could potentially lead to problematic cross-modal alignment, resulting in hallucinations.", "source_doc": "hallucination.pdf", "source_id": "77ce09f375", "retrieved_docs": ["Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "is a document and the output is its summary. So we can feed the input document into the language\nmodel and then produce the generated summary.\nDespite the successful applications in natural language processing, it is still struggling to natively use\nLLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal\nperception is a necessity to achieve arti\ufb01cial general intelligence, in terms of knowledge acquisition\nand grounding to the real world. More importantly, unlocking multimodal input [ TMC+21,HSD+22,\nWBD+22,ADL+22,AHR+22,LLSH23 ] greatly widens the applications of language models to\nmore high-value areas, such as multimodal machine learning, document intelligence, and robotics.\nIn this work, we introduce KOSMOS -1, a Multimodal Large Language Model (MLLM) that can\nperceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e.,", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "cient MLLMs, research state of efficient structures and strategies, and the appli-\ncations. Finally, we discuss the limitations of current efficient MLLM research\nand promising future directions. Please refer to our GitHub repository for more\ndetails: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.\n1 Introduction\nLarge-scale pretraining, a leading approach in Artificial Intelligence(AI), has seen general-purpose\nmodels like large language and multimodal models outperform specialized deep learning models\nacross many tasks. The remarkable abilities of Large Language Models (LLM) have inspired efforts\nto merge them with other modality-based models to enhance multimodal competencies. This con-\ncept is further supported by the remarkable success of proprietary models like OpenAI\u2019s GPT-4V [1]\nand Google\u2019s Gemini[2]. As a result, Multimodal Large Language Models (MLLMs) have emerged,\nincluding the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],", "Challenges and Applications of Large Language Models\nJean Kaddour\u03b1,\u2020,\u2217, Joshua Harris\u03b2,\u2217, Maximilian Mozes\u03b1,\nHerbie Bradley\u03b3,\u03b4,\u03f5, Roberta Raileanu\u03b6, and Robert McHardy\u03b7,\u2217\n\u03b1University College London\u03b2UK Health Security Agency\u03b3EleutherAI\n\u03b4University of Cambridge\u03f5Stability AI\u03b6Meta AI Research\u03b7InstaDeep\nAbstract\nLarge Language Models (LLMs) went from\nnon-existent to ubiquitous in the machine learn-\ning discourse within a few years. Due to the\nfast pace of the field, it is difficult to identify\nthe remaining challenges and already fruitful\napplication areas. In this paper, we aim to es-\ntablish a systematic set of open problems and\napplication successes so that ML researchers\ncan comprehend the field\u2019s current state more\nquickly and become productive.\nContents\n1 Introduction 1\n2 Challenges 2\n2.1 Unfathomable Datasets . . . . . . 2\n2.2 Tokenizer-Reliance . . . . . . . . 4\n2.3 High Pre-Training Costs . . . . . 6\n2.4 Fine-Tuning Overhead . . . . . . 10\n2.5 High Inference Latency . . . . . . 11"], "retrieved_docs_id": ["ac70fcc9f2", "74bb21ad4f", "542e5c49da", "e021f7788d", "bf695e58cf"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does Quantformer reduce quantization error during training?\n", "true_answer": "Quantformer reduces quantization error during training by leveraging entropy information to maintain consistency in self-attention ranks and introducing a differentiable search mechanism to optimally group patch feature dimensions, reducing rounding and clipping inaccuracies.", "source_doc": "multimodal.pdf", "source_id": "900b3dde3f", "retrieved_docs": ["to quantized values, the quantization error is significantly reduced under certain conditions. This\ntechnique successfully modifies heavy-tailed activation distributions to fit a given quantizer.\nQuantization-Aware Training (QAT) integrates quantization into the training cycle. This in-\ntegration is particularly advantageous when scaling down to ultra-low bit precision, such as 4 bits\nor lower, where PTQ struggles with significant performance loss. For example, Quantformer [124]\nleverages entropy information to maintain consistency in self-attention ranks and introduces a dif-\nferentiable search mechanism to optimally group patch feature dimensions, reducing rounding and\nclipping inaccuracies. Q-ViT [126] incorporates a distillation token and Information Rectification\nModule (IRM) to counteract altered distributions in quantized attention modules. TerViT [127] and\nBit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain", "ing. Works like Kim et al. (2019); Mishra & Marr (2017)\nexploit student-teacher training to improve quantized model\nperformance during training. Although quantization-aware\ntraining is potent and often gives good results, the process\nis often tedious and time-consuming. Our work seeks to get\nhigh accuracy models without this hassle.\nSeveral easy-to-use methods for quantization of networks\nwithout quantization-aware training have been proposed as\nof recent. These methods are often referred to as post-\ntraining quantization methods. Krishnamoorthi (2018)\nshow several results of network quantization without \ufb01ne-", "this dependence, we provide a tight lower bound on the output errors for quantization. For pruning\nwe provide a way to solve the problem exactly for moderate dimensionalities. This way, we can\nprovide a comparison that holds regardless of the algorithm used for each method.\n4.1 Post-training quantization\nWe set out to formulate a way by which we can get relatively tight bounds for comparison when\nquantizing a single layer with the MSE as the objective. The higher bound is simple to obtain by using\na solution with a heuristic quantization algorithm, but for the lower bound, we have to reformulate the\nproblem. The mean-squared error of the output activations of a quantized layer can be expressed as:\nmin\nwE(w) =\u2225X\u03b4w\u2212Xw orig\u22252\n2(4)\ns.t.w\u2208Zn,\nwmin\u2264wi\u2264wmax,\nwhere Xis the input data in an unfolded form, and worigare the floating point weights. The quantized\nweights are computed as the product of the quantization scale \u03b4, and the integer weights w.wmin", "in these experiments to isolate the impact of weight decay. A larger weight decay value ( 0.1vs\n0.001) results in better post-training performance (0.09% vs 1.36% degradation). Furthermore, as\nshown in Figure 3, combining lower weight decay with fp16can further amplify sensitivity to post-\ntraining quantization. A small weight decay value (0.01) can cause higher performance degradation\nin post-quantization after training (1.73%).\nDropout and Gradient Clipping In Figure 2b we observe that higher levels of dropout corre-\nspond to sharper degradation in post-training quantization. Note that Pdropout = 0.8unsurprisingly\nleads to a poor absolute downstream performance, however, it helps establish a clear trend given\nother data points. Figure 2c shows the relative quantization degradation for models with and with-\nout gradient clipping. When varying gradient clipping, a control weight decay value of 0.001 is used", "performance drop (Frantar et al., 2022; Xiao et al., 2022). Zeng et al. (2022) hypothesize that the\nobserveddifferenceinweightdistributioncharacteristicsmaybeduetothedifferenceinoptimization\nchoices made during pre-training.\nIn this work, we seek to reconcile these observations. We posit that it is possible to optimize for a\nquantization friendly training recipe that suppresses large activation magnitude outliers. This leads\nto a distribution of activations and weights that are more amenable to simple INT8 quantization\nrecipes and does not necessitate the need for complex and inefficient mixed-precision computations.\nOur results show that we can introduce simple INT8 post-training quantization with negligible\nimpact on performance due to choices we make during the pre-training stage. As shown in Figure 1,\nacross 8 zero-shot downstream tasks, our models do not present any significant performance drop,\nhaving only 0.24% average degradation in a 52 billion parameter model."], "retrieved_docs_id": ["900b3dde3f", "e7556ddd3b", "a14d42001c", "099a3ab61e", "a9605c7507"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the score of LLaMA-2-Chat-7B in the InstructBLIP benchmark?\n", "true_answer": "The score of LLaMA-2-Chat-7B in the InstructBLIP benchmark is 58.8.", "source_doc": "multimodal.pdf", "source_id": "88e0397250", "retrieved_docs": ["4 Instruction Finetuning\nModelChatbot Arena\nELO RatingMT Bench\nWizardLM 13B v1.2 1047 7.2\nMistral 7B Instruct 1031 6.84 +/- 0.07\nLlama 2 13B Chat 1012 6.65\nVicuna 13B 1041 6.57\nLlama 2 7B Chat 985 6.27\nVicuna 7B 997 6.17\nAlpaca 13B 914 4.53\nTable 3: Comparison of Chat models. Mistral 7B \u2013\nInstruct outperforms all 7B models on MT-Bench, and\nis comparable to 13B \u2013 Chat models.To evaluate the generalization capabilities of\nMistral 7B, we fine-tuned it on instruction datasets\npublicly available on the Hugging Face repository.\nNo proprietary data or training tricks were utilized:\nMistral 7B \u2013 Instruct model is a simple and\npreliminary demonstration that the base model can\neasily be fine-tuned to achieve good performance.\nIn Table 3, we observe that the resulting model,\nMistral 7B \u2013 Instruct, exhibits superior perfor-\nmance compared to all 7B models on MT-Bench,\nand is comparable to 13B \u2013 Chat models. An\nindependent human evaluation was conducted on\nhttps://llmboxing.com/leaderboard .", "Figure12: Humanevaluationresults forLlama 2-Chat modelscomparedtoopen-andclosed-sourcemodels\nacross ~4,000 helpfulness prompts with three raters per prompt.\nThe largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\nmodel by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\nInter-Rater Reliability (IRR). In our human evaluations, three different annotators provided independent\nassessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as\nbetter from a data quality perspective, however, context is important. Highly subjective tasks like evaluating\nthe overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling", "Figure12: Humanevaluationresults forLlama 2-Chat modelscomparedtoopen-andclosed-sourcemodels\nacross ~4,000 helpfulness prompts with three raters per prompt.\nThe largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\nmodel by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\nInter-Rater Reliability (IRR). In our human evaluations, three different annotators provided independent\nassessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as\nbetter from a data quality perspective, however, context is important. Highly subjective tasks like evaluating\nthe overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling", "on proprietary user-shared conversations from ShareGPT and is thus the result of distillation from\nOpenAI GPT models.\n5.2 Evaluation\nTable 5: MMLU 5-shot test results for different\nsizes of LLaMA finetuned on the corresponding\ndatasets using QLoRA.\nDataset 7B 13B 33B 65B\nLLaMA no tuning 35.1 46.9 57.8 63.4\nSelf-Instruct 36.4 33.3 53.0 56.7\nLongform 32.1 43.2 56.6 59.7\nChip2 34.5 41.6 53.6 59.8\nHH-RLHF 34.9 44.6 55.8 60.1\nUnnatural Instruct 41.9 48.1 57.3 61.3\nGuanaco (OASST1) 36.6 46.4 57.0 62.2\nAlpaca 38.8 47.8 57.3 62.5\nFLAN v2 44.5 51.4 59.2 63.9Following common practice, we use the MMLU (Mas-\nsively Multitask Language Understanding) benchmark\n[24] to measure performance on a range of language un-\nderstanding tasks. This is a multiple-choice benchmark\ncovering 57 tasks including elementary mathematics,\nUS history, computer science, law, and more. We report\n5-shot test accuracy.\nWe also test generative language capabilities through\nboth automated and human evaluations. This second", "approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\nSize and Efficiency. We computed \u201cequivalent model sizes\u201d of the Llama 2 family, aiming to\nunderstand Mistral 7B models\u2019 efficiency in the cost-performance spectrum (see Figure 5). When\nevaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B\nmirrored performance that one might expect from a Llama 2 model with more than 3x its size. On\nthe Knowledge benchmarks, Mistral 7B\u2019s performance achieves a lower compression rate of 1.9x,\nwhich is likely due to its limited parameter count that restricts the amount of knowledge it can store.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation\nprotocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\non TriviaQA, we do not provide Wikipedia contexts.\n4 Instruction Finetuning\nModelChatbot Arena\nELO RatingMT Bench"], "retrieved_docs_id": ["56f904611e", "69a8c044aa", "69a8c044aa", "7eda03b09e", "b7d8c4df51"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "When was the first issue of Preprint published?\n", "true_answer": "The first issue of Preprint was published in April 2024.", "source_doc": "hallucination.pdf", "source_id": "6158839d4c", "retrieved_docs": ["Preprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "employs an LLM to generate sentences based on the extracted keywords. Ultimately, the framework\nproduces a set of high-quality image-caption pairs. Experiment results show that the model trained\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "preprint arXiv:1904.10509 , 2019.\n13", "95\nabs/2206.14858, 2022.\n[204] T. Saier, J. Krause, and M. F \u00a8arber, \u201cunarxive 2022:\nAll arxiv publications pre-processed for nlp, includ-\ning structured full-text and citation network,\u201d arXiv\npreprint arXiv:2303.14957 , 2023.\n[205] H. A. Simon, \u201cExperiments with a heuristic compiler,\u201d\nJ. ACM , vol. 10, no. 4, pp. 493\u2013506, 1963.\n[206] Z. Manna and R. J. Waldinger, \u201cToward automatic\nprogram synthesis,\u201d Commun. ACM , vol. 14, no. 3, pp.\n151\u2013165, 1971.\n[207] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,\nL. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou,\n\u201cCodebert: A pre-trained model for programming and\nnatural languages,\u201d in Findings of EMNLP , 2020.\n[208] J. Austin, A. Odena, M. I. Nye, M. Bosma,\nH. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry,\nQ. V . Le, and C. Sutton, \u201cProgram synthesis with large\nlanguage models,\u201d CoRR , vol. abs/2108.07732, 2021.\n[209] S. Black, L. Gao, P . Wang, C. Leahy, and S. Bi-\nderman, \u201cGPT-Neo: Large Scale Autoregressive Lan-", "preprint arXiv:1906.08237 , 2019.\nJason Yosinski, Je\ufb00 Clune, Yoshua Bengio, and Hod Lipson. How transferable are features\nin deep neural networks? In Advances in neural information processing systems , 2014.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad\nNorouzi, and Quoc V. Le. QAnet: Combining local convolution with global self-attention\nfor reading comprehension. arXiv preprint arXiv:1804.09541 , 2018.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roes-\nner, and Yejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616 ,\n2019.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin\nVan Durme. ReCoRD: Bridging the gap between human and machine commonsense\nreading comprehension. arXiv preprint arXiv:1810.12885 , 2018.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu. Freelb: En-"], "retrieved_docs_id": ["6158839d4c", "f5c7517032", "9915e27c49", "e144d5f70b", "645a423c3a"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How are the parameters of a pre-trained model updated during adapter-based tuning?\n", "true_answer": "During adapter-based tuning, only the adapter parameters are updated, while the pre-trained model\u2019s parameters remain fixed.", "source_doc": "multimodal.pdf", "source_id": "004ffc5dd9", "retrieved_docs": ["expressiveness and generalization capabilities. Adapter-based tuning introduces lightweight adapter\nmodules into the pre-trained model\u2019s architecture. These adapter modules, typically composed of\nfeed-forward neural networks with a small number of parameters, are inserted between the layers\nof the original model. During fine-tuning, only the adapter parameters are updated, while the pre-\ntrained model\u2019s parameters remain fixed. This method significantly reduces the number of trainable\nparameters, leading to faster training and inference times without compromising the model\u2019s per-\nformance. LLM-Adapters [154] presents a framework for integrating various adapters into large\nlanguage models, enabling parameter-efficient fine-tuning for diverse tasks. This framework en-\n16", "Weights\n\ud835\udc4a\u2208\u211d\ud835\udc51\u00d7\ud835\udc51\nxh\n\ud835\udc35=0\n\ud835\udc34=\ud835\udca9(0,\ud835\udf0e2)\n\ud835\udc51\ud835\udc5fPretrained \nWeights\n\ud835\udc4a\u2208\u211d\ud835\udc51\u00d7\ud835\udc51\nxf(x)\n\ud835\udc51\nFigure 1: Our reparametriza-\ntion. We only train AandB.Many applications in natural language processing rely on adapt-\ningonelarge-scale, pre-trained language model to multiple down-\nstream applications. Such adaptation is usually done via \ufb01ne-tuning ,\nwhich updates all the parameters of the pre-trained model. The ma-\njor downside of \ufb01ne-tuning is that the new model contains as many\nparameters as in the original model. As larger models are trained\nevery few months, this changes from a mere \u201cinconvenience\u201d for\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\n175 billion trainable parameters.1\nMany sought to mitigate this by adapting only some parameters or\nlearning external modules for new tasks. This way, we only need\nto store and load a small number of task-speci\ufb01c parameters in ad-", "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nFine-tuning method GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo\n\u22c6All parameters 83.28 19 .24 80 .88 71 .36 26 .98 39 .82 27 .65\nAdapter layers, d= 32 80 .52 15 .08 79 .32 60 .40 13 .84 17 .88 15 .54\nAdapter layers, d= 128 81 .51 16 .62 79 .47 63 .03 19 .83 27 .50 22 .63\nAdapter layers, d= 512 81 .54 17 .78 79 .18 64 .30 23 .45 33 .98 25 .81\nAdapter layers, d= 2048 81 .51 16 .62 79 .47 63 .03 19 .83 27 .50 22 .63\nGradual unfreezing 82.50 18 .95 79 .17 70.79 26.71 39 .02 26 .93\nTable 10: Comparison of di\ufb00erent alternative \ufb01ne-tuning methods that only update a subset\nof the model\u2019s parameters. For adapter layers, drefers to the inner dimensionality\nof the adapters.\n218steps. As such, we subdivide the \ufb01ne-tuning process into 12episodes of 218/12steps each\nand train from layers 12\u2212nto12in thenth episode. We note that Howard and Ruder\n(2018) suggested \ufb01ne-tuning an additional layer after each epoch of training. However, since", "Radford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform \ufb01ne-tuning in\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\nhas the same memory footprint as pre-training.\nParameter-Ef\ufb01cient Adaptation. Many have proposed inserting adapter layers between existing\nlayers in a neural network (Houlsby et al., 2019; Rebuf\ufb01 et al., 2017; Lin et al., 2020). Our method\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\nkey functional difference is that our learned weights can be merged with the main weights during\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).", "focus on two alternative \ufb01ne-tuning approaches that update only a subset of the parameters\nof our encoder-decoder model.\nThe \ufb01rst, \u201cadapter layers\u201d (Houlsby et al., 2019; Bapna et al., 2019), is motivated by\nthe goal of keeping most of the original model \ufb01xed while \ufb01ne-tuning. Adapter layers are\nadditional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward\nnetworks in each block of the Transformer. These new feed-forward networks are designed\nso that their output dimensionality matches their input. This allows them to be inserted\ninto the network with no additional changes to the structure or parameters. When \ufb01ne-\ntuning, only the adapter layer and layer normalization parameters are updated. The main\nhyperparameter of this approach is the inner dimensionality dof the feed-forward network,\nwhich changes the number of new parameters added to the model. We experiment with\nvarious values for d."], "retrieved_docs_id": ["004ffc5dd9", "b1e71d3b0e", "a7b1516ffb", "48e53ebd1d", "d8dc4b310a"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "Which model outperforms several 13 billion parameter MLLMs in document understanding?\n", "true_answer": "TinyChart [37]", "source_doc": "multimodal.pdf", "source_id": "6b47636d3a", "retrieved_docs": ["7.2 Document Understanding\nDocuments or charts serve as a crucial source of information, offering an intuitive visualization\nof data in various forms. They have become an indispensable part of information dissemination,\nbusiness decision-making, and academic research. However, current chart understanding models\nstill face two primary limitations: (1) The considerable number of parameters makes training and\ndeployment challenging. For instance, ChartLlama [196], a 13-billion-parameter model, is difficult\nto deploy on a single consumer-grade GPU. (2) These models struggle with efficiently encoding\nhigh-resolution images, as vision transformers tend to produce lengthy feature sequences.\nTo address the challenges of fine-grained visual perception and visual information compression for\ndocument-oriented MLLMs. TinyChart [37] outperforms several 13B MLLMs with Program-of-\nThoughts (PoT) learning and Visual Token Merging strategy while excelling in faster inference", "its selection is closely related to the lightweight nature of MLLM. In comparison to conventional\nMLLMs with parameter sizes ranging from 7 billion to tens of billions[87, 88], efficient MLLMs\ntypically employ language models with less than 3 billion parameters, such as phi2-2.7B[74] by\nMicrosoft and Gemma-2B[78] by Google. Phi-2 trained on special data recipes can match the per-\nformance of models 25 times larger trained on regular data. Phi-3-mini [86] can be easily deployed\nlocally on a modern phone and achieves a quality that seems on-par with models such as Mixtral\n8x7B [89] and GPT-3.5. In addition to utilizing pre-trained models, MobileVLM[20] downscales\nLLaMA[87] and trains from scratch using open-source datasets. The specific model scaling is illus-\ntrated in the Table.1 and Table.4.\n2.4 Vision Token Compression\nInitial research has underscored the potential of MLLMs across various tasks, including visual ques-", "Figure 11: Organization of efficient large language models advancements.\nOccupying a significant majority of the parameter volume in MLLMs, LLM serves as a crucial entry\npoint for enhancing the efficiency of MLLMs. In this section, similar to the survey paper [160], we\nprovide a brief overview of the research progress in efficient LLMs, offering inspiration for the\ndevelopment of Efficient MLLMs.\n4.1 Attention\nIn the standard self-attention mechanism, the time complexity is O(n2), where nis the sequence\nlength. This quadratic complexity arises due to the pairwise interactions between all input tokens,\nwhich can lead to scalability issues, especially when dealing with long sequences in LLMs. To\ntackle this, researchers have developed techniques to expedite attention mechanisms and reduce\ntime complexity, such as sharing-based attention, feature information reduction, kernelization or\nlow-rank, fixed and learnable pattern strategies, and hardware-assisted attention.", "\u2022 At present, efficient MLLMs face challenges in processing extended-context multimodal\ninformation, and they are typically limited to accepting single images. This constrains the\nadvancement of more sophisticated models capable of handling an increased number of\nmultimodal tokens. Such models would be beneficial for applications like comprehending\nlengthy videos and analyzing extensive documents that incorporate a mix of images and\ntext, creating more versatile and powerful systems.\n\u2022 The predominant efficient MLLMs mainly support dual input modalities - images and texts,\nand a singular output modality - text. However, the tangible world encompasses a more\nextensive array of modalities. By expanding the scope of efficient MLLMs to accommodate\n23", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-"], "retrieved_docs_id": ["6b47636d3a", "26327c579e", "323641b323", "a1bd2d5193", "ac70fcc9f2"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How much computation does LLaV A-UHD use compared to previous models when supporting 6 times larger resolution images?\n", "true_answer": "LLaV A-UHD uses 94% of the inference computation compared to previous models when supporting 6 times larger resolution images.", "source_doc": "multimodal.pdf", "source_id": "986687f08e", "retrieved_docs": ["Token Processing Techniques designed to process lengthy visual token squence are critical in ef-\nficient MLLMs as they address the dual challenges of preserving fine-grained details and reducing\ncomputational complexity. LLaV A-UHD [35] presents a novel approach to manage the computa-\ntional burden associated with high-resolution images. It puts forward two key components: (1) a\ncompression module that further condenses image tokens from visual encoders, significantly re-\nducing the computational load, and (2) a spatial schema to organize slice tokens for LLMs. No-\ntably, LLaV A-UHD demonstrates its efficiency by supporting 6 times larger resolution images using\nonly 94% of the inference computation compared to previous models. Furthermore, the model\ncan be efficiently trained in academic settings, completing the process within 23 hours on 8 A100\nGPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-", "Figure 6: Comparision of Phi[86] (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-small) versus\nLlama-2 [91] family of models(7B, 13B, 34B, 70B) that were trained on the same fixed data.\nLLaV A-UHD [35] proposes an image modularization strategy that divides native-resolution im-\nages into smaller variable-sized slices for efficient and extensible encoding. Inaddition, InternLM-\nXComposer2-4KHD [90] introduces a strategy that dynamically adjusts resolution with an automatic\nlayout arrangement, which not only maintains the original aspect ratios of images but also adaptively\nalters patch layouts and counts, thereby enhancing the efficiency of image information extraction.\nBy implementing an adaptive input strategy for images of varying resolutions, a balance between\nperceptual capability and efficiency can be achieved.\nToken Processing Techniques designed to process lengthy visual token squence are critical in ef-", "MEGABYTE matches the state-of-the-art performance of\nPerceiverAR whilst using only half the compute.\n6.3. Scaling to higher resolutions\nWe compare three transformer variants (vanilla, Per-\nceiverAR, MEGABYTE) to test scalability to long sequences\non increasingly large image resolutions. We use our own\nimplementations of these in the same framework and budget\nthe same amount of GPU hours and data to train each of\nthese model variants.\nMEGABYTE is able to handle all sequence lengths with a\nsingle forward pass of up to 1.2M tokens. We found nei-\nther the standard Transformer nor PerceiverAR could model\nsuch long sequences at a reasonable model size, so instead\nwe split images into segments of size 1024 and 12000 re-\nspectively. For Megabyte, we set patch size as 12 for Im-\nage64 and patch size as 192 for Image256 and Image640\ndatasets. Model sizes are adjusted to match overall training\nspeeds across models and we do not use any form of sliding", "ModelVision Encoder LLMVision-LLM ProjectorVariants Resolution Parameter Size Variants Parameter Size\nMobileVLM [20] CLIP ViT-L/14 [73] 336 0.3B MobileLLaMA[20] 2.7B LDP[20]\nLLaV A-Phi [21] CLIP ViT-L/14 [73] 336 0.3B Phi-2[74] 2.7B MLP\nImp-v1 [22] SigLIP [75] 384 0.4B Phi-2[74] 2.7B -\nTinyLLaV A [23] SigLIP-SO [75] 384 0.4B Phi-2[74] 2.7B MLP\nBunny [24] SigLIP-SO [75] 384 0.4B Phi-2[74] 2.7B MLP\nMobileVLM-v2-3B [17] CLIP ViT-L/14 [73] 336 0.3B MobileLLaMA[17] 2.7B LDPv2[17]\nMoE-LLaV A-3.6B [25] CLIP-Large [73] 384 - Phi-2[74] 2.7B MLP\nCobra [13]DINOv2 [76]\nSigLIP-SO [75]384 0.3B+0.4B Mamba-2.8b-Zephyr[77] 2.8B MLP\nMini-Gemini [26] CLIP-Large [73] 336 - Gemma[78] 2B MLP\nVary-toy [27] CLIP [73] 224 - Qwen[79] 1.8B -\nTinyGPT-V [28] EV A [80] 224/448 - Phi-2[74] 2.7B Q-Former [15]\nSPHINX-Tiny [14]DINOv2 [76]\nCLIP-ConvNeXt [81]448 - TinyLlama[82] 1.1B -\nALLaV A-Longer [29] CLIP-ViT-L/14 [73] 336 0.3B Phi-2[74] 2.7B -\nMM1-3B-MoE-Chat [30] CLIP DFN-ViT-H [83] 378 - - 3B\u2217C-Abstractor [19]", "640\u00d7640 pixels \u2013 the latter requiring the effective modeling\nof sequences with over 1.2M tokens. This generation task\nbecomes increasingly challenging as the image\u2019s resolution\ngrows: doing well on this task requires the modeling of\nlocal patterns (textures, lines, etc.) and long-range context\nthat provides information about the high level structure of\nthe image. Inspired by recent works in Vision Transform-\ners (Dosovitskiy et al., 2020), we model image data patch\nby patch (more details can be found in Appendix D.1).\n6.2. Comparison with State of the Art\nWe train a large MEGABYTE model on ImageNet 64x64\nwith Global and Local models sized 2.7B and 350M parame-\nters, respectively, for 1.4T tokens. We estimate that training\nthis model consumed less than half the GPU hours we would\nhave needed to reproduce the best PerceiverAR model de-\nscribed by (Hawthorne et al., 2022). As shown in Table 4,\nMEGABYTE matches the state-of-the-art performance of"], "retrieved_docs_id": ["986687f08e", "c0bdc4830f", "705a34eae0", "14f018b2c6", "8518fe7b13"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does FaithScore evaluate free-form responses from MLLMs?\n", "true_answer": "FaithScore evaluates free-form responses from MLLMs by decomposing the response into elements, evaluating, and analyzing them in detail.", "source_doc": "hallucination.pdf", "source_id": "23d981a684", "retrieved_docs": ["HaELM [ 104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4\nmodels to assess the quality of the MLLM response. In contrast, the work of Hallucination Evaluation\nbased on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination\ndetection. It collects a set of hallucination data generated by a wide range of MLLMs, simulates data\nusing ChatGPT, and trains an LLM based on LLaMA [ 99]. After that, the HaELM model becomes\nproficient in hallucination evaluation, leveraging reference descriptions of images as the basis of\nassessment.\nFaithScore [ 55]Considering the natural forms of interaction between humans and MLLMs,\nFaithScore aims to evaluate free-form responses to open-ended questions. Different from LLM-based\noverall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate,\nand analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence", "and analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence\nidentification, atomic fact generation, and fact verification. The evaluation metric involves fine-\ngrained object hallucination categories, including entity, count, color, relation, and other attributes.\nThe final computation of FaithScore is the ratio of hallucinated content.\nBingo [ 21]Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark\nspecifically designed for assessing and analyzing the limitations of current popular MLLMs, such as\nGPT-4V [ 83]. It comprises 190 failure instances, along with 131 success instances as a comparison.\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\nBias refers to the model\u2019s susceptibility to generating hallucinatory outputs on specific types of\nexamples, such as OCR bias, region bias, etc. Interference refers to scenarios in which the judgment", "manner. Traditional multimodal tasks often rely on a closed-\nended evaluation framework, where the assessment is based\non the exact match between the model\u2019s response and the\nground-truth answer. Examples include the VQA score [849]\nfor visual question answering tasks and the CIDEr [850]\nscore for captioning tasks. However, MLLMs generate re-\nsponses in an open-ended way, which may contain the\ncorrect answer but not exactly match the ground-truth per-\nfectly. This discrepancy can lead to the underestimation of\nthe model\u2019s performance in previous evaluation paradigms.\nTo address this issue, recent approaches have incorporated\nhumans or LLMs as evaluators [829]. For instance, MM-Bench [838] employs ChatGPT to align the model responses\nwith the most relevant option in a set of multiple-choice\nquestions. Similarly, LLaVA [851] utilizes GPT-4 for eval-\nuating MLLMs\u2019 output, where GPT-4 takes the generated\nimage captions and object bounding boxes as visual inputs", "from what appears in the document being summarized. Consequently, it is important to measure and\nimprove the faithfulness of these systems since unfaithful systems may be harmful by potentially spreading\nmisinformation, including dangerous, yet hard to detect errors, when deployed in real-world settings. We\nevaluate the LMs using recently proposed reference-free evaluation metrics that have been shown to get high\ncorrelations with human scores for faithfulness (Laban et al., 2022; Fabbri et al., 2022). We note recent\nwork has shown that some reference-free evaluation metrics may be mostly relying on spurious correlations\n(Durmus et al., 2022).\nDatasets. There is a growing collection of summarization datasets, including datasets that capture finer-\ngrained and more specific summarization functions (e.g. summarizing multiple documents or conditional\non a user query). Bommasani & Cardie (2020) show that there is significant diversity in summarization", "open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify"], "retrieved_docs_id": ["23d981a684", "db8870dfa6", "c4a72ae7c0", "e99622852f", "ffd5c8b41e"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the length of the context window in RAG affect its performance?\n", "true_answer": "The performance of RAG's generation phase is constrained by the context window of LLMs. If the window is too short, it may not contain enough relevant information, and if it's too long, it might lead to information loss.", "source_doc": "RAG.pdf", "source_id": "6080afb1ff", "retrieved_docs": ["that require further investigation.\nFirstly, the issue of long context in RAG is a significant\nchallenge. As mentioned in the literature [Xuet al. , 2023c ],\nRAG\u2019s generation phase is constrained by the context win-\ndow of LLMs. If the window is too short, it may not contain\nenough relevant information; if it\u2019s too long, it might lead to\ninformation loss. Currently, expanding the context window\nof LLMs, even to the extent of limitless context, is a critical\ndirection in LLM development. However, once the context\nwindow constraint is removed, how RAG should adapt re-\nmains a noteworthy question.\nSecondly, the robustness of RAG is another important re-\nsearch focus. If irrelevant noise appears during retrieval, or\nif the retrieved content contradicts facts, it can significantly\nimpact RAG\u2019s effectiveness. This situation is figuratively\nreferred to as \u201dopening a book to a poisonous mushroom\u201d.", "has highlighted that the fine-tuning approach tends to be\ninherently slow when adapting LLMs for long texts [240].\n\u2022Position interpolation. This method downscales the po-\nsition indices within the original context window, to avoid\nout-of-distribution rotation angles during pre-training [240,\n289]. To be more specific, this approach multiplies all posi-\ntion indices by a coefficient L/L\u2032(L < L\u2032), where Land\nL\u2032represent the original and target context window length,\nrespectively. Experimental results [240] have shown that\nthis method can extend the context window effectively and\nefficiently, compared to the above approach of direct model\nfine-tuning. However, it is worth noting that this technique\nmay have an adverse impact on the model\u2019s performance\nwhen handling shorter texts[240, 290].\n\u2022Position truncation. To mitigate the challenges posed\nby out-of-distribution rotation angles, another practical ap-\nproach is to truncate longer relative positions to satisfy the", "ple, different chunking models should be selected for longer\nor shorter content. Additionally, different embedding mod-\nels perform differently at different block sizes; for example,\nsentence-transformer is more suitable for single sentences,while text-embedding-ada-002 is better for blocks containing\n256 or 512 tokens. Furthermore, the length and complexity\nof the user\u2019s input question text, as well as the specific needs\nof your application such as semantic search or Q&A, will all\naffect the choice of chunking strategy. This might directly\ncorrelate with the token limits of your chosen LLM, and may\nrequire you to adjust the block size. In fact, accurate query\nresults are achieved by adaptively applying several chunking\nstrategies; there is no best, only most suitable.\nCurrent research in RAG employs diverse block optimiza-\ntion methods to improve retrieval efficiency and accuracy.\nTechniques such as sliding window technology implement", "and required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the differencein retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022ChipNeMo-70B-Steer outperforms similar sized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and", "attention enables up to 32x larger context lengths.\nmultihead variant.\nAt large batch sizes and context lengths, the KV cache can\nbecome very large, putting us at the risk of running out\nof memory. Table 1 shows that the optimized multiquery\nlayout can \ufb01t up to 32\u201364 times longer context lengths than\nthe multihead and baseline multiquery variant.\nDuring pre\ufb01ll, multiquery and multihead attention incur sim-\nilar inference latencies because we compute many attention\nqueries in parallel and the attention computation becomes\ncompute-limited on the attention matrix multiplies. During\ngeneration, Figure 8 shows that the optimized multiquery\nlayout improves speed. The speed improvement is small\nwhen the context length is short because almost all of the\ntime is spent on the feedforward layer. As the context length\ngrows longer, the time to load the KV cache in the attention\nlayer becomes a much larger portion of overall inference\ntime. Multiquery attention scales up to sequence lengths of"], "retrieved_docs_id": ["6080afb1ff", "17615db54d", "20000f1ef4", "1ed1c2ae54", "981b25c11e"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the focus of the analysis in the given context?\n", "true_answer": "The focus of the analysis in the given context is the unique origins of hallucinations in modern large-scale language models (MLLMs), spanning a spectrum of contributing factors from data, model, training, to the inference stage.", "source_doc": "hallucination.pdf", "source_id": "6da15b5bb7", "retrieved_docs": ["suggests that the global trend of focusing on the nearby context is largely contributed by this content-\ndependent positional bias.\n\u2022The overall trend of term (d)is also focusing more on nearby words. However, compared to the trend\nof term (b), it is clearly \ufb02atter and biases towards a longer context.", "et al., 2021; Power et al., 2022; Olsson et al., 2022; Zhang et al., 2022) such as recall and induction, to distill\nthree properties of attention correlated with its performance and the quality gap with existing subquadratic\napproaches:\na.Data control: Attention implements an expressive data-controlled (Massaroli et al., 2020) linear operator1,\nencoding an entire family of linear functions in a single block.\nb.Sublinear parameter scaling: Parameter counts of attention layers are decoupled from sequence length,\nallowing Transformers to allocate more parameters elsewhere e.g., the feed-forward neural networks (FFNs)\nbetween attention layers.\nc.Unrestricted context: For a given input, attention has an unrestricted context i.e., it can approximate\ndependencies between any two inputs, without arbitrary restrictions such as locality (except in cases using\nmasking such as autoregressive models).", "a few locations are clearly attended more than others.\nFinally, as we have discussed in section 3.3, the attention score can be decomposed into four intuitive\nterms. Here, we want to further investigate how these four terms contribute to the overall attention trend\nin Fig. 5. Since the term (c)represents the global content bias, i.e., the prior importance of each word\nregardless of the context, we will leave it out and focus on the terms (a),(b)and(d). So, for each term,\nwe take the Softmax w.r.t. the memory span and average the resulted distribution of all tokens in the\nvalidation set. The results are visualized in Fig. 7:\n\u2022Since term (a)is fully content-based addressing, when averaging over all target words, the result is\nessentially uniform over the entire context, except for a few very close words, which are likely to be\nsemantically similar to the target word.\n\u2022The overall trend of term (b)highly resembles that of the entire attention distribution in Fig. 5. It", "in this report. These sections are intended to help downstream developers assess potential harms in their speci\ufb01c\napplication contexts (Shelby et al., 2023), so that they can prioritize additional procedural and technical safeguards\nearlier in development.\nD.1 Dataset analysis\nWe conduct a responsible AI-focused analysis of the PaLM 2 pre-training data, focusing our analysis on representations\nof people in pre-training data. Dataset analysis and transparency artifacts are a key part of Responsible AI practices\n(Bender & Friedman, 2018; Mitchell et al., 2019; Gebru et al., 2021; Pushkarna et al., 2022). Documenting pre-training\ndata is a challenging but critical element of understanding language models (Paullada et al., 2021; Dodge et al., 2021;\nLuccioni & Viviano, 2021). We focus our analysis on forms of bias observed in prior NLP systems (Abid et al., 2021;\n63", "Published in Transactions on Machine Learning Research (08/2023)\n10.1 Missing scenarios\nSince we define scenarios in terms of tasks, domains, and languages, we begin by considering what we miss\nfor each of these. For tasks, we emphasize that we deliberately chose to prioritize classical user-facing tasks,\nbut other tasks also can confer significant societal impact, even if they are not user-facing (e.g. syntactic\nparsing and natural language inference). Beyond this, even while our focus is oriented by societal impact\nin task selection, other tasks may be more discriminative in identifying specific properties, limitations, or\ncapabilities of models (e.g. natural language inference often plays such a diagnostic role, which may also align\nwith our component skill on linguistic capabilities. We also explicitly acknowledge certain tasks that we do\nnot currently implement as an oversight in terms of determining what was user-facing, such as data-to-text"], "retrieved_docs_id": ["e8db9c0070", "7070f29504", "4fa9964baf", "809df15e2a", "0ace790de2"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does iterative retrieval improve the robustness of answer generation in large language models?\n", "true_answer": "Iterative retrieval improves robustness by regularly collecting documents based on the original query and generated text, providing additional materials for large language models. The robustness is further enhanced by providing additional references in multiple iterative retrievals.", "source_doc": "RAG.pdf", "source_id": "f24827ee1d", "retrieved_docs": ["Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "models must retrieve from the middle of the input context.\nplaced at the start of the input context, LongChat-\n13B (16K) tends to generate code to retrieve the\nkey, rather than outputting the value directly.\n4Why Are Language Models Not Robust\nto Changes in the Position of Relevant\nInformation?\nOur multi-document question answering and key-\nvalue retrieval results show that language models\nstruggle to robustly access and use information in\nlong input contexts, since performance degrades\nsignificantly when changing the position of rele-\nvant information. To better understand why, we per-\nform some preliminary investigations into the role\nof model architecture (decoder-only vs. encoder-\ndecoder), query-aware contextualization, and in-\nstruction fine-tuning.\n4.1 Effect of Model Architecture\nThe open models we evaluated are all decoder-only\nmodels\u2014at each timestep, they may only attend\nto prior tokens. To better understand the poten-\ntial effects of model architecture on how language", "50 documents instead of 20 retrieved documents\nonly marginally improves performance ( \u223c1.5% for\nGPT-3.5-Turbo and \u223c1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future long-\ncontext models; to claim that a language model can\nrobustly use information within long input con-\ntexts, it is necessary to show that its performance\nis minimally affected by the position of the rele-\nvant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data.1\n2 Multi-Document Question Answering\nOur goal is to better understand how language mod-\nels use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant", "pp. 2206\u20132240.\n[658] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua,\n\u201cSearch-in-the-chain: Towards accurate, credible and\ntraceable large language models for knowledge-\nintensive tasks,\u201d CoRR , vol. abs/2304.14732, 2023.\n[659] B. Peng, M. Galley, P . He, H. Cheng, Y. Xie, Y. Hu,\nQ. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao,\n\u201cCheck your facts and try again: Improving large\nlanguage models with external knowledge and auto-\nmated feedback,\u201d CoRR , vol. abs/2302.12813, 2023.\n[660] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-\nYu, Y. Yang, J. Callan, and G. Neubig, \u201cActive retrieval\naugmented generation,\u201d CoRR , vol. abs/2305.06983,\n2023.\n[661] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang,\nQ. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, \u201cA sur-\nvey on hallucination in large language models: Prin-\nciples, taxonomy, challenges, and open questions,\u201d\nCoRR , vol. abs/2311.05232, 2023.\n[662] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and", "provide enough information about whether the produced answer would contradict.\nRetrieval-augmented language model pre-training\n(REALM) [ 186] inserts retrieved documents\ninto the pre-training examples. While Guu et al.\n[186] designed REALM for extractive tasks\nsuch as question-answering, Lewis et al. [304]\npropose retrieval-augmented generation (RAG), a\nlanguage generation framework using retrievers\nfor knowledge-intensive tasks that humans could\nnot solve without access to an external knowledge\nsource. Yogatama et al. [646] propose the adaptive\nSemiparametric Language Models architecture,\nwhich incorporates the current local context, a\nshort-term memory that caches earlier-computed\nhidden states, and a long-term memory based on a\nkey-value store of (hidden-state, output) tuples. To\nequip a retrieval-augmented LLM with few-shot\nabilities that were before only emergent in LLMs\nwith many more parameters, Izacard et al. [236]\npropose a KL-divergence loss term for retrieval"], "retrieved_docs_id": ["af911eac69", "91a4302548", "b410d0d2f3", "058f03bb53", "f7770d2394"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does post-training quantization reduce the precision of ViT models?\n", "true_answer": "Post-training quantization compresses trained ViT models by converting their parameters from high-precision floating-point numbers to lower-precision fixed-point numbers, such as 8-bit integers.", "source_doc": "multimodal.pdf", "source_id": "354a427ccf", "retrieved_docs": ["improved performance for the ViT student model.\n3.4 Quantization\nViT quantization is the process of reducing the precision of numerical representations in ViT models,\ntypically transitioning from floating-point to fixed-point arithmetic [140]. This reduction in preci-\nsion aims to decrease memory usage, computational complexity, and energy consumption while\npreserving model accuracy to an acceptable level. Current research can be mainly categorized into\npost-training quantization, quantization-aware training, and hardware-aware quantization.\nPost-Training Quantization (PTQ) compresses trained ViT models by converting their param-\neters from high-precision floating-point numbers to lower-precision fixed-point numbers, such as\n8-bit integers. For example, Liu et al. [141] introduced a ranking loss method to identify opti-\nmal low-bit quantization intervals for weights and inputs, ensuring the functionality of the attention", "to quantized values, the quantization error is significantly reduced under certain conditions. This\ntechnique successfully modifies heavy-tailed activation distributions to fit a given quantizer.\nQuantization-Aware Training (QAT) integrates quantization into the training cycle. This in-\ntegration is particularly advantageous when scaling down to ultra-low bit precision, such as 4 bits\nor lower, where PTQ struggles with significant performance loss. For example, Quantformer [124]\nleverages entropy information to maintain consistency in self-attention ranks and introduces a dif-\nferentiable search mechanism to optimally group patch feature dimensions, reducing rounding and\nclipping inaccuracies. Q-ViT [126] incorporates a distillation token and Information Rectification\nModule (IRM) to counteract altered distributions in quantized attention modules. TerViT [127] and\nBit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain", "produce negative values.\nSummary (QAT). QAT has been shown to work\ndespite the coarse approximation of STE. However, the\nmain disadvantage of QAT is the computational cost of\nre-training the NN model. This re-training may need\nto be performed for several hundred epochs to recover\naccuracy, especially for low-bit precision quantization. If\na quantized model is going to be deployed for an extended\nperiod, and if ef\ufb01ciency and accuracy are especially\nimportant, then this investment in re-training is likely\nto be worth it. However, this is not always the case, as\nsome models have a relatively short lifetime. Next, we\nnext discuss an alternative approach that does not have\nthis overhead.\n2) Post-Training Quantization: An alternative to the\nexpensive QAT method is Post-Training Quantization\n(PTQ) which performs the quantization and the adjust-\nments of the weights, without any \ufb01ne-tuning [ 11,24,40,\n60,61,68,69,89,108,142,148,174,182,223,281].", "quantization in a uni\ufb01ed setting, is time- and space-ef\ufb01cient, and considerably\nimproves upon the practical performance of existing post-training methods. At the\ntechnical level, our approach is based on an exact and ef\ufb01cient realization of the\nclassical Optimal Brain Surgeon (OBS) framework of [LeCun, Denker, and Solla,\n1990] extended to also cover weight quantization at the scale of modern DNNs.\nFrom the practical perspective, our experimental results show that it can improve\nsigni\ufb01cantly upon the compression-accuracy trade-offs of existing post-training\nmethods, and that it can enable the accurate compound application of both pruning\nand quantization in a post-training setting.\n1 Introduction\nThe impressive recent progress of deep learning for solving challenging tasks across several domains\nhas been accompanied by a signi\ufb01cant increase in parameter counts and computational costs for\nexecuting such models. A natural consequence has been a growing effort to reduce such costs via", "performance drop (Frantar et al., 2022; Xiao et al., 2022). Zeng et al. (2022) hypothesize that the\nobserveddifferenceinweightdistributioncharacteristicsmaybeduetothedifferenceinoptimization\nchoices made during pre-training.\nIn this work, we seek to reconcile these observations. We posit that it is possible to optimize for a\nquantization friendly training recipe that suppresses large activation magnitude outliers. This leads\nto a distribution of activations and weights that are more amenable to simple INT8 quantization\nrecipes and does not necessitate the need for complex and inefficient mixed-precision computations.\nOur results show that we can introduce simple INT8 post-training quantization with negligible\nimpact on performance due to choices we make during the pre-training stage. As shown in Figure 1,\nacross 8 zero-shot downstream tasks, our models do not present any significant performance drop,\nhaving only 0.24% average degradation in a 52 billion parameter model."], "retrieved_docs_id": ["354a427ccf", "900b3dde3f", "94a6ffe197", "49779f135e", "a9605c7507"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does REALM model the pre-training process?\n", "true_answer": "REALM models the pre-training process as a retrieve-then-predict process, where the language model pre-trains by predicting masked tokens based on masked sentences, modeling P(x|y).", "source_doc": "RAG.pdf", "source_id": "6240233238", "retrieved_docs": ["As a knowledge-intensive task, RAG employs different tech-\nnical approaches during the language model training\u2019s pre-\ntraining, fine-tuning, and inference stages.\nPre-training Stage\nSince the emergence of pre-trained models, researchers have\ndelved into enhancing the performance of Pre-trained Lan-\nguage Models (PTMs) in open-domain Question Answering\n(QA) through retrieval methods at the pre-training stage. Rec-\nognizing and expanding implicit knowledge in pre-trained\nmodels can be challenging. REALM [Arora et al. , 2023 ]in-\ntroduces a more modular and interpretable knowledge em-\nbedding approach. Following the Masked Language Model\n(MLM) paradigm, REALM models both pre-training and\nfine-tuning as a retrieve-then-predict process, where the lan-\nguage model pre-trains by predicting masked tokens ybased\non masked sentences x, modeling P(x|y).\nRETRO [Borgeaud et al. , 2022 ]leverages retrieval aug-\nmentation for pre-training a self-regressive language model,", "of data for pre-training LLMs. To determine both settings, a\npractical way is to first train several small language models\nwith multiple candidate plans and then select a good plan\namong them [59]. Overall, it is more difficult to find a\nsuitable data curriculum. In practice, one can monitor the\nperformance of intermediate model checkpoints on specific\nevaluation benchmarks, and dynamically tune the data mix-\nture and distribution during pre-training. In this process, it\nis also useful to explore the potential relations between data\nsources and model abilities to instruct the design of data\ncurriculum.\n4.2 Architecture\nIn this section, we review the architecture design of LLMs,\ni.e.,mainstream architecture, pre-training objective, and de-\ntailed configuration. Table 5 presents the model cards of\nseveral representative LLMs with public details.\n4.2.1 Typical Architectures\nDue to the excellent parallelizability and capacity, the Trans-", "propose to combine the following techniques: Domain-\nAdaptive Pre-Training (DAPT) (Gururangan et al., 2020) of\nfoundation models with domain-adapted tokenizers, model\nalignment using general and domain-specific instructions,\nand retrieval-augmented generation (RAG) (Lewis et al.,\n2021b) with a trained domain-adapted retrieval model.\nAs shown in Figure 1, our approach is to start with a base\nfoundational model and apply DAPT followed by model\nalignment. DAPT, also known as continued pretraining with\nin-domain data, has been shown to be effective in areas such\nas biomedical and computer science publications, news, and\nreviews. In our case, we construct our domain-specific pre-\ntraining dataset from a collection of proprietary hardware-\nrelated code (e.g. software, RTL, verification testbenches,\netc.) and natural language datasets (e.g. hardware specifi-\ncations, documentation, etc.). We clean up and preprocess\nthe raw dataset, then continued-pretrain a foundation model", "acceleration methods, and optimization techniques need to\nbe well designed. In what follows, we first discuss the data\ncollection and processing in Section 4.1, then introduce the\ncommonly used model architectures in Section 4.2, and fi-\nnally present the training techniques to stably and efficiently\noptimize LLMs in Section 4.3.\n4.1 Data Collection and Preparation\nCompared with small-scale language models, LLMs have\na stronger demand for high-quality data for model pre-\ntraining, and their model capacities largely rely on the pre-\ntraining corpus and how it has been preprocessed. In this\npart, we discuss the collection and processing of pre-training\ndata, including data sources, preprocessing methods, and\nimportant analysis of how pre-training data affects the\nperformance of LLMs.", "Pre-training/\ufb01ne-tuning has become a popular\nparadigm for solving many tasks in natural lan-\nguage processing (NLP) (Devlin et al., 2018; Liu\net al., 2019; Brown et al., 2020) and Computer Vi-\nsion (Simonyan and Zisserman, 2014; He et al.,\n2016; Howard et al., 2019; Bochkovskiy et al.,\n1github.com/huawei-noah/KD-NLP/tree/main/DyLoRA2020; Chen et al., 2020; Dosovitskiy et al., 2020).\npretrained models (PMs) such as pretrained lan-\nguage models (PLMs) (Devlin et al., 2018; Brown\net al., 2020), and pretrained visual-language mod-\nels (Lu et al., 2019; Li et al., 2019; Su et al., 2019;\nXia et al., 2021) have advanced a lot in recent years.\nWith the ever-growing size of these pretrained mod-\nels, \ufb01ne-tuning them on downstream tasks becomes\nmore expensive. Moreover, as the ratio of the num-\nber of parameters of models with respect to the\nlabeled data increases, the \ufb01ne-tuning process will\nbe more prone to over\ufb01tting (Karimi Mahabadi\net al., 2021). There are two categories of solutions:"], "retrieved_docs_id": ["6240233238", "57073057c9", "926168a67f", "a8f1212d5f", "3c072f09a9"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does SURGE minimize the semantic similarity between documents and queries?\n", "true_answer": "SURGE minimizes the semantic similarity between documents and queries by updating the generator and retriever during the fine-tuning phase, which leverages relevant background knowledge.", "source_doc": "RAG.pdf", "source_id": "977e0e1405", "retrieved_docs": ["swers given a retrieval-enhanced directive. It updates the gen-\nerator and retriever to minimize the semantic similarity be-\ntween documents and queries, effectively leveraging relevant\nbackground knowledge.\nAdditionally, SUGRE [Kang et al. , 2023 ]introduces the\nconcept of contrastive learning. It conducts end-to-end fine-\ntuning of both retriever and generator, ensuring highly de-\ntailed text generation and retrieved subgraphs. Using a\ncontext-aware subgraph retriever based on Graph Neural Net-\nworks (GNN), SURGE extracts relevant knowledge from a\nknowledge graph corresponding to an ongoing conversation.\nThis ensures the generated responses faithfully reflect the re-\ntrieved knowledge. SURGE employs an invariant yet efficient\ngraph encoder and a graph-text contrastive learning objective\nfor this purpose.\nIn summary, the enhancement methods during the fine-\ntuning phase exhibit several characteristics. Firstly, fine-\ntuning both LLM and retriever allows better adaptation", "formation to the edges of the prompt is a straightfor-\nward idea. This concept has been implemented in frame-\nworks such as LlamaIndex, LangChain, and HayStack\n[Blagojevi, 2023 ]. For instance, Diversity Ranker pri-\noritizes reordering based on document diversity, while\nLostInTheMiddleRanker alternates placing the best doc-\nument at the beginning and end of the context window.\nSimultaneously, addressing the challenge of interpreting\nvector-based simulated searches for semantic similarity,\napproaches like cohereAI rerank [Cohere, 2023 ], bge-\nrerank5, or LongLLMLingua [Jiang et al. , 2023a ]recal-\nculate the semantic similarity between relevant text and\nquery.\n\u2022Prompt Compression Research indicates that noise\nin retrieved documents adversely affects RAG perfor-\nmance. In post-processing, the emphasis lies in com-\npressing irrelevant context, highlighting pivotal para-\ngraphs, and reducing the overall context length. Ap-\nproaches such as Selective Context [Litman et al. , 2020 ]", "This demonstrates the ability of our model to handle long-range contexts effectively.\nSemantic Similarity Semantic similarity (or paraphrase detection) tasks involve predicting whether\ntwo sentences are semantically equivalent or not. The challenges lie in recognizing rephrasing of\nconcepts, understanding negation, and handling syntactic ambiguity. We use three datasets for this\ntask \u2013 the Microsoft Paraphrase corpus (MRPC) [ 14] (collected from news sources), the Quora\nQuestion Pairs (QQP) dataset [ 9], and the Semantic Textual Similarity benchmark (STS-B) [ 6].\nWe obtain state-of-the-art results on two of the three semantic similarity tasks (Table 4) with a 1\npoint absolute gain on STS-B. The performance delta on QQP is signi\ufb01cant, with a 4.2% absolute\nimprovement over Single-task BiLSTM + ELMo + Attn.\nClassi\ufb01cation Finally, we also evaluate on two different text classi\ufb01cation tasks. The Corpus", "the semantic space of the user\u2019s query and documents is very\nnecessary. This section introduces two key technologies to\nachieve this goal.\nQuery Rewrite\nThe most intuitive way to align the semantics of\nquery and document is to rewrite the query. As\nmentioned in Query2Doc [Wang et al. , 2023b ]and ITER-\nRETGEN [Shao et al. , 2023 ], the inherent capabilities of\nlarge language models are utilized to generate a pseudo-\ndocument by guiding it, and then the original query is\nmerged with this pseudo-document to form a new query.\nIn HyDE [Gao et al. , 2022 ], query vectors are established\nthrough the use of text indicators, using these indicators to\ngenerate a \u2019hypothetical\u2019 document that is relevant, yet may\nnot truly exist, it only needs to capture the relevant pattern.\nRRR [Maet al. , 2023a ]introduced a new framework that in-\nverts the order of retrieval and reading, focusing on query\nrewriting. This method generates a query using a large lan-", "as dates and purposes used for filtering. Adding\nmetadata like chapters and subsections of refer-\nences could also be beneficial for improving re-\ntrieval. When we divide the index into numerous\nchunks, retrieval efficiency becomes a concern. Fil-\ntering through metadata first can enhance efficiency\nand relevance.\n4.Alignment Optimization: This strategy primarily\naddresses alignment issues and differences between\ndocuments. The alignment concept involves intro-\nducing hypothetical questions , creating questions\nwhich are suitable to answer with each document,\nand embedding (or replacing) these questions with\nthe documents. This helps address alignment prob-\nlems and discrepancies between documents.\n5.Mixed Retrieval: The advantage of this strategy\nlies in leveraging the strengths of different retrieval\ntechnologies. Intelligently combining various tech-\nniques, including keyword-based search, semantic\nsearch, and vector search, adapts to different query"], "retrieved_docs_id": ["977e0e1405", "e2c2dc1d50", "fb28b3acaa", "71a4057422", "9cdcf53e15"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does NoisyQuant reduce quantization error?\n", "true_answer": "NoisyQuant reduces quantization error by adding a fixed Uniform noisy bias to quantized values.", "source_doc": "multimodal.pdf", "source_id": "b9d5e110ca", "retrieved_docs": ["efficiency [ 4,8,22,31,33,34]. Our key insight is that quantization can be most effective when weight\nand proxy Hessian matrices are incoherent \u2014that the weights themselves are even in magnitude,\nand the directions in which it is important to have good rounding accuracy are not too large in any\none coordinate. Intuitively, incoherence can be thought of as a principled form of outlier reduction,\nwhich makes it easier to adaptively round the weights to a finite set of compressed values. We use\nthis intuition to develop theoretically sound two-bit quantization algorithms that scale to LLM-sized\nmodels.\nSpecifically, we introduce quantization with incoherence processing (QuIP), a new method motivated\nby the above insight. QuIP consists of two steps: (1) an adaptive rounding [ 20] procedure, which\nminimizes a quadratic proxy objective \u2113(\u02c6W) = tr(( \u02c6W\u2212W)H(\u02c6W\u2212W)T)of the error between the\noriginal weights Wand the quantized weights \u02c6Wusing an estimate of the Hessian H; (2) efficient pre-", "and symmetric quantization is mostly employed for the weights. In all our experiments, we use a\nquantization range estimator minimizing the mean-squared error on weights by grid search [46].\n3 Comparison on statistical distributions\nBefore diving into comparison results, we first describe theoretically what the quantization error\nand pruning error are. Looking at this with a theoretical lens helps with understanding the later\nexperimental difference between the two methods. We start off by describing and analyzing both\nmethods on simple data distributions.\nIn order to compare the error of pruning and quantization, we will frequently use the signal-to-noise\nratio measure defined in the log scale: SNR dB= 10 log10\u0000\nE\u0002\nW2\u0003\n/E\u0002\n(W\u2212F(W))2\u0003\u0001\n, where\nF(W)is the quantization or pruning function. This measure is the same as a scaled logarithm of\n2", "showing how appropriately modeling outliers is critical to effective quantization.\nMulti-billion Scale Transformer Quantization. There are two methods that were developed in\nparallel to ours: nuQmm (Park et al., 2022) and ZeroQuant (Yao et al., 2022). Both use the same\nquantization scheme: group-w2ise quantization, which has even \ufb01ner quantization normalization\nconstant granularity than vector-wise quantization. This scheme offers higher quantization precision\nbut also requires custom CUDA kernels. Both nuQmm and ZeroQuant aim to accelerate inference\nand reduce the memory footprint while we focus on preserving predictive performance under an\n8-bit memory footprint. The largest models that nuQmm and ZeroQuant evaluate are 2.7B and 20B\nparameter transformers, respectively. ZeroQuant achieves zero-degradation performance for 8-bit\nquantization of a 20B model. We show that our method allows for zero-degradation quantization", "Figure 2: Comparing the error of pruning and quantization for a student-t distribution, simulating\nthe presence of significant outliers. We plot the results for different magnitudes of the outliers, as\nper the kurtosis on the x-axis. (left) the pruning error, which does not change under the presence of\nmore severe outliers. (middle) the quantization SNR, which is reduced greatly when outliers increase\n(right) the trade-off regions where quantization and pruning are better.\nan MSE measure. Both are often employed to analyze the sensitivity of neural network layers to\nquantization, and they are theoretically well-founded to correlate with network performance [ 38,45].\n3.1 Quantization error\nFor quantization, we consider symmetric uniform quantization, which is also called integer quan-\ntization. Given a bit-width band the scale \u03b4, the grid nodes are defined as qi=\u03b4i, i\u2208\n{\u22122b, . . . , 0,2b\u22121}. The quantization operation rounding-to-nearest Q(w)and the corresponding", "Figure 3: Comparison on all the weights from PyTorch model zoo (46 models). (left) Pruning SNR\nversus quantization SNR for every tensor. (right) Pruning is preferable at high compression ratios for\ntensors with high sample kurtosis values.\nthe comparison. As we can see from figure 1 (middle), the errors for both methods have very different\nbehavior. The quantization error oscillates between the quantization nodes and has a moderate range.\nThe pruning error effectively corresponds to rounding many weights to zero and thus has a higher\nerror. As we can see in figure 1 (right), this results in a higher SNR for quantization, e.g. 19.1\ndB for INT4 quantization versus only 5.6 dB for 75% pruning. We see similar results for different\ncompression ratios. For this distribution, quantization achieves a much higher signal-to-noise ratio.\nDistributions with heavy tails. The trade-off is expected to change when more significant outliers"], "retrieved_docs_id": ["22ac0714ae", "de15032b1a", "219286a7af", "19d7216008", "353eada9b4"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How is the ChipNeMo chat model aligned with queries in the chip design domain?\n", "true_answer": "The ChipNeMo chat model is aligned with queries in the chip design domain by using a general-purpose chat instruction dataset and a small amount of domain-specific instruction datasets. This alignment is further improved by adding a small amount of task-specific instruction data.", "source_doc": "ChipNemo.pdf", "source_id": "411c489c58", "retrieved_docs": ["We use largely publicly available general-purpose chat in-\nstruction datasets for multi-turn chat together with a small\namount of domain-specific instruction datasets to perform\nalignment on the ChipNeMo foundation model, which pro-\nduces the ChipNeMo chat model. We observe that align-\nment with a general purpose chat instruction dataset is\nadequate to align the ChipNeMo foundation models with\nqueries in the chip design domain. We also added a small\namount of task-specific instruction data, which further im-\nproves the alignment. We trained multiple ChipNeMo foun-\ndation and chat models based on variants of LLaMA2 mod-\nels used as the base foundation model.\nTo improve performance on the engineering assistant chat-\nbot application, we also leverage Retrieval Augmented Gen-\neration (RAG). RAG is an open-book approach for giving\nLLMs precise context for user queries. It retrieves rele-\nvant in-domain knowledge from its data store to augment", "fine-tuning and also masked the attribute labels and trained\non ChipNeMo models for 2 epochs. We refer readers to\nAppendix A.4 for details on the alignment datasets and A.7\non implementations details.\nWe also experimented with DAPT directly on a chat aligned\nmodel, such as the LLaMA2-Chat model. We found that\nDAPT significantly degraded the model\u2019s alignment, mak-\ning the resulting model useless for downstream tasks.\n2.4. Domain-Adapted Retrieval Model\nIt is well known that LLMs can generate inaccurate text,\nso-called hallucination (Ji et al., 2023). Although the phe-\nnomenon is not completely understood, we still must miti-\ngatehallucinations since they are particularly problematic\nin an engineering assistant chatbot context, where accu-\nracy is critical. Our proposal is to leverage the retrieval\naugmented generation (RAG) method. RAG tries to re-trieve relevant passages from a database to be included in\nthe prompt together with the question, which grounds the", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat.", "ChipNeMo: Domain-Adapted LLMs for Chip Design\n2Domain -Adaptive\nPretraining\n24B tokens of chip \ndesign docs/code\nThousands GPU hrs\nModel\nAlignmen t\n56K/128K \n(SteerLM /SFT)  insts\n+ 1.4K task insts\n100+ GPU hrsFoundation Models\nLLaMA2 \n(7B, 13B, 70B) \nChipNeMo \nChat Models\n(7B, 13B, 70B)ChipNeMo \nFoundation Models\n(7B, 13B, 70B)Pretraining\nTrillions tokens of \ninternet data\n105 \u2013 106 GPU hrs\nFigure 1: ChipNeMo Training Flow\n2023)) fine-tuned on additional Verilog data can outperform\nstate-of-art OpenAI GPT-3.5 models. Customizing LLMs\nin this manner also avoids security risks associated with\nsending proprietary chip design data to third party LLMs\nvia APIs. However, it would be prohibitively expensive to\ntrain domain-specific models for every domain from scratch,\nsince this often requires millions of GPU training hours. To\ncost-effectively train domain-specific models, we instead\npropose to combine the following techniques: Domain-"], "retrieved_docs_id": ["411c489c58", "aec87069e2", "a6c3d05123", "273b593026", "2079d05356"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How do large language models (LLMs) fail when dealing with real-world production environments?\n", "true_answer": "LLMs may fail to provide accurate answers when the information sought extends beyond the model\u2019s training data or requires the latest data. This limitation poses challenges when deploying generative artificial intelligence in real-world production environments.", "source_doc": "RAG.pdf", "source_id": "b40c0db2f1", "retrieved_docs": ["maintaining user privacy and data security are crucial con-\nsiderations when applying LLMs to real-world scenarios.\n9 C ONCLUSION AND FUTURE DIRECTIONS\nIn this survey, we have reviewed the recent progress of large\nlanguage models (LLMs), and introduced the key concepts,\nfindings, and techniques for understanding and utilizing\nLLMs. We focus on the large-sized models ( i.e.,having a size\nlarger than 10B) while excluding the contents of early pre-\ntrained language models ( e.g., BERT and GPT-2) that have\nbeen well covered in the existing literature. In particular,our survey has discussed four important aspects of LLMs,\ni.e.,pre-training, adaptation, utilization, and evaluation. For\neach aspect, we highlight the techniques or findings that are\nkey to the success of LLMs. Furthermore, we also summa-\nrize the available resources for developing LLMs and dis-\ncuss important implementation guidelines for reproducing\nLLMs. This survey tries to cover the most recent literature", "Challenges and Applications of Large Language Models\nJean Kaddour\u03b1,\u2020,\u2217, Joshua Harris\u03b2,\u2217, Maximilian Mozes\u03b1,\nHerbie Bradley\u03b3,\u03b4,\u03f5, Roberta Raileanu\u03b6, and Robert McHardy\u03b7,\u2217\n\u03b1University College London\u03b2UK Health Security Agency\u03b3EleutherAI\n\u03b4University of Cambridge\u03f5Stability AI\u03b6Meta AI Research\u03b7InstaDeep\nAbstract\nLarge Language Models (LLMs) went from\nnon-existent to ubiquitous in the machine learn-\ning discourse within a few years. Due to the\nfast pace of the field, it is difficult to identify\nthe remaining challenges and already fruitful\napplication areas. In this paper, we aim to es-\ntablish a systematic set of open problems and\napplication successes so that ML researchers\ncan comprehend the field\u2019s current state more\nquickly and become productive.\nContents\n1 Introduction 1\n2 Challenges 2\n2.1 Unfathomable Datasets . . . . . . 2\n2.2 Tokenizer-Reliance . . . . . . . . 4\n2.3 High Pre-Training Costs . . . . . 6\n2.4 Fine-Tuning Overhead . . . . . . 10\n2.5 High Inference Latency . . . . . . 11", "LLM-Pruner: On the Structural Pruning\nof Large Language Models\nXinyin Ma Gongfan Fang Xinchao Wang\u2217\nNational University of Singapore\nmaxinyin@u.nus.edu, gongfan@u.nus.edu, xinchao@nus.edu.sg\nAbstract\nLarge language models (LLMs) have shown remarkable capabilities in language un-\nderstanding and generation. However, such impressive capability typically comes\nwith a substantial model size, which presents significant challenges in both the\ndeployment, inference, and training stages. With LLM being a general-purpose\ntask solver, we explore its compression in a task-agnostic manner, which aims to\npreserve the multi-task solving and language generation ability of the original LLM.\nOne challenge to achieving this is the enormous size of the training corpus of LLM,\nwhich makes both data transfer and model post-training over-burdensome. Thus,\nwe tackle the compression of LLMs within the bound of two constraints: being task-", "which the model responses are presented to\nGPT-4arerandomlyswappedtoalleviatebias.\n1 Introduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have", "which the model responses are presented to\nGPT-4arerandomlyswappedtoalleviatebias.\n1 Introduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have"], "retrieved_docs_id": ["3ba0afeccb", "bf695e58cf", "45d34e3817", "dfa8d53d52", "dfa8d53d52"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is the process of creating an index for a language model?\n", "true_answer": "The process involves data cleaning and extraction, converting file formats into plain text, dividing the text into smaller chunks, encoding the text into vectors through a language model, and requiring high inference speed for the embedding models.", "source_doc": "RAG.pdf", "source_id": "58f1dd1f89", "retrieved_docs": ["is also summarized as a \u201cRetrieve\u201d-\u201cRead\u201d framework\n[Maet al. , 2023a ].\nIndexing\nThe pipeline for obtaining data from the source and building\nan index for it generally occurs in an offline state. Specifi-\ncally, the construction of a data index involves the following\nsteps:1.Data Indexing: This involves cleaning and extracting the\noriginal data, converting different file formats such as PDF,\nHTML, Word, Markdown, etc., into plain text.\n2.Chunking: This involves dividing the loaded text into\nsmaller chunks. This is necessary because language mod-\nels typically have a limit on the amount of context they can\nhandle, so it is necessary to create as small text chunks as\npossible.\n3. Embedding and Creating Index: This is the process of\nencoding text into vectors through a language model. The re-\nsulting vectors will be used in the subsequent retrieval process\nto calculate the similarity between the vector and the problem\nvector.The embedding models require a high inference speed.", "(ES) index. ES is a search engine that allows retrieving strings from a corpus, the documents where\nthey appeared, and the number of times they appeared. (2) a count functionality, built using map-\nreduce (Dean & Ghemawat, 2008), allowing quick iteration over an entire dataset and extraction\nof relevant information, e.g., the character length distribution of documents, duplicates, domain\ncounts, finding personally identifiable information (PII), and more. WIMBD is extendable and can\nbe used to index, count, and analyze other corpora at scale, and the code is publicly available at\ngithub.com/allenai/wimbd .\nUsing these tools, we perform a set of sixteen analyses on ten different corpora used to train language\nmodels, including C4(used to train T5; Raffel et al., 2020), The Pile (used to train Pythia; Gao\net al., 2020; Biderman et al., 2022; 2023), and RedPajama (used to reproduce Llama Touvron et al.,", "Published in Transactions on Machine Learning Research (08/2023)\nFigure 1: Language model. A language model takes text (a prompt) and generates text (a completion)\nprobabilistically. Despite their simple interface, language models can be adapted to a wide range of language\ntasks from question answering to summarization.\n1 Introduction\nBenchmarks orient AI. They encode values and priorities (Ethayarajh & Jurafsky, 2020; Birhane et al., 2022)\nthat specify directions for the AI community to improve upon (Sp\u00e4rck Jones & Galliers, 1995; Sp\u00e4rck Jones,\n2005; Kiela et al., 2021; Bowman & Dahl, 2021; Raji et al., 2021). When implemented and interpreted\nappropriately, they enable the broader community to better understand AI technology and influence its\ntrajectory.\nIn recent years, the AI technology that has arguably advanced the most is foundation models (Bommasani\net al., 2021), headlined by the rise of language models (LMs; Peters et al., 2018; Devlin et al., 2019; Brown", "mixed with training corpora. The tuning process is conducted as language modeling. Notice that\ninstructions and inputs are not accounted for in the loss. Section 4.9.1 shows that the improvements\nin the instruction-following capability can transfer across modalities.\nWe combine Unnatural Instructions [ HSLS22 ] and FLANv2 [ LHV+23] as our instruction dataset.\nUnnatural Instructions is a dataset that was created by using a large language model to generate\ninstructions for various natural language processing tasks. It has 68,478 instruction-input-output\ntriplets in its core dataset. FLANv2 is a collection of datasets that cover diverse types of language\nunderstanding tasks, such as reading comprehension, commonsense reasoning, and closed-book\nquestion answering. We randomly select 54k examples of instructions from FLANv2 to augment our\ninstruction dataset. Details of the training hyperparameter settings are described in Appendix A.2.\n7", "1 Introduction\nLarge language models such as PaLM (Chowdhery et al., 2022), Chinchilla (Ho\ufb00mann et al., 2022), and\nChatGPTamongothers(Brownetal.,2020;Ouyangetal.,2022)haveunlockednewcapabilitiesinperforming\nnatural language processing (NLP) tasks from reading instructive prompts. Prior art has shown that\ninstruction tuning\u2014\ufb01netuning language models on a collection of NLP tasks formatted with instructions\u2014\nfurther enhances the ability of language models to perform an unseen task from an instruction (Wei et al.,\n2021; Sanh et al., 2021; Min et al., 2022).\nInthiswork,weevaluatethemethodsandresultsof open sourced instructiongeneralizatione\ufb00orts,comparing\ntheir \ufb01netuning techniques and methods. And in particular, we identify and evaluate the critical method-\nological improvements in the \u201cFlan 2022 Collection\u201d, which is the term we use for the collection of data and\nmethods for data augmentation and instruction tuning , \ufb01rst implemented and used in Chung et al. (2022). Where"], "retrieved_docs_id": ["58f1dd1f89", "a50ef72c0f", "3ee50c5426", "c60a7f9f84", "82c42a6390"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the ChipNeMo-70B model perform compared to GPT-4 in engineering assistant chatbot and EDA scripts generation?\n", "true_answer": "The ChipNeMo-70B model outperforms the GPT-4 on two use cases, namely engineering assistant chatbot and EDA scripts generation.", "source_doc": "ChipNemo.pdf", "source_id": "a6c3d05123", "retrieved_docs": ["processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving", "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "results on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks\nwhere we check for fully accurate code. For \u201cHard\u201d bench-\nmarks in Figure 9 we check for partial correctness of the\ncode, which is evaluated by a human user on a 0-10 scale.\nChipNeMo-70B-Steer performs significantly better than off-\nthe-shelf GPT-4 and LLaMA2-70B-Chat model.\nFigure 8: EDA Script Generation Evaluation Results, Pass@5\nAs seen in Figure 8, models like GPT-4 and LLaMA2-70B-\nChat have close to zero accuracy for the Python tool where\nthe domain knowledge related to APIs of the tool are neces-\nsary. This shows the importance of DAPT. Without DAPT,\nthe model had little to no understanding of the underlying\nAPIs and performed poorly on both automatic and human\nevaluated benchmarks. Our aligned model further improved\nthe results of DAPT because our domain instructional data\nhelps guide the model to present the final script in the most\nuseful manner. An ablation study on inclusion of domain", "uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "5.3 Guanaco: QL ORA trained on OASST1 is a State-of-the-art Chatbot\nBased on our automated and human evaluations, we find that the top QLORAtuned model, Guanaco\n65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\npairwise comparisons - the highest reported to date.\nThe Vicuna benchmark [ 10] results relative to ChatGPT are shown in Table 6. We find that Guanaco\n65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.\nGuanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its\nweights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage\npoints of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a"], "retrieved_docs_id": ["e6b9ba907a", "af6e8c3fb2", "cf9d13203d", "c7d05c4b43", "646d2aa195"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How is LLM performance used in reinforcement learning for a rewriter module?\n", "true_answer": "LLM performance is used as a reward in reinforcement learning for a rewriter module, allowing the rewriter to adjust retrieval queries and improve the downstream task performance of the reader.", "source_doc": "RAG.pdf", "source_id": "79cd640612", "retrieved_docs": ["Retrieve-Read process, utilizing LLM performance as a\nreward in reinforcement learning for a rewritter module.\nThis allows the rewritter to adjust retrieval queries, im-\nproving the downstream task performance of the reader.\nSimilarly, modules can be selectively replaced in ap-\nproaches like Generate-Read [Yuet al. , 2022 ], where the\nLLM generation module replaces the retrieval module.", "igate alignment issues. PRCA [Yang et al. , 2023b ]lever-\naged reinforcement learning to train a context adapter\ndriven by LLM rewards, positioned between the re-\ntriever and generator. It optimizes the retrieved in-\nformation by maximizing rewards in the reinforcement\nlearning phase within the labeled autoregressive pol-\nicy. AAR [Yuet al. , 2023b ]proposed a universal plu-\ngin that learns LM preferences from known-source\nLLMs to assist unknown or non-co-finetuned LLMs.\nRRR [Maet al. , 2023a ]designed a module for rewriting\nqueries based on reinforcement learning to align queries\nwith documents in the corpus.\n\u2022Validation Module: In real-world scenarios, it is notalways guaranteed that the retrieved information is reli-\nable. Retrieving irrelevant data may lead to the occur-\nrence of illusions in LLM. Therefore, an additional val-\nidation module can be introduced after retrieving docu-\nments to assess the relevance between the retrieved doc-", "propose the Automatic Prompt Engineer (APE)\nmethod, which leverages LLMs to generate, score,\nand rephrase instruction-following zero- and few-\nshot prompts. Longpre et al. [340] describe and an-\nalyze the steps taken to create an improved version\nof the Flan collection [ 598] used to train FLAN-\nPaLM [ 93]. When trained on this data, the authors\nfind that the improved model performance stems\nfrom more diverse tasks by inverting input-output\npairs and data augmentation techniques such as\nmixing zero-shot and few-shot prompts. Honovich\net al. [209] generate a large dataset of natural lan-\nguage instructions using a pre-trained LLM to gen-\nerate and then rephrase instructions. They show\nthat a T5 (\"LM-adapted\") fine-tuned on this data\noutperforms other instruction fine-tuned T5 models\nsuch as T0++ [475] and Tk-Instruct [589].\nReinforcement Learning From Human Feed-\nback (RLHF) is a variation of RL that incor-\nporates feedback from humans in the form of re-", "32\nof training computations to CPUs if desired.\n5 A DAPTATION OF LLM S\nAfter pre-training, LLMs can acquire the general abilities\nfor solving various tasks. However, an increasing number\nof studies have shown that LLM\u2019s abilities can be further\nadapted according to specific goals. In this section, we\nintroduce two major approaches to adapting pre-trained\nLLMs, namely instruction tuning and alignment tuning. The\nformer approach mainly aims to enhance (or unlock) the\nabilities of LLMs, while the latter approach aims to align the\nbehaviors of LLMs with human values or preferences. Fur-\nther, we will also discuss efficient tuning and quantization\nfor model adaptation in resource-limited settings. In what\nfollows, we will introduce the four parts in detail.\n5.1 Instruction Tuning\nIn essence, instruction tuning is the approach to fine-tuning\npre-trained LLMs on a collection of formatted instances in\nthe form of natural language [67], which is highly related", "30\ntures and pre-training objectives are in need to analyze how\nthe choices of the architecture and pre-training tasks affect\nthe capacity of LLMs, especially for encoder-decoder archi-\ntectures. Despite the effectiveness of decoder-only architec-\nture, it is also suggested to make more diverse exploration\non architecture design. Besides the major architecture, the\ndetailed configuration of LLM is also worth attention, which\nhas been discussed in Section 4.2.2.\n4.3 Model Training\nIn this part, we review the important settings, techniques,\nor tricks for training LLMs.\n4.3.1 Optimization Setting\nFor parameter optimization of LLMs, we present the com-\nmonly used settings for batch training, learning rate, opti-\nmizer, and training stability.\nBatch Training. For language model pre-training, existing\nwork generally sets the batch size to a large number ( e.g.,\n2,048 examples or 4M tokens) to improve the training\nstability and throughput. For LLMs such as GPT-3 and"], "retrieved_docs_id": ["79cd640612", "8d0a82337c", "65665a1a99", "eaf8999042", "6f172cf8fc"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does IRCoT use retrieval in its thought chain?\n", "true_answer": "IRCoT retrieves documents for each generated sentence, introducing retrieval at every step of the thought chain.", "source_doc": "RAG.pdf", "source_id": "89c7fd1852", "retrieved_docs": ["ITER-RETGEN [Shao et al. , 2023 ]collaboratively utilizes\n\u201dretrieval-enhanced generation\u201d and \u201dgeneration-enhanced\nretrieval\u201d for tasks requiring reproduction of information.\nThat is, the model uses the content needed to complete the\ntask to respond to the input task, and these target contents\nserve as the information context for retrieving more relevant\nknowledge. This helps to generate better responses in another\niteration.\nIRCoT [Trivedi et al. , 2022 ]also explores retrieving docu-\nments for each generated sentence, introducing retrieval at\nevery step of the thought chain. It uses CoT to guide the re-\ntrieval and uses the retrieval results to improve CoT, ensuring\nsemantic completeness.\nAdaptive Retrieval\nIndeed, the RAG methods described in the previous two\nsections follow a passive approach where retrieval is prior-", "models for complementing with each other in solving com-\nplex cases of classic NLP tasks [772]. Another promising di-\nrection is to conduct human-machine collaborative research\n(e.g., conversational translation [768]) on NLP tasks, sinceLLMs can effectively understand human instructions and\nmake meaningful responses.\n8.1.2 LLM for Information Retrieval\nThe goal of information retrieval (IR) systems is to assist\nusers in discovering ideal information resources (typically\ndocuments) and mitigating the information overload issue.\nTypically, contemporary IR systems adopt a retrieve-then-\nrerank pipeline framework [54]. Within this framework,\nthe retriever initially retrieves relevant information from a\nlarge-scale corpus, and the reranker subsequently performs\nmulti-stage ranking procedure to acquire the most relevant\ninformation [773]. Since the advent of LLMs has significant\nimpact on the way of information access, we discuss how\nit advances the development of IR from two main aspects,", "contexts, but we perform a case study with open-\ndomain question answering on NaturalQuestions-\nOpen to better understand this trade-off in existing\nlanguage models.\nWe use language models in a standard retriever-\nreader setup. A retrieval system (Contriever, fine-\ntuned on MS-MARCO) takes an input query from\nNaturalQuestions-Open and returns the kdocu-\nments from Wikipedia with the highest relevance\nscore. To condition language models on these re-\ntrieved documents, we simply include them in the\nprompt. We evaluate retriever recall and reader\naccuracy (whether any of the annotated answers\nappear in the predicted output) as a function of the\nnumber of retrieved documents k. We use a subset\nof NaturalQuestions-Open where the long answer\nis a paragraph (as opposed to a table or a list).\nFigure 11 presents retriever recall and open-", "Figure 12: Example of information retrieval (passage ranking). An example instance for information\nretrieval from MS MARCO.\nWe focus here on the passage ranking task: given a query qand a large corpus Cof passages, systems\nmust output a list of the top- kpassages from Cin decreasing \u201crelevance\u201d to q. We specifically study this\nin the context of re-ranking : since Cis typically extremely large (e.g. |C|>10Mpassages), we consider\nonly ranking the top- kpassages among a set retrieved for q(i.e.M(q)where|M(q)|\u226a|C|) by an efficient\nexternal retrieval mechanism (e.g. BM25; Robertson & Zaragoza, 2009).\nIR differs fundamentally from the other tasks we consider in this work, as each test example (i.e. a query)\nentails processing a large set of passages and will likely invoke the LM numerous times to do so.37Because\nof this, IR tasks have received very little attention in the few-shot in-context learning with language models,", "phase to capture key semantic meanings. In the later\nstages of this process, larger blocks with more contex-\ntual information are provided to the language model\n(LM). This two-step retrieval method helps strike a bal-\nance between efficiency and contextually rich responses.\n\u2022StepBack-prompt: Integrated into the RAG process,\nthe StepBack-prompt approach [Zheng et al. , 2023 ]en-\ncourages LLM to step back from specific instances and\nengage in reasoning about the underlying general con-\ncepts or principles. Experimental findings indicate a sig-\nnificant performance improvement in various challeng-\ning, inference-intensive tasks with the incorporation of\nbackward prompts, showcasing its natural adaptability\nto RAG. The retrieval-enhancing steps can be applied in\nboth the generation of answers to backward prompts and\nthe final question-answering process.\n\u2022Subqueries: Various query strategies can be employed in\ndifferent scenarios, including using query engines pro-"], "retrieved_docs_id": ["89c7fd1852", "f738db1ca0", "3d4e4c3f8a", "fb0c34583a", "ad03b3dcc5"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How is the IT stage typically conducted in the context of Multi-Task Language Model Learning?\n", "true_answer": "The IT stage is typically conducted within the paradigm of Supervised Fine-Tuning (SFT).", "source_doc": "multimodal.pdf", "source_id": "0806fe2e1a", "retrieved_docs": ["multi-task \ufb01ne-tuning rather than for in-context learning without weight updates.\nAnother approach to increasing generality and transfer-learning capability in language models is multi-task learning\n[Car97 ], which \ufb01ne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for\neach one. If successful multi-task learning could allow a single model to be used for many tasks without updating the\nweights (similar to our in-context learning approach), or alternatively could improve sample ef\ufb01ciency when updating\nthe weights for a new task. Multi-task learning has shown some promising initial results [ LGH+15,LSP+18] and\nmulti-stage \ufb01ne-tuning has recently become a standardized part of SOTA results on some datasets [ PFB18 ] and pushed\nthe boundaries on certain tasks [ KKS+20], but is still limited by the need to manually curate collections of datasets and", "their performance on the target task. The benefits of IT in efficient MLLMs are manifold. Firstly,\nit enables the model to adapt to a wide range of tasks with minimal changes to its architecture\nor training data. This makes it a flexible and efficient approach for fine-tuning on diverse tasks.\nSecondly, IT allows for better generalization, as the model learns to follow instructions and apply\nits knowledge to new and unseen tasks.\nThe IT stage is typically conducted within the paradigm of Supervised Fine-Tuning (SFT). SFT\ndatasets are often derived from a portion of the pre-training data, which is transformed into an\ninstruction-based format, presented in the form of single-turn or multi-turn dialogue structures.\nGiven an image Xvand its caption, a conversation data (X1\nq, X1\na, . . . , XT\nq, XT\na)can be generated,\nwhere T is the total number of turns. Typically, we can organize the data into a sequence of instruc-\ntions and responses following [7], where the instruction Xt", "Language Models are Unsupervised Multitask Learners\nAlec Radford*1Jeffrey Wu*1Rewon Child1David Luan1Dario Amodei**1Ilya Sutskever**1\nAbstract\nNatural language processing tasks, such as ques-\ntion answering, machine translation, reading com-\nprehension, and summarization, are typically\napproached with supervised learning on task-\nspeci\ufb01c datasets. We demonstrate that language\nmodels begin to learn these tasks without any ex-\nplicit supervision when trained on a new dataset\nof millions of webpages called WebText. When\nconditioned on a document plus questions, the an-\nswers generated by the language model reach 55\nF1 on the CoQA dataset - matching or exceeding\nthe performance of 3 out of 4 baseline systems\nwithout using the 127,000+ training examples.\nThe capacity of the language model is essential\nto the success of zero-shot task transfer and in-\ncreasing it improves performance in a log-linear\nfashion across tasks. Our largest model, GPT-2,\nis a 1.5B parameter Transformer that achieves", "One potential route towards addressing these issues is meta-learning1\u2013 which in the context of language models means\nthe model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities\nat inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [ RWC+19]\nattempts to do this via what we call \u201cin-context learning\u201d, using the text input of a pretrained language model as a form\nof task speci\ufb01cation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task\nand is then expected to complete further instances of the task simply by predicting what comes next.\nWhile it has shown some initial promise, this approach still achieves results far inferior to \ufb01ne-tuning \u2013 for example\n[RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind", "short year later, Radford et al. (2019), Ra\ufb00el et al. (2020), and Lewis et al. (2020) popularized the notion that\ndownstream tasks\u2014and multiple tasks\u2014can be jointly learned by directly using the pretrained LM head to\ngenerate the answers in natural language (cf. task-speci\ufb01c numerical class labels), the task-general nature of\nthese generative models became the precursor to many multitask transfer learning studies (McCann et al.,\n2018; Khashabi et al., 2020; Ye et al., 2021; Vu et al., 2020), which in turn led to the \ufb01rst wave of instruction\ntuning as described in Section 2.\nThe continuing advancement in research on the pretraining corpora, architectures and pretraining objectives\nofLMsalsohasalargeimpactoninstructiontuning. Asof2022,decoder-onlyleft-to-rightcausalTransformers\ndominate the market of models larger than 100B (Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021;\n10"], "retrieved_docs_id": ["4e6d19e6c4", "0806fe2e1a", "5981cc3258", "aa33f94ed2", "e0bc5677c2"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "What is the computational complexity of State Space Models (SSMs) during inference?\n", "true_answer": "State Space Models (SSMs) offer near-linear computational complexity during inference.", "source_doc": "multimodal.pdf", "source_id": "bb2e9ee3f0", "retrieved_docs": ["Figure 13: The elements(left) block(middle) and architecture(right) in RWKV [151].\nThis approach parallelizes computations during training and maintains constant computational and\nmemory complexity during inference.\nState Space Models (SSMs) [152] can be formulated as a type of RNN for efficient autoregressive\ninference and have emerged as a promising alternative to attention mechanisms, offering near-linear\ncomputational complexity compared to the quadratic complexity of attention. SSMs are formulated\nas x\u2019(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), mapping a single-dimension input signal u(t) to an N-\ndimension latent state x(t) before projecting it to a single-dimension output signal y(t), with A, B, C,\nand D being parameters learned by gradient descent [152]. Several techniques have been proposed\nto enhance SSMs, such as the Structured State Space sequence model (S4) [152], which refines\nSSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)", "sequence length requires /u1D442./u1D435/u1D43F/u1D437/u1D441 )time and memory; this is the root of the fundamental e\ufb03ciency bottleneck\naddressed in Section 3.3.\nGeneral State Space Models. We note that the term state space model has a very broad meaning which simply\nrepresents the notion of any recurrent process with a latent state. It has been used to refer to many disparate\nconcepts in di\ufb00erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner\net al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)),\nKalman \ufb01lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS)\n(machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4", "SSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)\nmodel [153], which proposes fully diagonal parameterization of state spaces for greater efficiency.\nH3 stacks two SSMs to interact with their output and input projection, bridging the gap between\nSSMs and attention while adapting to modern hardware. Mamba [77], a selective state space model,\nhas been introduced as a strong competitor to the Transformer architecture in large language models.\nMamba incorporates a selection mechanism to eliminate irrelevant data and develops a hardware-\naware parallel algorithm for recurrent operation. This results in competitive performance compared\nto LLMs of the same capacity, with faster inference speeds that scale linearly with time and con-\nstant memory usage. In conclusion, State Space Models offer significant potential as an alternative\nto attention mechanisms by providing near-linear computational complexity and effectively captur-", "ProjectDiscretize\ud835\udc65!\u210e!\"#\u210e!\ud835\udc66!\ud835\udc34\ud835\udc36!\ud835\udc35!Selection MechanismGPU SRAMGPU HBM\u2206!Selective State Space ModelwithHardware-aware State ExpansionFigure 1: ( Overview .) Structured SSMs independently map each channel (e.g. /u1D437= 5) of an input xto output /u1D466through a higher\ndimensional latent state /uni210E(e.g./u1D441= 4). Prior SSMs avoid materializing this large e\ufb00ective state ( /u1D437/u1D441, times batch size /u1D435and sequence\nlength /u1D43F) through clever alternate computation paths requiring time-invariance: the .\u2206,A,B,C)parameters are constant across\ntime. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to\nonly materialize the expanded states in more e\ufb03cient levels of the GPU memory hierarchy.\n2 State Space Models\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are", "insufficient for length generalization in the context\nof reasoning tasks. Instead, they propose combin-\ning in-context learning and scratchpad/chain-of-\nthought reasoning to enable LLMs to generalize to\nunseen sequence lengths in- and out-of-distribution,\nwith performance scaling with model size. The au-\nthors report that fine-tuning can further improve\nmodel performance dependent on the task perfor-\nmance of the baseline.\nTransformer Alternatives While Transformers\nare the dominant paradigm in LLMs today due to\ntheir strong performance, several more efficient\nalternative architectures exist. One line of work\ntries to replace the attention mechanism using state\nspace models (SSMs), which offer near-linear com-\nputational complexity w.r.t. the sequence length.\nDao et al. [108] investigate the weaknesses of state\nspace models (SSMs) in language modeling and\nfind that existing approaches struggle with recall-\ning previous tokens and comparing tokens in the"], "retrieved_docs_id": ["bb2e9ee3f0", "79e095312d", "85b5cac71b", "ac9544cf55", "9e5f877b03"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How did the domain-adapted retrieval model perform compared to Sentence Transformer and e5small unsupervised in the Specs category?\n", "true_answer": "The domain-adapted model performed 2x better than the original e5small unsupervised model and 30% better than the sentence transformer in the Specs category.", "source_doc": "ChipNemo.pdf", "source_id": "79a9ff88c8", "retrieved_docs": ["retrieval. It includes about 1.8K documents, which were\nsegmented into 67K passages, each about 512 characters.\nFirst, we compare our domain adapted retrieval model with\nSentence Transformer (Reimers & Gurevych, 2019) and\ne5small unsupervised (Wang et al., 2022) on each category.\nEach model fetches its top 8 passages from the data store.\nAs shown in Figure 6, our domain-adapted model performed\n2x better than the original e5small unsupervised model and\n30% better than sentence transformer.\nFigure 6: Retrieval Model Accuracy Comparison\nThe queries in the Specs category are derived directly from\npassages in the documents, so their answers are often nicely\ncontained in a concise passage and clearly address the query.\nOn the other hand, the queries of the Testbench and Build\ncategories are not directly derived from passages, so their\nanswers were often not as apparent in the fetched passages\nand required more context (see Appendix A.8 for detailed", "the prompt together with the question, which grounds the\nLLM to produce more accurate answers. We find that using\na domain adapted language model for RAG significantly\nimproves answer quality on our domain specific questions.\nAlso, we find that fine-tuning an off-the-shelf unsupervised\npre-trained dense retrieval model with a modest amount\nof domain specific training data significantly improves re-\ntrieval accuracy. Our domain-adapted RAG implementation\ndiagram is illustrated on Figure 3.\nFigure 3: RAG Implementation Variations\nWe created our domain adapted retrieval model by fine-\ntuning the e5small unsupervised model (Wang et al., 2022)\nwith 3000 domain specific auto-generated samples using the\nTevatron framework (Gao et al., 2022). We refer readers to\nthe details on the sample generation and training process in\nAppendix A.8.\nEven with the significant gains that come with fine-tuning a\nretrieval model, the fact remains that retrieval still struggles", "vant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation", "the potential of combining context from an in-\nformation retrieval system with a pretrained lan-\nguage models for unsupervised question answering.\nO\u2019Connor and Andreas (2021) found that many\ninformation-destroying operations had marginal ef-\nfects on Transformer LMs\u2019 predictions. Krishna\net al. (2022) found that long-context neural gen-\neration in modestly-sized Transformer language\nmodels degenerates because models fail to prop-\nerly condition on long context. Finally, studying\nlong-context models, Sun et al. (2021) found that\nlonger contexts improves prediction of only a few\ntokens, an empirical finding consistent with the\ntheory of Sharan et al. (2018), who showed that\nsequence distributions with bounded mutual infor-\nmation necessarily lead to marginal average predic-\ntion benefits from increasingly long context. Qin\net al. (2023) analyze how efficient Transformers\nperform on a variety of long-context downstream\nNLP tasks, finding that long-context transformers", "Table 1: A list of the different tasks and datasets used in our experiments.\nTask Datasets\nNatural language inference SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]\nQuestion Answering RACE [30], Story Cloze [40]\nSentence similarity MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6]\nClassi\ufb01cation Stanford Sentiment Treebank-2 [54], CoLA [65]\nbut is shuf\ufb02ed at a sentence level - destroying long-range structure. Our language model achieves a\nvery low token level perplexity of 18.4 on this corpus.\nModel speci\ufb01cations Our model largely follows the original transformer work [ 62]. We trained a\n12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12\nattention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\nWe used the Adam optimization scheme [ 27] with a max learning rate of 2.5e-4. The learning rate"], "retrieved_docs_id": ["79a9ff88c8", "ad55562468", "28f0897bcb", "cc85f3aa39", "f3ec3a526d"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a significant challenge in the practical application of multimodal large language models?\n", "true_answer": "The phenomenon of hallucination is a significant challenge in the practical application of multimodal large language models.", "source_doc": "hallucination.pdf", "source_id": "114f3dada8", "retrieved_docs": ["Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "is a document and the output is its summary. So we can feed the input document into the language\nmodel and then produce the generated summary.\nDespite the successful applications in natural language processing, it is still struggling to natively use\nLLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal\nperception is a necessity to achieve arti\ufb01cial general intelligence, in terms of knowledge acquisition\nand grounding to the real world. More importantly, unlocking multimodal input [ TMC+21,HSD+22,\nWBD+22,ADL+22,AHR+22,LLSH23 ] greatly widens the applications of language models to\nmore high-value areas, such as multimodal machine learning, document intelligence, and robotics.\nIn this work, we introduce KOSMOS -1, a Multimodal Large Language Model (MLLM) that can\nperceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e.,", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "ods, to deploy LLMs efficiently and effectively in real-world\nrecommender systems. In addition, existing LLMs have\nlimited capacities in long context modeling, make it difficult\nto process the huge amount of user-item interaction data.\nImproved context length extension and context information\nutilization approaches should be developed to improve the\nmodeling capacities of LLMs in long interaction sequences.\n8.1.4 Multimodal Large Language Model\nIn existing literature [823, 824], multimodal models mainly\nrefer to the models that can process and integrate informa-\ntion of various modalities ( e.g., text, image, and audio) frominput, and further produce corresponding output in certain\nmodalities. In this part, we mainly focus on the multimodal\nextension of LLMs by enabling the information modeling\nof non-textual modalities, especially the vision modality,\ncalled multimodal large language models (MLLMs) [797]49. To\nstart our discussion, we specify the input to be text-image", "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the"], "retrieved_docs_id": ["542e5c49da", "74bb21ad4f", "ac70fcc9f2", "593e38b258", "da0a465b6c"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How do large language models (LLMs) decide when to search for relevant queries?\n", "true_answer": "LLMs decide to search for a relevant query when they think it is necessary.", "source_doc": "RAG.pdf", "source_id": "8d605d7952", "retrieved_docs": ["what we say and what we want.\n5 Do LLMs Really Know Anything?\nTurning an LLM into a question-answering sys-\ntem by a) embedding it in a larger system, and\nb) using prompt engineering to elicit the required\nbehaviour exempli\ufb01es a pattern found in much\ncontemporary work. In a similar fashion, LLMs\ncan be used not only for question-answering,\nbut also to summarise news articles, to generate\nscreenplays, to solve logic puzzles, and to trans-\nlate between languages, among other things.\nThere are two important takeaways here. First,\nthe basic function of a large language model,\nnamely to generate statistically likely continua-\ntions of word sequences, is extraordinarily versa-\ntile. Second, notwithstanding this versatility, at\nthe heart of every such application is a model do-\ning just that one thing: generating statistically\nlikely continuations of word sequences.\nWith this insight to the fore, let\u2019s revisit the\nquestion of how LLMs compare to humans, and", "Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "Challenges and Applications of Large Language Models\nJean Kaddour\u03b1,\u2020,\u2217, Joshua Harris\u03b2,\u2217, Maximilian Mozes\u03b1,\nHerbie Bradley\u03b3,\u03b4,\u03f5, Roberta Raileanu\u03b6, and Robert McHardy\u03b7,\u2217\n\u03b1University College London\u03b2UK Health Security Agency\u03b3EleutherAI\n\u03b4University of Cambridge\u03f5Stability AI\u03b6Meta AI Research\u03b7InstaDeep\nAbstract\nLarge Language Models (LLMs) went from\nnon-existent to ubiquitous in the machine learn-\ning discourse within a few years. Due to the\nfast pace of the field, it is difficult to identify\nthe remaining challenges and already fruitful\napplication areas. In this paper, we aim to es-\ntablish a systematic set of open problems and\napplication successes so that ML researchers\ncan comprehend the field\u2019s current state more\nquickly and become productive.\nContents\n1 Introduction 1\n2 Challenges 2\n2.1 Unfathomable Datasets . . . . . . 2\n2.2 Tokenizer-Reliance . . . . . . . . 4\n2.3 High Pre-Training Costs . . . . . 6\n2.4 Fine-Tuning Overhead . . . . . . 10\n2.5 High Inference Latency . . . . . . 11", "maintaining user privacy and data security are crucial con-\nsiderations when applying LLMs to real-world scenarios.\n9 C ONCLUSION AND FUTURE DIRECTIONS\nIn this survey, we have reviewed the recent progress of large\nlanguage models (LLMs), and introduced the key concepts,\nfindings, and techniques for understanding and utilizing\nLLMs. We focus on the large-sized models ( i.e.,having a size\nlarger than 10B) while excluding the contents of early pre-\ntrained language models ( e.g., BERT and GPT-2) that have\nbeen well covered in the existing literature. In particular,our survey has discussed four important aspects of LLMs,\ni.e.,pre-training, adaptation, utilization, and evaluation. For\neach aspect, we highlight the techniques or findings that are\nkey to the success of LLMs. Furthermore, we also summa-\nrize the available resources for developing LLMs and dis-\ncuss important implementation guidelines for reproducing\nLLMs. This survey tries to cover the most recent literature", "the query. As a solution, LLM can be utilized to rewrite the\nquery for enhancing the understanding of the query intent\nand incorporating additional knowledge into the query\nthrough well-designed instructions. The rewritten query\ncan take the form of an improved version of the original\nquery [791], a document in the corpus that related to the\nquery [792], or an expansion of the query that concatenated\nwith a pseudo generated document [793]. In addition, docu-\nments can also be expanded with queries that are generated\nbased on the original documents using LLMs for context\nextension [794].\nRemaining Issues. In this part, we further discuss several\nimportant issues to apply LLMs to improve IR systems.\nFirst, though LLMs are capable of being as general-purpose\ntask solvers, they are not directly well suited for existing\nIR systems: they require high overhead for inference [774,\n782], have limitations in modeling long texts or document"], "retrieved_docs_id": ["95f3a4d61c", "af911eac69", "bf695e58cf", "3ba0afeccb", "de435e084d"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is one potential application of large language models in GPU ASIC and Architecture design?\n", "true_answer": "One potential application of large language models in GPU ASIC and Architecture design is as an engineering assistant chatbot that understands internal hardware terminology and can assist engineers with their tasks.", "source_doc": "ChipNemo.pdf", "source_id": "f23b3625e0", "retrieved_docs": ["[25] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V . Korthikanti, D. Vainbrand,\nP. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia. Efficient large-scale\nlanguage model training on gpu clusters using megatron-lm. In Proceedings of the International\nConference for High Performance Computing, Networking, Storage and Analysis , SC \u201921, New\nYork, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450384421. doi:\n10.1145/3458817.3476209. URL https://doi.org/10.1145/3458817.3476209 .\n[26] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with\nhuman feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems , volume 35, pages 27730\u201327744.", "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU\u2217\nYixin Song, Zeyu Mi\u2020, Haotong Xie and Haibo Chen\nInstitute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University\nAbstract\nThis paper introduces PowerInfer, a high-speed Large Lan-\nguage Model (LLM) inference engine on a personal computer\n(PC) equipped with a single consumer-grade GPU. The key\nunderlying the design of PowerInfer is exploiting the high\nlocality inherent in LLM inference, characterized by a power-\nlaw distribution in neuron activation. This distribution indi-\ncates that a small subset of neurons, termed hot neurons , are\nconsistently activated across inputs, while the majority, cold\nneurons , vary based on speci\ufb01c inputs. PowerInfer exploits\nsuch an insight to design a GPU-CPU hybrid inference en-\ngine: hot-activated neurons are preloaded onto the GPU for\nfast access, while cold-activated neurons are computed on\nthe CPU, thus signi\ufb01cantly reducing GPU memory demands", "Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism\nMohammad Shoeybi1 2Mostofa Patwary1 2Raul Puri1 2Patrick LeGresley2Jared Casper2\nBryan Catanzaro2\nAbstract\nRecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in Natural Language Processing\napplications. However, very large models can be\nquite dif\ufb01cult to train due to memory constraints.\nIn this work, we present our techniques for train-\ning very large transformer models and implement\na simple, ef\ufb01cient intra-layer model parallel ap-\nproach that enables training transformer models\nwith billions of parameters. Our approach does\nnot require a new compiler or library changes, is\northogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with\nthe insertion of a few communication operations\nin native PyTorch. We illustrate this approach\nby converging transformer based models up to", "on a GPU, there is no need for communicating updated\nparameter values in this formulation.\nWe present further details about the hybrid model and data\nparallelism and handling random number generation in Ap-\npendix B for reference. In summary, our approach as de-\nscribed above is simple to implement, requiring only a few\nextra all-reduce operations added to the forward and back-\nward pass. It does not require a compiler, and is orthogonal\nand complementary to the pipeline model parallelism advo-\ncated by approaches such as (Huang et al., 2018).4. Setup\nPretrained language understanding models are central tasks\nin natural language processing and language understanding.\nThere are several formulations of language modeling. In\nthis work we focus on GPT-2 (Radford et al., 2019), a left-\nto-right generative transformer based language model, and\nBERT (Devlin et al., 2018), a bi-directional transformer\nmodel based on language model masking. We explain our", "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 1. Model (blue) and model+data (green) parallel FLOPS\nas a function of number of GPUs. Model parallel (blue): up to\n8-way model parallel weak scaling with approximately 1 billion\nparameters per GPU (e.g. 2 billion for 2 GPUs and 4 billion for\n4 GPUs). Model+data parallel (green): similar con\ufb01guration as\nmodel parallel combined with 64-way data parallel.\na baseline by training a model of 1.2 billion parameters\non a single NVIDIA V100 32GB GPU, that sustains 39\nTeraFLOPs. This is 30% of the theoretical peak FLOPS\nfor a single GPU as con\ufb01gured in a DGX-2H server, and\nis thus a strong baseline. Scaling the model to 8.3 billion\nparameters on 512 GPUs with 8-way model parallelism,\nwe achieve up to 15.1 PetaFLOPs per second sustained\nover the entire application. This is 76% scaling ef\ufb01ciency\ncompared to the single GPU case. Figure 1 shows more\ndetailed scaling results."], "retrieved_docs_id": ["7d7f8b95fd", "079fda9e8c", "56f7d6d82f", "340e70d67f", "e5e81bda86"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What are the two types of evaluation instances in VHTest?\n", "true_answer": "The two types of evaluation instances in VHTest are \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).", "source_doc": "hallucination.pdf", "source_id": "8ef8344de6", "retrieved_docs": ["VHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual\nproperties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which\nemerge from comparisons across multiple objects, such as relative size, relative position, and\ncounting. Based on such categorization, the authors further defined 8 visual hallucination modes,\nproviding a very detailed evaluation of hallucination in MLLMs. Furthermore, the collected 1,200\nevaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no\nquestion\" (YNQ). Such design enables this benchmark to evaluate both generative and discriminative\ntasks.\nComparison of mainstream models We compare the mainstream MLLMs on some represen-\ntative benchmarks, providing a holistic overview of their performance from different dimensions.\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We", "68\non various abilities ( e.g., knowledge utilization and hu-\nman alignment), and thus it is common that they are as-\nsessed with multiple evaluation approaches. In addition\nto benchmark-based evaluation, human-based and model-\nbased approaches have also been widely used to evaluate\nthe advanced abilities of fine-tuned LLMs. Next, we will\nintroduce the two evaluation methods.\n\u2022Human-based evaluation. Unlike automatic evaluation\nfor basic abilities, human evaluation typically considers\nmore factors or abilities in real-world use, such as hu-\nman alignment and tool manipulation. In this evaluation\napproach, test tasks are usually in the form of open-\nended questions, and human evaluators are invited to make\njudgments on the quality of answers generated by LLMs.\nTypically, there are two main types of scoring methods\nfor human evaluators: pairwise comparison and single-\nanswer grading. In pairwise comparison, given the same\nquestion, humans are assigned two answers from different", "F Comparison with other evaluations 155\nG Contamination 155\nH Priority system 156\nI Models 158\nJ Adaptation 160\nJ.1 Formatting test instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nJ.2 Formatting the remainder of the prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\nJ.3 Decoding parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\nJ.4 Adaptation methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n13", "tonomously judge inputs and generate accurate answers.\n7 RAG Evaluation\nIn exploring the development and optimization of RAG, ef-\nfectively evaluating its performance has emerged as a central\nissue. This chapter primarily discusses the methods of eval-\nuation, key metrics for RAG, the abilities it should possess,\nand some mainstream evaluation frameworks.\n7.1 Evaluation Methods\nThere are primarily two approaches to evaluating the ef-\nfectiveness of RAG: independent evaluation and end-to-endevaluation [Liu, 2023 ].\nIndependent Evaluation\nIndependent evaluation includes assessing the retrieval mod-\nule and the generation (read/synthesis) module.\n1.Retrieval Module\nA suite of metrics that measure the effectiveness of sys-\ntems (like search engines, recommendation systems, or\ninformation retrieval systems) in ranking items accord-\ning to queries or tasks are commonly used to evaluate\nthe performance of the RAG retrieval module. Exam-\nples include Hit Rate, MRR, NDCG, Precision, etc.", "thermore, it can evolve into a multi-turn interaction frame-\nwork, where LLM-based evaluators provide natural lan-\nguage feedback to existing solutions from task solvers [891].\nThis framework evaluates the ability of LLMs to leverage\nlanguage feedback for refining self-generated solutions.\nEvaluation Methods. A common method for LLM-based\nevaluation involves prompting LLMs with specific instruc-\ntions. To further improve the quality of LLM-based eval-\nuation, recent work proposes to prompt LLMs with varied\ncontexts to generate diverse evaluation feedback. These con-\ntexts vary in aspects such as the candidate order [647, 727],\nevaluation perspectives [892, 893] ( e.g., relevance, clarity,\noriginality), and evaluation explanation [647]. The gener-\nated multiple evaluation feedbacks are then aggregated to\nproduce a final evaluation result, which makes the evalua-\ntion process less prone to biases from individual feedback\nand allows for a more thorough evaluation by covering"], "retrieved_docs_id": ["8ef8344de6", "3603224301", "968f18c524", "a580bf7e9b", "a730d879eb"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the name of the state-of-the-art proprietary chat model?\n", "true_answer": "GPT-4", "source_doc": "ChipNemo.pdf", "source_id": "8f6b70d3f1", "retrieved_docs": ["are observed with other models). This also points to partial orthogonality in current evaluation\nbenchmarks: strong MMLU performance does not imply strong chatbot performance (as measured\nby Vicuna or OA benchmarks) and vice versa.\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1\ndataset collection guidelines explicitly forbid the use of GPT models. The next best model trained\non only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points\nlower than Guanaco on the Vicuna benchmark (see Table 6). Overall, these results show that 4-bit\nQLORAis effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\npotential for future work via QLORAtuning on specialized open-source data, which produces models\nthat can compete with the very best commercial models that exist today.", "5.3 Guanaco: QL ORA trained on OASST1 is a State-of-the-art Chatbot\nBased on our automated and human evaluations, we find that the top QLORAtuned model, Guanaco\n65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\npairwise comparisons - the highest reported to date.\nThe Vicuna benchmark [ 10] results relative to ChatGPT are shown in Table 6. We find that Guanaco\n65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.\nGuanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its\nweights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage\npoints of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a", "[50] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model\nwith parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196 , 2023.\n[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\n2022.\n[52] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068 , 2022.\n13", "}} </s> ... <s >[ INST ] {{ user_msg_N }} [/ INST ] {{ model_answer_N }} </s>\nFine-tuning was performed on the same hardware and software configuration used for Language Adaptation based\non Huggingface Python Library [ 27], using the SFTTrainer over the Leonardo HPC infrastructure. We set the maximum\ntextual content length to 2048 (such as the original chat LLaMA 2-Chat model) and 15k training steps (i.e., a standard\nvalue of 3 epochs). The final models obtained are the following:\n\u2022LLaMAntino-2-7b-chat-hf-ITA\n\u2022LLaMAntino-2-7b-chat-hf-ITA-Ultra\n\u2022LLaMAntino-2-13b-chat-hf-ITA\n\u2022LLaMAntino-2-13b-chat-hf-ITA-Ultra\nIn the future, we aim to be able to release the 70B parameters version, also. In the following snap, it is possible to\nfind a conversation conducted with our LLaMAntino-2-13b-chat-hf-ITA-Ultra model.\n1User : Ciao ! Sono Marco . Oggi sono un po 'triste . Puoi raccontarmi una storia ?", "tools to compensate for the deficiencies of LLMs [80, 81].\nFor example, LLMs can utilize the calculator for accurate\ncomputation [80] and employ search engines to retrieve\nunknown information [81]. More recently, ChatGPT has\nenabled the mechanism of using external plugins (existing\nor newly created apps)12, which are by analogy with the\n\u201ceyes and ears \u201d of LLMs. Such a mechanism can broadly\nexpand the scope of capacities for LLMs.\nIn addition, many other factors ( e.g., the upgrade of\nhardware) also contribute to the success of LLMs. Currently,\nwe limit our discussion to the major technical approaches\nand key findings for developing LLMs.\n2.2 Technical Evolution of GPT-series Models\nDue to the excellent capacity in communicating with hu-\nmans, ChatGPT has ignited the excitement of the AI com-\nmunity since its release. ChatGPT is developed based on the\npowerful GPT model with specially optimized conversation\ncapacities. Considering the ever-growing interest in Chat-"], "retrieved_docs_id": ["1ee92ac678", "646d2aa195", "37f31bb995", "21226ef7e8", "562c27324d"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does the proposed decoding method for MLLMs address the issue of over-trust?\n", "true_answer": "The proposed decoding method for MLLMs addresses the issue of over-trust by introducing a penalty term on the model logits during the beam-search decoding process to mitigate the over-trust issue.", "source_doc": "hallucination.pdf", "source_id": "4f752eeea2", "retrieved_docs": ["are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,\ni.e.,MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all\nthe previous tokens. Such a partial over-trust inclination results in neglecting image tokens and\ndescribing the image content with hallucination. Based on this observation, a decoding method for\nMLLMs grounded in an Over-trust Penalty and a Retrospection- Allocation strategy is proposed.\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding\nprocess to mitigate the over-trust issue. Additionally, to handle the hard cases that cannot be\naddressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed\nto retrospect the presence of summary tokens in the previously generated tokens and reallocate\nthe token selection if necessary.\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered", "images. In addition, Resampler [ ADL+22] is used as an attentive pooling mechanism to reduce the\nnumber of image embeddings.\n2.2 Multimodal Large Language Models (MLLMs)\nAfter obtaining the embeddings of an input sequence, we feed them into the Transformer-based\ndecoder. The left-to-right causal model processes the sequence in an auto-regressive manner, which\nproduces the next token by conditioning on past timesteps. The causal masking is used to mask\nout future information. A softmax classi\ufb01er upon Transformer is used to generate tokens over the\nvocabulary.\nMLLMs serve as general-purpose interfaces [ HSD+22] that can perform interactions with both\nnatural language and multimodal input. The framework is \ufb02exible to handle various data types,\nas long as we can represent input as vectors. MLLMs combine the best of two worlds. First, the\nlanguage models naturally inherit the capabilities of in-context learning and instruction following.", "and preserving user privacy.\nIn light of these challenges, there has been growing attention on the study of efficient MLLMs.\nThe primary objective of these endeavors is to decrease the resource consumption of MLLMs\nand broaden their applicability while minimizing performance degradation. Research on efficient\nMLLMs began with replacing large language models with lightweight counterparts and performing\ntypical visual instruction tuning. Subsequent studies further enhanced capabilities and expanded\nuse cases in the following ways: (1) lighter architectures were introduced with an emphasis on ef-\nficiency, aiming to reduce the number of parameters or computational complexity[25, 13, 18]; (2)\nmore specialized components were developed, focusing on efficiency optimizations tailored to ad-\nvanced architectures or imbuing specific properties, such as locality[19, 17, 12]; and (3) support\nfor resource-sensitive tasks was provided, with some works employing visual token compression", "Figure 1: The timeline of efficient MLLMs.\ninference constitutes the major portion of resource consumption in mllm. Consider a typical scenario\nwhere the model input consists of an image with dimensions of 336\u00d7336pixels and a text prompt\nwith a length of 40 tokens, performing inference with LLaV A-1.5 and a Vicuna-13B LLM backbone\nrequires 18.2T FLOPS and 41.6G of memory usage. The resource-intensive nature of large-scale\nmodels has also sparked concerns about democratization and privacy protection, considering that the\ncurrent mainstream MLLMs, represented by GPT-4V and Gemini, are controlled by a few dominant\ncorporations and operate in the cloud. As demonstrated in the aforementioned experiments, even for\nopen-source MLLMs, high requirements for computation resources make it challenging to run them\non edge devices. This further exacerbates the challenges associated with ensuring equitable access\nand preserving user privacy.", "GPTQ (g128) 3.25 (4.92) 7.06 5.31 4.25 (3.77) 6.57 4.96\nAWQ (g128) 3.25 (4.92) 6.94 5.32 4.25 (3.77) 6.56 4.97\nSQLLM (0.45%) 3.24 (4.94) 6.82 5.23 4.26 (3.76) 6.54 4.96\nA.6 A DDITIONAL EXPERIMENT RESULTS\nA.6.1 P ERPLEXITY EVALUATION\nIn Tab. A.4, we provide the full experimental results on LLaMA Touvron et al. (2023a). Further-\nmore, in Tab. A.5 and A.6, we provide additional experimental results on LLaMA2 Touvron et al.\n(2023b) and OPT Zhang et al. (2022) models.\nA.6.2 MMLU E VALUATION\nIn Tab. A.7, we provide additional experimental results for Vicuna v1.3 on MMLU.\nA.7 L IMITATIONS\nWhile our empirical results primarily focus on generation tasks, the proposed ideas in this work\nare not inherently limited to decoder architectures. However, we have not yet conducted thorough\nassessments of our framework\u2019s effectiveness on encoder-only or encoder-decoder architectures, as\nwell as other neural network architectures. Additionally, it is important to note that our hardware"], "retrieved_docs_id": ["4f752eeea2", "5107b7792e", "04b6ebc53f", "37128cb48f", "732981ffa5"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does Cait enhance model compression in Vision Transformers?\n", "true_answer": "Cait enhances model compression in Vision Transformers by introducing asymmetric token merging to integrate neighboring tokens efficiently and preserving the spatial structure, along with consistent dynamic channel pruning for uniform pruning of unimportant channels.", "source_doc": "multimodal.pdf", "source_id": "28380a85e1", "retrieved_docs": ["in computer vision applications. However, as ViT models have grown in size, the number of train-\nable parameters and operations has also increased, impacting their deployment and performance.\nAdditionally, the computational and memory cost of self-attention grows quadratically with image\nresolution. Referring to the paper [95], this survey aims to explore the most efficient vision encoding\nmethodologies that may be used for efficient MLLMs.\nEfficient VisionCompact Architecture (\u00a73.1)Architecture Design MethodsReformer[96], EfficientFormer[97],\nEfficientFormerV2[98]\nArchitecture Search MethodsAutoformer [99], NASViT [100],\nTF-TAS [101], UniNet [102]\nOptimization of Attention\nMechanisms MethodsPatchMerger [103], DynamicViT [104],\nSepvit [105]\nPruning (\u00a73.2)Unstructured Pruning Cap [106], Cait [107]\nStructured PruningWDPruning [108], X-Pruner [109],\nVTP[110], PS-ViT[111]\nHybrid Pruning SPViT [112], ViT-Slim [113]", "Instead of achieving SoTA performance, the goal of this\npaper is to show that our parameter deployment frame-\nwork can improve the transformer backbone with less train-\nable parameters. Therefore, we employ LAMB instead of\nAdamW for more general and typical experiments. For MoE\nbased models ( i.e., ViT-MoE and WideNet), we set the\nweight of load balance loss \u03bbas 0.01. Without special in-\nstructions, we use 4 experts in total and Top 2 experts se-\nlected in each transformer block. The capacity ratio Cis\nset as 1.2 for a trade-off between accuracy and speed. We\npretrain our models on 256 TPUv3 cores. According to re-\ncent work (Zhai et al. 2021), different types of the prediction\nhead have no signi\ufb01cant difference on ImageNet\u2019s few-shot\nperformance. We also verify this conclusion on training Im-\nageNet from scratch. In this work, for ViT, we use the typical\ntoken head, which means we insert [CLS] token at the start\nof patch tokens and use it to classify the image. For MoE", "low rank bias in Figures 12, 13, 15, 17 and 19. In these experiments we use off-the-shelf vision\ntransformers (ViT) [ DBK+20] trained on popular vision benchmarks, as well as off-the-shelf GPT-2\ntrained on a popular language benchmark. We use no weight decay or dropout in our experiments.\nAll models were initialized using the default initialization scale.\nB.1 SGD-trained transformers\nCIFAR-10/100 We trained a 6-layer ViT with 8 heads per layer, embedding dimension 512, head\ndimension 128, and MLP dimension 512 and patch-size 4 for 500 epochs on CIFAR10/CIFAR100\nwith SGD and learning rate 3e-1 and warmup. See Figures 12 and 13. Each run took 2 hours on\none A100 GPU.\n0 10000 20000 30000 40000 50000\nIteration1.01.52.02.53.03.54.04.55.0Stable rankLayer 0\nWQWT\nK\nWVWT\nO\n0 10000 20000 30000 40000 50000\nIteration1.01.52.02.53.03.54.04.55.0Stable rankLayer 1\nWQWT\nK\nWVWT\nO\n0 10000 20000 30000 40000 50000\nIteration1.01.52.02.53.03.54.04.55.0Stable rankLayer 2\nWQWT\nK\nWVWT\nO", "at:https://github.com/IST-DASLab/\nsparsegpt .\n1. Introduction\nLarge Language Models (LLMs) from the Generative Pre-\ntrained Transformer (GPT) family have shown remarkable\nperformance on a wide range of tasks, but are dif\ufb01cult to de-\nploy because of their massive size and computational costs.\nFor illustration, the top-performing GPT-175B models have\n175 billion parameters, which total at least 320GB (count-\ning multiples of 1024) of storage in half-precision (FP16)\nformat, leading it to require at least \ufb01ve A100 GPUs with\n80GB of memory each for inference. It is therefore natu-\nral that there has been signi\ufb01cant interest in reducing these\ncosts via model compression . To date, virtually all existing\nGPT compression approaches have focused on quantiza-\ntion (Dettmers et al., 2022; Yao et al., 2022; Xiao et al.,\n2022; Frantar et al., 2022a), that is, reducing the precision\nof the model\u2019s numerical representation.\nA complementary approach for compression is pruning ,", "Further, it supports a number of language models such as\nGPT-2 and LLaMA, and also covers several representative\nvision Transformer models ( e.g.,ViT and Swin Transformer).\nAs discussed in Section 5.3.1, there have been a large\nnumber of efficient tuning methods proposed in the existing\nliterature. However, most of these approaches are tested\non small-sized pre-trained language models, instead of the\nLLMs. So far, there still lacks a thorough investigation on\nthe effect of different efficient tuning methods on large-sized\nlanguage models at different settings or tasks.\n5.4 Memory-Efficient Model Adaptation\nDue to the huge number of model parameters, LLMs take a\nsignificant memory footprint for inference, making it very\ncostly to be deployed in real-world applications. In this\nsection, we discuss how to reduce the memory footprint\nof LLMs via a popular model compression approach ( i.e.,\nmodel quantization), so that large-sized LLMs can be used"], "retrieved_docs_id": ["20b3b3179f", "11b3541f30", "b678599b0c", "7fb981b421", "0df03879ce"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What type of information is often inaccurately described by multimodal large language models (MLLMs) in images?\n", "true_answer": "MLLMs often inaccurately describe the attributes of objects in images, such as color, shape, material, content, counting, action, etc.", "source_doc": "hallucination.pdf", "source_id": "f2b3e09bb2", "retrieved_docs": ["2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "is a document and the output is its summary. So we can feed the input document into the language\nmodel and then produce the generated summary.\nDespite the successful applications in natural language processing, it is still struggling to natively use\nLLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal\nperception is a necessity to achieve arti\ufb01cial general intelligence, in terms of knowledge acquisition\nand grounding to the real world. More importantly, unlocking multimodal input [ TMC+21,HSD+22,\nWBD+22,ADL+22,AHR+22,LLSH23 ] greatly widens the applications of language models to\nmore high-value areas, such as multimodal machine learning, document intelligence, and robotics.\nIn this work, we introduce KOSMOS -1, a Multimodal Large Language Model (MLLM) that can\nperceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e.,", "Vision Audition\n31 8 \u2026 70 2\nA B C\nD E F\nFigure 1: KOSMOS -1is a multimodal large language model (MLLM) that is capable of perceiving\nmultimodal input, following instructions, and performing in-context learning for not only language\ntasks but also multimodal tasks. In this work, we align vision with large language models (LLMs),\nadvancing the trend of going from LLMs to MLLMs.\n\u2217Equal contribution. \u2020Corresponding author.arXiv:2302.14045v2  [cs.CL]  1 Mar 2023"], "retrieved_docs_id": ["da0a465b6c", "f49f3b54ce", "ac70fcc9f2", "74bb21ad4f", "f0ea146bbd"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "Which company offers a low-code AI solution for implementing various RAG applications?\n", "true_answer": "Flowise AI", "source_doc": "RAG.pdf", "source_id": "9ff21c1039", "retrieved_docs": ["quickly with the popularity of ChatGPT. They both offer a\nrich set of RAG-related APIs, gradually becoming one of\nthe indispensable technologies in the era of large models.\nMeanwhile, new types of technical stacks are constantly be-\ning developed. Although they do not offer as many features\nas LangChain and LLamaIndex, they focus more on their\nunique characteristics. For example, Flowise AI6emphasizes\nlow-code, allowing users to implement various AI applica-\ntions represented by RAG without writing code, simply by\ndragging and dropping. Other emerging technologies include\nHayStack, Meltno, and Cohere Coral.\nIn addition to AI-native frameworks, traditional software\nor cloud service providers have also expanded their service\nrange. For instance, Verba7, provided by the vector database\ncompany Weaviate, focuses on personal assistants. Amazon\noffers its users the intelligent enterprise search service tool\nKendra, based on RAG thinking. Users can search in different", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "Kendra, based on RAG thinking. Users can search in different\ncontent repositories through built-in connectors.\nThe development of the technical stack and RAG are mu-\ntually reinforcing. New technologies pose higher demands\n6https://flowiseai.com\n7https://github.com/weaviate/Verbaon the existing technical stack, while the optimization of the\ntechnical stack\u2019s functions further promotes the development\nof RAG technology. Overall, the technical stack of RAG\u2019s\ntoolchain has initially formed, and many enterprise-level ap-\nplications have gradually emerged, but an all-in-one platform\nstill needs to be refined.\n9 Conclusion\nThis paper thoroughly explores Retrieval-Augmented Gener-\nation (RAG), a technique that uses an external knowledge\nbase to supplement the context of Large Language Models\n(LLMs) and generate responses. Notably, RAG combines pa-\nrameterized knowledge from LLMs and non-parameterized\nexternal knowledge, alleviates hallucination issues, identifies", "providing an effective solution to the incomplete and insuf-\nficient knowledge problem inherent in purely parameterized\nmodels.\nThe paper systematically reviews and analyzes the current\nresearch approaches and future development paths of RAG,\nsummarizing them into three main paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG. Subsequently, the paper\nprovides a consolidated summary of the three core compo-\nnents: Retrieval, Augmented, and Generation, highlighting\nthe improvement directions and current technological char-\nacteristics of RAG. In the section on augmentation methods,the current work is organized into three aspects: the augmen-\ntation stages of RAG, augmentation data sources, and aug-\nmentation process. Furthermore, the paper summarizes the\nevaluation system, applicable scenarios, and other relevant\ncontent related to RAG. Through this article, readers gain a\nmore comprehensive and systematic understanding of large\nmodels and retrieval-Augmented generation. They become", "tial future research directions for RAG. As a method that\ncombines retrieval and generation, RAG has numerous po-\ntential development directions in future research. By contin-\nuously improving the technology and expanding its applica-\ntions, the performance and practicality of RAG can be further\nenhanced.\nReferences\n[Alon et al. , 2022 ]Uri Alon, Frank Xu, Junxian He, Sudipta\nSengupta, Dan Roth, and Graham Neubig. Neuro-\nsymbolic language modeling with automaton-augmented\nretrieval. In International Conference on Machine Learn-\ning, pages 468\u2013485. PMLR, 2022."], "retrieved_docs_id": ["9ff21c1039", "4fffd3dc2b", "f3d56bbc09", "1bd400d39e", "08c361d4ad"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What are the three core metrics primarily focused on in the latest evaluation frameworks like RAGAS and ARES?\n", "true_answer": "Faithfulness of the answer, Answer Relevance, and Context Relevance.", "source_doc": "RAG.pdf", "source_id": "57b75e5528", "retrieved_docs": ["evaluation metrics. Additionally, the latest evalu-\nation frameworks like RAGAS [Eset al. , 2023 ]and\nARES [Saad-Falcon et al. , 2023 ]also involve RAG eval-\nuation metrics. Summarizing these works, three core metrics\nare primarily focused on: Faithfulness of the answer, Answer\nRelevance, and Context Relevance.\n1.Faithfulness\nThis metric emphasizes that the answers generated by\nthe model must remain true to the given context, ensur-\ning that the answers are consistent with the context infor-\nmation and do not deviate or contradict it. This aspect of\nevaluation is vital for addressing illusions in large mod-\nels.\n2.Answer Relevance\nThis metric stresses that the generated answers need to\nbe directly related to the posed question.\n3.Context Relevance\nThis metric demands that the retrieved contextual infor-\nmation be as accurate and targeted as possible, avoid-\ning irrelevant content. After all, processing long texts\nis costly for LLMs, and too much irrelevant information", "open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "ARES\nARES aims to automatically evaluate the performance of\nRAG systems in three aspects: Context Relevance, Answer\nFaithfulness, and Answer Relevance. These evaluation met-\nrics are similar to those in RAGAS. However, RAGAS, being\na newer evaluation framework based on simple handwritten\nprompts, has limited adaptability to new RAG evaluation set-\ntings, which is one of the significances of the ARES work.\nFurthermore, as demonstrated in its assessments, ARES per-\nforms significantly lower than RAGAS.\nARES reduces the cost of evaluation by using a small\namount of manually annotated data and synthetic data,\nand utilizes Predictive-Driven Reasoning (PDR) to provide\nstatistical confidence intervals, enhancing the accuracy of\nevaluation [Saad-Falcon et al. , 2023 ].\nAlgorithm Principles\n1. Generating Synthetic Dataset: ARES initially generates\nsynthetic questions and answers from documents in the\ntarget corpus using a language model to create positive\nand negative samples.", "in retrieved information. Counterfactual robustness tests\ninclude questions that the LLM can answer directly, but\nthe related external documents contain factual errors.\n7.3 Evaluation Frameworks\nRecently, the LLM community has been exploring the use\nof \u201dLLMs as judge\u201d for automatic assessment, with many\nutilizing powerful LLMs (such as GPT-4) to evaluate their\nown LLM applications outputs. Practices by Databricks us-\ning GPT-3.5 and GPT-4 as LLM judges to assess their chatbot\napplications suggest that using LLMs as automatic evaluation\ntools is effective [Leng et al. , 2023 ]. They believe this method\ncan also efficiently and cost-effectively evaluate RAG-based\napplications.\nIn the field of RAG evaluation frameworks, RAGAS and\nARES are relatively new. The core focus of these evaluations\nis on three main metrics: Faithfulness of the answer, answer\nrelevance, and context relevance. Additionally, TruLens, an\nopen-source library proposed by the industry, also offers a", "ous downstream tasks and with different retrievers may yield\ndivergent results. However, some academic and engineering\npractices have focused on general evaluation metrics for RAG\nand the abilities required for its effective use. This section\nprimarily introduces key metrics for evaluating RAG\u2019s effec-\ntiveness and essential abilities for assessing its performance.\nKey Metrics\nRecent OpenAI report [Jarvis and Allard, 2023 ]have\nmentioned various techniques for optimizing large\nlanguage models (LLMs), including RAG and its"], "retrieved_docs_id": ["57b75e5528", "ffd5c8b41e", "1b1cdfdd79", "a05a21efce", "8e161396f8"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the reduction in parameters achieved by LDPv2 compared to the original LDP in CNN-based MobileVLMv2?\n", "true_answer": "LDPv2 achieves a 99.8% reduction in parameters compared to the original LDP in CNN-based MobileVLMv2.", "source_doc": "multimodal.pdf", "source_id": "f4853839e9", "retrieved_docs": ["cross-attention, while image features are unfolded and concatenated with Q to serve as K and V in\ncross-attention. By this means, the transformer output at the corresponding positions of the learn-\nable latent queries is taken as the aggregated representation of visual features, thereby standardizing\nvariable-length video frame features into fixed-size features. MEQ-Former in BRA VE [12] designs\na multi-encoder querying transformer to amalgamate features from multiple frozen vision encoders\ninto a versatile representation that can be directly inputted into a frozen language model.\nCNN-based MobileVLMv2[17] proposes LDPv2, a new projector consisting of three parts: fea-\nture transformation, token reduction, and positional information enhancement. By using point-wise\nconvolution layers, average pooling, and a PEG module with a skip connection, LDPv2 achieves\nbetter efficiency, a 99.8% reduction in parameters, and slightly faster processing compared to the\noriginal LDP[20].", "(EAS) module[52] proposes a novel parameter and computation-efficient tuning method for MLLMs\nto retain the high performance and reduce both parameter and computation expenditures on down-\nstream tasks. MemVP [53] argues that this transfer learning paradigm still exhibits inefficiency\nsince it significantly increases the input length of the language models. Visual prompts in MemVP\nare concatenated with the weights of Feed Forward Networks for visual knowledge injection to re-\nduce the training time and inference latency of the finetuned MLLMs and surpass the performance\nof previous PEFT methods.\n6 Data and Benchmarks\nIn this section, we provide an overview of the data and benchmarks used for training and evaluating\nefficient MLLMs. We discuss the significance of pre-training data, instruction-tuning data, and the\n19", "the inputs and targets as described in Section 3.2.1.\n3.2.4. Results\nThe scores achieved by each of the architectures we compare are shown in Table 2. For\nall tasks, the encoder-decoder architecture with the denoising objective performed best.\nThis variant has the highest parameter count ( 2P) but the same computational cost as the\nP-parameter decoder-only models. Surprisingly, we found that sharing parameters across the\nencoder and decoder performed nearly as well. In contrast, halving the number of layers in\nthe encoder and decoder stacks signi\ufb01cantly hurt performance. Concurrent work (Lan et al.,\n2019) also found that sharing parameters across Transformer blocks can be an e\ufb00ective means\nof lowering the total parameter count without sacri\ufb01cing much performance. XLNet also\nbears some resemblance to the shared encoder-decoder approach with a denoising objective\n(Yang et al., 2019). We also note that the shared parameter encoder-decoder outperforms", "in computer vision applications. However, as ViT models have grown in size, the number of train-\nable parameters and operations has also increased, impacting their deployment and performance.\nAdditionally, the computational and memory cost of self-attention grows quadratically with image\nresolution. Referring to the paper [95], this survey aims to explore the most efficient vision encoding\nmethodologies that may be used for efficient MLLMs.\nEfficient VisionCompact Architecture (\u00a73.1)Architecture Design MethodsReformer[96], EfficientFormer[97],\nEfficientFormerV2[98]\nArchitecture Search MethodsAutoformer [99], NASViT [100],\nTF-TAS [101], UniNet [102]\nOptimization of Attention\nMechanisms MethodsPatchMerger [103], DynamicViT [104],\nSepvit [105]\nPruning (\u00a73.2)Unstructured Pruning Cap [106], Cait [107]\nStructured PruningWDPruning [108], X-Pruner [109],\nVTP[110], PS-ViT[111]\nHybrid Pruning SPViT [112], ViT-Slim [113]", "Jian Sun. Shuf\ufb02enet V2: Practical guidelines for\nef\ufb01cient cnn architecture design. In Proceedings\nof the European Conference on Computer Vision\n(ECCV) , pages 116\u2013131, 2018.\n[168] Franck Mamalet and Christophe Garcia. Simpli-\nfying convnets for fast learning. In International\nConference on Arti\ufb01cial Neural Networks , pages\n58\u201365. Springer, 2012.\n[169] Brais Martinez, Jing Yang, Adrian Bulat, and\nGeorgios Tzimiropoulos. Training binary neural\nnetworks with real-to-binary convolutions. arXiv\npreprint arXiv:2003.11535 , 2020.\n[170] Julieta Martinez, Shobhit Zakhmi, Holger H Hoos,\nand James J Little. Lsq++: Lower running time\nand higher recall in multi-codebook quantization.\nInProceedings of the European Conference on\nComputer Vision (ECCV) , pages 491\u2013506, 2018.\n[171] Warren S McCulloch and Walter Pitts. A logical\ncalculus of the ideas immanent in nervous activity.\nThe bulletin of mathematical biophysics , 5(4):115\u2013\n133, 1943.\n[172] Jeffrey L McKinstry, Steven K Esser, Rathinaku-"], "retrieved_docs_id": ["f4853839e9", "100e4a1fcc", "be665c14e5", "20b3b3179f", "845fd1539a"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does RAG increase trust in its output?\n", "true_answer": "By citing sources, RAG allows users to verify the accuracy of the answers, which increases trust in its output.", "source_doc": "RAG.pdf", "source_id": "62ff7bd487", "retrieved_docs": ["maintains the timeliness and accuracy of responses.\n\u2022 Transparency is an advantage of RAG. By citing\nsources, users can verify the accuracy of the answers,\nincreasing trust in the model\u2019s output.\n\u2022 RAG has customization capabilities. Models can be tai-\nlored to different domains by indexing relevant textual\ncorpora, providing knowledge support for specific fields.\n\u2022 In terms of security and privacy management, RAG,\nwith its built-in roles and security controls in the\ndatabase, can better control data usage. In contrast, fine-\ntuned models may lack clear management of who can\naccess which data.\n\u2022 RAG is more scalable. It can handle large-scale datasets\nwithout the need to update all parameters and create\ntraining sets, making it more economically efficient.\n\u2022 Lastly, results produced by RAG are more trustworthy.\nRAG selects deterministic results from the latest data,\nwhile fine-tuned models may exhibit hallucinations and\ninaccuracies when dealing with dynamic data, lacking", "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "external knowledge, alleviates hallucination issues, identifies\ntimely information via retrieval technology, and enhances re-\nsponse accuracy. Additionally, by citing sources, RAG in-\ncreases transparency and user trust in model outputs. RAG\ncan also be customized based on specific domains by index-\ning relevant text corpora. RAG\u2019s development and charac-\nteristics are summarized into three paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG, each with its models,\nmethods, and shortcomings. Naive RAG primarily involves\nthe \u2019retrieval-reading\u2019 process. Advanced RAG uses more\nrefined data processing, optimizes the knowledge base in-\ndexing, and introduces multiple or iterative retrievals. As\nexploration deepens, RAG integrates other techniques like\nfine-tuning, leading to the emergence of the Modular RAG\nparadigm, which enriches the RAG process with new mod-\nules and offers more flexibility.\nIn the subsequent chapters, we further analyze three key", "such as professional domain knowledge question-answering,\nRAG might offer lower training costs and better performance\nbenefits than fine-tuning.\nSimultaneously, improving the evaluation system of RAG\nfor assessing and optimizing its application in different down-\nstream tasks is crucial for the model\u2019s efficiency and bene-\nfits in specific tasks. This includes developing more accurate\nevaluation metrics and frameworks for different downstream\ntasks, such as context relevance, content creativity, and harm-\nlessness, among others.\nFurthermore, enhancing the interpretability of models\nthrough RAG, allowing users to better understand how and\nwhy the model makes specific responses, is also a meaning-\nful task.\nTechnical Stack\nIn the ecosystem of RAG, the development of the related\ntechnical stack has played a driving role. For instance,\nLangChain and LLamaIndex have become widely known\nquickly with the popularity of ChatGPT. They both offer a", "quently, it utilizes this retrieved information to generate re-\nsponses or text, thereby enhancing the quality of predictions.\nThe RAG method allows developers to avoid the need for\nretraining the entire large model for each specific task. In-\nstead, they can attach a knowledge base, providing additional\ninformation input to the model and improving the accuracy\nof its responses. RAG methods are particularly well-suited\nfor knowledge-intensive tasks. In summary, the RAG system\nconsists of two key stages:1. Utilizing encoding models to retrieve relevant docu-\nments based on questions, such as BM25, DPR, Col-\nBERT, and similar approaches [Robertson et al. , 2009,\nKarpukhin et al. , 2020, Khattab and Zaharia, 2020 ].\n2. Generation Phase: Using the retrieved context as a con-\ndition, the system generates text.\n2.2 RAG vs Fine-tuning\nIn the optimization of Large Language Models (LLMs), in\naddition to RAG, another important optimization technique\nis fine-tuning."], "retrieved_docs_id": ["62ff7bd487", "4fffd3dc2b", "123a2dcc44", "da182b99e8", "80558327ad"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a lightweight vision model designed for vision and language tasks?\n", "true_answer": "ViTamin [11]", "source_doc": "multimodal.pdf", "source_id": "18b9cdbf0e", "retrieved_docs": ["Vision Audition\n31 8 \u2026 70 2\nA B C\nD E F\nFigure 1: KOSMOS -1is a multimodal large language model (MLLM) that is capable of perceiving\nmultimodal input, following instructions, and performing in-context learning for not only language\ntasks but also multimodal tasks. In this work, we align vision with large language models (LLMs),\nadvancing the trend of going from LLMs to MLLMs.\n\u2217Equal contribution. \u2020Corresponding author.arXiv:2302.14045v2  [cs.CL]  1 Mar 2023", "Perception-language tasks\nCOCO Caption [LMB+14] Image captioning CIDEr, etc. \u0013 \u0013\nFlicker30k [YLHH14] Image captioning CIDEr, etc. \u0013 \u0013\nVQAv2 [GKSS+17] Visual question answering VQA acc. \u0013 \u0013\nVizWiz [GLS+18] Visual question answering VQA acc. \u0013 \u0013\nWebSRC [CZC+21] Web page question answering F1 score \u0013\nVision tasks\nImageNet [DDS+09] Zero-shot image classi\ufb01cation Top-1 acc. \u0013\nCUB [WBW+11] Zero-shot image classi\ufb01cation with descriptions Accuracy \u0013\nTable 1: We evaluate the capabilities of KOSMOS -1on language, perception-language, and vision\ntasks under both zero- and few-shot learning settings.\n1 Introduction: From LLMs to MLLMs\nLarge language models (LLMs) have successfully served as a general-purpose interface across various\nnatural language tasks [ BMR+20]. The LLM-based interface can be adapted to a task as long as we\nare able to transform the input and output into texts. For example, the input of the summarization task", "Further, it supports a number of language models such as\nGPT-2 and LLaMA, and also covers several representative\nvision Transformer models ( e.g.,ViT and Swin Transformer).\nAs discussed in Section 5.3.1, there have been a large\nnumber of efficient tuning methods proposed in the existing\nliterature. However, most of these approaches are tested\non small-sized pre-trained language models, instead of the\nLLMs. So far, there still lacks a thorough investigation on\nthe effect of different efficient tuning methods on large-sized\nlanguage models at different settings or tasks.\n5.4 Memory-Efficient Model Adaptation\nDue to the huge number of model parameters, LLMs take a\nsignificant memory footprint for inference, making it very\ncostly to be deployed in real-world applications. In this\nsection, we discuss how to reduce the memory footprint\nof LLMs via a popular model compression approach ( i.e.,\nmodel quantization), so that large-sized LLMs can be used", "[VLZP15] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based\nimage description evaluation. In CVPR , pages 4566\u20134575, 2015.\n[WBD+22]Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti\nAggarwal, Owais Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image\nas a foreign language: BEiT pretraining for all vision and vision-language tasks. ArXiv ,\nabs/2208.10442, 2022.\n[WBW+11]Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Belongie.\nThe caltech-ucsd birds-200-2011 dataset. 2011.\n[WCW+23]Chengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo\nChen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei.\nNeural codec language models are zero-shot text to speech synthesizers. ArXiv ,\nabs/2301.02111, 2023.\n[WDC+23]Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jian-\nfeng Gao, and Furu Wei. Visually-augmented language modeling. In International", "A. Hu, P . Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian,\nQ. Qi, J. Zhang, and F. Huang, \u201cmplug-owl: Mod-\nularization empowers large language models with\nmultimodality,\u201d CoRR , vol. abs/2304.14178, 2023.\n[830] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P . Wang, J. Lin,\nC. Zhou, and J. Zhou, \u201cQwen-vl: A frontier large\nvision-language model with versatile abilities,\u201d CoRR ,\nvol. abs/2308.12966, 2023.\n[831] H. Liu, C. Li, Y. Li, and Y. J. Lee, \u201cImproved\nbaselines with visual instruction tuning,\u201d CoRR , vol.\nabs/2310.03744, 2023.\n[832] P . Zhang, X. Dong, B. Wang, Y. Cao, C. Xu, L. Ouyang,\nZ. Zhao, S. Ding, S. Zhang, H. Duan, W. Zhang,\nH. Yan, X. Zhang, W. Li, J. Li, K. Chen, C. He,\nX. Zhang, Y. Qiao, D. Lin, and J. Wang, \u201cInternlm-\nxcomposer: A vision-language large model for ad-\nvanced text-image comprehension and composition,\u201d\nCoRR , vol. abs/2309.15112, 2023.\n[833] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and\nR. Zhao, \u201cShikra: Unleashing multimodal llm\u2019s ref-"], "retrieved_docs_id": ["f0ea146bbd", "4c8f867655", "0df03879ce", "e4285a7bd9", "3d9acf0ed0"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How well did the domain-adapted LLM perform for the engineering assistant chatbot based on expert evaluations?\n", "true_answer": "The domain-adapted LLM achieved a score of 6.0 on a 7-point Likert scale for the engineering assistant chatbot based on expert evaluations.", "source_doc": "ChipNemo.pdf", "source_id": "28f0897bcb", "retrieved_docs": ["uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "vant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation", "We use largely publicly available general-purpose chat in-\nstruction datasets for multi-turn chat together with a small\namount of domain-specific instruction datasets to perform\nalignment on the ChipNeMo foundation model, which pro-\nduces the ChipNeMo chat model. We observe that align-\nment with a general purpose chat instruction dataset is\nadequate to align the ChipNeMo foundation models with\nqueries in the chip design domain. We also added a small\namount of task-specific instruction data, which further im-\nproves the alignment. We trained multiple ChipNeMo foun-\ndation and chat models based on variants of LLaMA2 mod-\nels used as the base foundation model.\nTo improve performance on the engineering assistant chat-\nbot application, we also leverage Retrieval Augmented Gen-\neration (RAG). RAG is an open-book approach for giving\nLLMs precise context for user queries. It retrieves rele-\nvant in-domain knowledge from its data store to augment", "Bard, etc.) and open-source (Vicuna (Chiang et al., 2023),\nLLaMA2 (Touvron et al., 2023), etc.) large language mod-\nels (LLM) provide an unprecedented opportunity to help\nautomate these language-related chip design tasks. Indeed,\nearly academic research (Thakur et al., 2023; Blocklove\net al., 2023; He et al., 2023) has explored applications of\nLLMs for generating Register Transfer Level (RTL) code\nthat can perform simple tasks in small design modules as\nwell as generating scripts for EDA tools.\nWe believe that LLMs have the potential to help chip de-\nsign productivity by using generative AI to automate many\nlanguage-related chip design tasks such as code generation,\nresponses to engineering questions via a natural language\ninterface, analysis and report generation, and bug triage. In\nthis study, we focus on three specific LLM applications: an\nengineering assistant chatbot for GPU ASIC and Architec-\nture design engineers, which understands internal hardware"], "retrieved_docs_id": ["c7d05c4b43", "a6c3d05123", "28f0897bcb", "411c489c58", "f23b3625e0"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.3333333333333333, "hit": 1}, {"question": "Which model outperforms GPT-4 in all categories, including both RAG misses and hits, according to the human evaluation?\n", "true_answer": "ChipNeMo-70B-Steer outperforms GPT-4 in all categories, including both RAG misses and hits, according to the human evaluation.", "source_doc": "ChipNemo.pdf", "source_id": "1ed1c2ae54", "retrieved_docs": ["and required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the differencein retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022ChipNeMo-70B-Steer outperforms similar sized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and", "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "GPT-NeoX and FairSeq models both exhibit\ndominant performance on MMMLU compared\nto GPT-3 in the \ufb01ve-shot setting (Figure 7), their\nperformance is much closer in the zero-shot setting\n(Tables 10 to 13). Hendrycks et al. (2021b) claim\nto \ufb01nd that few-shot evaluation does not improve\nperformance relative to zero-shot, but they only\nstudy GPT-3. By contrast, we \ufb01nd that GPT-NeoX\nand FairSeq models do improve substantially\nwith as few as \ufb01ve examples. We view this as a\nwarning against drawing strong conclusions about\nevaluation metrics based only on one model, and\nencourage researchers developing new evaluation\nbenchmarks to leverage multiple different classes\nof models to avoid over\ufb01tting their conclusions to\na speci\ufb01c model.\n5.2 Powerful Few-Shot Learning\nOur experiments indicate that GPT-J-6B and GPT-\nNeoX-20B bene\ufb01t substantially more from few-\nshot evaluations than the FairSeq models do. When\ngoing from 0-shot to 5-shot evaluations, GPT-J-6B", "shot evaluations than the FairSeq models do. When\ngoing from 0-shot to 5-shot evaluations, GPT-J-6B\nimproves by 0.0526 and GPT-NeoX-20B improves", "raters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\nagainst each other in matches to produce the best response for a given prompt. The winner of a\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We"], "retrieved_docs_id": ["1ed1c2ae54", "af6e8c3fb2", "03fb936ebf", "75e832b9a2", "b849cf1a9b"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does SUGRE embed relevant information from a knowledge graph?\n", "true_answer": "SUGRE embeds relevant subgraphs retrieved from the knowledge graph using Graph Neural Networks (GNN).", "source_doc": "RAG.pdf", "source_id": "812e372c75", "retrieved_docs": ["formation of the preceding blocks (C1, . . . , C i\u22121)and the\nretrieval information of N(Ci\u22121)through cross-attention to\nguide the generation of the next block Ci. To maintain causal-\nity, the autoregressive generation of the i-th block Cican only\nuse the nearest neighbor of the previous block N(Ci\u22121)and\nnotN(Ci).\nAugmented with Structured Data\nStructured data sources like Knowledge Graphs (KG) are\ngradually integrated into the paradigm of RAG. Verified KGs\ncan offer higher-quality context, reducing the likelihood of\nmodel hallucinations.\nRET-LLM [Modarressi et al. , 2023 ]constructs a per-\nsonalized knowledge graph memory by extracting\nrelation triples from past dialogues for future use.\nSUGRE [Kang et al. , 2023 ]embeds relevant subgraphs\nretrieved from the knowledge graph using Graph Neural\nNetworks (GNN) to prevent the model from generating\ncontextually irrelevant replies. SUGRE [Kang et al. , 2023 ]\nemploys a graph encoding method that reflects the graph", "swers given a retrieval-enhanced directive. It updates the gen-\nerator and retriever to minimize the semantic similarity be-\ntween documents and queries, effectively leveraging relevant\nbackground knowledge.\nAdditionally, SUGRE [Kang et al. , 2023 ]introduces the\nconcept of contrastive learning. It conducts end-to-end fine-\ntuning of both retriever and generator, ensuring highly de-\ntailed text generation and retrieved subgraphs. Using a\ncontext-aware subgraph retriever based on Graph Neural Net-\nworks (GNN), SURGE extracts relevant knowledge from a\nknowledge graph corresponding to an ongoing conversation.\nThis ensures the generated responses faithfully reflect the re-\ntrieved knowledge. SURGE employs an invariant yet efficient\ngraph encoder and a graph-text contrastive learning objective\nfor this purpose.\nIn summary, the enhancement methods during the fine-\ntuning phase exhibit several characteristics. Firstly, fine-\ntuning both LLM and retriever allows better adaptation", "[CLS] helikesplay## ing[SEP] mydogiscute[SEP]Input \nE[CLS] Ehe Elikes Eplay E## ing E[SEP] Emy Edog Eis Ecute E[SEP] Token \nEmbeddings \nEA EB EB EB EB EB EA EA EA EA EASegment \nEmbeddings \nE0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5Position \nEmbeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings.\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\nPre-training data The pre-training procedure\nlargely follows the existing literature on language\nmodel pre-training. For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al.,\n2015) and English Wikipedia (2,500M words).\nFor Wikipedia we extract only the text passages", "employs a graph encoding method that reflects the graph\nstructure into PTMs\u2019 representation space and utilizes a\nmulti-modal contrastive learning objective between graph-\ntext modes to ensure consistency between retrieved facts\nand generated text. KnowledgeGPT [Wang et al. , 2023c ]\ngenerates search queries for Knowledge Bases (KB) in code\nformat and includes predefined KB operation functions.\nApart from retrieval, KnowledgeGPT also offers the ca-\npability to store knowledge in a personalized knowledge\nbase to meet individual user needs. These structured data\nsources provide RAG with richer knowledge and context,\ncontributing to improved model performance.\nLLM Generated Content RAG\nObserving that the auxiliary information recalled by RAG\nis not always effective and may even have negative effects,\nsome studies have expanded the paradigm of RAG by delving\ndeeper into the internal knowledge of LLM. This approach\nutilizes the content generated by LLM itself for retrieval, aim-", "[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In\nNeural Information Processing Systems (NeuRIPS) . 2019.\n[49] Giovanni Puccetti, Alessio Miaschi, and Felice Dell\u2019Orletta. How do BERT embeddings organize linguistic\nknowledge? In Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge\nExtraction and Integration for Deep Learning Architectures , pages 48\u201357, Online, June 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.deelio-1.6. URL https://aclanthology.org/\n2021.deelio-1.6 .\n13"], "retrieved_docs_id": ["812e372c75", "977e0e1405", "0d90fb5d35", "4c3ac6cb2e", "d61bbb666c"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How can multi-view low-resolution vision encoders capture detailed information for MLLM?\n", "true_answer": "By inputting multi-view high-resolution images, specifically a global view (low-resolution images from resizing) and a local view (image patches from splitting).", "source_doc": "multimodal.pdf", "source_id": "f8392fc0db", "retrieved_docs": ["Multi-view Input Directly employing high-resolution vision encoders for fine-grained percep-\ntion is prohibitively costly and does not align with practical usage requirements. Therefore, to\nutilize low-resolution vision encoders while enabling MLLM to perceive detailed information, a\ncommon approach is to input multi-view HR images, i.e., a global view: low-resolution images\nobtained through resizing, and a local view: image patches derived from splitting. For example,\n7", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "tion answering and image captioning. However, MLLMs face considerable challenges in tasks ne-\ncessitating intricate recognition, including crowd counting and OCR of small characters. A direct\napproach to address these challenges involves increasing the image resolution, practically, the num-\nber of visual tokens. This strategy, nonetheless, imposes a substantial computational burden on\nMLLMs, primarily due to the quadratic scaling of computational costs with the number of input to-\nkens in the Transformer architecture. Motivated by this challenge, vision token compression, aimed\nto reduce the prohibitive computation budget caused by numerous tokens, has become an essential\naspect of efficient MLLMs. We will explore this topic through several key techniques, including\nmulti-view input, token processing, multi-scale information fusion, vision expert agents and video-\nspecific methods.\nMulti-view Input Directly employing high-resolution vision encoders for fine-grained percep-", "to lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent\nstudies have proposed complementing this information loss by incorporating visual features from\nother vision encoders. The work of [ 98] proposes mixing features from CLIP ViT and DINO ViT.\nSpecifically, it experimented with additive and interleaved features. Both settings show that there\nis a trade-off between the two types of features. A more dedicated mechanism is needed.\nConcurrently, a visual expert-based model proposed in [ 38] aims to mitigate the information\nloss caused by the CLIP image encoder. Instead of merely mixing features, this paper enhances\nthe visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two\npivotal modules: multi-task encoders and the structural knowledge enhancement module. The multi-\ntask encoders are dedicated to integrating various types of latent visual information extracted by", "task encoders are dedicated to integrating various types of latent visual information extracted by\nmultiple visual encoders. Additionally, the structural knowledge enhancement module is designed\nto utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from\nvisual inputs.\nFollowing the approach of the structural knowledge enhancement module in [ 38], another line\nof research investigates the utilization of vision tool models to enhance the perception of MLLMs.\nVCoder [ 49] utilizes additional perception formats, such as segmentation masks and depth maps,\nto enhance the object identification ability of the MLLM. Another work [ 54] ensembles additional\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024."], "retrieved_docs_id": ["f8392fc0db", "4ee780b19c", "8beea9b82e", "c20c82af54", "c461600dc0"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the vision encoder modify the input image in a mathematical formula?\n", "true_answer": "The vision encoder compresses the original image (X) into more compact patch features (Zv) using the formula Zv=g(Xv).", "source_doc": "multimodal.pdf", "source_id": "11ffce909a", "retrieved_docs": ["Taking the input image Xvas input, the vision encoder compresses the original image into more\ncompact patch features Zv, as represented by the following formula:\nZv=g(Xv). (1)\n4", "Multi-view Input Directly employing high-resolution vision encoders for fine-grained percep-\ntion is prohibitively costly and does not align with practical usage requirements. Therefore, to\nutilize low-resolution vision encoders while enabling MLLM to perceive detailed information, a\ncommon approach is to input multi-view HR images, i.e., a global view: low-resolution images\nobtained through resizing, and a local view: image patches derived from splitting. For example,\n7", "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "20 Bai, et al.\nfocuses more on the image information. The image-based model is created by modifying the\nattention weight matrix structure within the original model, without altering its parameters. This\napproach emphasizes the knowledge of the image-biased model and diminishes that of the original\nmodel, which may be text-biased. Thus, it encourages the extraction of correct content while\nsuppressing hallucinations resulting from textual over-reliance.\nGuided Decoding. MARINE [ 131] proposes a training-free approach. It employs an additional\nvision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\nSpecifically, it innovatively adapts the classifier-free guidance [ 40] technique to implement guided\ndecoding, showing promising performance in emphasizing the detected objects while reducing\nhallucination in the text response.\nSimilarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that", "VL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to\n448\u00d7448. InternVL [ 2] scales up the vision encoder to 6 billion parameters, enabling processing of\nhigh-resolution images. Regarding hallucination, HallE-Switch [ 123] has investigated the impact\nof vision encoder resolution on its proposed CCEval benchmark. Among the three studied vision\nencoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results\nin lower degrees of hallucination. These works indicate that scaling up vision resolution is a\nstraightforward yet effective solution.\n5.2.2 Versatile Vision Encoders. Several studies [ 38,49,98] have investigated vision encoders for\nMLLMs. Typically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs\nthanks to its remarkable ability to extract semantic-rich features. However, CLIP has been shown\nto lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent"], "retrieved_docs_id": ["11ffce909a", "f8392fc0db", "4ee780b19c", "9e707211bd", "3f64cf9b55"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does Vid2Seq improve language model prediction?\n", "true_answer": "Vid2Seq improves language model prediction by introducing special time markings, enabling it to predict event boundaries and text descriptions seamlessly.", "source_doc": "RAG.pdf", "source_id": "535efdce62", "retrieved_docs": ["accuracy for VQAv2/VizWiz.\nTable 14 shows the experimental results. Language-only instruction tuning boosts our model\u2019s\nperformance by 1.9 points on Flickr30k, 4.3 points on VQAv2, and 1.3 points on VizWiz. Our experi-\nments show that language-only instruction tuning can signi\ufb01cantly improve the model\u2019s instruction-\nfollowing capabilities across modalities. The results also indicate that our model can transfer the\ninstruction-following capability from language to other modalities.\n16", "continued scaling of language models is yielding diminishing returns on this dif\ufb01cult benchmark. [ BHT+20] re\ufb02ect on\nthe small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([ SPP+19]\n11", "iand\nv\u2032\niare initialized similar to the weights of a feedforward sublayer.\nFor character level language modeling, we set the model dimension to d= 512 , and the number of\nheads to 8. Our small (large) models have 18(36) all-attention layers, N= 1024 (2048 ) persistent\nvectors and a dropout rate of 0.3(0.4) applied to attention weights. The adaptive span has the same\nhyperparameters as Sukhbaatar et al. [39] with a maximum span of 8192 , except the loss coef\ufb01cient\nis set to 10\u22127. We use Adagrad [ 11] with a learning rate of 0.07. We clip individual gradients with a\nnorm larger than 0.03[31]. We warmup the learning rate linearly for 32k timesteps [ 40]. A training\nbatch consists of 64samples, each with 512consecutive tokens. When the loss on validation stops\ndecreasing, we divide the learning rate by 10for an additional 20-30k steps. Training large models\ntakes about a day on 64V100 GPUs.\nFor word level language modeling, we use a model with d= 512 and36layers, each with 8heads", "H2O: Heavy-Hitter Oracle for Efficient Generative\nInference of Large Language Models\nZhenyu Zhang1, Ying Sheng2, Tianyi Zhou3, Tianlong Chen1, Lianmin Zheng4, Ruisi Cai1,\nZhao Song5,Yuandong Tian6,Christopher R\u00e92,Clark Barrett2,Zhangyang Wang1,Beidi Chen6,7\n1University of Texas at Austin,2Stanford University,3University of California, San Diego,\n4University of California, Berkeley,5Adobe Research,6Meta AI (FAIR),7Carnegie Mellon University\n{zhenyu.zhang,tianlong.chen,ruisi.cai,atlaswang}@utexas.edu ,ying1123@stanford.edu ,\n{chrismre,barrett}@cs.stanford.edu ,t8zhou@ucsd.edu ,lianminzheng@gmail.com ,\nzsong@adobe.com ,yuandong@meta.com ,beidic@andrew.cmu.edu\nAbstract\nLarge Language Models (LLMs), despite their recent impressive accomplishments,\nare notably cost-prohibitive to deploy, particularly for applications involving long-\ncontent generation, such as dialogue systems and story writing. Often, a large", "Training Compute-Optimal Large Language Models\nJordan Ho\ufb00mann\u2605, Sebastian Borgeaud\u2605, Arthur Mensch\u2605, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,\nErich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre\u2605\n\u2605Equal contributions\nWe investigate the optimal model size and number of tokens for training a transformer language model\nunder a given compute budget. We \ufb01nd that current large language models are signi\ufb01cantly under-\ntrained, a consequence of the recent focus on scaling language models whilst keeping the amount of\ntrainingdataconstant. Bytrainingover400languagemodelsrangingfrom70milliontoover16billion\nparameters on 5 to 500 billion tokens, we \ufb01nd that for compute-optimal training, the model size and"], "retrieved_docs_id": ["a1656af7b2", "bfe0e7767c", "cded4ceba4", "80cede70cf", "da1bac24b9"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is an example of a method for mitigating data-related hallucinations in NLP?\n", "true_answer": "Introducing negative data, such as LRV-Instruction, is an example of a method for mitigating data-related hallucinations in NLP.", "source_doc": "hallucination.pdf", "source_id": "19a4c2c778", "retrieved_docs": ["ofhallucination snowballing , where the LLM over-\ncommits to early mistakes (before outputting the\nexplanation) in its generation, which it otherwise\nwould not make.\nRetrieval Augmentation One way to mitigate\nhallucinations is to ground the model\u2019s input on\nexternal knowledge, which is often referred to as\nretrieval augmentation . In other words, we can\ndecouple (i) memory storage of knowledge (e.g.,\ndatabases or search indexes [ 290]) and (ii) process-\ning of the knowledge to arrive at a more modular\narchitecture. For (i), a retriever module retrieves\nthe top- krelevant documents (or passages) for a\nquery from a large corpus of text. Then, for (ii),\nwe feed these retrieved documents to the language\nmodel together with the initial prompt. In theory,\nusing an external data source may also make it eas-\nier to interpret which knowledge is retrieved and\nupdate it without tediously fine-tuning the model.\nShuster et al. [507] demonstrate hallucinations in", "Data quality relevant to hallucinations can be further categorized into the following three facets.\n\u2022Noisy data. As mentioned in the definition section, training MLLMs involves two stages. The\npre-training stage employs image-text pairs crawled from the web, which contain inaccurate,\nmisaligned, or corrupted data samples. The noisy data would limit the cross-modal feature\nalignment [ 117,120], which serves as the foundation of MLLMs. As for the instruction tuning\ndata, prevalent methods, such as LLaVA [ 75], utilize the advanced GPT-4 [ 82] model to\ngenerate instructions. However, ChatGPT is a language model that cannot interpret visual\ncontent, leading to the risk of noisy data. Moreover, language models themselves suffer\nfrom the issue of hallucination [ 44], further increasing the risk. LLaVA-1.5 [ 74] adds human\nannotated QA data into instruction following and shows improved results, revealing the\neffect of noisy data.", "MLLMs. Based on the detection result, the hallucinated content can be eliminated. Secondly, this\nwork observes that long-tail distribution and object co-occurrence in the training data are two\nprimary factors of hallucination. Thus, a counterfactual visual instruction generation strategy is\nproposed to expand the dataset. Using the proposed methods, the instruction tuning data can be\nbalanced and experience reduced hallucination. MLLMs trained on the calibrated dataset are shown\nto be less prone to hallucination.\nReCaption [ 105]This work proposes a framework called ReCaption to rewrite the text captions\nof existing image-text pairs in datasets. The framework comprises two steps: 1) keyword extraction,\nwhich extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which\nemploys an LLM to generate sentences based on the extracted keywords. Ultimately, the framework", "Another interesting study observes that the hallucination of MLLMs seems to be easily triggered\nby paragraph break \u2018\\n\\n\u2019 [ 36]. Based on this observation, this work proposes two simple methods\nto reduce hallucination by avoiding generating \u2018\\n\u2019 during generation. First, intuitively, users can\ndesign the prompt to instruct the model to output responses within one paragraph, avoiding \u2018\\n\u2019.\nBesides, the authors tried to alter the output logits during generation by manually lowering the\nprobability of generating \u2018\\n\u2019. Experimental results show that this simple strategy can alleviate\nhallucination on popular benchmarks.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the"], "retrieved_docs_id": ["662143e448", "dcdb797076", "294848c460", "3fc78f0ef0", "114f3dada8"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does Woodpecker, an early attempt on hallucination detection and correction, identify and correct hallucinations?\n", "true_answer": "Woodpecker identifies and corrects hallucinations by extracting key concepts from the generated text and validating them using visual content. It then detects and corrects any hallucinated concepts by asking questions around the extracted concepts.", "source_doc": "hallucination.pdf", "source_id": "b4dda01e19", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 21\n5.4.2 Post-hoc Correction. Post-hoc correction refers to first allowing the MLLM to generate a text\nresponse and then identifying and eliminating hallucinating content, resulting in less hallucinated\noutput. This is usually achieved by grounding on visual content [ 114], pre-trained revisior [ 137],\nand self-revision [63].\nWoodpecker [ 114] is an early attempt on hallucination detection and correction. Similar to how\na woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated\ntext. The key idea of Woodpecker is to extract key concepts from the generated text and validate\nthem using visual content. Subsequently, the hallucinated concepts can be detected and corrected\naccordingly. Specifically, it consists of five stages: 1) Key concept extraction identifies the main objects\nmentioned in the generated sentences; 2) Question formulation asks questions around the extracted", "mentioned in the generated sentences; 2) Question formulation asks questions around the extracted\nobjects; 3) Visual knowledge validation answers the formulated questions via expert models; 4)\nVisual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge\nbase; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence\nunder the guidance of the visual knowledge base. Woodpecker is a training-free method, where\neach component can be implemented using either hand-crafted rules or off-the-shelf pre-trained\nmodels.\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\nSpecifically, inspired by denoising autoencoders [ 101], which are designed to reconstruct clean data\nfrom corrupted input, LURE [ 137] employs a hallucination revisor that aims to transform potentially\nhallucinatory descriptions into accurate ones. To train such a revisor model, a dataset has been", "Reinforcement Learning from Human Feedback (RLHF). HalDetect [ 32] first introduces the M-\nHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content,\nincluding non-existent objects, unfaithful descriptions, and inaccurate relationships. It then proposes\na multimodal reward model to detect hallucinations generated by MLLMs. The reward model is\ntrained on the M-HalDetect dataset to identify hallucinations in the generated text. To utilize\nthe trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "in real-world applications. This problem has attracted increasing attention, prompting efforts to detect\nand mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these\nhallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and\nstrategies developed to address this issue. Additionally, we analyze the current challenges and limitations,\nformulating open questions that delineate potential pathways for future research. By drawing the granular\nclassification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this\nsurvey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in\nthe field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the\nrobustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners", "MiniGPT-4 (13B) [138] 13B 15.9 76.7 - - - - -\nmPLUG-Owl2 [112] 7B 10.6 84.0 47.30 - - - -\nLLaVA-1.5 (7B) [74] 7B 8.6 82.9 - - - 44.6 46.4\nLLaVA-1.5 (13B) [74] 13B - - 46.94 0.8566 0.9425 - -\nCogVLM [106] 7B 7.9 86.1 - - - - -\nQwen-VL-Chat [2] 7B - - 39.15 - - - -\nOpen-Flamingo [1] 9B - - 38.44 - - - -\nLRV-Instruction [73] - - - 42.78 - - - -\n5 HALLUCINATION MITIGATION\nIn this section, we present a comprehensive review of contemporary methods aimed at mitigating\nhallucinations in MLLMs. Based on the properties and perspectives of these methods, we sys-\ntematically categorize them into four groups. Specifically, we investigate approaches addressing\nhallucination from Data, Model, Training, and Inference.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024."], "retrieved_docs_id": ["b4dda01e19", "ceeab98980", "7a0f374e2c", "c7f1da1e07", "e515c37930"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "Which model outperforms GPT-4 in generating EDA tools scripts and as an engineering assistant chatbot?\n", "true_answer": "The fine-tuned LLaMA2 70B model, as demonstrated by ChipEDA (He et al., 2023), outperforms the GPT-4 model on these tasks.", "source_doc": "ChipNemo.pdf", "source_id": "e6b9ba907a", "retrieved_docs": ["processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving", "uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "results on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks\nwhere we check for fully accurate code. For \u201cHard\u201d bench-\nmarks in Figure 9 we check for partial correctness of the\ncode, which is evaluated by a human user on a 0-10 scale.\nChipNeMo-70B-Steer performs significantly better than off-\nthe-shelf GPT-4 and LLaMA2-70B-Chat model.\nFigure 8: EDA Script Generation Evaluation Results, Pass@5\nAs seen in Figure 8, models like GPT-4 and LLaMA2-70B-\nChat have close to zero accuracy for the Python tool where\nthe domain knowledge related to APIs of the tool are neces-\nsary. This shows the importance of DAPT. Without DAPT,\nthe model had little to no understanding of the underlying\nAPIs and performed poorly on both automatic and human\nevaluated benchmarks. Our aligned model further improved\nthe results of DAPT because our domain instructional data\nhelps guide the model to present the final script in the most\nuseful manner. An ablation study on inclusion of domain", "Chatbots Many instruction following models are structured as dialogue-based chatbots, often using\nReinforcement Learning from Human Feedback (RLHF) [ 11] or generating data from an existing\nmodel to train with AI model feedback (RLAIF) [ 5]. Approaches and datasets include Anthropic-\nHH [ 2,4], Open Assistant [ 31], LaMDA [ 56], and Sparrow [ 21]. We do not use reinforcement\nlearning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open\nAssistant dataset which was designed to be used for RLHF training [ 31]. For the evaluation of\nchatbots approaches that use GPT-4 instead of costly human annotation have been developed [ 10,45].\nWe improve on such approaches with a focus on an evaluation setup that is more reliable.\n8 Limitations and Discussion\nWe have shown evidence that our method, QLORA, can replicate 16-bit full finetuning performance\nwith a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish", "tests the models\u2019 ability to select superior answers from two reference answers10. We calculate the\nlikelihood of the model preferring one answer over the other when presented with two candidate\nanswers simultaneously. The MC accuracy results are displayed in Table 2. It can be observed that\nDromedary demonstrates significantly improved performance compared to other open-source models,\nsuch as LLaMA andAlpaca , particularly in the Hamrless metric. Furthermore, it only marginally\nunderperforms when compared to the powerful ChatGPT model.\n4.2.3 Vicuna Benchmark Questions (Evaluated by GPT-4)\nChiang et al. [8]introduced an evaluation framework leveraging GPT-4 [27] to automate the as-\nsessment of chatbot performance. In this framework, GPT-4 generates challenging questions\nacross diverse categories, and answers from five chatbots\u2014 LLaMA ,Alpaca ,ChatGPT ,Bard , and\nVicuna \u2014are collected. We directly use these data to compare Dromedary with these chatbots."], "retrieved_docs_id": ["e6b9ba907a", "c7d05c4b43", "cf9d13203d", "52897d5339", "5149356966"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What issue do large language models (LLMs) face that affects their reliability in knowledge-intensive tasks?\n", "true_answer": "LLMs like ChatGPT often struggle with hallucinations, knowledge updates, and data-related issues, which can affect their performance in tasks requiring access to a vast amount of knowledge, such as open-domain question answering and common-sense reasoning.", "source_doc": "RAG.pdf", "source_id": "ee184b2a82", "retrieved_docs": ["Challenges and Applications of Large Language Models\nJean Kaddour\u03b1,\u2020,\u2217, Joshua Harris\u03b2,\u2217, Maximilian Mozes\u03b1,\nHerbie Bradley\u03b3,\u03b4,\u03f5, Roberta Raileanu\u03b6, and Robert McHardy\u03b7,\u2217\n\u03b1University College London\u03b2UK Health Security Agency\u03b3EleutherAI\n\u03b4University of Cambridge\u03f5Stability AI\u03b6Meta AI Research\u03b7InstaDeep\nAbstract\nLarge Language Models (LLMs) went from\nnon-existent to ubiquitous in the machine learn-\ning discourse within a few years. Due to the\nfast pace of the field, it is difficult to identify\nthe remaining challenges and already fruitful\napplication areas. In this paper, we aim to es-\ntablish a systematic set of open problems and\napplication successes so that ML researchers\ncan comprehend the field\u2019s current state more\nquickly and become productive.\nContents\n1 Introduction 1\n2 Challenges 2\n2.1 Unfathomable Datasets . . . . . . 2\n2.2 Tokenizer-Reliance . . . . . . . . 4\n2.3 High Pre-Training Costs . . . . . 6\n2.4 Fine-Tuning Overhead . . . . . . 10\n2.5 High Inference Latency . . . . . . 11", "Eight Things to Know about Large Language Models\nSamuel R. Bowman1 2\nAbstract\nThe widespread public deployment of large lan-\nguage models (LLMs) in recent months has\nprompted a wave of new attention and engage-\nment from advocates, policymakers, and scholars\nfrom many \ufb01elds. This attention is a timely re-\nsponse to the many urgent questions that this tech-\nnology raises, but it can sometimes miss important\nconsiderations. This paper surveys the evidence\nfor eight potentially surprising such points:\n1.LLMs predictably get more capable with in-\ncreasing investment, even without targeted\ninnovation.\n2.Many important LLM behaviors emerge un-\npredictably as a byproduct of increasing in-\nvestment.\n3.LLMs often appear to learn and use repre-\nsentations of the outside world.\n4.There are no reliable techniques for steering\nthe behavior of LLMs.\n5.Experts are not yet able to interpret the inner\nworkings of LLMs.\n6.Human performance on a task isn\u2019t an upper\nbound on LLM performance.", "which the model responses are presented to\nGPT-4arerandomlyswappedtoalleviatebias.\n1 Introduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have", "which the model responses are presented to\nGPT-4arerandomlyswappedtoalleviatebias.\n1 Introduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have", "and application, along with analysis on\naspects that influence the outcome of IT (e.g.,\ngeneration of instruction outputs, size of the\ninstruction dataset, etc). We also review the\npotential pitfalls of IT along with criticism\nagainst it, along with efforts pointing out\ncurrent deficiencies of existing strategies and\nsuggest some avenues for fruitful research.\n1 Introduction\nThe field of large language models (LLMs)\nhas witnessed remarkable progress in recent\nyears. LLMs such as GPT-3 (Brown et al.,\n2020b), PaLM (Chowdhery et al., 2022), and\nLLaMA (Touvron et al., 2023a) have demonstrated\nimpressive capabilities across a wide range of\nnatural language tasks (Zhao et al., 2021; Wang\net al., 2022b, 2023a; Wan et al., 2023; Sun et al.,\n2023c; Wei et al., 2023; Li et al., 2023a; Gao et al.,\n2023a; Yao et al., 2023; Yang et al., 2022a; Qian\net al., 2022; Lee et al., 2022; Yang et al., 2022b;\nGao et al., 2023b; Ning et al., 2023; Liu et al.,\n\u2660Zhejiang University,\u2663Shannon.AI,\u25b2Nanyang"], "retrieved_docs_id": ["bf695e58cf", "54e413d363", "dfa8d53d52", "dfa8d53d52", "ef8b65a87b"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "Which area of application has potential for efficient Multimodal Large Language Models?\n", "true_answer": "Efficient Multimodal Large Language Models have potential for widespread application in academia and industry, especially in edge computing scenarios.", "source_doc": "multimodal.pdf", "source_id": "ac70fcc9f2", "retrieved_docs": ["Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "ods, to deploy LLMs efficiently and effectively in real-world\nrecommender systems. In addition, existing LLMs have\nlimited capacities in long context modeling, make it difficult\nto process the huge amount of user-item interaction data.\nImproved context length extension and context information\nutilization approaches should be developed to improve the\nmodeling capacities of LLMs in long interaction sequences.\n8.1.4 Multimodal Large Language Model\nIn existing literature [823, 824], multimodal models mainly\nrefer to the models that can process and integrate informa-\ntion of various modalities ( e.g., text, image, and audio) frominput, and further produce corresponding output in certain\nmodalities. In this part, we mainly focus on the multimodal\nextension of LLMs by enabling the information modeling\nof non-textual modalities, especially the vision modality,\ncalled multimodal large language models (MLLMs) [797]49. To\nstart our discussion, we specify the input to be text-image", "is a document and the output is its summary. So we can feed the input document into the language\nmodel and then produce the generated summary.\nDespite the successful applications in natural language processing, it is still struggling to natively use\nLLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal\nperception is a necessity to achieve arti\ufb01cial general intelligence, in terms of knowledge acquisition\nand grounding to the real world. More importantly, unlocking multimodal input [ TMC+21,HSD+22,\nWBD+22,ADL+22,AHR+22,LLSH23 ] greatly widens the applications of language models to\nmore high-value areas, such as multimodal machine learning, document intelligence, and robotics.\nIn this work, we introduce KOSMOS -1, a Multimodal Large Language Model (MLLM) that can\nperceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e.,", "Challenges and Applications of Large Language Models\nJean Kaddour\u03b1,\u2020,\u2217, Joshua Harris\u03b2,\u2217, Maximilian Mozes\u03b1,\nHerbie Bradley\u03b3,\u03b4,\u03f5, Roberta Raileanu\u03b6, and Robert McHardy\u03b7,\u2217\n\u03b1University College London\u03b2UK Health Security Agency\u03b3EleutherAI\n\u03b4University of Cambridge\u03f5Stability AI\u03b6Meta AI Research\u03b7InstaDeep\nAbstract\nLarge Language Models (LLMs) went from\nnon-existent to ubiquitous in the machine learn-\ning discourse within a few years. Due to the\nfast pace of the field, it is difficult to identify\nthe remaining challenges and already fruitful\napplication areas. In this paper, we aim to es-\ntablish a systematic set of open problems and\napplication successes so that ML researchers\ncan comprehend the field\u2019s current state more\nquickly and become productive.\nContents\n1 Introduction 1\n2 Challenges 2\n2.1 Unfathomable Datasets . . . . . . 2\n2.2 Tokenizer-Reliance . . . . . . . . 4\n2.3 High Pre-Training Costs . . . . . 6\n2.4 Fine-Tuning Overhead . . . . . . 10\n2.5 High Inference Latency . . . . . . 11"], "retrieved_docs_id": ["ac70fcc9f2", "542e5c49da", "593e38b258", "74bb21ad4f", "bf695e58cf"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How much does RAG improve the scores of ChipNeMo-70B-Steer, GPT-4, and LLaMA2-70b-Chat?\n", "true_answer": "RAG improves ChipNeMo-70B-Steer by 0.56, GPT-4 by 1.68, and LLaMA2-70b-Chat by 2.05.", "source_doc": "ChipNemo.pdf", "source_id": "af6e8c3fb2", "retrieved_docs": ["scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "and required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the differencein retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022ChipNeMo-70B-Steer outperforms similar sized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and", "results on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks\nwhere we check for fully accurate code. For \u201cHard\u201d bench-\nmarks in Figure 9 we check for partial correctness of the\ncode, which is evaluated by a human user on a 0-10 scale.\nChipNeMo-70B-Steer performs significantly better than off-\nthe-shelf GPT-4 and LLaMA2-70B-Chat model.\nFigure 8: EDA Script Generation Evaluation Results, Pass@5\nAs seen in Figure 8, models like GPT-4 and LLaMA2-70B-\nChat have close to zero accuracy for the Python tool where\nthe domain knowledge related to APIs of the tool are neces-\nsary. This shows the importance of DAPT. Without DAPT,\nthe model had little to no understanding of the underlying\nAPIs and performed poorly on both automatic and human\nevaluated benchmarks. Our aligned model further improved\nthe results of DAPT because our domain instructional data\nhelps guide the model to present the final script in the most\nuseful manner. An ablation study on inclusion of domain", "processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving", "ChipNeMo: Domain-Adapted LLMs for Chip Design\ncost of pretraining a foundational model from scratch.\nModel Size Pretraining DAPT SFT\n7B 184,320 2,620 90\n13B 368,640 4,940 160\n70B 1,720,320 20,500 840\nTable 1: Training cost of LLaMA2 models in A100 GPU hours.\nPretraining cost from (Touvron et al., 2023).\n3.5. RAG and Engineering Assistant Chatbot\nWe created a benchmark to evaluate the performance of\ndesign chat assistance, which uses the RAG method. This\nbenchmark includes 88 questions in three categories: archi-\ntecture/design/verification specifications (Specs), testbench\nregression documentation (Testbench), and build infrastruc-\nture documentation (Build). For each question, we specify\nthe golden answer as well as the paragraphs in the design\ndocument that contains the relevant knowledge for the an-\nswer. These questions are created by designers manually\nbased on a set of design documents as the data store for\nretrieval. It includes about 1.8K documents, which were"], "retrieved_docs_id": ["af6e8c3fb2", "1ed1c2ae54", "cf9d13203d", "e6b9ba907a", "7313e64a59"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does Flare decide when to retrieve references during text generation?\n", "true_answer": "Flare decides to retrieve references based on the probability of the generated text. When the probability of a term falls below a predefined threshold, Flare's information retrieval system retrieves references and removes terms with lower probabilities.", "source_doc": "RAG.pdf", "source_id": "b844a74991", "retrieved_docs": ["they can decide to search for a relevant query to collect the\nnecessary materials, similar to the tool call of the agent.\nWebGPT [Nakano et al. , 2021 ]employs a reinforcement\nlearning framework to automatically train the GPT-3 model\nto use a search engine for text generation. It uses special to-\nkens to perform actions, including querying on a search en-\ngine, scrolling rankings, and citing references. This allows\nGPT-3 to leverage a search engine for text generation.\nFlare [Jiang et al. , 2023b ], on the other hand, automates the\ntiming of retrieval and addresses the cost of periodic docu-\nment retrieval based on the probability of the generated text.\nIt uses probability as an indicator of LLMs\u2019 confidence during\nthe generation process. When the probability of a term falls\nbelow a predefined threshold, the information retrieval sys-\ntem would retrieve references and removes terms with lower\nprobabilities. This approach is designed to handle situations", "kens (e.g., kNN-LM [Khandelwal et al. , 2019 ]), phrases (e.g.,\nNPM [Leeet al. , 2020 ], COG [Vaze et al. , 2021 ]), and docu-\nment paragraphs. Finer-grained retrieval units can often bet-\nter handle rare patterns and out-of-domain scenarios but come\nwith an increase in retrieval costs.\nAt the word level, FLARE employs an active retrieval strat-\negy, conducting retrieval only when the LM generates low-\nprobability words. The method involves generating a tempo-\nrary next sentence for retrieval of relevant documents, then\nre-generating the next sentence under the condition of the re-\ntrieved documents to predict subsequent sentences.\nAt the chunk level, RETRO uses the previous chunk to re-\ntrieve the nearest neighboring chunk and integrates this infor-\nmation with the contextual information of the previous chunk\nto guide the generation of the next chunk. RETRO achieves\nthis by retrieving the nearest neighboring block N(Ci\u22121)\nfrom the retrieval database, then fusing the contextual in-", "corporating a retrieval mechanism using the T5 architecture\n[Raffel et al. , 2020 ]in both the pre-training and fine-tuning\nstages. Prior to pre-training, it initializes the encoder-decoder\nLM backbone with a pre-trained T5, and initializes the dense\nretriever with a pre-trained Contriever. During the pre-\ntraining process, it refreshes the asynchronous index every\n1000 steps.\nCOG [Vaze et al. , 2021 ]is a text generation model that for-\nmalizes its generation process by gradually copying text frag-\nments (such as words or phrases) from an existing collection\nof text. Unlike traditional text generation models that select\nwords sequentially, COG utilizes efficient vector search tools\nto calculate meaningful context representations of text frag-\nments and index them. Consequently, the text generation task\nis decomposed into a series of copy and paste operations,\nwhere at each time step, relevant text fragments are sought\nfrom the text collection instead of selecting from an indepen-", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "(up to millions of samples), filtering, and clustering\nof candidate solutions generated by AlphaCode to\nselect the final submissions.\nHowever, whilst these existing code-generation\nLLMs have achieved impressive results, a criti-\ncal current constraint in applying LLMs to code\ngeneration is the inability to fit the full code base\nand dependencies within the context window. To\ndeal with this constraint, a few frameworks have\nbeen proposed to retrieve relevant information or\nabstract the relevant information into an API defi-\nnition.\nLong-Range Dependencies [ 660,504]\nLong-range dependencies across a code\nrepository usually cannot be regarded be-\ncause of limited context lengths (Sec. 2.6).\nZhang et al. [660] introduce RepoCoder, a\nretrieval-based framework for repository-level code\ncompletion that allows an LLM to consider the\nbroader context of the repository. A multi-step\nretrieval-augmented generation approach is taken,\nwhere the initial code generated is then used to re-"], "retrieved_docs_id": ["b844a74991", "92f5901d31", "bff4917f9e", "fefa202c19", "5c21f0d3d2"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the role of GShard in MoE?\n", "true_answer": "GShard is a module composed of a set of lightweight annotation APIs and XLA compiler extensions that offers an elegant way to express various parallel computation patterns while making minimal changes to existing model code, enabling scaling of multi-lingual neural machine translation in MoE.", "source_doc": "multimodal.pdf", "source_id": "a7d187c571", "retrieved_docs": ["model. During the inference phase, given an input, all experts are ranked, and the most relevant\nones are selected for computation. This approach considerably reduces the amount of computa-\ntion, as only a subset of experts is involved in the calculation.By distributing computational tasks\namong different experts, MoE achieves more efficient utilization of computational resources during\nboth training and inference phases. In MoE, each expert has its own set of parameters; however,\nthese parameters are shared during the training process. This parameter-sharing strategy reduces\nthe overall number of parameters in the model, consequently lowering storage and computational\ncosts. GShard [149] is a module composed of a set of lightweight annotation APIs and XLA com-\npiler extensions, which offers an elegant way to express various parallel computation patterns while\nmaking minimal changes to existing model code. It enables us to scale multi-lingual neural machine", "work component: a Sparsely-Gated Mixture-of-Experts Layer (MoE). The MoE consists of a num-\nber of experts, each a simple feed-forward neural network, and a trainable gating network which\nselects a sparse combination of the experts to process each input (see Figure 1). All parts of the\nnetwork are trained jointly by back-propagation.\n2", "layer\u2019s execution, tokens meant to be processed by a specific expert are routed to the corresponding\nGPU for processing, and the expert\u2019s output is returned to the original token location. Note that EP\nintroduces challenges in load balancing, as it is essential to distribute the workload evenly across the\nGPUs to prevent overloading individual GPUs or hitting computational bottlenecks.\nIn a Transformer model, the MoE layer is applied independently per token and replaces the\nfeed-forward (FFN) sub-block of the transformer block. For Mixtral we use the same SwiGLU\narchitecture as the expert function Ei(x)and set K= 2. This means each token is routed to two\nSwiGLU sub-blocks with different sets of weights. Taking this all together, the output yfor an input\ntoken xis computed as:\ny=n\u22121X\ni=0Softmax (Top2 (x\u00b7Wg))i\u00b7SwiGLU i(x).\nThis formulation is similar to the GShard architecture [ 21], with the exceptions that we replace all", "FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a\nmore elaborate gating strategy for the second expert assigned to each token.\n3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair\ncomparison. We measure performance on a wide variety of tasks categorized as follow:\n\u2022Commonsense Reasoning (0-shot): Hellaswag [ 32], Winogrande [ 26], PIQA [ 3], SIQA [ 27],\nOpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]\n\u2022World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19]\n\u2022Reading Comprehension (0-shot): BoolQ [7], QuAC [5]\n\u2022Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4\n\u2022Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot)\n\u2022Popular aggregated results: MMLU [ 16] (5-shot), BBH [ 29] (3-shot), and AGI Eval [ 34]\n(3-5-shot, English multiple-choice questions only)", "depth. To this end, we employ MoE to scale transformer\nalong with width. As a typical conditional computation\nmodel (Bengio 2013), MoE only activates a few experts, i.e.,\nsubsets of a network. For each input, we feed only a part of\nhidden representations required to be processed into the se-\nlected experts.\nFollowing Shazeer et al. (2017), given Etrainable experts\nand input representation x\u2208RD, the output of MoE model\ncan be formulated as:\nMoE(x) =E\u2211\ni=1g(x)ie(x)i(1)\nwheree(\u00b7)iis a non-linear transformation RD\u2192RDof\nithexpert, andg(\u00b7)iisithelement of the output of trainable\nrouterg(\u00b7), a non-linear mapping RD\u2192RE. Usually, both\ne(\u00b7)andg(\u00b7)are parameterized by neural networks.\nAccording to the formulation above, when g(\u00b7)is a sparse\nvector, only part of experts would be activated and updated\nby back-propagation during training. In this paper, for both\nvanilla MoE and our WideNet, each expert is an FFN layer.\nRouting\nTo ensure a sparse routing g(\u00b7), we use TopK() to select the"], "retrieved_docs_id": ["a7d187c571", "1bc3705cf4", "db00f64555", "481b718aab", "b1caa8817a"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the main difference in hallucination research between LLMs and MLLMs?\n", "true_answer": "The main difference is that hallucination research in LLMs typically focuses on discrepancies between generated content and real-world facts or user instructions, while research in MLLMs mainly focuses on cross-modal inconsistency between generated text response and provided visual content.", "source_doc": "hallucination.pdf", "source_id": "ce4c90f626", "retrieved_docs": ["The problem of hallucination originates from LLMs themselves. In the NLP community, the\nhallucination problem is empirically categorized into two types [ 44]: 1) factuality hallucination\nemphasizes the discrepancy between generated content and verifiable real-world facts, typically\nmanifesting as factual inconsistency or fabrication; 2) faithfulness hallucination refers to the di-\nvergence of generated content from user instructions or the context provided by the input, as\nwell as self-consistency within generated content. In contrast to pure LLMs, research efforts of\nhallucination in MLLMs mainly focus on the discrepancy between generated text response and\nprovided visual content [69,76,137],i.e., cross-modal inconsistency. This difference suggests that\nstudies in LLMs cannot be seemingly transferred to MLLMs. Therefore, there is a growing need to\ncomprehensively survey recent advancements in MLLMs\u2019 hallucination phenomena to inspire new\nideas and foster the field\u2019s development.", "may be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature\nRecently, discussions on social media [ 56] have suggested that hallucination can be regarded as\nan inherent feature of LLMs and MLLMs. The models are like dream machines. Human users\ndirect their dreams with prompts. The prompts start the dream, and based on the model\u2019s hazy\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "offer valuable insights that deepen understanding of the opportunities and challenges associated\nwith hallucinations in MLLMs. This exploration not only enhances our understanding of the limita-\ntions of current MLLMs but also offers essential guidance for future research and the development\nof more robust and trustworthy MLLMs.\nComparison with existing surveys. In pursuit of reliable generative AI, hallucination stands\nout as a major challenge, leading to a series of survey papers on its recent advancements. For pure\nLLMs, there are several surveys [ 44,129], describing the landscape of hallucination in LLMs. In\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "HaELM [ 104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4\nmodels to assess the quality of the MLLM response. In contrast, the work of Hallucination Evaluation\nbased on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination\ndetection. It collects a set of hallucination data generated by a wide range of MLLMs, simulates data\nusing ChatGPT, and trains an LLM based on LLaMA [ 99]. After that, the HaELM model becomes\nproficient in hallucination evaluation, leveraging reference descriptions of images as the basis of\nassessment.\nFaithScore [ 55]Considering the natural forms of interaction between humans and MLLMs,\nFaithScore aims to evaluate free-form responses to open-ended questions. Different from LLM-based\noverall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate,\nand analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence", "6.4 Establishing Standardized Benchmarks\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in as-\nsessing the degree of hallucination in MLLMs. In Table 1, it can be observed that there is a variety\nof evaluation benchmarks, but a lack of unified standards. Among them, one of the most popular\nbenchmarks might be POPE [ 69], which employs a \u2019Yes-or-No\u2019 evaluation protocol. However, this\nbinary-QA manner does not align with how humans use MLLMs. Accordingly, some benchmarks\nspecifically evaluate the hallucination of MLLMs in the (free-form) generative context. Yet, they\noften rely on external models, such as vision expert models or other LLMs, which limits their wide-\nspread application. Moving forward, future research can investigate standardized benchmarks that\nare theoretically sound and easy to use. Otherwise, research on methods to mitigate hallucinations\nmay be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature"], "retrieved_docs_id": ["ce4c90f626", "f565d0de3b", "76835931c1", "23d981a684", "312439a972"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the issue associated with the rapid advancement of multimodal large language models?\n", "true_answer": "These models, also known as large vision-language models, have a tendency to generate hallucinations, producing seemingly plausible but factually inaccurate content.", "source_doc": "hallucination.pdf", "source_id": "da0a465b6c", "retrieved_docs": ["2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "Figure 2: Organization of efficient multimodal large language models advancements.\n\u2022 Training surveys the landscape of training methodologies that are pivotal in the devel-\nopment of efficient MLLMs. It addresses the challenges associated with the pre-training\nstage, instruction-tuning stage, and the overall training strategy for state-of-the-art results.\n\u2022 Data and Benchmarks evaluates the efficiency of datasets and benchmarks used in the\nevaluation of multimodal language models. It assesses the trade-offs between dataset size,\ncomplexity, and computational cost, while advocating for the development of benchmarks\nthat prioritize efficiency and relevance to real-world applications.\n\u2022 Application investigates the practical implications of efficient MLLMs in various do-\nmains, emphasizing the balance between performance and computational cost. By ad-\ndressing resource-intensive tasks such as high-resolution image understanding and medical\n3", "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "cient MLLMs, research state of efficient structures and strategies, and the appli-\ncations. Finally, we discuss the limitations of current efficient MLLM research\nand promising future directions. Please refer to our GitHub repository for more\ndetails: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.\n1 Introduction\nLarge-scale pretraining, a leading approach in Artificial Intelligence(AI), has seen general-purpose\nmodels like large language and multimodal models outperform specialized deep learning models\nacross many tasks. The remarkable abilities of Large Language Models (LLM) have inspired efforts\nto merge them with other modality-based models to enhance multimodal competencies. This con-\ncept is further supported by the remarkable success of proprietary models like OpenAI\u2019s GPT-4V [1]\nand Google\u2019s Gemini[2]. As a result, Multimodal Large Language Models (MLLMs) have emerged,\nincluding the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],", "is a document and the output is its summary. So we can feed the input document into the language\nmodel and then produce the generated summary.\nDespite the successful applications in natural language processing, it is still struggling to natively use\nLLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal\nperception is a necessity to achieve arti\ufb01cial general intelligence, in terms of knowledge acquisition\nand grounding to the real world. More importantly, unlocking multimodal input [ TMC+21,HSD+22,\nWBD+22,ADL+22,AHR+22,LLSH23 ] greatly widens the applications of language models to\nmore high-value areas, such as multimodal machine learning, document intelligence, and robotics.\nIn this work, we introduce KOSMOS -1, a Multimodal Large Language Model (MLLM) that can\nperceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e.,"], "retrieved_docs_id": ["da0a465b6c", "542e5c49da", "ac70fcc9f2", "e021f7788d", "74bb21ad4f"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does HyDE generate a hypothetical document relevant to the query?\n", "true_answer": "HyDE generates a hypothetical document relevant to the query by establishing query vectors through the use of text indicators and using these indicators to generate a document that captures the relevant pattern, even if it may not truly exist.", "source_doc": "RAG.pdf", "source_id": "71a4057422", "retrieved_docs": ["the semantic space of the user\u2019s query and documents is very\nnecessary. This section introduces two key technologies to\nachieve this goal.\nQuery Rewrite\nThe most intuitive way to align the semantics of\nquery and document is to rewrite the query. As\nmentioned in Query2Doc [Wang et al. , 2023b ]and ITER-\nRETGEN [Shao et al. , 2023 ], the inherent capabilities of\nlarge language models are utilized to generate a pseudo-\ndocument by guiding it, and then the original query is\nmerged with this pseudo-document to form a new query.\nIn HyDE [Gao et al. , 2022 ], query vectors are established\nthrough the use of text indicators, using these indicators to\ngenerate a \u2019hypothetical\u2019 document that is relevant, yet may\nnot truly exist, it only needs to capture the relevant pattern.\nRRR [Maet al. , 2023a ]introduced a new framework that in-\nverts the order of retrieval and reading, focusing on query\nrewriting. This method generates a query using a large lan-", "different scenarios, including using query engines pro-\nvided by frameworks like LlamaIndex, employing tree\nqueries, utilizing vector queries, or employing the most\nbasic sequential querying of chunks.\u2022HyDE: This approach is grounded on the assumption\nthat the generated answers may be closer in the embed-\nding space than a direct query. Utilizing LLM, HyDE\ngenerates a hypothetical document (answer) in response\nto a query, embeds the document, and employs this em-\nbedding to retrieve real documents similar to the hypo-\nthetical one. In contrast to seeking embedding similarity\nbased on the query, this method emphasizes the embed-\nding similarity from answer to answer. However, it may\nnot consistently yield favorable results, particularly in\ninstances where the language model is unfamiliar with\nthe discussed topic, potentially leading to an increased\ngeneration of error-prone instances.\nModular RAG\nThe modular RAG structure breaks away from the traditional", "the query. As a solution, LLM can be utilized to rewrite the\nquery for enhancing the understanding of the query intent\nand incorporating additional knowledge into the query\nthrough well-designed instructions. The rewritten query\ncan take the form of an improved version of the original\nquery [791], a document in the corpus that related to the\nquery [792], or an expansion of the query that concatenated\nwith a pseudo generated document [793]. In addition, docu-\nments can also be expanded with queries that are generated\nbased on the original documents using LLMs for context\nextension [794].\nRemaining Issues. In this part, we further discuss several\nimportant issues to apply LLMs to improve IR systems.\nFirst, though LLMs are capable of being as general-purpose\ntask solvers, they are not directly well suited for existing\nIR systems: they require high overhead for inference [774,\n782], have limitations in modeling long texts or document", "formulating responses in different cases. From the human-written principles, ICL exemplars, and\nthe incoming self-instructed prompts, the LLM can trigger the matching rules and generate the\nexplanations for a refused answer if the query is detected as a harmful or ill-formed one.\n3.Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on\nthe self-aligned responses, generated by the LLM itself through prompting, while pruning the\nprinciples and demonstrations for the fine-tuned model. The fine-tuning process enables our\nsystem to directly generate responses that are well-aligned with the helpful, ethical, and reliable\nprinciples across a wide range of queries, due to shared model parameters. Notice that the fine-\ntuned LLM can directly generate high-quality responses for new queries without explicitly using\nthe principle set and the ICL exemplars.\n4.Verbose Cloning: Lastly, we employ context distillation [ 18,3] to enhance the system\u2019s capability", "information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-"], "retrieved_docs_id": ["71a4057422", "d96393bb4b", "de435e084d", "28b8b4b020", "7fabdba415"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a model that tailors for medical applications and significantly lowers parameter demands?\n", "true_answer": "MoE-TinyMed [64]", "source_doc": "multimodal.pdf", "source_id": "51c7c3d212", "retrieved_docs": ["paradigm in biomedicine, achieving state-of-the-art results on many applications, including medical\nquestion answering [194] and medical image classification [195]. Recently, multimodal generative\nAI has emerged as an exciting frontier in the biomedical domain, expanding the application scope\nfrom single-modality to multi-modality, such as VQA and radiology report generation.\nThe mixture of Expert Tuning has effectively enhanced the performance of general MLLMs with\nfewer parameters, yet its application in resource-limited medical settings has not been fully explored.\nMoE-TinyMed [64] is a model tailored for medical applications that significantly lower parameter\ndemands. LLaV A-Rad [63] is a state-of-the-art tool that demonstrates rapid performance on a sin-\ngle V100 GPU in private settings, making it highly applicable for real-world clinical scenarios. It\nemploys a modular approach, integrating unimodal pre-trained models and emphasizing the training", "parameters, usually require significant computational resources. To democratize LLMs, considerable\nefforts have been taken to mitigate their high computational cost. Many of the notable advancements\nto date have centered on model quantization, a process where parameters are quantized into lower\nbit-level representations. The fast pace of LLM quantization research (Dettmers et al., 2022; Frantar\net al., 2023a; Xiao et al., 2023; Ahmadian et al., 2023) has led to substantial resource savings for\nthese models (Sheng et al., 2023; Lin et al., 2023).\nNetwork pruning (LeCun et al., 1989; Hassibi et al., 1993; Han et al., 2015), on the other hand,\nshrinks network sizes by removing specific weights from the model \u2013 essentially setting them to\nzero. Along with quantization, it is often considered another popular approach for compressing\nneural networks. However, it has received relatively little focus in compressing LLMs. This seems", "relevant as a way to minimize memory requirements and minimize compute intensity (Dettmers\net al., 2022; Xiao et al., 2022; Frantar et al., 2022; Park et al., 2022a; Kim et al.).\nModelEfficiencyatInferenceTime Researchinmodelcompressionmostlyfallsinthecategories\nof quantization techniques (Jacob et al., 2018; Courbariaux et al., 2014; Hubara et al., 2016; Gupta\net al., 2015), efforts to start with a network that is more compact with fewer parameters, layers or\ncomputations (architecture design) (Howard et al., 2017; Iandola et al., 2016; Kumar et al., 2017),\nstudent networks with fewer parameters that learn from a larger teacher model (model distillation)\n(Hinton et al., 2015) and finally pruning by setting a subset of weights or filters to zero (Louizos\net al., 2017; Wen et al., 2016; LeCun et al., 1990; Hassibi et al., 1993a; Str\u00f6m, 1997; Hassibi et al.,\n1993b; See et al., 2016; Narang et al., 2017; Frantar & Alistarh, 2023; Sanh et al., 2020). Often, a", "seen relatively little adoption (Narang et al., 2021); adopting some of these recommended\npractices could yield a significantly better model. We take a middle ground and focus on\nmodel families that have been shown to scale well, and that have reasonable support in\npublicly available tools and codebases. We ablate components and hyperparameters of the\nmodels, seeking to make the best use of our final compute budget.\nExperimental Design for Ablations One of the main draws of LLMs has been their\nability to perform tasks in a \u201czero/few-shot\u201d way: large enough models can perform novel\ntasks simply from in-context instructions and examples (Radford et al., 2019), without ded-\n16.github.com/bigscience-workshop/promptsource\n14", "number of single training runs, and the lack of suf-\nficient experimental coverage can severely inhibit\nscientific understanding of what makes an LLM\nperform well. While this issue is not unique to\nLLMs, they tend to be larger in the number of\nparameters\u2014and therefore compute requirements,\nfeedback loop times, and training costs\u2014than mod-\nels in most other fields.\nCurse of (Design) Dimensionality\nCommon design spaces of LLM experi-\nments are high-dimensional.\nOne possible way forward is to encourage the\ncommunity to use techniques like Bayesian opti-\nmization (BO) with dimensionality reduction [ 594,\n374], where we use a non-linear feature mapping to\nmap the input (the hyper-parameter configuration)\nonto a lower dimensional manifold followed by a\nBO procedure to optimize the underlying black-\nbox function (the LLM with respect to the hyper-\nparameters). Another suitable tool to explore the\ndesign space efficiently can be treatment effect es-"], "retrieved_docs_id": ["51c7c3d212", "c7b0d8c41e", "8ad1c4fdc7", "89d315a0af", "ec6ec438c3"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How can increasing the scale of data affect bias in a model?\n", "true_answer": "Increasing the scale of data can alleviate the effect of bias in a model, but it cannot fully resolve it due to the long-tail distribution of the real world.", "source_doc": "hallucination.pdf", "source_id": "44cf8ffcb0", "retrieved_docs": ["dataset has strong effects on the behavior of the model. Frequently appeared objects and object\nco-occurrence are two prominent types of statistical bias, as discussed in [ 69,90,137]. For example,\n\u2018person \u2019 might be one of the most frequently appearing objects in the training data. During inference,\neven if the given image does not contain a person, the model still tends to predict the presence\nof a person. On the other hand, object co-occurrence refers to the phenomenon that the model\nwill remember which two objects usually \u2018go together\u2019 [ 90]. For instance, given an image of a\nkitchen with a refrigerator, MLLMs are prone to answer \u2018 Yes\u2019 when asked about a microwave, as\nrefrigerators and microwaves frequently appear together in kitchen scenes. Bias exists in most\ndatasets. Increasing the scale of data may alleviate the effect, but cannot fully resolve it, given the\nlong-tail distribution of the real world.\n3.2 Model", "\u2022Increasing the diversity of data sources. Recent studies\nhave empirically shown that training on excessive data\nabout a certain domain would degrade the generalization\ncapability of LLMs on other domains [35, 64]. In contrast,\nincreasing the data source heterogeneity ( e.g., including\ndiverse data sources) is critical for improving the down-\nstream performance of LLMs [212, 229, 230]. To further\nexamine the effect of different data sources, some studies\nhave conducted ablation experiments by removing each\ndata source one by one, and pre-train LLMs with specially\ncurated datasets [212]. It has been shown that dropping data\nsources with high heterogeneity ( e.g., webpages) impacts\nLLM\u2019s abilities more severely than dropping sources with\nlow heterogeneity ( e.g., academic corpus).\n\u2022Optimizing data mixtures. In addition to manually set-\nting the data mixtures, several studies have proposed to\noptimize the data mixtures for improving the model pre-", "from small models could also apply to large models. For\ninstance, small proxy models can be trained to find the\noptimal schedule of the data mixture for large models [59].\nSecondly, the training of large-scale models takes a long\ntime, often suffering from issues such as training loss spike,\nand scaling law can be employed to monitor the training\nstatus of LLMs, e.g.,identifying abnormal performance at an\nearly time. Despite that scaling law characterizes a smooth\ntrend of performance increase (or loss decrease), it also\nindicates that diminishing returns7might occur as model\nscaling. An empirical study [58] from the OpenAI team\nhas shown that representation quality or semantic content\ncan still effectively improve even if approaching the point\nof diminishing returns ( i.e., approaching the irreducible\nloss) [58]. This finding suggests that training large models\nare promising for improving the performance of down-\nstream tasks. To further explore scaling effect, a potential", "We use these key properties of Pythia in order to study for\nthe first time how properties like gender bias, memorization,\nand few-shot learning are affected by the precise training\ndata processed and model scale. We intend the following ex-\nperiments to be case studies demonstrating the experimental\nsetups Pythia enables, and to additionally provide directions\nfor future work.\nMitigating Gender Bias There is much work cataloging\nhow language models reflect the biases encoded in their\ntraining data. However, while some work has explored\nfinetuning\u2019s effects on bias in language models (Gira et al.,\n2022; Kirtane et al., 2022; Choenni et al., 2021), or the\nrelationship between the corpus statistics and the measured\nbias (Bordia & Bowman, 2019; Van der Wal et al., 2022b),\nresearchers have generally lacked the tools to study the role\nof the training data on the learning dynamics of bias in large\nlanguage models of different sizes. To demonstrate what is", "and data distributions between different SFT\nabilities, the impact of data ratio is minimal .\nHowever, when there is some degree of similari-\nties, the data ratio can lead to noticeable perfor-\nmance fluctuations.\nQ2: Under extremely limited general data re-\nsources, does the ratio of specialized data have\nan impact on the model\u2019s performance? We\nfurther explore the impact of different ratios of\nspecialized data when the model has just acquired\na certain level of general human-aligning ability\n(k= 1/64). The bottom 3 graphs of Figure 4\npresent comparative experiments between two set-\ntings. We observe that regardless of whether the\ndata amount for general capabilities is abundant\n(k= 1) or scarce ( k= 1/64), the performance on\nMT-Bench shows no significant fluctuations with\nvarying proportions of specialized data. Further-\nmore, in mathematical reasoning, 1/64 general data\nsetup exhibited a scaling trend that is almost iden-\ntical to the full general data setup. However, for"], "retrieved_docs_id": ["44cf8ffcb0", "99b58f7ffa", "e95f9c7721", "a72d5b292c", "9c30eb9b32"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the challenge faced by MLLMs in tasks requiring intricate recognition?\n", "true_answer": "MLLMs face challenges in tasks like crowd counting and OCR of small characters, which require detailed recognition.", "source_doc": "multimodal.pdf", "source_id": "8beea9b82e", "retrieved_docs": ["\u2022 At present, efficient MLLMs face challenges in processing extended-context multimodal\ninformation, and they are typically limited to accepting single images. This constrains the\nadvancement of more sophisticated models capable of handling an increased number of\nmultimodal tokens. Such models would be beneficial for applications like comprehending\nlengthy videos and analyzing extensive documents that incorporate a mix of images and\ntext, creating more versatile and powerful systems.\n\u2022 The predominant efficient MLLMs mainly support dual input modalities - images and texts,\nand a singular output modality - text. However, the tangible world encompasses a more\nextensive array of modalities. By expanding the scope of efficient MLLMs to accommodate\n23", "and preserving user privacy.\nIn light of these challenges, there has been growing attention on the study of efficient MLLMs.\nThe primary objective of these endeavors is to decrease the resource consumption of MLLMs\nand broaden their applicability while minimizing performance degradation. Research on efficient\nMLLMs began with replacing large language models with lightweight counterparts and performing\ntypical visual instruction tuning. Subsequent studies further enhanced capabilities and expanded\nuse cases in the following ways: (1) lighter architectures were introduced with an emphasis on ef-\nficiency, aiming to reduce the number of parameters or computational complexity[25, 13, 18]; (2)\nmore specialized components were developed, focusing on efficiency optimizations tailored to ad-\nvanced architectures or imbuing specific properties, such as locality[19, 17, 12]; and (3) support\nfor resource-sensitive tasks was provided, with some works employing visual token compression", "ing images from public sources with manually-collected\ntext instructions for perception and cognition evaluations.\nMMBench [838] transforms these instructions into multiple-\nchoice questions and introduces CircularEval to ensure\nevaluation consistency. SEED-Bench [854] further considers\ntemporal understanding tasks and enlarges the evaluation\nscale to 19K multiple-choice questions with the assistance of\nLLMs. MM-Vet [855] presents more complex tasks to assess\nthe integrated multimodal capabilities of MLLMs. It starts\nby defining six essential multimodal abilities and then cre-\nates intricate questions by combining multiple abilities. In\nsummary, the above benchmarks collectively contribute to\nthe comprehensive evaluation and improved development\nof MLLMs.\nKey Points for Improving MLLMs. To develop capable\nMLLMs, we continue to discuss three key points to improve\nthe model capacities, from the perspectives of instruction\ndata, training strategy, and safety and alignment.", "(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"\nthe world via images or videos. Combined with strong reasoning and language generation abilities,\nMLLMs trigger a series of downstream tasks in multimodal domains, such as image/video captioning\nand visual question answering. Additionally, MLLMs serve as the foundation for applications in\nother fields, such as AI assistants, embodied agents, and robotics.\nIntegrating the two modalities of vision and language involves primarily two types of approaches.\nThe first line of work is built upon off-the-shelf pre-trained uni-modal models. Specifically, these\nMLLMs usually incorporate a learnable interface between pre-trained visual encoders and LLMs.\nThe interface extracts and integrates information from visual modalities. Such interfaces can be\nfurther categorized into 1) learnable query-based and 2) projection layer based. Learnable query-", "for resource-sensitive tasks was provided, with some works employing visual token compression\nto boost efficiency, enabling the transfer of MLLM capabilities to resource-intensive tasks such as\nhigh-resolution image and video understanding[35, 39, 14, 40].\nIn this survey, we aim to present an exhaustive organization of the recent advancements in the rapidly\nevolving field of efficient MLLMs, as depicted in Figure.2. We organize the literature in a taxonomy\nconsisting of six primary categories, encompassing various aspects of efficient MLLMs, including\narchitecture ,efficient vision ,efficient LLMs ,training ,data and benchmarks , and applications .\n\u2022 Architecture focuses on the MLLM framework developed by efficient techniques to reduce\nthe computational cost. The architecture is composed of multiple modality-based funda-\nmental models, exhibits characteristics distinct from single-modal models, thus promoting\nthe development of novel technologies."], "retrieved_docs_id": ["a1bd2d5193", "04b6ebc53f", "df8870e586", "3799aaa107", "cd55ca1477"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does Mixture of Experts (MoE) decompose a large-scale model?\n", "true_answer": "MoE decomposes a large-scale model into several smaller models, each focusing on learning a specific part of the input data.", "source_doc": "multimodal.pdf", "source_id": "811840a2cd", "retrieved_docs": ["into a high-dimensional space, where task-related information can be more readily captured. In\nthis new space, each word in the text sequence is represented as a high-dimensional vector, and the\ndistances between these vectors serve to measure their similarities. Low-Rank [147] aims to decom-\npose a high-dimensional matrix into the product of two lower-dimensional matrices. Consequently,\nby calculating the inverses of these two lower-dimensional matrices, an approximate inverse of the\nattention matrix can be obtained, thereby significantly reducing computational complexity.\n4.2 Framework\nMixture of Experts The core idea behind MoE [89] is to decompose a large-scale model into sev-\neral smaller models, each of which focuses on learning a specific part of the input data. During the\ntraining process, each expert is assigned a weight that determines its importance within the overall\nmodel. During the inference phase, given an input, all experts are ranked, and the most relevant", "resentative LLMs based on prefix decoders include GLM-\n130B [93] and U-PaLM [118].\nMixture-of-Experts. For the above three types of archi-\ntectures, we can further extend them via the mixture-of-\nexperts (MoE) scaling, in which a subset of neural network\nweights for each input are sparsely activated, e.g., Switch\nTransformer [25] and GLaM [112]. The major merit is that\nMoE is a flexible way to scale up the model parameter while\nmaintaining a constant computational cost [25]. It has been\nshown that substantial performance improvement can be\nobserved by increasing either the number of experts or the\ntotal parameter size [246]. Despite the merits, training large\nMoE models may suffer from instability issues due to the\ncomplex, hard-switching nature of the routing operation.\nTo enhance the training stability of MoE-based language\nmodels, techniques such as selectively using high-precision\ntensors in the routing module or initializing the model with", "Under review as a conference paper at ICLR 2017\nResults: Results are reported in Table 6. All the combinations containing at least one the two\nlosses led to very similar model quality, where having no loss was much worse. Models with higher\nvalues ofwloadhad lower loads on the most overloaded expert.\nB H IERACHICAL MIXTURE OF EXPERTS\nIf the number of experts is very large, we can reduce the branching factor by using a two-level\nhierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted com-\nbination of \u201cexperts\", each of which is itself a secondary mixture-of-experts with its own gating\nnetwork.3If the hierarchical MoE consists of agroups ofbexperts each, we denote the primary gat-\ning network by Gprimary , the secondary gating networks by (G1,G2..Ga), and the expert networks\nby(E0,0,E0,1..Ea,b). The output of the MoE is given by:\nyH=a\u2211\ni=1b\u2211\nj=1Gprimary (x)i\u00b7Gi(x)j\u00b7Ei,j(x) (12)\nOur metrics of expert utilization change to the following:", "The Mixture-of-Experts (MoE) layer consists of a set of n\u201cexpert networks\" E1,\u00b7\u00b7\u00b7,En, and a\n\u201cgating network\" Gwhose output is a sparse n-dimensional vector. Figure 1 shows an overview\nof the MoE module. The experts are themselves neural networks, each with their own parameters.\nAlthough in principle we only require that the experts accept the same sized inputs and produce the\nsame-sized outputs, in our initial investigations in this paper, we restrict ourselves to the case where\nthe models are feed-forward networks with identical architectures, but with separate parameters.\nLet us denote by G(x)andEi(x)the output of the gating network and the output of the i-th expert\nnetwork for a given input x. The output yof the MoE module can be written as follows:\ny=n\u2211\ni=1G(x)iEi(x) (1)\nWe save computation based on the sparsity of the output of G(x). WhereverG(x)i= 0, we need not\ncomputeEi(x). In our experiments, we have up to thousands of experts, but only need to evaluate", "Under review as a conference paper at ICLR 2017\nWhile the introduced technique is generic, in this paper we focus on language modeling and machine\ntranslation tasks, which are known to bene\ufb01t from very large models. In particular, we apply a MoE\nconvolutionally between stacked LSTM layers (Hochreiter & Schmidhuber, 1997), as in Figure 1.\nThe MoE is called once for each position in the text, selecting a potentially different combination\nof experts at each position. The different experts tend to become highly specialized based on syntax\nand semantics (see Appendix E Table 9). On both language modeling and machine translation\nbenchmarks, we improve on best published results at a fraction of the computational cost.\n1.3 R ELATED WORK ON MIXTURES OF EXPERTS\nSince its introduction more than two decades ago (Jacobs et al., 1991; Jordan & Jacobs, 1994),\nthe mixture-of-experts approach has been the subject of much research. Different types of expert"], "retrieved_docs_id": ["811840a2cd", "3c084f5868", "5f55e9e161", "906a8625a5", "00ef434257"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the name of the concurrent work that also uses preference-based reinforcement learning to improve the faithfulness of MLLMs?\n", "true_answer": "Silkie [68]", "source_doc": "hallucination.pdf", "source_id": "0b66cff7c9", "retrieved_docs": ["samples) and hallucinatory descriptions (negative samples). HA-DPO then trains the model using\nthese sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions. This\ngoal is achieved through direction preference optimization (DPO), which optimizes a specific loss\nfunction designed to maximize the model\u2019s preference for positive samples while minimizing its\npreference for negative samples.\nA concurrent work, Silkie [ 68], introduces a similar approach of utilizing preference-based\nreinforcement learning to enhance the faithfulness of MLLMs. Specifically, it emphasizes the\nconcept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more\nrobust MLLM, i.e., GPT-4V [ 83]. Responses are first generated by models from 12 MLLMs, and then\nassessed by GPT-4V. The constructed dataset, termed as VLFeedback, contains preferences distilled\nfrom GPT-4V and is utilized to train other MLLMs through direct preference optimization.", "from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based\nmethods. These methods rely on the traditional preference data generation process in LLMs, where\nboth preferred and dispreferred responses may potentially be incorrect. Therefore, this work pro-\nposes the Preference Optimization in VLLM with AI-Generated Dispreferences (POVID) framework,\naiming to exclusively generate dispreferred feedback data using AI models. The dispreferred data\nis generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and\n2) provoking inherent hallucination by introducing noise into MLLMs. In the DPO optimization\nframework, the ground-truth multimodal instructions serves as the preferred answers.\nReinforcement Learning from Human Feedback (RLHF). HalDetect [ 32] first introduces the M-", "Contrastive Loss e.g.HACL [52]\nOthers e.g.EOS [120]\nReinforcement LearningAutomatic\nMetric-basede.g.MOCHa [5]\nRLAIF-based e.g.HA-DPO [133], POVID [136]\nRLHF-based e.g.LLaVA-RLHF [96], RLHF-V [119]\nMitigating Inference-related\nHallucinations (\u00a75.4)Generation InterventionContrastive Decoding e.g.VCD [64], IBD [139]\nGuided Decoding e.g.MARINE [131], GCD [24]\nOthers e.g.OPERA [45], Skip\u2018\\n\u2019 [36]\nPost-hoc Correction e.g.Woodpecker [114], Volcano [63], LURE [137]\nFig. 1. The main content flow and categorization of this survey.\n(RLHF) [ 19,84,94] emerges as a notable approach for achieving alignment through reinforcement\nlearning. RLHF typically employs a preference model [ 7], trained to predict preference rankings\nbased on prompts and human-labeled responses. To better align with human preferences, RLHF opti-\nmizes the LLM to generate outputs that maximize rewards provided by the trained preference model,", "preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\na large text dataset. While the most straightforward approach to preference learning is supervised\nfine-tuning on human demonstrations of high quality responses, the most successful class of methods\nis reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [ 12,2]). RLHF methods fit\na reward model to a dataset of human preferences and then use RL to optimize a language model\npolicy to produce responses assigned high reward without drifting excessively far from the original\nmodel. While RLHF produces models with impressive conversational and coding abilities, the RLHF\npipeline is considerably more complex than supervised learning, involving training multiple LMs and\nsampling from the LM policy in the loop of training, incurring significant computational costs.\nIn this paper, we show how to directly optimize a language model to adhere to human preferences,", "as image captions.\nSimilarly, RLHF-V [ 119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect\nhuman feedback in the form of fine-grained segment-level corrections, providing a clear, dense,\nand fine-grained human preference. 2) at the method level, it proposes dense direct preference\noptimization (DDPO) that directly optimizes the policy model against dense and fine-grained\nsegment-level preference.\nAnother similar work, ViGoR [ 110], also designs a fine-grained reward model to update pre-\ntrained MLLMs, aiming to improve visual grounding and reduce hallucination. The reward modeling\nin this work encompasses both human preferences and automatic metrics. Specifically, it collects\nhuman judgment and preferences for the responses generated by MLLMs by asking crowd-workers\nto provide fine-grained feedback at the sentence level. The collected human preference data is"], "retrieved_docs_id": ["0b66cff7c9", "4ed1c08405", "9644813f29", "6baa286ac1", "9da785fedf"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How can the CHAIR metric be affected in the context of MLLMs?\n", "true_answer": "The CHAIR metric can be affected by the instruction designs and the length of generated captions in the context of MLLMs.", "source_doc": "hallucination.pdf", "source_id": "84a3c00c17", "retrieved_docs": ["MHaluBench [13] arXiv\u201924 Feb. MSCOCO [70] 1,860 Gen Acc/P/R/F \u2713 \u2713 \u2717 T2I\nVHTest [46] arXiv\u201924 Feb. MSCOCO [70] 1,200 Dis & Gen Acc \u2713 \u2713 \u2717 \u2713\nHal-Eavl [53] arXiv\u201924 Feb.MSCOCO [70] &\nLAION [92]10,000 Dis & GenAcc/P/R/F &\nLLM Assessment\u2713 \u2713 \u2713 Obj. Event\n(denoted as CHAIR \ud835\udc60):\nCHAIR \ud835\udc56=|{hallucinated objects }|\n|{all objects mentioned }|,\nCHAIR \ud835\udc60=|{sentences with hallucinated object }|\n|{all sentences}|.\nIn the paper of CHAIR [ 90], the range of objects is restricted to the 80 MSCOCO objects. Sentence\ntokenization and synonyms mapping are applied to determine whether a generated sentence\ncontains hallucinated objects. Ground-truth caption and object segmentations both serve as ground-\ntruth objects in the computation. In the MLLM era, this metric is still widely used for assessing the\nresponse of MLLMs.\nPOPE [ 69]. When used in MLLMs, the work of [ 69] argues that the CHAIR metric can be\naffected by the instruction designs and the length of generated captions. Therefore, it proposes a", "tokens. The issue of \u2019losing attention\u2019 would also lead to the model\u2019s output response being\nirrelevant to the visual content.\n4 HALLUCINATION METRICS AND BENCHMARKS\nIn this section, we present a comprehensive overview of existing hallucination metrics and bench-\nmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge\nMLLMs. Currently, the primary focus of these benchmarks is on evaluating the object hallucination\nof MLLM-generated content. Tab. 1 illustrates a summary of related benchmarks.\nCHAIR [ 90]. As one of the early works, the metric of CHAIR was proposed to evaluate ob-\nject hallucination in the traditional image captioning task. This is achieved by computing what\nproportion of words generated are actually in the image according to the ground truth sentences\nand object segmentations. The computation of the CHAIR metric is straightforward and easy", "10 Bai, et al.\nTable 1. Summary of most relevant benchmarks and metrics of object hallucination in MLLMs. The order is\nbased on chronological order on arxiv. In the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1-\nScore.\nBenchmark VenueUnderlying\nData SourceSizeTask\nTypeMetricHallucination Type\nCategory Attribute Relation Others\nCHAIR [90] EMNLP\u201918 MSCOCO [70] 5,000 Gen CHAIR \u2713 \u2717 \u2717 \u2717\nPOPE [69] EMNLP\u201923 MSCOCO [70] 3,000 Dis Acc/P/R/F1 \u2713 \u2717 \u2717 \u2717\nMME [113] arXiv\u201923 Jun MSCOCO [70] 1457 Dis Acc/Score \u2713 \u2713 \u2717 \u2713\nCIEM [42] NeurIPS-W\u201923 MSCOCO [70] 78120 Dis Acc \u2713 \u2717 \u2717 \u2717\nM-HalDetect [32] arXiv\u201923 Aug. MSCOCO [70] 4,000 Dis Reward Model Score \u2713 \u2717 \u2717 \u2717\nMMHal-Bench [96] arXiv\u201923 Sep. Open-Images [61] 96 Gen LLM Assessment \u2713 \u2717 \u2717 \u2713\nGAVIE [73] ICLR\u201924 Visual-Genome [59] 1,000 Gen LLM Assessment Not Explicitly Stated\nNOPE [77] arXiv\u201923 Oct. Open-Images [61] 36,000 Dis Acc/METEOR [3] \u2713 \u2717 \u2717 \u2717\nHaELM [104] arXiv\u201923 Oct. MSCOCO [70] 5,000 Gen LLM Assessment Not Explicitly Stated", "OpenCHAIR [ 5]The traditional CHAIR metric relies on the closed list of 80 objects in the\nMS-COCO dataset, limiting its application. To measure object hallucination in the open-vocabulary\nsettings, OpenCHAIR expands CHAIR by relaxing the strong reliance on the closed vocabulary.\nThe \u2019open-vocabulary\u2019 manifests in two ways. Firstly, when building the benchmark, it organizes a\ndataset consisting of synthetic images with corresponding captions, which include diverse, open-\nvocabulary objects using a text-to-image diffusion model. Secondly, during computing the metric,\nCHAIR checks if words or their synonyms (as given by fixed vocabulary lists) are found in ground-\ntruth annotations. In contrast, OpenCHAIR extracts concrete objects from a predicted caption and\nidentifies hallucinated objects from this list by querying an LLM. Similar to CHAIR, the final metric\ncomputation is based on the hallucination rate.", "and object segmentations. The computation of the CHAIR metric is straightforward and easy\nto understand. The metric has two variants: per-instance (denoted as CHAIR \ud835\udc56) and per-sentence\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024."], "retrieved_docs_id": ["84a3c00c17", "52c95dc6e8", "8705831e19", "c7d602443c", "7168b77a46"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "Why is parametric knowledge not updated dynamically in this model?\n", "true_answer": "The model parameters cannot be updated dynamically, making the parametric knowledge susceptible to becoming outdated over time.", "source_doc": "RAG.pdf", "source_id": "9ebdac13a1", "retrieved_docs": ["and more specific knowledge. Secondly, since the model\nparameters cannot be updated dynamically, the parametric\nknowledge is susceptible to becoming outdated over time.\nLastly, an expansion in parameters leads to increased com-arXiv:2312.10997v1  [cs.CL]  18 Dec 2023", "can be updated during inference time to reflect\nan updated state of the underlying knowledge.\nE.g., Lewis et al. [304] demonstrate that swapping\ntheir model\u2019s non-parametric memory with an up-\ndated version enabled it to answer questions about\nworld leaders who had changed between the mem-\nory collection dates. Similarly, Izacard et al. [236]\ndemonstrate that their retrieval-augmented model\ncan update its knowledge forward and backward in\ntime by swapping the index.\n2.11 Brittle Evaluations\nOne reason why the evaluation of language models\nis a challenging problem is that they have an un-\neven capabilities surface \u2014a model might be able\nto solve a benchmark problem without issues, but\na slight modification of the problem (or even a sim-\nple change of the prompt) can give the opposite\nresult [ 675,342,533] (see Section 2.7). Unlike\nhumans, we cannot easily infer that an LLM that\ncan solve one problem will have other related capa-\nbilities. This means that it is difficult to assess the", "play distinct roles. Parametric knowledge is acquired through\ntraining LLMs and stored in the neural network weights, rep-\nresenting the model\u2019s understanding and generalization of\nthe training data, forming the foundation for generated re-\nsponses. Non-parametric knowledge, on the other hand, re-\nsides in external knowledge sources such as vector databases,\nnot encoded directly into the model but treated as updatable\nsupplementary information. Non-parametric knowledge em-\npowers LLMs to access and leverage the latest or domain-\nspecific information, enhancing the accuracy and relevance\nof responses.\nPurely parameterized language models (LLMs) store their\nworld knowledge, which is acquired from vast corpora, in\nthe parameters of the model. Nevertheless, such models have\ntheir limitations. Firstly, it is difficult to retain all the knowl-\nedge from the training corpus, especially for less common\nand more specific knowledge. Secondly, since the model", "to be still at a superficial level. In addition, existing studies\nalso explore editing parameters of language models to up-\ndate intrinsic knowledge [669\u2013671]. Nevertheless, previous\nwork [672] has shown that several parameter editing meth-\nods perform not well on LLMs, though they can improve\nthe performance of small language models. Therefore, it\nis still difficult to directly amend intrinsic knowledge or\ninject specific knowledge into LLMs, which remains an\nopen research problem [672]. Recently, a useful framework\nEasyEdit [673] has been released to facilitate the research of\nknowledge editing for LLMs.\nKnowledge Recency\nThe parametric knowledge of LLMs is hard to be\nupdated in a timely manner. Augmenting LLMs\nwith external knowledge sources is a practical\napproach to tackling the issue. However, how\nto effectively update knowledge within LLMs\nremains an open research problem.\n7.1.3 Complex Reasoning\nComplex reasoning refers to the ability of understanding", "numerous shortcomings. They often fabricate\nfacts [Zhang et al. , 2023b ]and lack knowledge when\ndealing with specific domains or highly specialized\nqueries [Kandpal et al. , 2023 ]. For instance, when the infor-\nmation sought extends beyond the model\u2019s training data or\nrequires the latest data, LLM may fail to provide accurate\nanswers. This limitation poses challenges when deploying\ngenerative artificial intelligence in real-world production\nenvironments, as blindly using a black-box LLM may not\nsuffice.\nTraditionally, neural networks adapt to specific domains\nor proprietary information by fine-tuning models to param-\neterize knowledge. While this technique yields significant\nresults, it demands substantial computational resources, in-\ncurs high costs, and requires specialized technical expertise,\nmaking it less adaptable to the evolving information land-\nscape. Parametric knowledge and non-parametric knowledge\nplay distinct roles. Parametric knowledge is acquired through"], "retrieved_docs_id": ["9ebdac13a1", "e1118fca72", "fc82ce8e28", "66aa6c0fd1", "b40c0db2f1"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "Which model outperforms both GPT-4V and Med-PaLM in terms of efficiency and effectiveness?\n", "true_answer": "LLaV A-Rad", "source_doc": "multimodal.pdf", "source_id": "0da5fa4a36", "retrieved_docs": ["employs a modular approach, integrating unimodal pre-trained models and emphasizing the training\nof lightweight adapters. As a result, LLaV A-Rad outperforms larger models such as GPT-4V and\nMed-PaLM in terms of standard metrics, showcasing its superior efficiency and effectiveness.\n22", "2017) to compare answers and guard against false negatives, which arise from equivalent answers with different surface\nforms. For MGSM, we use 8-shot chain-of-thought prompts and in-language exemplars provided by Shi et al. (2023).\nWe show the results in Table 7. PaLM 2 outperforms PaLM dramatically on all datasets. On MATH, PaLM 2 is\ncompetitive with the state-of-the-art performance achieved by the dedicated Minerva model. On GSM8K, PaLM 2\noutperforms Minerva and GPT-4 while on MGSM, it surpasses the state of the art even without self-consistency.\n4.4 Coding\nCode language models are among the most economically signi\ufb01cant and widely-deployed LLMs today; code LMs\nare deployed in diverse developer tooling (Github, 2021; Tabachnyk & Nikolov, 2022), as personal programming\nassistants (OpenAI, 2022; Hsiao & Collins, 2023; Replit, 2022), and as competent tool-using agents (OpenAI, 2023a).", "LLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 \u00d7smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. \u201cBooks \u2013 2TB\u201d or\n\u201cSocial media conversations\u201d). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modi\ufb01cations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our", "LLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 \u00d7smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. \u201cBooks \u2013 2TB\u201d or\n\u201cSocial media conversations\u201d). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modifications we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our", "and previous models have only benchmarked on a subset of tasks and shots, comparisons are more limited\nthan other more established benchmarks. The three models evaluated on 58 tasks in common, so this \ufb01gure\npresents results only on these 58 tasks. We see that PaLM signi\ufb01cantly outperforms both GPT-3,7Gopher,\nand Chinchilla, and 5-shot PaLM 540B achieves a higher score than the average score of the humans asked to\nsolve the same tasks. PaLM 540B 5-shot outperforms the prior SOTA on 44 out of the 58 common tasks,\nwith per-task results shown in Figure 4. In addition, the performance of PaLM models as a function of\nscale appears to follow log-linear behavior, indicating that further scaling up is likely to result in increased\n6The benchmark is hosted at https://github.com/google/BIG-bench. The full evaluation results of PaLM on BIG-bench will\nbe made available there.\n7Note that due to timing issues in the paper writing process, the models from OpenAI evaluated here are those associated"], "retrieved_docs_id": ["0da5fa4a36", "eac4e5934c", "acbecf1628", "acbecf1628", "5155d20891"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the only concurrent survey on the hallucination problem in Multimodal Large Language Models (MLLMs)?\n", "true_answer": "The only concurrent survey on the hallucination problem in MLLMs is a short survey on the hallucination problem of LVLMs, as mentioned in the context.", "source_doc": "hallucination.pdf", "source_id": "33d47ad8cc", "retrieved_docs": ["Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the"], "retrieved_docs_id": ["33d47ad8cc", "114f3dada8", "72dc971633", "92e73c053a", "da0a465b6c"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is a technique for efficiently encoding images of varying resolutions?\n", "true_answer": "LLaVA A-UHD [35] proposes an image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding.", "source_doc": "multimodal.pdf", "source_id": "c0bdc4830f", "retrieved_docs": ["Figure 6: Comparision of Phi[86] (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-small) versus\nLlama-2 [91] family of models(7B, 13B, 34B, 70B) that were trained on the same fixed data.\nLLaV A-UHD [35] proposes an image modularization strategy that divides native-resolution im-\nages into smaller variable-sized slices for efficient and extensible encoding. Inaddition, InternLM-\nXComposer2-4KHD [90] introduces a strategy that dynamically adjusts resolution with an automatic\nlayout arrangement, which not only maintains the original aspect ratios of images but also adaptively\nalters patch layouts and counts, thereby enhancing the efficiency of image information extraction.\nBy implementing an adaptive input strategy for images of varying resolutions, a balance between\nperceptual capability and efficiency can be achieved.\nToken Processing Techniques designed to process lengthy visual token squence are critical in ef-", "Token Processing Techniques designed to process lengthy visual token squence are critical in ef-\nficient MLLMs as they address the dual challenges of preserving fine-grained details and reducing\ncomputational complexity. LLaV A-UHD [35] presents a novel approach to manage the computa-\ntional burden associated with high-resolution images. It puts forward two key components: (1) a\ncompression module that further condenses image tokens from visual encoders, significantly re-\nducing the computational load, and (2) a spatial schema to organize slice tokens for LLMs. No-\ntably, LLaV A-UHD demonstrates its efficiency by supporting 6 times larger resolution images using\nonly 94% of the inference computation compared to previous models. Furthermore, the model\ncan be efficiently trained in academic settings, completing the process within 23 hours on 8 A100\nGPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-", "Multi-view Input Directly employing high-resolution vision encoders for fine-grained percep-\ntion is prohibitively costly and does not align with practical usage requirements. Therefore, to\nutilize low-resolution vision encoders while enabling MLLM to perceive detailed information, a\ncommon approach is to input multi-view HR images, i.e., a global view: low-resolution images\nobtained through resizing, and a local view: image patches derived from splitting. For example,\n7", "tion answering and image captioning. However, MLLMs face considerable challenges in tasks ne-\ncessitating intricate recognition, including crowd counting and OCR of small characters. A direct\napproach to address these challenges involves increasing the image resolution, practically, the num-\nber of visual tokens. This strategy, nonetheless, imposes a substantial computational burden on\nMLLMs, primarily due to the quadratic scaling of computational costs with the number of input to-\nkens in the Transformer architecture. Motivated by this challenge, vision token compression, aimed\nto reduce the prohibitive computation budget caused by numerous tokens, has become an essential\naspect of efficient MLLMs. We will explore this topic through several key techniques, including\nmulti-view input, token processing, multi-scale information fusion, vision expert agents and video-\nspecific methods.\nMulti-view Input Directly employing high-resolution vision encoders for fine-grained percep-", "for resource-sensitive tasks was provided, with some works employing visual token compression\nto boost efficiency, enabling the transfer of MLLM capabilities to resource-intensive tasks such as\nhigh-resolution image and video understanding[35, 39, 14, 40].\nIn this survey, we aim to present an exhaustive organization of the recent advancements in the rapidly\nevolving field of efficient MLLMs, as depicted in Figure.2. We organize the literature in a taxonomy\nconsisting of six primary categories, encompassing various aspects of efficient MLLMs, including\narchitecture ,efficient vision ,efficient LLMs ,training ,data and benchmarks , and applications .\n\u2022 Architecture focuses on the MLLM framework developed by efficient techniques to reduce\nthe computational cost. The architecture is composed of multiple modality-based funda-\nmental models, exhibits characteristics distinct from single-modal models, thus promoting\nthe development of novel technologies."], "retrieved_docs_id": ["c0bdc4830f", "986687f08e", "f8392fc0db", "8beea9b82e", "cd55ca1477"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What model is used to evaluate all prompts in the RAGAS framework?\n", "true_answer": "The gpt-3.5-turbo-16k model, which is available through the OpenAI API, is used to evaluate all prompts in the RAGAS framework.", "source_doc": "RAG.pdf", "source_id": "ffd5c8b41e", "retrieved_docs": ["open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "ARES\nARES aims to automatically evaluate the performance of\nRAG systems in three aspects: Context Relevance, Answer\nFaithfulness, and Answer Relevance. These evaluation met-\nrics are similar to those in RAGAS. However, RAGAS, being\na newer evaluation framework based on simple handwritten\nprompts, has limited adaptability to new RAG evaluation set-\ntings, which is one of the significances of the ARES work.\nFurthermore, as demonstrated in its assessments, ARES per-\nforms significantly lower than RAGAS.\nARES reduces the cost of evaluation by using a small\namount of manually annotated data and synthetic data,\nand utilizes Predictive-Driven Reasoning (PDR) to provide\nstatistical confidence intervals, enhancing the accuracy of\nevaluation [Saad-Falcon et al. , 2023 ].\nAlgorithm Principles\n1. Generating Synthetic Dataset: ARES initially generates\nsynthetic questions and answers from documents in the\ntarget corpus using a language model to create positive\nand negative samples.", "the prompt together with the question, which grounds the\nLLM to produce more accurate answers. We find that using\na domain adapted language model for RAG significantly\nimproves answer quality on our domain specific questions.\nAlso, we find that fine-tuning an off-the-shelf unsupervised\npre-trained dense retrieval model with a modest amount\nof domain specific training data significantly improves re-\ntrieval accuracy. Our domain-adapted RAG implementation\ndiagram is illustrated on Figure 3.\nFigure 3: RAG Implementation Variations\nWe created our domain adapted retrieval model by fine-\ntuning the e5small unsupervised model (Wang et al., 2022)\nwith 3000 domain specific auto-generated samples using the\nTevatron framework (Gao et al., 2022). We refer readers to\nthe details on the sample generation and training process in\nAppendix A.8.\nEven with the significant gains that come with fine-tuning a\nretrieval model, the fact remains that retrieval still struggles", "phase to capture key semantic meanings. In the later\nstages of this process, larger blocks with more contex-\ntual information are provided to the language model\n(LM). This two-step retrieval method helps strike a bal-\nance between efficiency and contextually rich responses.\n\u2022StepBack-prompt: Integrated into the RAG process,\nthe StepBack-prompt approach [Zheng et al. , 2023 ]en-\ncourages LLM to step back from specific instances and\nengage in reasoning about the underlying general con-\ncepts or principles. Experimental findings indicate a sig-\nnificant performance improvement in various challeng-\ning, inference-intensive tasks with the incorporation of\nbackward prompts, showcasing its natural adaptability\nto RAG. The retrieval-enhancing steps can be applied in\nboth the generation of answers to backward prompts and\nthe final question-answering process.\n\u2022Subqueries: Various query strategies can be employed in\ndifferent scenarios, including using query engines pro-", "Figure 3: Comparison between the three paradigms of RAG\n\u2022Task Adaptable Module: Focused on trans-\nforming RAG to adapt to various downstream\ntasks, UPRISE [Cheng et al. , 2023a ] automati-\ncally retrieves prompts for given zero-shot task\ninputs from a pre-constructed data pool, en-\nhancing universality across tasks and models.\nPROMPTAGATOR [Daiet al. , 2022 ]utilizes LLM\nas a few-shot query generator and, based on the gener-\nated data, creates task-specific retrievers. Leveraging\nthe generalization capability of LLM, PROMPTAGA-\nTOR enables the creation of task-specific end-to-end\nretrievers with just a few examples.\n\u2022Alignment Module: The alignment between queries\nand texts has consistently been a critical issue influenc-\ning the effectiveness of RAG. In the era of Modular\nRAG, researchers have discovered that adding a train-\nable Adapter module to the retriever can effectively mit-\nigate alignment issues. PRCA [Yang et al. , 2023b ]lever-"], "retrieved_docs_id": ["ffd5c8b41e", "1b1cdfdd79", "ad55562468", "ad03b3dcc5", "bbfa682738"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does IBD compute a more reliable next-token probability distribution?\n", "true_answer": "IBD calculates a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model.", "source_doc": "hallucination.pdf", "source_id": "9a2cc490f3", "retrieved_docs": ["decoding probability distribution is calibrated using the reference (distorted) distribution.\nFollowing the same idea of contrastive decoding, IBD [ 139] proposes an image-biased decoding\nstrategy. Specifically, IBD involves computing a more reliable next-token probability distribution\nby contrasting the predictions of the original model with those of an image-biased model, which\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "2 3 4 5 6 7 8 9\n0.40.60.81.01.2\n\u03c4\u03bb\n(a)\u03c4and\u03bbparameters relationship\n2 3 4 5 6 7 8 9\n2345678# of selected tokens\n\u03c4# of selected tokens\n(b)\u03c4and # of selected tokens relationship\n2 3 4 5 6 7 8 9\n# of selected tokens0.00.20.40.60.8Probabilities=3\n=5\n=7\n=9\nProbabilities\n# of selected tokens\n(c) Distribution of # selected tokens over varying \u03c4\nFigure 12: Behavior of GD when selecting multiple tokens.\nvia GD and setting the rank limit to m=d. Observing both subfigures, we note that a larger nnecessitates a larger m\nfor attention weights KQ\u22a4to accurately converge to the SVM solution (Figure 10(lower)). Meanwhile, performances\nremain consistent across varying Tvalues (Figure 10(upper)). This observation further validates Lemma 1. Furthermore,\nthe results demonstrate that Wconverges directionally towards Wmm\n\u22c6,\u03b1as long as m\u2273n, thereby confirming the assertion\nin our Theorem 7.\nBehavior of GD with nonlinear nonconvex prediction head and multi-token compositions (Figure 11). To better", "identically distributed (IID) pattern and tokens exhibit IID isotropic distributions, we posit that d\u2273(T+n)log(T+n)\nwill su ffice. More generally, the extent of required overparameterization will be contingent on the covariance of tokens\n[BLLT20, MNS+21] and the distribution characteristics of input sequences [WT22].\nAssumption B.2stipulates that non-optimal tokens possess identical scores which constitutes a relatively stringent\nassumption that we will subsequently relax. Under Assumption B, we establish that optimization problems (W-ERM)\nand (KQ-ERM) lack stationary points, and when trained using GD, the norm of parameters will diverge.\nTheorem 3 Suppose Assumption A on the loss function \u2113and Assumption B on the tokens hold. Then,\n\u2022There is no W\u2208Rd\u00d7dsatisfying\u2207L(W)=0.\n\u2022Algorithm W-GD with the step size \u03b7\u2264O(1/LW)and any starting point W(0)satisfies lim k\u2192\u221e\u2225\u2207L(W(k))\u2225F=0,\nandlim k\u2192\u221e\u2225W(k)\u2225F=\u221e.", "the actual probability distribution of the generated text is not known, then the minimum lossless\ncompression rate achievable is equal to the cross-entropy of the assumed distribution, which is the\nLM here, with respect to the actual unknown distribution, which is obtained from adaptive top- k\nsampling here.\nExample 2. We generate 200 tokens for different values of target surprise values using the GPT-2\nmodel with 117M parameters to show the quality of the text generated using Alg. 1 for different target\nsurprise values. We also measure the compression rates obtained using arithmetic coding (Witten\net al., 1987; Rissanen & Langdon, 1979) with the LM as the probability distribution. So, in a way,\nmirostat can generate text that has a predetermined minimum lossless compression rate for a given\nmodel.\nContext: \u201cFor two months early in 1943, Shannon came into contact with the leading British\nmathematician Alan Turing. Turing had been posted to Washington to share with the U.S. Navy\u2019s", "Switch Transformers\nB. Preventing Token Dropping with No-Token-Left-Behind\nDue to software constraints on TPU accelerators, the shapes of our Tensors must be stat-\nically sized. As a result, each expert has a \ufb01nite and \ufb01xed capacity to process token\nrepresentations. This, however, presents an issue for our model which dynamically routes\ntokens at run-time that may result in an uneven distribution over experts. If the number of\ntokens sent to an expert is less than the expert capacity, then the computation may simply\nbe padded \u2013 an ine\ufb03cient use of the hardware, but mathematically correct. However, when\nthe number of tokens sent to an expert is larger than its capacity (expert over\ufb02ow), a proto-\ncol is needed to handle this. Lepikhin et al. (2020) adapts a Mixture-of-Expert model and\naddresses expert over\ufb02ow by passing its representation to the next layer without processing\nthrough a residual connection which we also follow."], "retrieved_docs_id": ["9a2cc490f3", "d10ccb16c7", "8ffb57762e", "969be68aca", "1f420ae819"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How can the adaptability of MLLMs be improved according to the context?\n", "true_answer": "The adaptability of MLLMs can be improved by incorporating a more varied set of lightweight LLMs, which can be customized to cater to a broad spectrum of requirements.", "source_doc": "multimodal.pdf", "source_id": "f10976c224", "retrieved_docs": ["can reserve the original capacities of LLMs but likely have\na weak an adaptation performance, while the latter can\nfully adapt to multimodal tasks but suffer from the loss of\noriginal capacities of LLMs. More efforts should be made to\ninvestigate how to effectively balance the two aspects, so as\nto achieving improved multimodal capacities. In addition,\nexisting MLLMs are still overly dependent on the capacities\nof LLMs, which pose the limits on many multimodal tasks\n(e.g., space positioning). It will be meaningful to explore\nimproved training approaches of language models, so that\nmultimodal information can be also utilized in this process.\n\u2022Safety and alignment. Safety and alignment has been\nwidely discussed in LLMs, which aim to regulate the behav-\niors of models by technical approaches [66]. This topic is also\nimportant to MLLMs. Even a highly advanced MLLM ( e.g.,\nGPT-4V [133]) can be susceptible to safety issues. For exam-", "a richer diversity of input modalities, and augmenting their generative capacities, we can\nsignificantly bolster their multifunctionality and widen their applicability.\n\u2022 There are two principal pathways to fortify efficient MLLM models. Firstly, the incorpora-\ntion of a more varied set of lightweight LLMs can render the design of MLLMs more adapt-\nable, facilitating their customization to cater to a broad spectrum of requirements. Sec-\nondly, leveraging high-quality instruction tuning datasets can empower efficient MLLMs\nto better comprehend and implement a vast array of instructions, thereby amplifying their\nzero-shot learning capabilities.\n\u2022 The development of embodied agents capable of deployment on edge devices represents a\ncrucial application prospect for efficient MLLMs. An agent possessing specialized knowl-\nedge and the capability to interact with the real world has far-reaching implications, poten-", "\u2022 At present, efficient MLLMs face challenges in processing extended-context multimodal\ninformation, and they are typically limited to accepting single images. This constrains the\nadvancement of more sophisticated models capable of handling an increased number of\nmultimodal tokens. Such models would be beneficial for applications like comprehending\nlengthy videos and analyzing extensive documents that incorporate a mix of images and\ntext, creating more versatile and powerful systems.\n\u2022 The predominant efficient MLLMs mainly support dual input modalities - images and texts,\nand a singular output modality - text. However, the tangible world encompasses a more\nextensive array of modalities. By expanding the scope of efficient MLLMs to accommodate\n23", "including the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],\nMiniGPT-v2 [9], and MiniGPT-4[10]. These models circumvent the computational cost of train-\ning from scratch by effectively leveraging the pre-training knowledge of each modality. MLLMs\ninherit the cognitive capabilities of LLMs, showcasing numerous remarkable features such as robust\nlanguage generation and transfer learning abilities. Moreover, by establishing strong representa-\ntional connections and alignments with other modality-based models, MLLMs can process inputs\nfrom multiple modalities, significantly broadening their application scope.\nThe success of MLLMs is largely attributed to the scaling law: the performance of an AI model\nimproves as more resources, such as data, computational power, or model size, are invested into it.\nHowever, scalability comes at the cost of high resource demands, which hinders the development", "start our discussion, we specify the input to be text-image\npairs and the output to be text responses. Similar discus-\nsions can be made for other modalities, e.g., language-audio\nmodels [825], which is beyond our scope here. In essence,\nMLLMs are developed by adapting the information from\nother modalities to the text modality, so as to leverage the\nexcellent model capacities of LLMs that are learned based on\nworld text. Typically, a MLLM comprises an image encoder\nfor image encoding and a LLM for text generation, associ-\nated by a connection module that aligns vision and language\nrepresentations. During generation, the image is first split\ninto patches, and then transformed into patch embeddings\nby the image encoder and the connection module, to derive\na visual representation that can be understood by the LLM.\nSubsequently, the patch embeddings and text embeddings\nare concatenated, and fed into the MLLM, allowing the\nlanguage model to generate the response autoregressively."], "retrieved_docs_id": ["32fb098424", "f10976c224", "a1bd2d5193", "7a547e4fbb", "f96143f9ba"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "What is one of the processes involved in post-retrieval processing to enhance the quality of information retrieval?\n", "true_answer": "Information compression is one of the processes involved in post-retrieval processing. It is used to optimize the relevant information retrieved by the retriever from a large document database.", "source_doc": "RAG.pdf", "source_id": "faf8e03358", "retrieved_docs": ["rely on well-recognized large language models like GPT-\n4[OpenAI, 2023 ]to leverage their robust internal knowl-\nedge for the comprehensive retrieval of document knowledge.\nHowever, inherent issues of these large models, such as con-\ntext length restrictions and vulnerability to redundant infor-\nmation, persist. To mitigate these issues, some research has\nmade efforts in post-retrieval processing. Post-retrieval pro-\ncessing refers to the process of further treating, filtering, or\noptimizing the relevant information retrieved by the retriever\nfrom a large document database. Its primary purpose is to en-\nhance the quality of retrieval results to better meet user needs\nor for subsequent tasks. It can be understood as a process of\nreprocessing the documents obtained in the retrieval phase.\nThe operations of post-retrieval processing usually involve in-\nformation compression and result rerank.\nInformation Compression\nEven though the retriever can fetch relevant information from", "put forward various methods to optimize the retrieval process.\nIn terms of specific implementation, Advanced RAG can be\nadjusted either through a pipeline or in an end-to-end manner.\nPre-Retrieval Process\n\u2022Optimizing Data Indexing\nThe purpose of optimizing data indexing is to enhance\nthe quality of indexed content. Currently, there are five\nmain strategies employed for this purpose: increasing\nthe granularity of indexed data, optimizing index struc-\ntures, adding metadata, alignment optimization, and\nmixed retrieval.\n1.Enhancing Data Granularity: The objective of\npre-index optimization is to improve text standard-\nization, consistency, and ensure factual accuracy\nand contextual richness to guarantee the perfor-\nmance of the RAG system. Text standardization in-\nvolves removing irrelevant information and special\ncharacters to enhance the efficiency of the retriever.\nIn terms of consistency, the primary task is to elim-\ninate ambiguity in entities and terms, along with", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "challenging, and the augmentation process needs to balance\nthe value of each passage appropriately. The retrieved con-\ntent may also come from different writing styles or tones, and\nthe augmentation process needs to reconcile these differences\nto ensure output consistency. Lastly, generation models may\noverly rely on augmented information, resulting in output thatmerely repeats the retrieved content, without providing new\nvalue or synthesized information.\n3.2 Advanced RAG\nAdvanced RAG has made targeted improvements to over-\ncome the deficiencies of Naive RAG. In terms of the quality\nof retrieval generation, Advanced RAG has incorporated pre-\nretrieval and post-retrieval methods. To address the indexing\nissues encountered by Naive RAG, Advanced RAG has op-\ntimized indexing through methods such as sliding window,\nfine-grained segmentation, and metadata. Concurrently, it has\nput forward various methods to optimize the retrieval process.", "dle\u201d phenomenon [Liuet al. , 2023 ]. This redundant informa-\ntion can obscure key information or contain information con-\ntrary to the real answer, negatively impacting the generation\neffect [Yoran et al. , 2023 ]. Additionally, the information ob-\ntained from a single retrieval is limited in problems requiring\nmulti-step reasoning.\nCurrent methods to optimize the retrieval process mainly\ninclude iterative retrieval and adaptive retrieval. These allow\nthe model to iterate multiple times during the retrieval process\nor adaptively adjust the retrieval process to better accommo-\ndate different tasks and scenarios.\nIterative Retrieval\nRegularly collecting documents based on the original query\nand generated text can provide additional materials for\nLLMs [Borgeaud et al. , 2022, Arora et al. , 2023 ]. Providing\nadditional references in multiple iterative retrievals has im-\nproved the robustness of subsequent answer generation.\nHowever, this method may be semantically discontinuous and"], "retrieved_docs_id": ["faf8e03358", "8a71abd00a", "fefa202c19", "873e6df003", "f24827ee1d"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "What is the speed difference in training between Switch Transformer and Google's T5-XXL?\n", "true_answer": "The Switch Transformer trains four times faster than T5-XXL under the same computational resources.", "source_doc": "multimodal.pdf", "source_id": "45effa0e86", "retrieved_docs": ["C4 data set after 250k and 500k steps, respectively. We observe that the Switch-\nC Transformer variant is 4x faster to a \ufb01xed perplexity (with the same compute\nbudget) than the T5-XXL model, with the gap increasing as training progresses.\ndepth, number of heads, and so on, are all much smaller than the T5-XXL model. In\ncontrast, the Switch-XXL is FLOP-matched to the T5-XXL model, which allows for larger\ndimensions of the hyper-parameters, but at the expense of additional communication costs\ninduced by model-parallelism (see Section 5.5 for more details).\nSample e\ufb03ciency versus T5-XXL. In the \ufb01nal two columns of Table 9 we record\nthe negative log perplexity on the C4 corpus after 250k and 500k steps, respectively. After\n250k steps, we \ufb01nd both Switch Transformer variants to improve over the T5-XXL version\u2019s\nnegative log perplexity by over 0.061.10To contextualize the signi\ufb01cance of a gap of 0.061,\nwe note that the T5-XXL model had to train for an additional 250k steps to increase", "making minimal changes to existing model code. It enables us to scale multi-lingual neural machine\ntranslation Transformer models with sparse gated mixtures of experts to over 600 billion parameters\nusing automatic sharding. Switch Transformer [150] replaces the feedforward network (FFN) layer\nin the standard Transformer with a MoE routing layer, where each expert operates independently on\nthe tokens in the sequence. Its training speed is four times faster than Google\u2019s previously developed\nlargest model, T5-XXL, under the same computational resources. The proposed training techniques\nhave eliminated instability during the training process, demonstrating that large sparse models can\nalso be trained in a low-precision format, such as bfloat16.\nTransformer-Alternative Structures Although the Transformer is the dominant architecture in\ncurrent large-scale language models, models like RWKV [151] and Mamba [77] have emerged as", "Fedus, Zoph and Shazeer\nSwitch-Base is still more sample e\ufb03cient and yields a 2.5x speedup. Furthermore, more\ngains can be had simply by designing a new, larger sparse version, Switch-Large, which is\nFLOP-matched to T5-Large. We do this and demonstrate superior scaling and \ufb01ne-tuning\nin the following section.\n0 1 2 3 4\nTraining Step 1e52.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity\nSwitch-Base: 64e\nT5-Large\nT5-Base\n50 100 150 200 250 300 350\nTraining Time2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity7.0x Speedup2.5x Speedup\nSwitch-Base: 64e\nT5-Large\nT5-Base\nFigure 6: Scaling Transformer models with Switch layers or with standard dense model\nscaling. Left Plot: Switch-Base is more sample e\ufb03cient than both the T5-Base,\nand T5-Large variant, which applies 3.5x more FLOPS per token. Right Plot: As\nbefore, on a wall-clock basis, we \ufb01nd that Switch-Base is still faster, and yields a\n2.5x speedup over T5-Large.\n4. Downstream Results", "the same performance of the T5-Base model at step 60k at step 450k, which is a 7.5x\nspeedup in terms of step time. In addition, consistent with the \ufb01ndings of Kaplan et al.\n(2020), we \ufb01nd that larger models are also more sample e\ufb03cient \u2014learning more quickly\nfor a \ufb01xed number of observed tokens.\n1091010\nSparse Model Parameters4.85.05.25.45.65.86.0Test Loss\n1e\n2e\n4e\n8e\n16e\n32e\n64e\n128e\n256e\n0 1 2 3 4\nTraining Step 1e52.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log PerplexitySwitch-Base: 128e\nSwitch-Base: 64e\nSwitch-Base: 32e\nSwitch-Base: 16e\nT5-Base\nFigure 4: Scaling properties of the Switch Transformer. Left Plot: We measure the quality\nimprovement, as measured by perplexity, as the parameters increase by scaling\nthe number of experts. The top-left point corresponds to the T5-Base model with\n223M parameters. Moving from top-left to bottom-right, we double the number of\nexperts from 2, 4, 8 and so on until the bottom-right point of a 256 expert model", "Switch Transformers\n3.2 Scaling Results on a Time-Basis\nFigure 4 demonstrates that on a step basis, as we increase the number of experts, the\nperformance consistently improves. While our models have roughly the same amount of\nFLOPS per token as the baseline, our Switch Transformers incurs additional communication\ncosts across devices as well as the extra computation of the routing mechanism. Therefore,\nthe increased sample e\ufb03ciency observed on a step-basis doesn\u2019t necessarily translate to a\nbetter model quality as measured by wall-clock. This raises the question:\nFor a \ufb01xed training duration and computational budget, should one train a dense or a\nsparse model?\n50 100 150 200 250 300 350\nTraining Time2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity7x Speedup\nSwitch-Base: 128e\nSwitch-Base: 64e\nSwitch-Base: 32e\nT5-Base\nFigure 5: Speed advantage of Switch Transformer. All models trained on 32 TPUv3 cores\nwith equal FLOPs per example. For a \ufb01xed amount of computation and training"], "retrieved_docs_id": ["526652a233", "45effa0e86", "7c79461c75", "155ec011de", "0ec3d81b96"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "How does Efficient Vision aim to optimize visual fracture extraction strategies?\n", "true_answer": "Efficient Vision explores optimizing visual fracture extraction strategies by emphasizing methods that enhance efficiency without compromising accuracy. It also focuses on integrating high-quality visual data for effective cross-modal understanding.", "source_doc": "multimodal.pdf", "source_id": "f53fc9e54d", "retrieved_docs": ["the development of novel technologies.\n\u2022 Efficient Vision explores optimizing efficient visual fracture extraction strategies, empha-\nsizing methods that boost efficiency while maintaining accuracy. It addresses integrating\nhigh-quality visual data for effective cross-modal understanding.\n\u2022 Efficient LLMs explores these strategies of improving the computational efficiency and\nscalability of language models. It examines the trade-offs between model complexity and\nperformance while suggesting promising avenues for balancing these competing factors.\n2", "Figure 10: Efficient vision transformer techniques in [138]. The dashed orange block highlights the\ncomponent on which each optimization technique mainly focuses.\ndetermining the pruning rate. Additionally, VTP [110] reduces embedding dimensions through the\nintegration of control coefficients, concurrently removing neurons with negligible coefficients. Tang\net al. [111] eliminate redundant patches by first identifying effective patches in the last layer and\nthen leveraging them to guide the selection process of previous layers, where patches with minimal\nimpact on the final output feature are subsequently discarded.\nHybrid Pruning , such as [137], investigates both unstructured and structured sparsity, intro-\nducing a first-order importance approximation approach for attention head removal. SPViT [112]\ndevelops a dynamic attention-based multi-head token selector for adaptive instance-wise token se-", "to lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent\nstudies have proposed complementing this information loss by incorporating visual features from\nother vision encoders. The work of [ 98] proposes mixing features from CLIP ViT and DINO ViT.\nSpecifically, it experimented with additive and interleaved features. Both settings show that there\nis a trade-off between the two types of features. A more dedicated mechanism is needed.\nConcurrently, a visual expert-based model proposed in [ 38] aims to mitigate the information\nloss caused by the CLIP image encoder. Instead of merely mixing features, this paper enhances\nthe visual perception ability of MLLMs by focusing on knowledge enhancement, relying on two\npivotal modules: multi-task encoders and the structural knowledge enhancement module. The multi-\ntask encoders are dedicated to integrating various types of latent visual information extracted by", "for resource-sensitive tasks was provided, with some works employing visual token compression\nto boost efficiency, enabling the transfer of MLLM capabilities to resource-intensive tasks such as\nhigh-resolution image and video understanding[35, 39, 14, 40].\nIn this survey, we aim to present an exhaustive organization of the recent advancements in the rapidly\nevolving field of efficient MLLMs, as depicted in Figure.2. We organize the literature in a taxonomy\nconsisting of six primary categories, encompassing various aspects of efficient MLLMs, including\narchitecture ,efficient vision ,efficient LLMs ,training ,data and benchmarks , and applications .\n\u2022 Architecture focuses on the MLLM framework developed by efficient techniques to reduce\nthe computational cost. The architecture is composed of multiple modality-based funda-\nmental models, exhibits characteristics distinct from single-modal models, thus promoting\nthe development of novel technologies.", "Pruning: Optimal Brain Surgeon. In Advances in Neural Information Processing Systems 5 , pp.\n164\u2013171. Morgan Kaufmann, 1993b.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2016. URL https://arxi\nv.org/abs/1606.08415 .\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. arXiv\ne-prints, art. arXiv:1503.02531, Mar 2015.\nGeoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdi-\nnov. Improving neural networks by preventing co-adaptation of feature detectors, 2012.\nSara Hooker. The hardware lottery. Commun. ACM , 64(12):58\u201365, nov 2021. ISSN 0001-0782. doi:\n10.1145/3467017. URL https://doi.org/10.1145/3467017 .\nA. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and\nH. Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.\nArXiv e-prints , April 2017."], "retrieved_docs_id": ["f53fc9e54d", "ad28faccc0", "c20c82af54", "cd55ca1477", "01070e1c89"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the VQAv2-IDK benchmark categorize \"I Know (IK)\" hallucination in machine-learned language models?\n", "true_answer": "The VQAv", "source_doc": "hallucination.pdf", "source_id": "d18c108916", "retrieved_docs": ["desired answer is \u2019I don\u2019t know\u2019. The concept is defined as \u2019I Know (IK)\u2019 hallucination in the work\nof [11]. Accordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of\nhallucination. VQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as\ndetermined by human annotators. In this benchmark, \u2019I Know (IK)\u2019 hallucination has been further\ncategorized into four types:\n\u2022Unanswerable: no one can know.\n\u2022Don\u2019t know: human may not know, but robot might.\n\u2022False questions: refers non-existing.\n\u2022Not sure: ambiguous to answer.\nThis benchmark opens a new track for the study of hallucination in MLLMs.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "VHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual\nproperties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which\nemerge from comparisons across multiple objects, such as relative size, relative position, and\ncounting. Based on such categorization, the authors further defined 8 visual hallucination modes,\nproviding a very detailed evaluation of hallucination in MLLMs. Furthermore, the collected 1,200\nevaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no\nquestion\" (YNQ). Such design enables this benchmark to evaluate both generative and discriminative\ntasks.\nComparison of mainstream models We compare the mainstream MLLMs on some represen-\ntative benchmarks, providing a holistic overview of their performance from different dimensions.\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "recently, HaluEval [602] creates a large-scale LLM-generated\nand human-annotated hallucinated samples to evaluate the\nability of language models to recognize hallucination in both\ntask-specific and general scenarios.\nHallucination\nLLMs are prone to generate untruthful informa-\ntion that either conflicts with the existing source\nor cannot be verified by the available source.\nEven the most powerful LLMs such as ChatGPT\nface great challenges in migrating the hallucina-\ntions of the generated texts. This issue can be\npartially alleviated by special approaches such as\nalignment tuning and tool utilization.\n\u2022Knowledge recency . As another major challenge, LLMs\nwould encounter difficulties when solving tasks that require"], "retrieved_docs_id": ["d18c108916", "8ef8344de6", "92e73c053a", "114f3dada8", "fa2581a685"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does PRCA train the adapter?\n", "true_answer": "PRCA trains the adapter through the Contextual Extraction Stage and the Reward-Driven Stage.", "source_doc": "RAG.pdf", "source_id": "af13cfcd4c", "retrieved_docs": ["igate alignment issues. PRCA [Yang et al. , 2023b ]lever-\naged reinforcement learning to train a context adapter\ndriven by LLM rewards, positioned between the re-\ntriever and generator. It optimizes the retrieved in-\nformation by maximizing rewards in the reinforcement\nlearning phase within the labeled autoregressive pol-\nicy. AAR [Yuet al. , 2023b ]proposed a universal plu-\ngin that learns LM preferences from known-source\nLLMs to assist unknown or non-co-finetuned LLMs.\nRRR [Maet al. , 2023a ]designed a module for rewriting\nqueries based on reinforcement learning to align queries\nwith documents in the corpus.\n\u2022Validation Module: In real-world scenarios, it is notalways guaranteed that the retrieved information is reli-\nable. Retrieving irrelevant data may lead to the occur-\nrence of illusions in LLM. Therefore, an additional val-\nidation module can be introduced after retrieving docu-\nments to assess the relevance between the retrieved doc-", "expressiveness and generalization capabilities. Adapter-based tuning introduces lightweight adapter\nmodules into the pre-trained model\u2019s architecture. These adapter modules, typically composed of\nfeed-forward neural networks with a small number of parameters, are inserted between the layers\nof the original model. During fine-tuning, only the adapter parameters are updated, while the pre-\ntrained model\u2019s parameters remain fixed. This method significantly reduces the number of trainable\nparameters, leading to faster training and inference times without compromising the model\u2019s per-\nformance. LLM-Adapters [154] presents a framework for integrating various adapters into large\nlanguage models, enabling parameter-efficient fine-tuning for diverse tasks. This framework en-\n16", "the adapters, while for all the other models, we upload only the difference in weights with respect to the fine-tuned\nmodel. For example, to obtain the LLaMAntino 2-13b-hf-evalita-ITA model, it is necessary to apply the LLaMAntino\n2-13b-hf-ITA adapters to the LLaMa 2-13b model and then apply the difference in weights to the obtained model. We\nreleased the training code together with a pipeline to simplify the model creation process on GitHub18.\n4 DISCUSSION\nThe initial qualitative analysis of the obtained models reveals their efficiency and ability to respond logically and\naccurately to various questions. Even in lengthy and complex conversations, the chat models demonstrate excellent\ndialogue capabilities, exhibiting minimal hallucinations and strong linguistic articulation in Italian.\nHowever, it is essential to note that these models often exhibit errors in sentence structure, primarily stemming from", "linearity. The Adapter module is applied in series\nwith the feed-forward network (FFN). Having the\nadaptor module in-line with other blocks in the\nmodel can increase the inference time of the model.\nPA is a faster version of the Adapter, which can be\napplied in parallel with the FFN block. The com-\npactor is a more memory-ef\ufb01cient version of the\nAdapter, which deploys the sum of Kronecker prod-\nucts to reconstruct each up-projection and down-\nprojection matrices. All these low-rank adapters\nsuffer from two major issues: \ufb01rst, \ufb01nding the best\nrank requires heavy exhaustive training and search;\nsecond, the tuned adapter module works well only\nwith a particular rank.\nWhile there have been some efforts in the lit-\nerature towards dynamic networks such as Dyn-\naBERT (Hou et al., 2020) and GradMax (Evci et al.,\n2022), to the best of our knowledge, this problem\nfor factorized networks and low-rank adapters is\nstill open. DRONE (Chen et al., 2021) propose a", "tokens, we learn the activations after every Transformer layer. The activations computed from pre-\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\n|\u0398|=L\u00d7dmodel\u00d7(lp+li), whereLis the number of Transformer layers.\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-\nattention module (and the MLP module) and the subsequent residual connection. There are two\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more ef\ufb01cient design with the\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\ninclude another baseline call AdapterDrop (R \u00a8uckl\u00b4e et al., 2020) which drops some adapter layers for"], "retrieved_docs_id": ["8d0a82337c", "004ffc5dd9", "9194c0656d", "54f8547c29", "eacae6e898"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is a hardware-aware quantization method for ViTs?\n", "true_answer": "GPUSQ-ViT", "source_doc": "multimodal.pdf", "source_id": "8a087225e4", "retrieved_docs": ["NN are grouped into sensitive/insensitive to quantization,\nand higher/lower bits are used for each layer. As such,\none can minimize accuracy degradation and still bene\ufb01t\nfrom reduced memory footprint and faster speed up with\nlow precision quantization. Recent work [ 267] has also\nshown that this approach is hardware-ef\ufb01cient as mixed-\nprecision is only used across operations/layers.\nC. Hardware Aware Quantization\nOne of the goals of quantization is to improve the\ninference latency. However, not all hardware provide\nthe same speed up after a certain layer/operation is\nquantized. In fact, the bene\ufb01ts from quantization is\nhardware-dependant, with many factors such as on-chip\nmemory, bandwidth, and cache hierarchy affecting the\nquantization speed up.\nIt is important to consider this fact for achieving\noptimal bene\ufb01ts through hardware-aware quantization [ 87,\n91,246,250,254,256,265,267]. In particular, the\nwork [ 246] uses a reinforcement learning agent to", "Bit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain\naccuracy throughout quantization. PackQViT [129] mitigates outlier effects during quantization.\nBiViT [128] introduces Softmax-aware Binarization to adjust the binarization process, minimizing\nerrors in binarizing softmax attention values. Xiao et al. [142] integrated a gradient regularization\nscheme to curb weight oscillation during binarization training and introduced an activation shift\nmodule to reduce information distortion in activations. Additionally, BinaryViT [130] integrates\nessential architectural elements from CNNs into a pure ViT framework, enhancing its capabilities.\nHardware-Aware Quantization optimizes the quantization process of neural network models for\nspecific hardware platforms ( e.g., GPUs [131], FPGA [132]). It adjusts precision levels and quan-\ntization strategies to maximize performance and energy efficiency during inference. For example,", "in the seminal work of Optimal Brain Damage [ 139].\nIn HAWQv2, this method was extended to mixed-\nprecision activation quantization [ 50], and was shown to\nbe more than 100x faster than RL based mixed-precision\nmethods [ 246]. Recently, in HAWQv3, an integer-only,\nhardware-aware quantization was introduced [ 267] that\nproposed a fast Integer Linear Programming method to\n\ufb01nd the optimal bit precision for a given application-\nspeci\ufb01c constraint (e.g., model size or latency). This work\nalso addressed the common question about hardware\nef\ufb01ciency of mixed-precision quantization by directly\ndeploying them on T4 GPUs, showing up to 50% speed\nup with mixed-precision (INT4/INT8) quantization as\ncompared to INT8 quantization.\nSummary (Mixed-precision Quantization). Mixed-\nprecision quantization has proved to be an effective and\nhardware-ef\ufb01cient method for low-precision quantizationof different NN models. In this approach, the layers of a", "VTP[110], PS-ViT[111]\nHybrid Pruning SPViT [112], ViT-Slim [113]\nKnowledge Distillation (\u00a73.3)Homomorphic KDDeiT [114], TinyViT [115], m2mKD [116],\nDeiT-Tiny [117], MiniViT [118]\nHeteromorphic KD DearKD [119], CiT [120]\nQuantization (\u00a73.4)Post-Training QuantizationPTQ4ViT [121], APQ-ViT [122],\nNoisyQuant [123]\nQuantization-Aware TrainingQuantformer [124] Bit-shrinking [125],\nQ-ViT [126], TerViT [127], BiViT [128],\nPackQViT [129], BinaryViT [130]\nHardware-Aware Quantization GPUSQ-ViT[131], Auto-ViT-Acc [132]\nFigure 9: Organization of efficient vision advancements.\n10", "64conv16464conv2/3+6464conv4/5+128128conv6/7+128128conv8/9+...\n...+512512conv16/17+4B i t s8B i t s4B i t s8B i t s4B i t s8B i t s4B i t s8B i t s4B i t s8B i t s4B i t s8B i t s4B i t s8B i t sFC&softmaxSensitivity: Flat vs. Sharp Local Minima\n\u00000.4\u00000.200.20.4\u00000.4\u00000.200.20.4\u00002\u0000101\n\u270f1\u270f2Loss(Log)Balance the Trade-offInference LatencyINT8INT4\n\u00000.4\u00000.200.20.4\u00000.4\u00000.200.20.400.51\n\u270f1\u270f2Loss(Log)17th Block\u00000=0.7Figure 8: Illustration of mixed-precision quantization. In mixed-precision quantization the goal is to keep sensitive\nand ef\ufb01cient layers in higher precision, and only apply low-precision quantization to insensitive and inef\ufb01cient\nlayers. The ef\ufb01ciency metric is hardware dependant, and it could be latency or energy consumption.\nhardware processors, including NVIDIA V100 and Titan\nRTX, support fast processing of low-precision arithmetic\nthat can boost the inference throughput and latency.\nMoreover, as illustrated in Figure 7 (right) for a 45nm"], "retrieved_docs_id": ["3e3cb80a9a", "31efe3044d", "64d03aeb69", "8a087225e4", "642c5f83c6"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.25, "hit": 1}, {"question": "How does MARINE implement guided decoding?\n", "true_answer": "MARINE implements guided decoding by employing an additional vision encoder for object grounding and utilizing the grounded objects to guide the decoding process, using the classifier-free guidance technique.", "source_doc": "hallucination.pdf", "source_id": "9e707211bd", "retrieved_docs": ["20 Bai, et al.\nfocuses more on the image information. The image-based model is created by modifying the\nattention weight matrix structure within the original model, without altering its parameters. This\napproach emphasizes the knowledge of the image-biased model and diminishes that of the original\nmodel, which may be text-biased. Thus, it encourages the extraction of correct content while\nsuppressing hallucinations resulting from textual over-reliance.\nGuided Decoding. MARINE [ 131] proposes a training-free approach. It employs an additional\nvision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\nSpecifically, it innovatively adapts the classifier-free guidance [ 40] technique to implement guided\ndecoding, showing promising performance in emphasizing the detected objects while reducing\nhallucination in the text response.\nSimilarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that", "shows these values as well as similar results for beam-searc h.\n1Due to system limitations requiring \ufb01xed shapes, we used pad ding and masking in our decoder-self-attention implementa tion.\nThe memory tensors were thus padded to the maximum length (12 8), or to the window-size (32) in the case of local attention.\nEach decoding step thus took the same amount of time. An alter native implementation of incrementally growing the tensor s\ncould save time near the beginning of the sequence.\n7", "to use the entire BART model (both encoder and de-\ncoder) as a single pretrained decoder for machine trans-\nlation, by adding a new set of encoder parameters that\nare learned from bitext (see Figure 3b).\nMore precisely, we replace BART\u2019s encoder embed-\nding layer with a new randomly initialized encoder.\nThe model is trained end-to-end, which trains the new\nencoder to map foreign words into an input that BART\ncan de-noise to English. The new encoder can use a\nseparate vocabulary from the original BART model.\nWe train the source encoder in two steps, in both\ncases backpropagating the cross-entropy loss from the\noutput of the BART model. In the \ufb01rst step, we freeze\nmost of BART parameters and only update the ran-\ndomly initialized source encoder, the BART positional\nembeddings, and the self-attention input projection ma-\ntrix of BART\u2019s encoder \ufb01rst layer. In the second step,\nwe train all model parameters for a small number of\niterations.\n4 Comparing Pre-training Objectives", "arXiv:1911.02150v1  [cs.NE]  6 Nov 2019Fast Transformer Decoding: One Write-Head is All\nYou Need\nNoam Shazeer\nGoogle\nnoam@google.com\nNovember 7, 2019\nAbstract\nMulti-head attention layers, as used in the Transformer neu ral sequence model, are a powerful alter-\nnative to RNNs for moving information across and between seq uences. While training these layers is\ngenerally fast and simple, due to parallelizability across the length of the sequence, incremental inference\n(where such paralleization is impossible) is often slow, du e to the memory-bandwidth cost of repeatedly\nloading the large \"keys\" and \"values\" tensors. We propose a v ariant called multi-query attention, where\nthe keys and values are shared across all of the di\ufb00erent atte ntion \"heads\", greatly reducing the size of\nthese tensors and hence the memory bandwidth requirements o f incremental decoding. We verify exper-\nimentally that the resulting models can indeed be much faste r to decode, and incur only minor quality", "of generations with length T= 200 \u00b15tokens. In the runs\nusing greedy and beam search decoding, we suppress the\nEOS token during generation to combat the tendency of\nbeam search to generate short sequences. We then truncate\nall sequences to T= 200 . A larger oracle language model"], "retrieved_docs_id": ["9e707211bd", "533b5f9a8e", "d708c93a55", "32682a487b", "6b97d69b0b"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does TinyLlava adjust pre-trained modules during pre-training?\n", "true_answer": "TinyLlava partially freezes pre-trained vision encoder and symmetric language model (SLM) modules to activate more parameters for learning alignment during the pre-training stage.", "source_doc": "multimodal.pdf", "source_id": "1bd741e7c9", "retrieved_docs": ["used in model pre-training [234, 235, 237, 238]. It aims to\norganize different parts of pre-training data for LLMs in\na specific order, e.g., starting with easy/general examples\nand progressively introducing more challenging/special-\nized ones. More generally, it can broadly refer to the adap-\ntive adjustment of data proportions for different sources\nduring pre-training. Existing work about data curriculum\nmainly focuses on continual pre-training, such as special-\nized coding LLMs ( e.g., CodeLLaMA [235]) or long context\nLLMs ( e.g., LongLLaMA [238]). However, it still lacks of\nmore detailed report about data curriculum for general-\npurpose LLMs ( e.g., LLaMA) in the literature. To determine\ndata curriculum, a practical approach is to monitor the de-\nvelopment of key abilities of LLMs based on specially con-\nstructed evaluation benchmarks, and then adaptively adjust\nthe data mixture during pre-training. Next, we take three\ncommon abilities as examples to introduce how the concept", "FastV[46], VTW[47]\nTraining (\u00a75)Pre-Training (\u00a75.1) Idefics2[48], TinyLLaV A[23], VILA[49]\nInstruction-Tuning (\u00a75.2) LaVIN[50], HyperLLaV A[51]\nDiverse Training Steps (\u00a75.3) SPHINX-X[14], Cobra[13], TinyGPT-V[28]\nParameter Efficient\nTransfer Learning (\u00a75.4)EAS [52], MemVP [53]\nData and Benchmarks (\u00a76)Pre-Training Data (\u00a76.1)CC595k[7], LLava-1.5-PT[54],\nShareGPT4V-PT[55],\nBunny-pretrain-LAION-2M[24],\nALLaV A-Caption-4V[29], etc.\nInstrcution-Tuning Data (\u00a76.2)LLaV A\u2019s IT[7], LLaV A-1.5\u2019s IT[54],\nShareGPT4V\u2019s IT[55], Bunny-695K[24],\nLVIS-INSTRUCT-4V[56], etc.\nBenchmarks (\u00a76.3)VQAv2[57], TextVQA[58], GQA[59],\nMME[60], MMBench[61], POPE[62]\nApplication (\u00a77)Biomedical Analysis (\u00a77.1) LLaV A-Rad [63], MoE-TinyMed [64]\nDocument Understanding (\u00a77.2)TextHawk [36], TinyChart [37],\nMonkey [65], HRVDA [66]\nVideo Comprehension (\u00a77.3)mPLUG-video [67], Video-LLaV A [44],\nMA-LMM [68], LLaMA-VID [69]\nFigure 2: Organization of efficient multimodal large language models advancements.", "This can be attributed to the pre-training phase already capturing significant information, leaving the fine-tuning\nstage primarily to focus on task-specific adjustments. In essence, LoRA offers a compelling approach to parameter\nreduction by leveraging the notion of intrinsic dimension in weight matrices. By adopting a mathematically rigorous\nframework, LoRA enables more efficient adaptation of pre-trained language models to new tasks during the fine-tuning", "There are many directions for future works. 1) LoRA can be combined with other ef\ufb01cient adapta-\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind \ufb01ne-tuning\nor LoRA is far from clear \u2013 how are features learned during pre-training transformed to do well\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full \ufb01ne-\n12", "the general domain. KD at pre-training improves\nthe initialization of the Kronecker model for the\ntask-speci\ufb01c KD stage. Similar to (Jiao et al., 2019)\nthe loss at pre-training stage only involves the in-\ntermediate layersLEmbedding (x)+LAttention (x)+\nLFFN(x). Unlike (Jiao et al., 2019), we perform\npre-training distillation only on a small portion of\nthe dataset (5% of the English Wikipedia) for a few\nepochs (3 epochs) which makes our training far\nmore ef\ufb01cient.\n3.6 Model Settings\nThe \ufb01rst step of the proposed framework is to de-\nsign the Kronecker layers by de\ufb01ning the shape of\nKronecker factors matrices AandB. To do this we\nneed to set the shape of one of these matrices and\nthe other one can be obtained accordingly. There-\nfore we only searched among different choices for\nm1andn1which are limited to the factors of the\noriginal weight matrix (m and n respectively). We\nused the same con\ufb01guration for all the matrices in\nthe MHA. Also For the FFN, we chose the con\ufb01g-"], "retrieved_docs_id": ["55df9c8a81", "d85947fa4f", "bf757f65af", "93e75b2efb", "4ed36f9fae"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is the focus of the survey presented in the paper?\n", "true_answer": "The focus of the survey presented in the paper is the latest developments regarding hallucinations in multimodal large language models (MLLMs).", "source_doc": "hallucination.pdf", "source_id": "e1b4ac9436", "retrieved_docs": ["G Details of Task Phrasing and Speci\ufb01cations\nThe following \ufb01gures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from\nthe ground truth datasets in this section, and no samples from GPT-3 are included here.\nContext\u2192Article:\nInformal conversation is an important part of any business\nrelationship.Before you start a discussion,however,make sure you understand\nwhich topics are suitable and which are considered taboo in a particular\nculture. Latin Americans enjoy sharing information about their local\nhistory, art and customs.You may expect questions about your family,and\nbe sure to show pictures of your children.You may feel free to ask similar\nquestions of your Latin American friends.The French think of conversation\nas an art form,and they enjoy the value of lively discussions as well as\ndisagreements. For them,arguments can be interesting and they can cover\npretty much or any topic ---- as long as they occur in are respectful and", "approaches to analyzing the data characteristics, includ-\ning data exploration, visualization, and deriving analytical\nconclusions [935, 936]. Fourth, during the paper writing\nstage, researchers can also benefit from the assistance of\nLLMs in scientific writing [937, 938], in which LLMs can\noffer valuable support for scientific writing through diverse\nmeans, such as summarizing the existing content and pol-\nishing the writing [939]. In addition, LLMs can aid in\nthe automated paper review process, encompassing tasks\nsuch as error detection, checklist verification, and candidate\nranking [940]. Despite these advances, there is much room\nfor improving the capacities of LLMs to serve as helpful,\ntrustworthy scientific assistants, to both increase the quality\nof the generated scientific content and reduce the harmful\nhallucinations.\nSummary . In addition to the aforementioned work, the\napplications of LLMs have been also discussed in several", "(a)Do the main claims made in the abstract and introduction accurately re\ufb02ect the paper\u2019s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See the limitation section\n(c)Did you discuss any potential negative societal impacts of your work?[Yes] See the\nBroader Impacts section\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\nthem?[Yes] Yes, we believe our work conforms to these guidelines.\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments...\n(a)Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] We will include\nour code in the supplemental material.\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they", "from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half\nhuman written and half model generated articles. Comprehension questions were added and articles were shown to\nparticipants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task.\nModel generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments.\nF Additional Samples from GPT-3\nGPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure\nF.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the\nstyle of Wallace Stevens. We \ufb01rst experimented with a few prompts, then generated four samples with no additional\nediting or selection (sampling at temperature 1using nucleus sampling [ HBFC19 ] withP= 0.9). Completions were", "way. Then, we have extensively revised the writing and\ncontents in several passes. Due to the space limit, we can\nonly include a fraction of existing LLMs in Figure 3 and\nTable 1, by setting the selection criterion. However, we set\na more relaxed criterion for model selection on our GitHub\npage (https://github.com/RUCAIBox/LLMSurvey), which\nwill be regularly maintained. We release the initial version\non March 31, 2023, the major revision on June 29, 2023,\nand second version on September 10, 2023, and this latest\nversion (major revision) on November 23, 2023.\nSeeking for Advice . Despite all our efforts, this survey\nis still far from perfect: we are likely to miss important\nreferences or topics, and might also have non-rigorous\nexpressions or discussions. We will continuously update\nthis survey, and improve the quality as much as we can.\nFor us, survey writing is also a learning process for LLMs\nby ourselves. For readers with constructive suggestions to"], "retrieved_docs_id": ["c742f5dcc3", "ce2931f287", "a09d2355b0", "25ab437c71", "affa67e9d3"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How does D-Abstractor maintain the local context in visual feature abstraction?\n", "true_answer": "D-Abstractor, or Deformable attention-based Abstractor, maintains the local context through a 2-D coordinate-based sampling process, using reference points and sampling offsets.", "source_doc": "multimodal.pdf", "source_id": "3a3d9edb48", "retrieved_docs": ["additional LResNet blocks, which facilitate the abstraction of visual features to any squared num-\nber of visual tokens. Conversely, D-Abstractor, or Deformable attention-based Abstractor utilizes\ndeformable attention, which maintains the local context through a 2-D coordinate-based sampling\nprocess, using reference points and sampling offsets.\n6", "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "vectors, or (right) the way how persistent vectors integrate with self-attention.\nimportance of feedforward layers in transformer models. However, it maintains decent performances\nbecause it still has a lot of parameters ( 38M) in the Wq,k,v,o matrices.\nWe also compare several different ways of integrating persistent vectors into self-attention:\n\u2022All-attn : this is our default model presented in Section 4 where persistent vectors are simply\nconcatenated to context vectors.\n\u2022Attn-split : this is the same as \u201call-attn\u201d except the attention over context and persistent\nvectors are computed separately. In other words, we replace the softmax in Eq. 13 with two\nseparate softmax functions: one for context vectors only and one for persistent vectors only.\n\u2022Head-split : this is the same as \u201call-attn\u201d except we constrain half of the heads to attend\nonly to context vectors, and the other half to attend only to persistent vectors.", "6 Conclusion\nIn this paper, we propose a novel attention layer that presents a uni\ufb01ed mechanism to aggregate\ngeneral and contextual information. It extends the self-attention layer of a transformer with a set of\npersistent vectors that are capable of storing information that is complementary to the short term\ninformation in contexts. We also show that these persistent vectors can replace the feedforward layers\nin a transformer network with no loss of performance. We think that this simpli\ufb01ed layer can help\nbetter understand how information is processed and stored in transformer-like sequence models.\nAcknowledgements\nWe thank Leon Bottou, Omer Levy for their helpful comments and suggestions. We also thank Lowik\nChanussot, J\u00e9r\u00e9my Rapin and other members of Facebook AI Research engineering team for their\nsupport in implementing and running the experiments.\nReferences\n[1]Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level", "100 with larger dreaffirm that overparameterization leads the attention weights to converge directionally towards the\noptimal max-margin direction outlined by (Att-SVM) and (Att-SVM \u22c6).\nIn the upcoming section, we will introduce locally-optimal directions, to which GD can be proven to converge when\nappropriately initialized. We will then establish a condition that ensures the globally-optimal direction is the sole viable\nlocally-optimal direction . This culmination will result in a formal conjecture detailed in Section 5.2.\n5 Understanding Local Convergence of the Attention Layer\nSo far, we have primarily focused on the convergence to the global direction dictated by (Att-SVM) . In this section, we\ninvestigate and establish the local directional convergence of GD as well as RP.\n5.1 Local convergence of gradient descent\nTo proceed, we introduce locally-optimal directions by adapting Definition 2 of [TLZO23]."], "retrieved_docs_id": ["3a3d9edb48", "3238be52f9", "a9c7970b35", "097ed983ff", "0537e071c4"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does Mamba Cobra [13] incorporate the efficient Mamba [77] language model into the vision modality?\n", "true_answer": "Mamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modality by exploring different modal fusion schemes to develop an effective multi-modal Mamba.", "source_doc": "multimodal.pdf", "source_id": "5510d4cc4e", "retrieved_docs": ["mising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework\nthat effectively increases the number of parameters without compromising computational efficiency.\nFurthermore, it introduces MoE-Tuning, a three-stage training strategy designed to adapt MoE [89]\nto MLLMs and prevent model degradation caused by sparsity. MM1[30] designs two variants of\nMoE models. The first is a 3B-MoE model that employs 64 experts and substitutes a dense layer\nwith a sparse one every two layers. The second is a 7B-MoE model that utilizes 32 experts and\nsubstitutes a dense layer with a sparse one every four layers.\nMamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modal-\nity and explores different modal fusion schemes to develop an effective multi-modal Mamba. Exper-\niments show that it not only achieves competitive performance with state-of-the-art efficient meth-", "iments show that it not only achieves competitive performance with state-of-the-art efficient meth-\nods but also boasts faster speeds due to its linear sequential modeling.It also excels in overcom-\ning visual illusions and spatial relationship judgments in closed-set challenging prediction bench-\nmarks and achieves performance comparable to LLaV A while using only 43% of the parameters.\nVL-Mamba[18] substitutes the Transformer-based backbone language model with the pre-trained\nMamba language model. It explores how to effectively implement the 2D vision selective scan\nmechanism for multimodal learning and the combinations of different vision encoders and pre-\ntrained Mamba language model variants.\nInference Acceleration SPD[45] proposes the speculative decoding with a language-only model\nto improve inference efficiency. By employing a language-only model as a draft model for specu-\n9", "improves with longer context up to million-length sequences.\n\u2022Language Modeling. Mamba is the \ufb01rst linear-time sequence model that truly achieves Transformer-quality\nperformance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,\nwe show that Mamba exceeds the performance of a large range of baselines, including very strong modern\nTransformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5 \u00d7\ngeneration throughput compared to Transformers of similar size, and Mamba-3B\u2019s quality matches that of\nTransformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and\neven exceeding Pythia-7B).\nModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba .\n2", "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "elements. (iii)Long context: the quality and e\ufb03ciency together yield performance improvements on real data up\nto sequence length 1M.\nWe empirically validate Mamba\u2019s potential as a general sequence FM backbone, in both pretraining quality and\ndomain-speci\ufb01c task performance, on several types of modalities and settings:\n\u2022Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being\nkey to large language models, Mamba not only solves them easily but can extrapolate solutions inde\ufb01nitely long\n(>1M tokens).\n\u2022Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-\ners on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g.\nreducing FID on a challenging speech generation dataset by more than half). In both settings, its performance\nimproves with longer context up to million-length sequences."], "retrieved_docs_id": ["5510d4cc4e", "6bebc6e320", "4349151e98", "3238be52f9", "22e268ba33"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}, {"question": "How does the RETRO system retrieve and integrate information for chunk-level language generation?\n", "true_answer": "RETRO retrieves the nearest neighboring chunk (N(Ci\u22121)) from the retrieval database and integrates this information with the contextual information of the previous chunk to guide the generation of the next chunk.", "source_doc": "RAG.pdf", "source_id": "92f5901d31", "retrieved_docs": ["mentation for pre-training a self-regressive language model,\nenabling large-scale pre-training from scratch by retrieving\nfrom a massive set of labeled data and significantly reducing\nmodel parameters. RETRO shares the backbone structure\nwith GPT models and introduces an additional RETRO\nencoder to encode features of neighboring entities retrieved\nfrom an external knowledge base. Additionally, RETRO\nincorporates block-wise cross-attention layers in its decoder\ntransformer structure to effectively integrate retrieval infor-\nmation from the RETRO encoder. RETRO achieves lower\nperplexity than standard GPT models. Moreover, it provides\nflexibility in updating knowledge stored in the language\nmodels by updating the retrieval database without the need\nfor retraining the language models [Petroni et al. , 2019 ].\nAtla[Izacard et al. , 2022 ]employs a similar approach, in-\ncorporating a retrieval mechanism using the T5 architecture", "Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "retrieved information. In RAG, the generator\u2019s input includes\nnot only traditional contextual information but also relevant\ntext segments obtained through the retriever. This allows the\ngenerator to better comprehend the context behind the ques-\ntion and produce responses that are more information-rich.\nFurthermore, the generator is guided by the retrieved text toensure consistency between the generated content and the re-\ntrieved information. It is the diversity of input data that has\nled to a series of targeted efforts during the generation phase,\nall aimed at better adapting the large model to the input data\nfrom queries and documents. We will delve into the intro-\nduction of the generator through aspects of post-retrieval pro-\ncessing and fine-tuning.\n5.1 How Can Retrieval Results be Enhanced via\nPost-retrieval Processing?\nIn terms of untuned large language models, most studies\nrely on well-recognized large language models like GPT-", "BLOOM\nProcessing: System Demonstrations , pages 175\u2013184, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.emnlp-demo.21. URL https://aclanthology.org/2021.emnlp-demo.21 .\nYujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hu-\nbert, Peter Choy, Cyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J.\nMankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray\nKavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode.\nCoRR, abs/2203.07814, 2022. doi: 10.48550/arXiv.2203.07814. URL https://doi.org/\n10.48550/arXiv.2203.07814 .\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Ya-\nsunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin New-", "pp. 2206\u20132240.\n[658] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua,\n\u201cSearch-in-the-chain: Towards accurate, credible and\ntraceable large language models for knowledge-\nintensive tasks,\u201d CoRR , vol. abs/2304.14732, 2023.\n[659] B. Peng, M. Galley, P . He, H. Cheng, Y. Xie, Y. Hu,\nQ. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao,\n\u201cCheck your facts and try again: Improving large\nlanguage models with external knowledge and auto-\nmated feedback,\u201d CoRR , vol. abs/2302.12813, 2023.\n[660] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-\nYu, Y. Yang, J. Callan, and G. Neubig, \u201cActive retrieval\naugmented generation,\u201d CoRR , vol. abs/2305.06983,\n2023.\n[661] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang,\nQ. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, \u201cA sur-\nvey on hallucination in large language models: Prin-\nciples, taxonomy, challenges, and open questions,\u201d\nCoRR , vol. abs/2311.05232, 2023.\n[662] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and"], "retrieved_docs_id": ["422e1adde8", "af911eac69", "fefa202c19", "4151c3f857", "058f03bb53"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "How can cross-modal alignment be improved in MLLMs training?\n", "true_answer": "Cross-modal alignment in MLLMs training can be improved by designing more advanced architectures, introducing additional learning objectives, or incorporating diverse supervision signals.", "source_doc": "hallucination.pdf", "source_id": "83c3718d9d", "retrieved_docs": ["pacities. To improve the alignment performance, it is crucial\nto design effective training strategies and select appropriate\npre-training data [829, 830]. Existing work mainly employs\nthe following strategies for cross-modality alignment: (1) if\nthe number of image-text pairs is not sufficiently large ( e.g.,\nless than 1M), it is often suggested to only update the\nconnection module [831]; (2) if the training data includes\nhigh-quality text corpora [832] or image-text pairs with\nfine-grained annotations [833], fine-tuning the LLM can be\nconducted to boost the performance; (3) if the number of\nimage-text pairs is very large ( e.g., about 1B), fine-tuning\nthe vision encoder is also plausible [829, 830], but the benefit\nremains further verification.\n\u2022Visual instruction tuning. After vision-language pre-\ntraining, the second-stage training, i.e., visual instruction\ntuning, aims to improve the instruction-following and task-\nsolving abilities of MLLMs. Generally, the input of vi-", "generated content remains consistent and contextually relevant to the input modality requires\nsophisticated techniques for capturing and modeling cross-modal relationships. The direction of\ncross-modal alignment encompasses both MLLMs training and hallucination evaluation. Regarding\ntraining, future research should explore methods for aligning representations between different\nmodalities. Achieving this goal may involve designing more advanced architectures, introducing\nadditional learning objectives [ 52], or incorporating diverse supervision signals [ 16]. Regarding\nevaluation, cross-modal consistency checking has been a long-standing topic, ranging from multi-\nmodal understanding [ 66,88] to text-to-image generation [ 13,17]. Drawing on proven experiences\nfrom these domains to improve the assessment of MLLM hallucination, or unifying them into an\noverall framework, may be promising research directions.\n6.3 Advancements in Model Architecture", "Figure 14: Training stages of efficient MLLMs.\nusing a standard cross-entropy loss function:\nmax\n\u03b8LX\ni=1logp\u03b8(xi|Xv, Xinstruct , Xa,<i), (4)\nwhere Lis the length of Xaand\u03b8denotes the trainable parameters. In order to better align different\nmodalities of knowledge and avoid catastrophic forgetting during the pre-training stage, \u03b8typically\nincludes only a learnable modality interface, i.e., a vision-language projector.\nWhich part to unfreeze? Considering that only training the connector may not well align the\nvision and text information when using SLMs, TinyLlava[23] also opt to partially freeze pre-\ntrained modules (i.e. vision encoder and SLM) to activate more parameters for learning alignment.\nVILA[49] reveals that updating the base LLM throughout the pre-training stage is essential to in-\nheriting some of the appealing LLM properties like in-context learning. ShareGPT4V[55] found\nthat unfreezing more parameters, particularly in the latter half of the vision encoder\u2019s layers, proves", "alignment between different modalities, fine-tune the models on specific tasks, and minimize the\ncomputational and parameter costs associated with the transfer learning process. Figure.14 presents\na schematic representation of the different training stages involved in the development of efficient\nMLLMs. In the following subsections, we delve deeper into each of these aspects and discuss their\nsignificance in the context of efficient MLLMs.\n5.1 Pre-Training\nIn the pre-training stage, the primary focus is on aligning different modalities in the embedding\nspace, enabling the language model to accept inputs from various modalities. This phase of training\nmainly involves large-scale text-paired data, predominantly in the form of image-caption pairs. An\nimage-caption pair (X, Y )is typically expanded into a single-turn conversation (Xinstruct , Xa),\nwhere Xinstruct contains an image Xvand a randomly sampled question Xqfrom a set of instruc-", "including the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],\nMiniGPT-v2 [9], and MiniGPT-4[10]. These models circumvent the computational cost of train-\ning from scratch by effectively leveraging the pre-training knowledge of each modality. MLLMs\ninherit the cognitive capabilities of LLMs, showcasing numerous remarkable features such as robust\nlanguage generation and transfer learning abilities. Moreover, by establishing strong representa-\ntional connections and alignments with other modality-based models, MLLMs can process inputs\nfrom multiple modalities, significantly broadening their application scope.\nThe success of MLLMs is largely attributed to the scaling law: the performance of an AI model\nimproves as more resources, such as data, computational power, or model size, are invested into it.\nHowever, scalability comes at the cost of high resource demands, which hinders the development"], "retrieved_docs_id": ["687ba2bcbe", "83c3718d9d", "1bd741e7c9", "3dbb490826", "7a547e4fbb"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.5, "hit": 1}, {"question": "How can a token-wise optimal visual context reduce hallucination in MLLMs?\n", "true_answer": "By providing the most informative visual grounding when decoding a specific token in the MLLM, it can effectively reduce hallucination.\n\nHere, MLLMs refer to Multimodal Language Learning Models. The factoid answer is derived from the context, specifically the key insight provided by HALC [15].", "source_doc": "hallucination.pdf", "source_id": "17a462daf3", "retrieved_docs": ["supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [ 57],\nguiding MLLMs to focus on highly-related image content. With the additional supervision from\nthe mask prediction loss, MLLMs are encouraged to extract features that can better represent these\ncrucial instances, thus generating more accurate responses and mitigating vision hallucination. The\nintuitive idea of supervising MLLMs with grounding shows promising performance in mitigating\nhallucination.\nAnother line of work analyzes the training loss from the perspective of embedding space distri-\nbution. As introduced earlier, popular MLLMs typically project the encoded vision features into the\ninput space of a specific LLM. A recent work, HACL [ 52], argues that an ideal projection should\nblend the distribution of visual and textual embeddings. However, despite visual projection, a sig-\nnificant modality gap exists between textual and visual tokens, suggesting that the current learned", "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "Organization of this survey. In this paper, we present a comprehensive survey of the latest\ndevelopments regarding hallucinations in MLLMs. The survey is organized as follows: We begin by\nproviding sufficient context and defining concepts related to LLMs, MLLMs, hallucination, etc. Next,\nwe delve into an in-depth analysis of the factors contributing to hallucinations in MLLMs. Following\nthis, we present a set of metrics and benchmarks employed for evaluating hallucinations in MLLMs.\nWe then elaborate on a range of approaches designed to mitigate hallucinations in MLLMs. Finally,\nwe delve into the challenges and open questions that frame the current limitations and future\nprospects of this field, offering insights and delineating potential pathways for forthcoming research.\n2 DEFINITIONS\n2.1 Large Language Models\nBefore moving to multimodal large language models, it is essential to introduce the concept of large", "overall framework, may be promising research directions.\n6.3 Advancements in Model Architecture\nDespite recent advancements in model architectures of LLMs and MLLMs, designing effective\narchitectures specifically tailored to hallucination remains a challenge. Developing advanced model\narchitectures capable of capturing complex linguistic structures and generating coherent and con-\ntextually relevant output based on input visual content is essential for improving the performance of\nMLLMs. Future research can explore innovative architectural designs based on identified causes of\nhallucination. This includes developing stronger visual perception models, innovative cross-modal\ninteraction modules capable of transferring cross-modal information seamlessly, and novel large\nlanguage model architectures faithful to input visual content and text instructions, etc.\n6.4 Establishing Standardized Benchmarks", "VHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual\nproperties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which\nemerge from comparisons across multiple objects, such as relative size, relative position, and\ncounting. Based on such categorization, the authors further defined 8 visual hallucination modes,\nproviding a very detailed evaluation of hallucination in MLLMs. Furthermore, the collected 1,200\nevaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no\nquestion\" (YNQ). Such design enables this benchmark to evaluate both generative and discriminative\ntasks.\nComparison of mainstream models We compare the mainstream MLLMs on some represen-\ntative benchmarks, providing a holistic overview of their performance from different dimensions.\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We"], "retrieved_docs_id": ["c505f06d1a", "92e73c053a", "e1b4ac9436", "c8e35c3848", "8ef8344de6"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "In what month and year was the preprint with arXiv ID 2404.1893v1 published in the field of computer vision?\n", "true_answer": "The preprint was published in the field of computer vision in April 2024.", "source_doc": "hallucination.pdf", "source_id": "35a7709274", "retrieved_docs": ["abling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615 .\nKovaleva, O., Kulshreshtha, S., Rogers, A., and Rumshisky, A. (2021). Bert busters: Outlier\ndimensions that disrupt transformers. arXiv preprint arXiv:2105.06990 .\nLi, R., Wang, Y ., Liang, F., Qin, H., Yan, J., and Fan, R. (2019). Fully quantized network for object\ndetection. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long\nBeach, CA, USA, June 16-20, 2019 , pages 2810\u20132819. Computer Vision Foundation / IEEE.\nLin, Y ., Li, Y ., Liu, T., Xiao, T., Liu, T., and Zhu, J. (2020). Towards fully 8-bit integer inference for\nthe transformer model. arXiv preprint arXiv:2009.08034 .\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and\nStoyanov, V . (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692 .", "vision, pages 6023\u20136032, 2019.\nS. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free\ntransformer. arXiv preprint arXiv:2105.14103 , 2021.\nH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412 , 2017.\nY. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transformers with lego:\na synthetic reasoning task. arXiv preprint arXiv:2206.04301 , 2022.\nZ. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. In Proceedings of the\nAAAI conference on arti\ufb01cial intelligence , volume 34, pages 13001\u201313008, 2020.\n16", "plications. arXiv preprint arXiv:1704.04861 , 2017.\nGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional\nnetworks. In Conference on Computer Vision and Pattern Recognition , 2017.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In International Conference on Machine Learning , pp. 448\u2013456, 2015.\nKevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture for object\nrecognition? In 2009 IEEE 12th International Conference on Computer Vision , 2009.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference\non Learning Representations , 2015.\nG\u00a8unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural net-\nworks. arXiv preprint arXiv:1706.02515 , 2017.", "preprint arXiv:2005.09904 , 2020.\n[119] Tianchu Ji, Shraddhan Jain, Michael Ferdman,\nPeter Milder, H Andrew Schwartz, and Niranjan\nBalasubramanian. On the distribution, sparsity, and\ninference-time quantization of attention values in\ntransformers. arXiv preprint arXiv:2106.01335 ,\n2021.\n[120] Kai Jia and Martin Rinard. Ef\ufb01cient exact veri\ufb01-\ncation of binarized neural networks. Advances in\nneural information processing systems , 2020.\n[121] Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou,\nand Zhiliang Gan. Kdlsq-bert: A quantized bert\ncombining knowledge distillation with learned step\nsize quantization. arXiv preprint arXiv:2101.05938 ,\n2021.\n[122] Qing Jin, Linjie Yang, and Zhenyu Liao. Adabits:\nNeural network quantization with adaptive bit-\nwidths. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition ,\npages 2146\u20132156, 2020.\n[123] Jeff Johnson. Rethinking \ufb02oating point for deep\nlearning. arXiv preprint arXiv:1811.01721 , 2018.", "complexity. arXiv preprint arXiv:2006.04768 , 2020.\n[66]Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with\nlightweight and dynamic convolutions. In The International Conference on Learning Representations\n(ICLR), 2019.\n[67]Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint\narXiv:2102.03902 , 2021.\n[68]Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237 ,\n2019.\n[69]Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986 , 2021."], "retrieved_docs_id": ["4f4cbd5041", "ca4bcb16bd", "8844c39736", "62baa4cf63", "28b324708e"], "reranker_type": "colbert", "search_type": "vector", "rr": 0.0, "hit": 0}, {"question": "What is the optimization goal of the information condenser training process in RECOMP [Xuet al., 2023a]?\n", "true_answer": "The optimization goal is to minimize the contrastive loss, which involves maximizing the similarity between a data point and its positive sample while minimizing the similarity between the data point and negative samples.", "source_doc": "RAG.pdf", "source_id": "ba4f3a6fe9", "retrieved_docs": ["input document. The objective of the training process is to\nminimize the discrepancy between Cextracted and the actual\ncontext Ctruth as much as possible. The loss function they\nadopted is as follows:\nminL (\u03b8) =\u22121\nNNX\ni=1C(i)\ntruthlog(f.(S(i)\ninput;\u03b8)) (3)\nwhere f.is the information extractor and \u03b8is the parameter\nof the extractor. RECOMP [Xuet al. , 2023a ]similarly trains\nan information condenser by leveraging contrastive learning.\nFor each training data point, there exists one positive sample\nand five negative samples. The encoder is trained using con-\ntrastive loss [Karpukhin et al. , 2020 ]during this process.The\nspecific optimization goals are as follows:\n\u2212logesim(xi,pi)\nsim(xi, pi) +P\nnj\u2208Niesim(xi,pi)(4)", "Information Compression\nEven though the retriever can fetch relevant information from\na vast knowledge base, we are still confronted with the chal-\nlenge of dealing with a substantial amount of information in\nretrieval documents. Some existing research attempts to solve\nthis problem by increasing the context length of large lan-\nguage models, but current large models still confront context\nlimitations. Thus, in certain situations, information conden-\nsation is necessary. In short, the importance of information\ncondensation mainly embodies in the following aspects: re-\nduction of noise, coping with context length restrictions, and\nenhancing generation effects.\nPRCA [Yang et al. , 2023b ]addressed this issue by train-\ning an information extractor. In the context extraction stage,\ngiven an input text Sinput , it can generate an output sequence\nCextracted , which represents the condensed context from the\ninput document. The objective of the training process is to", "zero-optimization.reduce-scatter True\nzero-optimization.stage 1\nTable 1: The full con\ufb01guration details for GPT-NeoX-\n20B training", "30\ntures and pre-training objectives are in need to analyze how\nthe choices of the architecture and pre-training tasks affect\nthe capacity of LLMs, especially for encoder-decoder archi-\ntectures. Despite the effectiveness of decoder-only architec-\nture, it is also suggested to make more diverse exploration\non architecture design. Besides the major architecture, the\ndetailed configuration of LLM is also worth attention, which\nhas been discussed in Section 4.2.2.\n4.3 Model Training\nIn this part, we review the important settings, techniques,\nor tricks for training LLMs.\n4.3.1 Optimization Setting\nFor parameter optimization of LLMs, we present the com-\nmonly used settings for batch training, learning rate, opti-\nmizer, and training stability.\nBatch Training. For language model pre-training, existing\nwork generally sets the batch size to a large number ( e.g.,\n2,048 examples or 4M tokens) to improve the training\nstability and throughput. For LLMs such as GPT-3 and", "cluster. In practice, it is very challenging to pre-train capable\nLLMs, due to the huge compute consumption and the\nsensitivity to data quality and training tricks [78, 93]. Thus,\nit becomes particularly important to develop systemic, eco-\nnomical pre-training approaches for optimizing LLMs, e.g.,\npredictable scaling [46] and proxy model training [59]. More\ntraining recipes or principles should be investigated and\nshared to reduce the potential risk of degradation or failure\nin large-scale model optimization. Although increasingly\nmore model checkpoints and cleaned datasets have been\nreleased, there still lacks reproducible work on pre-training\ndata preparation ( e.g., detailed cleaning strategies) and data\nscheduling ( e.g., data mixture and curriculum). Since it is\nvery costly to pre-train a LLM from scratch, it is important\nto design suitable mechanisms for continually pre-training\nor fine-tuning the LLM based on publicly available model"], "retrieved_docs_id": ["ba4f3a6fe9", "c5e55b3041", "44332bb85b", "6f172cf8fc", "c7e653937f"], "reranker_type": "colbert", "search_type": "vector", "rr": 1.0, "hit": 1}]