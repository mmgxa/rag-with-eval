[{"context": "VBR [Zhuet al. , 2022 ]method is used to generate images to\nguide the text generation of the language model, which has\nsignificant effects in open text generation tasks.\nIn the code field, RBPS [Nashid et al. , 2023 ]is used for\nsmall-scale learning related to code. By encoding or fre-\nquency analysis, similar code examples to the developers\u2019\ntasks are automatically retrieved. This technique has proven\nits effectiveness in test assertion generation and program re-\npair tasks. In the field of structured knowledge, methods like\nCoK [Liet al. , 2023c ]hints first retrieve facts related to the\ninput question from the knowledge graph and then add these\nfacts to the input in the form of hints. This method has per-\nformed well in knowledge graph question answering tasks.\nFor the field of audio and video, the\nGSS[Zhao et al. , 2022 ]method retrieves and concatenates\naudio clips from the spoken vocabulary bank, immediately\ntransforming MT data into ST data. UEOP [Chan et al. , 2023 ]", "question": "What method is used to transform media translation data (MT) into speech translation data (ST) using audio clips from a spoken vocabulary bank?\n", "answer": "The GSS method, introduced by Zhao et al. in 2022, is used to transform MT data into ST data by retrieving and concatenating audio clips from the spoken vocabulary bank.", "source": "RAG.pdf", "id": "150d364554"}, {"context": "SSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)\nmodel [153], which proposes fully diagonal parameterization of state spaces for greater efficiency.\nH3 stacks two SSMs to interact with their output and input projection, bridging the gap between\nSSMs and attention while adapting to modern hardware. Mamba [77], a selective state space model,\nhas been introduced as a strong competitor to the Transformer architecture in large language models.\nMamba incorporates a selection mechanism to eliminate irrelevant data and develops a hardware-\naware parallel algorithm for recurrent operation. This results in competitive performance compared\nto LLMs of the same capacity, with faster inference speeds that scale linearly with time and con-\nstant memory usage. In conclusion, State Space Models offer significant potential as an alternative\nto attention mechanisms by providing near-linear computational complexity and effectively captur-", "question": "What is a significant advantage of State Space Models over attention mechanisms?\n", "answer": "State Space Models offer near-linear computational complexity, providing an advantage over attention mechanisms.", "source": "multimodal.pdf", "id": "85b5cac71b"}, {"context": "task encoders are dedicated to integrating various types of latent visual information extracted by\nmultiple visual encoders. Additionally, the structural knowledge enhancement module is designed\nto utilize visual tools, such as OCR tools and object detectors, to extract prior knowledge from\nvisual inputs.\nFollowing the approach of the structural knowledge enhancement module in [ 38], another line\nof research investigates the utilization of vision tool models to enhance the perception of MLLMs.\nVCoder [ 49] utilizes additional perception formats, such as segmentation masks and depth maps,\nto enhance the object identification ability of the MLLM. Another work [ 54] ensembles additional\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How does VCoder enhance the object identification ability of MLLMs?\n", "answer": "VCoder enhances the object identification ability of MLLMs by utilizing additional perception formats, such as segmentation masks and depth maps.", "source": "hallucination.pdf", "id": "c461600dc0"}, {"context": "putational expenses for both training and inference. To ad-\ndress the limitations of purely parameterized models, lan-\nguage models can adopt a semi-parameterized approach by\nintegrating a non-parameterized corpus database with pa-\nrameterized models. This approach is known as Retrieval-\nAugmented Generation (RAG).\nThe term Retrieval-Augmented Generation (RAG) was\nfirst introduced by [Lewis et al. , 2020 ]. It combines a pre-\ntrained retriever with a pre-trained seq2seq model (generator)\nand undergoes end-to-end fine-tuning to capture knowledge\nin a more interpretable and modular way. Before the advent\nof large models, RAG primarily focused on direct optimiza-\ntion of end-to-end models. Dense retrievals on the retrieval\nside, such as the use of vector-based Dense Passage Retrieval\n(DPR) [Karpukhin et al. , 2020 ], and training smaller models\non the generation side are common practices. Due to the\noverall smaller parameter size, both the retriever and gener-", "question": "Who introduced the term Retrieval-Augmented Generation (RAG)?\n", "answer": "The term Retrieval-Augmented Generation (RAG) was first introduced by Lewis et al. , 2020.", "source": "RAG.pdf", "id": "33aae5a21d"}, {"context": "develops a dynamic attention-based multi-head token selector for adaptive instance-wise token se-\nlection, alongside a soft pruning technique consolidating less informative tokens into package tokens\nrather than discarding them. ViT-Slim [113] utilizes a learnable and unified sparsity constraint with\npre-defined factors to represent global importance within the continuous search space across various\ndimensions.\n3.3 Knowledge Distillation\nKnowledge distillation is a technique in which a smaller model learns from a larger, more com-\nplex model to replicate its performance, enabling efficient deployment while maintaining predictive\naccuracy [139]. Knowledge distillation (KD) techniques for Vision Transformers (ViTs) can be\ncategorized into two main types: 1) homomorphic KDs and 2) heteromorphic KDs.\nHomomorphic KDs can further classified into logit-level [114, 115], patch-level [117], module-", "question": "What is one type of knowledge distillation technique for Vision Transformers?\n", "answer": "Homomorphic KDs are one type of knowledge distillation technique for Vision Transformers.", "source": "multimodal.pdf", "id": "61a0681b1c"}, {"context": "implementing LaVIN is remarkably low, for instance, it only requires 1.4 training hours with 3.8M\ntrainable parameters. HyperLLaV A [51] studies the under-explored dynamic tuning strategy for\nMLLMs and leverages the visual and language-guided dynamic tuning for the projector and LLM\nin two-stage training.\n5.3 Diverse Training Steps\nThe traditional two-stage strategy, which demands the manual assignment of various adjustable\nparameters and dataset combinations to different training stages, can be a laborious task. To mit-\nigate this, SPHINX-X[14] devises a single-stage, all-encompassing training pipeline that impar-\ntially treats all gathered datasets and consistently converts them into multi-modal, multi-turn dia-\nlogue formats. Throughout this unified training phase, all parameters except vision ecnoders within\nSPHINX-X are activated. Cobra[13] also argues that the initial phase of pre-alignment may not", "question": "How long does it take to implement LaVIN with its number of trainable parameters?\n", "answer": "It takes 1.4 training hours with 3.8M trainable parameters to implement LaVIN.", "source": "multimodal.pdf", "id": "ba652acb16"}, {"context": "Figure 11: Organization of efficient large language models advancements.\nOccupying a significant majority of the parameter volume in MLLMs, LLM serves as a crucial entry\npoint for enhancing the efficiency of MLLMs. In this section, similar to the survey paper [160], we\nprovide a brief overview of the research progress in efficient LLMs, offering inspiration for the\ndevelopment of Efficient MLLMs.\n4.1 Attention\nIn the standard self-attention mechanism, the time complexity is O(n2), where nis the sequence\nlength. This quadratic complexity arises due to the pairwise interactions between all input tokens,\nwhich can lead to scalability issues, especially when dealing with long sequences in LLMs. To\ntackle this, researchers have developed techniques to expedite attention mechanisms and reduce\ntime complexity, such as sharing-based attention, feature information reduction, kernelization or\nlow-rank, fixed and learnable pattern strategies, and hardware-assisted attention.", "question": "How does the standard self-attention mechanism's time complexity affect large language models?\n", "answer": "The standard self-attention mechanism has a time complexity of O(n^2), where n is the sequence length. This quadratic complexity can lead to scalability issues, particularly when dealing with long sequences in large language models (LLMs).", "source": "multimodal.pdf", "id": "323641b323"}, {"context": "tailed and accurate image descriptions compared to their coarse-grained counterparts, enabling a\ncloser alignment of image and text modalities. However, this method often requires the use of\ncommercial MLLMs, leading to increased costs and a smaller data volume. ShareGPT4V[55] ad-\ndresses this issue by first training a captioner on 100K GPT-4V-generated data and then expanding\nthe dataset to 1.2M using the pretrained captioner. Moreover,VILA\u2019s[49] findings indicate that in-\ncorporating interleaved pre-training data proves advantageous, while solely relying on image-text\npairs is suboptimal in achieving the desired outcomes.\n6.2 Instruction-Tuning Data\nInstruction tuning(IT) is a crucial step in refining efficient MLLMs\u2019 capacity to accurately interpret\nuser instructions and effectively carry out the desired tasks. This procedure bears a strong connection\nto the concept of multi-task prompting.\n20", "question": "How does ShareGPT4V address the issue of increased costs and smaller data volume in tailed and accurate image descriptions?\n", "answer": "ShareGPT4V addresses this issue by first training a captioner on 100K GPT-4V-generated data and then expanding the dataset to 1.2M using the pretrained captioner.", "source": "multimodal.pdf", "id": "207b452ddf"}, {"context": "ARES\nARES aims to automatically evaluate the performance of\nRAG systems in three aspects: Context Relevance, Answer\nFaithfulness, and Answer Relevance. These evaluation met-\nrics are similar to those in RAGAS. However, RAGAS, being\na newer evaluation framework based on simple handwritten\nprompts, has limited adaptability to new RAG evaluation set-\ntings, which is one of the significances of the ARES work.\nFurthermore, as demonstrated in its assessments, ARES per-\nforms significantly lower than RAGAS.\nARES reduces the cost of evaluation by using a small\namount of manually annotated data and synthetic data,\nand utilizes Predictive-Driven Reasoning (PDR) to provide\nstatistical confidence intervals, enhancing the accuracy of\nevaluation [Saad-Falcon et al. , 2023 ].\nAlgorithm Principles\n1. Generating Synthetic Dataset: ARES initially generates\nsynthetic questions and answers from documents in the\ntarget corpus using a language model to create positive\nand negative samples.", "question": "How does ARES generate synthetic questions and answers?\n", "answer": "ARES generates synthetic questions and answers by using a language model to create positive and negative samples from documents in the target corpus.", "source": "RAG.pdf", "id": "1b1cdfdd79"}, {"context": "randomly samples 100 images from Visual Genome to form a benchmark. In evaluation, GPT-4\nis utilized to parse the captions generated by MLLMs and extract objects. Additionally, this work\nintroduces the \"coverage\" metric on top of CHAIR to ensure that the captions are detailed enough.\nThis metric computes the ratio of objects in the caption that match the ground truth to the total\nnumber of ground truth objects. It additionally records the average number of objects as well as\nthe average length of captions as auxiliary metric. Compared with CHAIR, CCEval employs more\ndiverse objects, as reflected in the source of ground truth (Visual Genome vs. COCO) and caption\nparsing (GPT-4 vs. rule-based tool).\nMERLIM [ 100]MERLIM ( Multi-modal Evaluation benchma Rk for Large Image-language\nModels) is a test-bed aimed at empirically evaluating MLLMs on core computer vision tasks,\nincluding object recognition, instance counting, and identifying object-to-object relationships.", "question": "What is the source of ground truth objects used in the CCEval metric?\n", "answer": "The source of ground truth objects used in the CCEval metric is Visual Genome.", "source": "hallucination.pdf", "id": "6e78496733"}, {"context": "Figure 3: Comparison between the three paradigms of RAG\n\u2022Task Adaptable Module: Focused on trans-\nforming RAG to adapt to various downstream\ntasks, UPRISE [Cheng et al. , 2023a ] automati-\ncally retrieves prompts for given zero-shot task\ninputs from a pre-constructed data pool, en-\nhancing universality across tasks and models.\nPROMPTAGATOR [Daiet al. , 2022 ]utilizes LLM\nas a few-shot query generator and, based on the gener-\nated data, creates task-specific retrievers. Leveraging\nthe generalization capability of LLM, PROMPTAGA-\nTOR enables the creation of task-specific end-to-end\nretrievers with just a few examples.\n\u2022Alignment Module: The alignment between queries\nand texts has consistently been a critical issue influenc-\ning the effectiveness of RAG. In the era of Modular\nRAG, researchers have discovered that adding a train-\nable Adapter module to the retriever can effectively mit-\nigate alignment issues. PRCA [Yang et al. , 2023b ]lever-", "question": "How does UPRISE address universality across tasks and models in RAG?\n", "answer": "UPRISE automatically retrieves prompts for given zero-shot task inputs from a pre-constructed data pool, enhancing universality across tasks and models in RAG.", "source": "RAG.pdf", "id": "bbfa682738"}, {"context": "is often combined with process optimization techniques such\nas step-wise reasoning , iterative reasoning, and adaptive re-\ntrieval to better meet the requirements of different tasks.\n6.2 Augmentation Data Source\nData source is crucial factors for RAG effectiveness. Vari-\nous data sources offer distinct granularities and dimensions\nof knowledge, requiring different processing methods. They\nprimarily fall into three categories: unstructured data, struc-\ntured data, and content generated by LLMs.\nAugmented with Unstructured Data\nUnstructured data mainly encompasses textual data , typi-\ncally derived from pure text corpora. Additionally, other text\ndata can serve as retrieval sources, such as Prompt data used\nfor large model fine-tuning [Cheng et al. , 2023a ]and cross-\nlanguage data [Liet al. , 2023b ].\nIn terms of text granularity, beyond the common\nchunks (including sentences), the retrieval unit can be to-\nkens (e.g., kNN-LM [Khandelwal et al. , 2019 ]), phrases (e.g.,", "question": "What are some examples of text data that can serve as retrieval sources in unstructured data?\n", "answer": "Prompt data used for large model fine-tuning and cross-language data are some examples of text data that can serve as retrieval sources in unstructured data.", "source": "RAG.pdf", "id": "1e001bba8b"}, {"context": "equally diverse. Hallucination is a prominent issue where the\nmodel fabricates an answer that doesn\u2019t exist in the context.\nIrrelevance is another concern where the model generates an\nanswer that fails to address the query. Further, toxicity or\nbias, where the model generates a harmful or offensive re-\nsponse, is another problem.\nFinally, the augmentation process also faces several chal-\nlenges. Crucially, the effective integration of the context from\nretrieved passages with the current generation task is of ut-\nmost importance. If mishandled, the output might appear in-\ncoherent or disjointed. Redundancy and repetition are another\nissue, particularly when multiple retrieved passages contain\nsimilar information, leading to content repetition in the gen-\neration step. Moreover, determining the importance or rele-\nvance of multiple retrieved passages to the generation task is\nchallenging, and the augmentation process needs to balance", "question": "What is a challenge in the integration process of retrieved passages in the generation task?\n", "answer": "If not handled properly, the output might appear incoherent or disjointed.", "source": "RAG.pdf", "id": "e75af48a5e"}, {"context": "Figure 1: The timeline of efficient MLLMs.\ninference constitutes the major portion of resource consumption in mllm. Consider a typical scenario\nwhere the model input consists of an image with dimensions of 336\u00d7336pixels and a text prompt\nwith a length of 40 tokens, performing inference with LLaV A-1.5 and a Vicuna-13B LLM backbone\nrequires 18.2T FLOPS and 41.6G of memory usage. The resource-intensive nature of large-scale\nmodels has also sparked concerns about democratization and privacy protection, considering that the\ncurrent mainstream MLLMs, represented by GPT-4V and Gemini, are controlled by a few dominant\ncorporations and operate in the cloud. As demonstrated in the aforementioned experiments, even for\nopen-source MLLMs, high requirements for computation resources make it challenging to run them\non edge devices. This further exacerbates the challenges associated with ensuring equitable access\nand preserving user privacy.", "question": "How many TFLOPS are needed to perform inference with LLaV A-1.5 and a Vicuna-13B LLM backbone on a 336\u00d7336 pixel image and 40 token text prompt?\n", "answer": "18.2 TFLOPS", "source": "multimodal.pdf", "id": "37128cb48f"}, {"context": "ChipNeMo: Domain-Adapted LLMs for Chip Design\nMingjie Liu* 1Teodor-Dumitru Ene* 1Robert Kirby* 1Chris Cheng* 1Nathaniel Pinckney* 1\nRongjian Liang* 1Jonah Alben1Himyanshu Anand1Sanmitra Banerjee1Ismet Bayraktaroglu1\nBonita Bhaskaran1Bryan Catanzaro1Arjun Chaudhuri1Sharon Clay1Bill Dally1Laura Dang1\nParikshit Deshpande1Siddhanth Dhodhi1Sameer Halepete1Eric Hill1Jiashang Hu1Sumit Jain1\nAnkit Jindal1Brucek Khailany1George Kokai1Kishor Kunal1Xiaowei Li1Charley Lind1Hao Liu1\nStuart Oberman1Sujeet Omar1Ghasem Pasandi1Sreedhar Pratty1Jonathan Raiman1Ambar Sarkar1\nZhengjiang Shao1Hanfei Sun1Pratik P Suthar1Varun Tej1Walker Turner1Kaizhe Xu1Haoxing Ren1\nAbstract\nChipNeMo aims to explore the applications of\nlarge language models (LLMs) for industrial chip\ndesign. Instead of directly deploying off-the-\nshelf commercial or open-source LLMs, we in-\nstead adopt the following domain adaptation tech-\nniques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment", "question": "How does the ChipNeMo project adapt large language models for chip design?\n", "answer": "The ChipNeMo project adapts large language models for chip design using domain-adaptive tokenization, domain-adaptive continued pretraining, and model alignment techniques.", "source": "ChipNemo.pdf", "id": "36c5c0c7f1"}, {"context": "FastV[46], VTW[47]\nTraining (\u00a75)Pre-Training (\u00a75.1) Idefics2[48], TinyLLaV A[23], VILA[49]\nInstruction-Tuning (\u00a75.2) LaVIN[50], HyperLLaV A[51]\nDiverse Training Steps (\u00a75.3) SPHINX-X[14], Cobra[13], TinyGPT-V[28]\nParameter Efficient\nTransfer Learning (\u00a75.4)EAS [52], MemVP [53]\nData and Benchmarks (\u00a76)Pre-Training Data (\u00a76.1)CC595k[7], LLava-1.5-PT[54],\nShareGPT4V-PT[55],\nBunny-pretrain-LAION-2M[24],\nALLaV A-Caption-4V[29], etc.\nInstrcution-Tuning Data (\u00a76.2)LLaV A\u2019s IT[7], LLaV A-1.5\u2019s IT[54],\nShareGPT4V\u2019s IT[55], Bunny-695K[24],\nLVIS-INSTRUCT-4V[56], etc.\nBenchmarks (\u00a76.3)VQAv2[57], TextVQA[58], GQA[59],\nMME[60], MMBench[61], POPE[62]\nApplication (\u00a77)Biomedical Analysis (\u00a77.1) LLaV A-Rad [63], MoE-TinyMed [64]\nDocument Understanding (\u00a77.2)TextHawk [36], TinyChart [37],\nMonkey [65], HRVDA [66]\nVideo Comprehension (\u00a77.3)mPLUG-video [67], Video-LLaV A [44],\nMA-LMM [68], LLaMA-VID [69]\nFigure 2: Organization of efficient multimodal large language models advancements.", "question": "What is one of the benchmarks used for evaluating video comprehension in multimodal large language models?\n", "answer": "Video-LLaV A", "source": "multimodal.pdf", "id": "d85947fa4f"}, {"context": "mark results are presented in Appendix A.6. Our research\nfindings can be summarized as follows:\n\u2022DAPT exerts a substantial positive impact on tasks\nwithin the domain itself. This effect is manifested in\nsignificant improvements in internal design knowledge\nas well as general circuit design knowledge.\n\u2022DAPT models exhibit a slight degradation in perfor-mance on open-domain academic benchmarks.\n\u2022The use of larger and more performant foundational\nmodels yields better zero-shot results on domain-\nspecific tasks. Furthermore, the employment of su-\nperior base models results in enhanced domain models\npost-DAPT, leading to heightened performance on in-\ndomain tasks.\n\u2022Improvements attributed to DAPT with in-domain\ntasks exhibit a positive correlation with model size,\nwith larger models demonstrating more pronounced\nenhancements in domain-specific task performance.\n3.3. Training Ablation Studies\nFor our ablation studies, we conducted multiple rounds of", "question": "How does DAPT affect performance on open-domain academic benchmarks?\n", "answer": "DAPT models exhibit a slight degradation in performance on open-domain academic benchmarks.", "source": "ChipNemo.pdf", "id": "1769e97a8f"}, {"context": "mentioned in the generated sentences; 2) Question formulation asks questions around the extracted\nobjects; 3) Visual knowledge validation answers the formulated questions via expert models; 4)\nVisual claim generation converts the above Question-Answer (QA) pairs into a visual knowledge\nbase; 5) Hallucination correction modifies the hallucinations and adds the corresponding evidence\nunder the guidance of the visual knowledge base. Woodpecker is a training-free method, where\neach component can be implemented using either hand-crafted rules or off-the-shelf pre-trained\nmodels.\nAnother line of work rectifies the generated text using a dedicatedly trained revisor model.\nSpecifically, inspired by denoising autoencoders [ 101], which are designed to reconstruct clean data\nfrom corrupted input, LURE [ 137] employs a hallucination revisor that aims to transform potentially\nhallucinatory descriptions into accurate ones. To train such a revisor model, a dataset has been", "question": "How does LURE correct hallucinations in generated text?\n", "answer": "LURE corrects hallucinations in generated text using a hallucination revisor, which transforms potentially hallucinatory descriptions into accurate ones. This is achieved by training the revisor model on a dataset, with the goal of reconstructing clean data from corrupted input.", "source": "hallucination.pdf", "id": "ceeab98980"}, {"context": "DIT[Linet al. , 2023 ]method fine-tunes both the generator\nand retriever by maximizing the probability of correct an-", "question": "How does the DIT[Linet al. , 2023] method treat the generator and retriever?\n", "answer": "The DIT[Linet al. , 2023] method fine-tunes both the generator and retriever.", "source": "RAG.pdf", "id": "00f00e3f9c"}, {"context": "edge and the capability to interact with the real world has far-reaching implications, poten-\ntially revolutionizing fields such as robotics, automation, and artificial intelligence.\n8.2 Conclusion\nIn this study, we take a deep dive into the realm of efficient MLLM literature, providing an all-\nencompassing view of its central themes, including foundational theories and their extensions. Our\ngoal is to identify and highlight areas that require further research and suggest potential avenues\nfor future studies. We aim to provide a comprehensive perspective on the current state of efficient\nMLLM, with the hope of inspiring additional research. Given the dynamic nature of this field, it\u2019s\npossible that some recent developments may not be fully covered. To counter this, we\u2019ve set up a\ndedicated website that uses crowdsourcing to keep up with the latest advancements. This platform\nis intended to serve as a continually updated source of information, promoting ongoing growth in", "question": "What is the purpose of the dedicated website mentioned in the context?\n", "answer": "The purpose of the dedicated website mentioned in the context is to keep up with the latest advancements in the field of efficient MLLM and serve as a continually updated source of information.", "source": "multimodal.pdf", "id": "17ac4e37d9"}, {"context": "Feature Comparison RAG Fine-tuning\nKnowledge UpdatesDirectly updates the retrieval knowledge\nbase, ensuring information remains current\nwithout the need for frequent retraining, suit-\nable for dynamic data environments.Stores static data, requiring retraining for\nknowledge and data updates.\nExternal KnowledgeProficient in utilizing external resources,\nparticularly suitable for documents or other\nstructured/unstructured databases.Can be applied to align the externally learned\nknowledge from pretraining with large lan-\nguage models, but may be less practical for\nfrequently changing data sources.\nData ProcessingRequires minimal data processing and han-\ndling.Relies on constructing high-quality datasets,\nand limited datasets may not yield significant\nperformance improvements.\nModel CustomizationFocuses on information retrieval and inte-\ngrating external knowledge but may not fully\ncustomize model behavior or writing style.Allows adjustments of LLM behavior, writ-", "question": "How does the Knowledge Updates feature affect the retrieval knowledge base?\n", "answer": "The Knowledge Updates feature directly updates the retrieval knowledge base, ensuring information remains current without the need for frequent retraining. This is suitable for dynamic data environments.", "source": "RAG.pdf", "id": "9c38efbac6"}, {"context": "into individual statements using an LLM and verify\nwhether each statement is consistent with the context.\nUltimately, a \u201dFaithfulness Score\u201d is calculated by com-\nparing the number of supported statements to the total\nnumber of statements.\n2. Assessing Answer Relevance: Generate potential ques-\ntions using an LLM and calculate the similarity between\nthese questions and the original question. The Answer\nRelevance Score is derived by calculating the average\nsimilarity of all generated questions to the original ques-\ntion.\n3. Assessing Context Relevance: Extract sentences directly\nrelevant to the question using an LLM, and use the ratio\nof these sentences to the total number of sentences in the\ncontext as the Context Relevance Score.", "question": "How is the \"Faithfulness Score\" calculated in the given context?\n", "answer": "The \"Faithfulness Score\" is calculated by comparing the number of statements supported by the context to the total number of statements, using a large language model (LLM) to break down the context into individual statements and verify their consistency with the original context.", "source": "RAG.pdf", "id": "716582522f"}, {"context": "Embedding model, so it is crucial whether Embedding can\nrepresent the corpus effectively. Nowadays, excellent Em-\nbedding models have appeared, such as [UAE [AngIE, 2023 ],\nV oyage [V oyageAI, 2023 ], BGE [BAAI, 2023 ], etc.], they\nhave been pre-trained on large-scale corpus, but they may\nnot accurately represent domain-specific corpus information\nwhen applied to specific domains. Furthermore, task-specific\nfine-tuning of Embedding models is critical to ensure that\nthe model understands the user query in relation to the con-\ntent relevance, whereas an un-fine-tuned model may not be\nable to fulfill the needs of a specific task. Thus, fine-tuning\nan Embedding model is essential for downstream applica-\ntions. There are two basic paradigms in Embedding fine-\ntuning methods\nDomain Knowledge Fine-tuning In order for an Embed-\nding model to correctly understand domain-specific informa-\ntion, we need to construct domain-specific datasets to fine-", "question": "How can an embedding model be made to understand domain-specific information?\n", "answer": "By constructing domain-specific datasets for Domain Knowledge Fine-tuning.", "source": "RAG.pdf", "id": "97e2fcdf55"}, {"context": "cross-attention scores, selecting the highest scoring input to-\nkens to effectively filter tokens. RECOMP [Xuet al. , 2023a ]\nproposes extractive and generative compressors, which gen-\nerate summaries by selecting relevant sentences or syn-\nthesizing document information to achieve multi-document\nquery focus summaries.In addition to that, a novel approach,\nPKG [Luoet al. , 2023 ], infuses knowledge into a white-box\nmodel through directive fine-tuning, and directly replaces the\nretriever module, used to directly output relevant documents\nbased on the query.\n5 Generator\nAnother core component in RAG is the generator, responsible\nfor transforming retrieved information into natural and fluent\ntext. Its design is inspired by traditional language models,\nbut in comparison to conventional generative models, RAG\u2019s\ngenerator enhances accuracy and relevance by leveraging the\nretrieved information. In RAG, the generator\u2019s input includes", "question": "How does RAG's generator improve the accuracy and relevance of the generated text?\n", "answer": "RAG's generator enhances accuracy and relevance by leveraging the retrieved information, in contrast to conventional generative models.", "source": "RAG.pdf", "id": "cd69a480bb"}, {"context": "ments to assess the relevance between the retrieved doc-\numents and the query. This enhances the robustness of\nRAG [Yuet al. , 2023a ].\nNew Pattern\nThe organizational approach of Modular RAG is flexible,\nallowing for the substitution or reconfiguration of modules\nwithin the RAG process based on specific problem con-\ntexts. For Naive RAG, which consists of the two modules\nof retrieval and generation ( referred as read or sythesis in\nsome literature), this framework offers adaptability and abun-\ndance. Present research primarily explores two organizational\nparadigms, involving the addition or replacement of modules,\nas well as the adjustment of the organizational flow between\nmodules.\n\u2022Adding or Replacing Modules\nThe strategy of adding or replacing modules entails\nmaintaining the structure of Retrieval-Read while intro-\nducing additional modules to enhance specific function-\nalities. RRR [Maet al. , 2023a ]proposes the Rewrite-\nRetrieve-Read process, utilizing LLM performance as a", "question": "What is the new organizational approach introduced in Modular RAG for specific problem contexts?\n", "answer": "The new organizational approach of Modular RAG is flexible, allowing for the substitution or reconfiguration of modules within the RAG process based on specific problem contexts. This includes adding or replacing modules, as well as adjusting the organizational flow between modules.", "source": "RAG.pdf", "id": "a016e8d322"}, {"context": "is intended to serve as a continually updated source of information, promoting ongoing growth in\nthe field. Due to space constraints, we can\u2019t cover all technical details in depth but have provided\nbrief overviews of the key contributions in the field. In the future, we plan to continuously update\nand enhance the information on our website, adding new insights as they come to light.\n24", "question": "What is the purpose of the source with the given context?\n", "answer": "The purpose of the source is to serve as a continually updated source of information, promoting ongoing growth in a certain field by providing brief overviews of key contributions.", "source": "multimodal.pdf", "id": "6e2ea7a4ef"}, {"context": "vision and language models. It commences with a convolutional stem, succeeded by Mobile Con-\nvolution Blocks in the first and second stages, and Transformer Blocks in the third stage. Remark-\nably, ViTamin-XL, with a modest count of 436M parameters, attains an 82.9% ImageNet zero-shot\naccuracy. This outperforms the 82.0% accuracy achieved by EV A-E [80], which operates with a pa-\nrameter count ten times larger, at 4.4B. Simply replacing LLaV A\u2019s image encoder with ViTamin-L\ncan establish new standards in various MLLM performance metrics.\n2.2 Vision-Language Projector\nThe task of the vision-language projector is to map the visual patch embeddings Zvinto the text\nfeature space:\nHv=P(Zv), (2)\nwhere Hvdenotes the projected visual embeddings. The aligned visual features are used as prompts\nand inputted into the language model along with the text embeddings. Vision-language projector\n5", "question": "What is the task of the vision-language projector in this context?\n", "answer": "The task of the vision-language projector is to map the visual patch embeddings into the text feature space.", "source": "multimodal.pdf", "id": "6ac775b4ef"}, {"context": "However, scalability comes at the cost of high resource demands, which hinders the development\nand deployment of large models. For example, the training of MiniGPT-v2 necessitates a total of\nover 800 GPU hours, as calculated based on NVIDIA A100 GPUs [9]. This imposes a substantial\nexpense that is difficult for researchers outside of major enterprises to bear. Aside from training,\n1* Equal contribution.\n2Yizhang Jin is an intern in Tencent, and Jian Li is the project leader.\nPreprint. Under review.arXiv:2405.10739v1  [cs.CV]  17 May 2024", "question": "How many GPU hours are needed to train MiniGPT-v2?\n", "answer": "Over 800 GPU hours are needed to train MiniGPT-v2, based on NVIDIA A100 GPUs.", "source": "multimodal.pdf", "id": "2f6f7fb082"}, {"context": "unsupervised learning styles. Auto-ViT-Acc [132] proposed a framework designed for quantizing\nViT architectures to run inference on FPGA-powered devices. They apply the quantization function\nfrom a prior study specifically to the FNN module within the attention block, aiming to optimize\nFPGA resource utilization and accelerate inference.\n4 Efficient LLMs\nEfficient LLMAttention (\u00a74.1)Sharing-based Attention GQA [143], Multi-Query Attention[144]\nFeature Information\nReductionFunnel-Transformer [145],\nSet Transformer[146]\nApproximate Attention Linformer [147], Performer[148]\nFramework (\u00a74.2)Mixture of Experts GShard[149], Switch Transformer [150]\nTransformer-Alternative\nArchitectureRWKV[151], S4[152],\nDSS[153], Mamba[77]\nFine-tuning (\u00a74.3)Parameter-Efficient\nFine-TuningLLM-Adapters [154], (IA)3[155],\nLoRA-FA [156], DyLoRa [157]\nFull-Parameter fine-tuning LOMO [158], MeZO[159]\nFigure 11: Organization of efficient large language models advancements.", "question": "What is the name of the framework proposed in the context for quantizing ViT architectures?\n", "answer": "Auto-ViT-Acc [132]", "source": "multimodal.pdf", "id": "3634d5989e"}, {"context": "the domain of efficient MLLMs. In addition to the survey, we have established a GitHub repository\nwhere we compile the papers featured in the survey, organizing them with the same taxonomy at\nhttps://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey. We will actively maintain it and\nincorporate new research as it emerges.\n2 Architecture\nFollowing the standard MLLM framework, efficient MLLMs can be divided into three main mod-\nules: a visual encoder gtasked with receiving and processing visual inputs, a pre-trained language\nmodel that manages the received multimodal signals and performs reasoning, and a visual-language\nprojector Pwhich functions as a bridge to align the two modalities. To enhance the efficiency of the\ngeneral MLLMs, the primary optimization lies in handling high-resolution images, compressing vi-\nsion tokens, implementing efficient structures, and utilizing compact language models, among other", "question": "What is the primary optimization for enhancing the efficiency of efficient MLLMs?\n", "answer": "The primary optimization for enhancing the efficiency of efficient MLLMs includes handling high-resolution images, compressing vision tokens, implementing efficient structures, and utilizing compact language models.", "source": "multimodal.pdf", "id": "de74717e46"}, {"context": "Where \u03b6,\u03beare learnable linear projection layers.z is the av-\nerage representations of the graph from Encoder,h is the mean\nof decoder representations. z\u2032,h\u2032represent the corresponding\nnegative samples respectively. In the given text, \u2019h\u201d and\n\u2019z\u201d represent negative samples. By introducing a contrastive\nlearning objective, the model can learn to generate diverse\nand reasonable replies better, rather than just the one seen in\nthe training data. This helps to mitigate the risk of overfitting\nand improves the model\u2019s generalization ability in real-world\nscenarios.\nWhen dealing with retrieval tasks that involve structured\ndata, the work of SANTA [Liet al. , 2023d ]utilized a three-\nstage training process to fully understand the structural and\nsemantic information. Specifically, in the training phase\nof the retriever, contrastive learning was adopted, with the\nmain goal of optimizing the embedding representations of the\nqueries and documents. The specific optimization objectives", "question": "How did the work of SANTA utilize contrastive learning in the training phase of the retriever?\n", "answer": "The work of SANTA utilized contrastive learning in the training phase of the retriever to optimize the embedding representations of the queries and documents.", "source": "RAG.pdf", "id": "29b4a935b2"}, {"context": "propose to combine the following techniques: Domain-\nAdaptive Pre-Training (DAPT) (Gururangan et al., 2020) of\nfoundation models with domain-adapted tokenizers, model\nalignment using general and domain-specific instructions,\nand retrieval-augmented generation (RAG) (Lewis et al.,\n2021b) with a trained domain-adapted retrieval model.\nAs shown in Figure 1, our approach is to start with a base\nfoundational model and apply DAPT followed by model\nalignment. DAPT, also known as continued pretraining with\nin-domain data, has been shown to be effective in areas such\nas biomedical and computer science publications, news, and\nreviews. In our case, we construct our domain-specific pre-\ntraining dataset from a collection of proprietary hardware-\nrelated code (e.g. software, RTL, verification testbenches,\netc.) and natural language datasets (e.g. hardware specifi-\ncations, documentation, etc.). We clean up and preprocess\nthe raw dataset, then continued-pretrain a foundation model", "question": "How is a foundation model adapted to a specific domain using DAPT?\n", "answer": "DAPT, or Domain-Adaptive Pre-Training, adapts a foundation model to a specific domain by continued pretraining with in-domain data. In this case, the domain-specific pre-training dataset is constructed from a collection of proprietary hardware-related code and natural language datasets.", "source": "ChipNemo.pdf", "id": "926168a67f"}, {"context": "challenging, and the augmentation process needs to balance\nthe value of each passage appropriately. The retrieved con-\ntent may also come from different writing styles or tones, and\nthe augmentation process needs to reconcile these differences\nto ensure output consistency. Lastly, generation models may\noverly rely on augmented information, resulting in output thatmerely repeats the retrieved content, without providing new\nvalue or synthesized information.\n3.2 Advanced RAG\nAdvanced RAG has made targeted improvements to over-\ncome the deficiencies of Naive RAG. In terms of the quality\nof retrieval generation, Advanced RAG has incorporated pre-\nretrieval and post-retrieval methods. To address the indexing\nissues encountered by Naive RAG, Advanced RAG has op-\ntimized indexing through methods such as sliding window,\nfine-grained segmentation, and metadata. Concurrently, it has\nput forward various methods to optimize the retrieval process.", "question": "How does Advanced RAG optimize the content retrieval process?\n", "answer": "Advanced RAG optimizes the content retrieval process through pre-retrieval and post-retrieval methods, as well as by optimizing indexing with techniques such as sliding window, fine-grained segmentation, and metadata.", "source": "RAG.pdf", "id": "873e6df003"}, {"context": "Reducing HallucinationsInherently less prone to hallucinations as\neach answer is grounded in retrieved evi-\ndence.Can help reduce hallucinations by training\nthe model based on specific domain data but\nmay still exhibit hallucinations when faced\nwith unfamiliar input.\nEthical and Privacy IssuesEthical and privacy concerns arise from\nstoring and retrieving text from external\ndatabases.Ethical and privacy concerns may arise due\nto sensitive content in the training data.\nTable 1: Comparison between RAG and Fine-tuning\nof Advanced RAG and Modular RAG were aimed at address-\ning specific deficiencies in the Naive RAG.\n3.1 Naive RAG\nThe Naive RAG research paradigm represents the earliest\nmethodology gained prominence shortly after the widespread\nadoption of ChatGPT. The naive RAG involves traditional\nprocess: indexing, retrieval, and generation. Naive RAG\nis also summarized as a \u201cRetrieve\u201d-\u201cRead\u201d framework\n[Maet al. , 2023a ].\nIndexing", "question": "What is the original methodology for RAG?\n", "answer": "The original methodology for RAG, known as Naive RAG, gained prominence shortly after the widespread adoption of ChatGPT and involves a \"Retrieve-Read\" framework. (Reference(s):\nContext)", "source": "RAG.pdf", "id": "16ccbc6afa"}, {"context": "rewriting. This method generates a query using a large lan-\nguage model, then uses a web search engine to retrieve con-\ntext, and finally uses a small language model as a train-\ning rewriter to serve the frozen large language model. The\nSTEP-BACKPROMPTING [Zheng et al. , 2023 ]method can\nmake large language models carry out abstract reasoning, ex-\ntract high-level concepts and principles, and conduct retrievalbased on this. Lastly, the method in Multi Query Retrieval\ninvolves using large language models to generate multiple\nsearch queries, these queries can be executed in parallel, and\nthe retrieval results are input together, which is very useful\nfor single problems that rely on multiple sub-problems\nEmbedding Transformation\nIf there is a coarse-grained method like rewriting queries,\nthere should also be a finer-grained implementation spe-\ncific for embedding operations. In LlamaIndex [Liu, 2023 ],\nit is possible to connect an adapter after the query en-", "question": "What is a method that uses a large language model to generate multiple search queries in parallel?\n", "answer": "Multi Query Retrieval in [Liu, 2023] involves using large language models to generate multiple search queries, which can be executed in parallel.", "source": "RAG.pdf", "id": "1aaa160d77"}, {"context": "bility to tailor models according to specific requirements and\ndata formats, reducing the resource consumption compared\nto the pre-training phase while retaining the ability to adjust\nthe model\u2019s output style.\nInference Stage\nThe integration of RAG methods with LLM has become a\nprevalent research direction in the inference phase. Notably,\nthe research paradigm of Naive RAG relies on incorporating\nretrieval content during the inference stage.\nTo overcome the limitations of Naive RAG, researchers\nhave introduced richer context in the RAG during the in-\nference phase. The DSP [Khattab et al. , 2022 ]framework re-\nlies on a complex pipeline that involves passing natural lan-\nguage text between a frozen Language Model (LM) and a Re-\ntrieval Model (RM), providing the model with more informa-\ntive context to enhance generation quality. PKG equips LLMs\nwith a knowledge-guided module that allows access to rele-\nvant knowledge without altering the parameters of LLMs, en-", "question": "How does the DSP framework enhance the generation quality in the RAG method?\n", "answer": "The DSP framework enhances the generation quality in the RAG method by passing natural language text between a frozen Language Model (LM) and a Retrieval Model (RM), providing the model with more informative context.", "source": "RAG.pdf", "id": "0264588829"}, {"context": "query heads into several groups, with each group\u2019s query heads sharing a common key-value head,\nthereby establishing a rigorous equilibrium between effectiveness and computational cost.\nFeature Information Reduction Feature Information Reduction, as evidenced by models such as\nFunnel-Transformer[145] and Set Transformer[146], addresses the crucial need for computational\nefficiency in attention mechanisms, specifically by reducing the dimensionality or quantity of input\nfeatures while preserving the essential information embedded within the data. A key motivation\nbehind this strategy stems from the potential redundancy in maintaining full-length hidden repre-\nsentations across all layers in Transformer models. Funnel-Transformer [145] tackles this issue by\nprogressively reducing the sequence size of hidden representations in self-attention models, such as\n14", "question": "How does the Funnel-Transformer model address the issue of computational efficiency in attention mechanisms?\n", "answer": "The Funnel-Transformer model addresses the issue of computational efficiency in attention mechanisms by progressively reducing the sequence size of hidden representations in self-attention models.", "source": "multimodal.pdf", "id": "3045b9cbb1"}, {"context": "original LDP[20].\nMamba-based VL-Mamba[18] implements the 2D vision selective scanning(VSS) technique\nwithin its vision-language projector, facilitating the amalgamation of diverse learning method-\nologies. The VSS module primarily resolves the distinct processing approaches between one-\ndimensional sequential processing and two-dimensional non-causal visual information.\nHybrid Structure Honeybee [19] put forward two visual projectors, namely C-Abstractor and D-\nAbstractor, which adhere to two primary design principles: (i) providing adaptability in terms of the\nnumber of visual tokens, and (ii) efficiently maintaining the local context. C-Abstractor, or Convo-\nlutional Abstractor, focuses on proficiently modeling the local context by employing a convolutional\narchitecture. This structure consists of LResNet blocks, followed by adaptive average pooling and\nadditional LResNet blocks, which facilitate the abstraction of visual features to any squared num-", "question": "What is the architecture of the C-Abstractor in the Hybrid Structure Honeybee project?\n", "answer": "The C-Abstractor, or Conventional Abstractor, in the Hybrid Structure Honeybee project employs a convolutional architecture consisting of LResNet blocks, followed by adaptive average pooling and additional LResNet blocks for abstracting visual features.", "source": "multimodal.pdf", "id": "3238be52f9"}, {"context": "as much context as possible to ensure \u201chealthy\u201d out-\ncomes.Built upon the principles of large language mod-\nels like GPT, OpenAI\u2019s embeddings-ada-02 is more so-\nphisticated than static embedding models, capturing a\ncertain level of context. While it excels in contextual\nunderstanding, it may not exhibit the same sensitivity to\ncontext as the latest full-size language models like GPT-\n4.\nPost-Retrieval Process\nAfter retrieving valuable context from the database, merg-\ning it with the query for input into LLM poses challenges.\nPresenting all relevant documents to the LLM at once may\nexceed the context window limit. Concatenating numerous\ndocuments to form a lengthy retrieval prompt is ineffective,\nintroducing noise and hindering the LLM\u2019s focus on crucial\ninformation. Additional processing of the retrieved content is\nnecessary to address these issues.\n\u2022ReRank: Re-ranking to relocate the most relevant in-\nformation to the edges of the prompt is a straightfor-", "question": "How does OpenAI's embeddings-ada-02 handle context compared to full-size language models like GPT-4?\n", "answer": "OpenAI's embeddings-ada-02 is more sophisticated than static embedding models and can capture a certain level of context, but it may not be as sensitive to context as the latest full-size language models like GPT-4.", "source": "RAG.pdf", "id": "5b18d3e068"}, {"context": "tomized embedding methods can improve retrieval rel-\nevance. The BGE [BAAI, 2023 ]embedding model is a\nfine-tunning and high-performance embedding model,such as BGE-large-EN developed by the BAAI3. To cre-\nate training data for fine-tuning the BGE model, start\nby using LLMs like gpt-3.5-turbo to formulate ques-\ntions based on document chunks, where questions and\nanswers (document chunks) form fine-tuning pairs for\nthe fine-tuning process.\n\u2022Dynamic Embedding: Dynamic embedding adjust\nbased on the context in which words appear, differing\nfrom static embedding that use a single vector for each\nword. For instance, in transformer models like BERT,\nthe same word can have varied embeddings depend-\ning on surrounding words. Evidence indicates unex-\npected high cosine similarity results, especially with text\nlengths less than 5 tokens, in OpenAI\u2019s text-embedding-\nada-002 model4. Ideally, embedding should encompass\nas much context as possible to ensure \u201chealthy\u201d out-", "question": "How does the BGE embedding model adjust its embeddings?\n", "answer": "The BGE embedding model uses dynamic embedding, which adjusts based on the context in which words appear, in contrast to static embedding that uses a single vector for each word.", "source": "RAG.pdf", "id": "331fc75c9e"}, {"context": "tonomously judge inputs and generate accurate answers.\n7 RAG Evaluation\nIn exploring the development and optimization of RAG, ef-\nfectively evaluating its performance has emerged as a central\nissue. This chapter primarily discusses the methods of eval-\nuation, key metrics for RAG, the abilities it should possess,\nand some mainstream evaluation frameworks.\n7.1 Evaluation Methods\nThere are primarily two approaches to evaluating the ef-\nfectiveness of RAG: independent evaluation and end-to-endevaluation [Liu, 2023 ].\nIndependent Evaluation\nIndependent evaluation includes assessing the retrieval mod-\nule and the generation (read/synthesis) module.\n1.Retrieval Module\nA suite of metrics that measure the effectiveness of sys-\ntems (like search engines, recommendation systems, or\ninformation retrieval systems) in ranking items accord-\ning to queries or tasks are commonly used to evaluate\nthe performance of the RAG retrieval module. Exam-\nples include Hit Rate, MRR, NDCG, Precision, etc.", "question": "What metrics are used to evaluate the performance of the RAG retrieval module?\n", "answer": "Hit Rate, MRR, NDCG, Precision, etc.", "source": "RAG.pdf", "id": "a580bf7e9b"}, {"context": "lows the decide-retrieve-reflect-read process, introduc-\ning a module for active judgment. This adaptive and\ndiverse approach allows for the dynamic organization of\nmodules within the Modular RAG framework.\n4 Retriever\nIn the context of RAG, the \u201dR\u201d stands for retrieval, serving\nthe role in the RAG pipeline of retrieving the top-k relevant\ndocuments from a vast knowledge base. However, crafting\na high-quality retriever is a non-trivial task. In this chapter,\nwe organize our discussions around three key questions: 1)\nHow to acquire accurate semantic representations? 2) How\nto match the semantic spaces of queries and documents? 3)\nHow to align the output of the retriever with the preferences\nof the Large Language Model ?\n4.1 How to acquire accurate semantic\nrepresentations?\nIn RAG, semantic space is the multidimensional space where\nquery and Document are mapped. When we perform re-\ntrieval, it is measured within the semantic space. If the se-", "question": "How is the \"semantic space\" defined in the RAG framework?\n", "answer": "The \"semantic space\" in the RAG framework is a multidimensional space where queries and documents are mapped for retrieval measurement.", "source": "RAG.pdf", "id": "8fe8499442"}, {"context": "retrieval model, the fact remains that retrieval still struggles\nwith queries that do not map directly to passages in the\ndocument corpus or require more context not present in\nthe passage. Unfortunately, these queries are also more\nrepresentative of queries that will be asked by engineers in\nreal situations. Combining retrieval with a domain adapted\nlanguage model is one way to address this issue.\n3. Evaluations\nWe evaluate our training methodology and application per-\nformance in this section. We study our 7B, 13B, and 70B\nmodels in the training methodology evaluation, and only our\nChipNeMo-70B model using SteerLM for model alignment\nin the application performance evaluation. For compari-\nson, we also evaluate two baseline chat models: LLaMA2-\n70B-Chat and GPT-4. LLaMA2-70B-Chat is the publicly\nreleased LLaMA2-Chat model trained with RLHF and is\nconsidered to be the state-of-the-art open-source chat model,\nwhile GPT-4 is considered to be the state-of-the-art propri-", "question": "What is the name of the state-of-the-art open-source chat model used for comparison in the evaluation?\n", "answer": "LLaMA2-70B-Chat", "source": "ChipNemo.pdf", "id": "0fb655a6fb"}, {"context": "Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\nDai1,Jiawei Sun1and Haofen Wang1,3\u2217\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n3College of Design and Innovation,Tongji University\ngaoyunfan1602@gmail.com\nAbstract\nLarge language models (LLMs) demonstrate pow-\nerful capabilities, but they still face challenges in\npractical applications, such as hallucinations, slow\nknowledge updates, and lack of transparency in\nanswers. Retrieval-Augmented Generation (RAG)\nrefers to the retrieval of relevant information from\nexternal knowledge bases before answering ques-\ntions with LLMs. RAG has been demonstrated\nto significantly enhance answer accuracy, reduce\nmodel hallucination, particularly for knowledge-\nintensive tasks. By citing sources, users can verify", "question": "How does Retrieval-Augmented Generation (RAG) improve the performance of large language models?\n", "answer": "RAG improves the performance of large language models by retrieving relevant information from external knowledge bases before answering questions, which enhances answer accuracy, reduces model hallucination, and is particularly beneficial for knowledge-intensive tasks.", "source": "RAG.pdf", "id": "af911eac69"}, {"context": "which can pose a significant computational challenge within the context window of LLMs. Ely-\nsium [92] provides a trade-off between performance and visual token consumption, where T-Selector\nis introduced as a visual token compression network to enable LLMs to distinguish individual frames\nwhile reducing visual token use. VideoLLaV A [44], building upon LanguageBind [93], unifies vi-\nsual representation into the language feature space to advance foundational LLMs towards a unified\nlanguage-vision LLM without incurring a large computational burden.\n2.5 Efficient Structures\nEfficient structures primarily explore three directions: Mixture-of-Experts, Mamba and Inference\nAcceleration.\nMixture of Experts MoE enhances model capacity by modulating the total count of model pa-\nrameters while maintaining the activated parameters unchanged, hence, not significantly compro-\nmising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework", "question": "How does MoE-LLaV increase model capacity without significantly compromising inference speed?\n", "answer": "MoE-LLaV increases model capacity by modulating the total count of model parameters while keeping the activated parameters unchanged, which does not significantly affect the inference speed.", "source": "multimodal.pdf", "id": "ffe176eb03"}, {"context": "providing an effective solution to the incomplete and insuf-\nficient knowledge problem inherent in purely parameterized\nmodels.\nThe paper systematically reviews and analyzes the current\nresearch approaches and future development paths of RAG,\nsummarizing them into three main paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG. Subsequently, the paper\nprovides a consolidated summary of the three core compo-\nnents: Retrieval, Augmented, and Generation, highlighting\nthe improvement directions and current technological char-\nacteristics of RAG. In the section on augmentation methods,the current work is organized into three aspects: the augmen-\ntation stages of RAG, augmentation data sources, and aug-\nmentation process. Furthermore, the paper summarizes the\nevaluation system, applicable scenarios, and other relevant\ncontent related to RAG. Through this article, readers gain a\nmore comprehensive and systematic understanding of large\nmodels and retrieval-Augmented generation. They become", "question": "What are the three main paradigms of RAG?\n", "answer": "The three main paradigms of RAG are Naive RAG, Advanced RAG, and Modular RAG.", "source": "RAG.pdf", "id": "1bd400d39e"}, {"context": "RAG, the search module, tailored to specific sce-\nnarios, incorporates direct searches on (additional)\ncorpora in the process using LLM-generated code,\nquery languages (e.g., SQL, Cypher), or other cus-\ntom tools. The data sources for searching can include\nsearch engines, text data, tabular data, or knowledge\ngraphs [Wang et al. , 2023c ].\n\u2022Memory Module: Leveraging the memory capabili-\nties of LLM itself to guide retrieval, the principle in-\nvolves finding memories most similar to the current in-\nput. Self-mem [Cheng et al. , 2023b ]iteratively employs\na retrieval-enhanced generator to create an unbounded\nmemory pool, combining the \u201coriginal question\u201d and\n\u201cdual question.\u201d A retrieval-enhanced generative model\ncan use its own outputs to enhance itself, making the\ntext closer to the data distribution in the reasoning pro-\ncess, with the model\u2019s own outputs rather than training\ndata[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-", "question": "How does the Memory Module in RAG find relevant memories?\n", "answer": "The Memory Module in RAG finds relevant memories by leveraging the memory capabilities of the LLM itself and finding memories most similar to the current input. It iteratively employs a retrieval-enhanced generator to create an unbounded memory pool.", "source": "RAG.pdf", "id": "9067222c76"}, {"context": "domain question-answering tasks. Concerning retriever fine-\ntuning, REPlUG [Shiet al. , 2023 ]treats the language model\n(LM) as a black box and enhances it through an adjustable re-\ntrieval model. By obtaining feedback from the black-box lan-\nguage model through supervised signals, REPLUG improves\nthe initial retrieval model. UPRISE [Cheng et al. , 2023a ], on\nthe other hand, fine-tunes retrievers by creating a lightweight\nand versatile retriever through fine-tuning on diverse task\nsets. This retriever can automatically provide retrieval\nprompts for zero-shot tasks, showcasing its universality and\nimproved performance across tasks and models.\nSimultaneously, methods for fine-tuning generators in-\nclude Self-Mem [Cheng et al. , 2023b ], which fine-tunes the\ngenerator through a memory pool of examples, and\nSelf-RAG [Asai et al. , 2023b ], which satisfies active re-\ntrieval needs by generating reflection tokens. The RA-\nDIT[Linet al. , 2023 ]method fine-tunes both the generator", "question": "How does REPLUG improve the initial retrieval model?\n", "answer": "REPLUG improves the initial retrieval model by obtaining feedback from the language model through supervised signals.", "source": "RAG.pdf", "id": "662eb558d5"}, {"context": "ous downstream tasks and with different retrievers may yield\ndivergent results. However, some academic and engineering\npractices have focused on general evaluation metrics for RAG\nand the abilities required for its effective use. This section\nprimarily introduces key metrics for evaluating RAG\u2019s effec-\ntiveness and essential abilities for assessing its performance.\nKey Metrics\nRecent OpenAI report [Jarvis and Allard, 2023 ]have\nmentioned various techniques for optimizing large\nlanguage models (LLMs), including RAG and its", "question": "What is mentioned in the recent OpenAI report as one of the techniques for optimizing large language models?\n", "answer": "RAG (Retrieval-Augmented Generation) is mentioned in the recent OpenAI report as one of the techniques for optimizing large language models.", "source": "RAG.pdf", "id": "8e161396f8"}, {"context": "Efficient MLLMArchitecture (\u00a72)Vision Encoder (\u00a72.1)ViTamin [11], BRA VE[12],\nCobra[13], SPHINX-X[14]\nVision-Language Projector (\u00a72.2)QFormer [15], Perceiver Resampler[16],\nLDPv2[17], VSS[18], C/D-Abstractor[19],\nMEQ-Former[12]\nSmall Language Models (\u00a72.3)MobileVLM [20], LLaV A-Phi [21],\nImp-v1 [22], TinyLLaV A [23],\nBunny [24], Gemini Nano-2 [2],\nMobileVLM-v2 [17], MoE-LLaV A [25],\nCobra [13], Mini-Gemini [26],\nVary-toy [27], TinyGPT-V [28],\nSPHINX-Tiny [14], ALLaV A [29],\nMM1 [30], LLaV A-Gemma [31],\nMipha [32], VL-Mamba [18]\nMiniCPM-V 2.0 [33], DeepSeek-VL [34]\nVision Token Compression (\u00a72.4)Mini-Gemini [26], LLaV A-UHD [35],\nTextHawk [36], TinyChart [37], P2G [38],\nIXC2-4KHD [39], SPHINX-X[14], S2[40]\nLLaV A-PruMerge[41], MADTP[42],\nMoV A[43], Video-LLaV A[44]\nEfficient Structures (\u00a72.5)SPD [45], MoE-LLaV A [25],\nMM1 [30], Cobra [13], VL-Mamba [18],\nFastV[46], VTW[47]\nTraining (\u00a75)Pre-Training (\u00a75.1) Idefics2[48], TinyLLaV A[23], VILA[49]", "question": "What is the name of the model mentioned in the context that is a small language model from section 2.3?\n", "answer": "MobileVLM", "source": "multimodal.pdf", "id": "93d03b64f9"}, {"context": "computation is based on the hallucination rate.\nHal-Eval [ 53]The work of Hal-Eval [ 53] identifies another type of object hallucination: event\nhallucination. This type of hallucination fabricates a fictional target and constructing an entire\nnarrative around it, including its attributes, relationships, and actions. This effort further completes\nthe definition of hallucination types. In addition, this work proposes an evaluation benchmark,\nwhich encompasses both discriminative and generative evaluation methods. This is achieved by\ncollecting two evaluation subsets, each tailored to the discriminative and generative evaluation\nmethods, respectively.\nCorrelationQA [ 35]CorrelationQA is a dedicated benchmark to quantify the effect of hal-\nlucination induced by the spurious visual input. This type of hallucination usually occurs when\nproviding the MLLM with images that are highly relevant but inconsistent with the answers,", "question": "What is the purpose of CorrelationQA [35]?\n", "answer": "The purpose of CorrelationQA [35] is to measure the impact of hallucination caused by irrelevant yet consistent visual input in machine learning language models (MLLM).", "source": "hallucination.pdf", "id": "ba38d348bd"}, {"context": "from GPT-4V and is utilized to train other MLLMs through direct preference optimization.\nA more recent work, POVID [136], challenges the assumption underlying previous DPO-based\nmethods. These methods rely on the traditional preference data generation process in LLMs, where\nboth preferred and dispreferred responses may potentially be incorrect. Therefore, this work pro-\nposes the Preference Optimization in VLLM with AI-Generated Dispreferences (POVID) framework,\naiming to exclusively generate dispreferred feedback data using AI models. The dispreferred data\nis generated by: 1) utilizing GPT-4V to introduce plausible hallucinations into the answer, and\n2) provoking inherent hallucination by introducing noise into MLLMs. In the DPO optimization\nframework, the ground-truth multimodal instructions serves as the preferred answers.\nReinforcement Learning from Human Feedback (RLHF). HalDetect [ 32] first introduces the M-", "question": "How does the POVID framework generate dispreferred feedback data?\n", "answer": "The POVID framework generates dispreferred feedback data by utilizing GPT-4V to introduce plausible hallucinations into the answer and provoking inherent hallucination by introducing noise into MLLMs.", "source": "hallucination.pdf", "id": "4ed1c08405"}, {"context": "ment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a\nrelatively minor portion of the MLLM parameters, the advantages of lightweight optimization are\nless pronounced compared to the language model. Therefore, efficient MLLMs generally continue\nto employ visual encoders that are widely used in large-scale MLLMs, as detailed in Table 1.\nMultiple Vision Encoders BRA VE[12] in Figure. 4 performs an extensive ablation of various vi-\nsion encoders with distinct inductive biases for tackling MLMM tasks. The results indicate that there\nisn\u2019t a single-encoder setup that consistently excels across different tasks, and encoders with diverse\nbiases can yield surprisingly similar results. Presumably, incorporating multiple vision encoders\ncontributes to capturing a wide range of visual representations, thereby enhancing the model\u2019s com-\nprehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,", "question": "Which vision backbone does Cobra integrate in its model?\n", "answer": "Cobra integrates DINOv2 and SigLIP as its vision backbone.", "source": "multimodal.pdf", "id": "4ee780b19c"}, {"context": "including the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],\nMiniGPT-v2 [9], and MiniGPT-4[10]. These models circumvent the computational cost of train-\ning from scratch by effectively leveraging the pre-training knowledge of each modality. MLLMs\ninherit the cognitive capabilities of LLMs, showcasing numerous remarkable features such as robust\nlanguage generation and transfer learning abilities. Moreover, by establishing strong representa-\ntional connections and alignments with other modality-based models, MLLMs can process inputs\nfrom multiple modalities, significantly broadening their application scope.\nThe success of MLLMs is largely attributed to the scaling law: the performance of an AI model\nimproves as more resources, such as data, computational power, or model size, are invested into it.\nHowever, scalability comes at the cost of high resource demands, which hinders the development", "question": "What is the main factor contributing to the success of multimodal large language models (MLLMs)?\n", "answer": "The success of MLLMs is largely attributed to the scaling law, which states that the performance of an AI model improves as more resources, such as data, computational power, or model size, are invested into it.", "source": "multimodal.pdf", "id": "7a547e4fbb"}, {"context": "DeepSeek-VL [34], Cobra[13]\nLAION GPT4V[184] SFT I+T \u2192T LAION [166] Auto. 12.4kMini-Gemini [26], SPHINX-X [14],\nDeepSeek-VL [34]\nMiniGPT-4\u2019s IT [10] SFT I+T \u2192T CC3M [162], CC12M [163] Auto. 5K TinyGPT-V [28],\nSVIT [185] SFT I+T \u2192TMS-COCO[173],\nVisual Genome[177]Auto. 3.2M MoE-LLaV A [25]\nBunny-695K [24] SFT I+T \u2192TSVIT-mix-665K[185],\nWizardLM-evol-instruct-70K[186]Auto. 695K Bunny [24]\nGQA[59] SFT I+T \u2192T Visual Genome[177] Auto. 22MMM1 [30], SPHINX-X [14],\nLLaV A-Gemma [31]\nTable 3: The statistics for common MLLM IT datasets. I \u2192O:Input to Output Modalities,T:Text.\n6.3 Benchmarks\nWith the aim of delivering an all-encompassing performance evaluation, we have assembled a table\nthat demonstrates the effectiveness of 22 MLLMs across 14 well-established VL benchmarks, as\ndepicted in Table.4. Additionally, for further reference, we have incorporated a comparison of results\nfrom 13 prominent and larger MLLMs.\n21", "question": "What is the number of MLLMs (Multimodal Large Language Models) evaluated in Table 4?\n", "answer": "22 MLLMs were evaluated in Table 4.", "source": "multimodal.pdf", "id": "de63235613"}, {"context": "data[Wang et al. , 2022a ].\n\u2022Extra Generation Module: In retrieved content, re-\ndundancy and noise are common issues. Instead of di-\nrectly retrieving from a data source, the Extra Gener-\nation Module leverages LLM to generate the required\ncontext [Yuet al. , 2022 ]. Content generated by LLM is\nmore likely to contain relevant information compared to\ndirect retrieval.", "question": "How does the Extra Generation Module generate required context according to Wang et al. (2022a)?\n", "answer": "The Extra Generation Module generates required context by using a large language model (LLM) to produce the content. This approach is more likely to contain relevant information compared to direct retrieval.", "source": "RAG.pdf", "id": "3ed835a82b"}, {"context": "Information Compression\nEven though the retriever can fetch relevant information from\na vast knowledge base, we are still confronted with the chal-\nlenge of dealing with a substantial amount of information in\nretrieval documents. Some existing research attempts to solve\nthis problem by increasing the context length of large lan-\nguage models, but current large models still confront context\nlimitations. Thus, in certain situations, information conden-\nsation is necessary. In short, the importance of information\ncondensation mainly embodies in the following aspects: re-\nduction of noise, coping with context length restrictions, and\nenhancing generation effects.\nPRCA [Yang et al. , 2023b ]addressed this issue by train-\ning an information extractor. In the context extraction stage,\ngiven an input text Sinput , it can generate an output sequence\nCextracted , which represents the condensed context from the\ninput document. The objective of the training process is to", "question": "How did PRCA address the challenge of dealing with substantial amount of information in retrieval documents?\n", "answer": "PRCA addressed this issue by training an information extractor to generate a condensed context from the input document.", "source": "RAG.pdf", "id": "c5e55b3041"}, {"context": "ChipNeMo: Domain-Adapted LLMs for Chip Design\ncost of pretraining a foundational model from scratch.\nModel Size Pretraining DAPT SFT\n7B 184,320 2,620 90\n13B 368,640 4,940 160\n70B 1,720,320 20,500 840\nTable 1: Training cost of LLaMA2 models in A100 GPU hours.\nPretraining cost from (Touvron et al., 2023).\n3.5. RAG and Engineering Assistant Chatbot\nWe created a benchmark to evaluate the performance of\ndesign chat assistance, which uses the RAG method. This\nbenchmark includes 88 questions in three categories: archi-\ntecture/design/verification specifications (Specs), testbench\nregression documentation (Testbench), and build infrastruc-\nture documentation (Build). For each question, we specify\nthe golden answer as well as the paragraphs in the design\ndocument that contains the relevant knowledge for the an-\nswer. These questions are created by designers manually\nbased on a set of design documents as the data store for\nretrieval. It includes about 1.8K documents, which were", "question": "What is the number of questions in the benchmark used to evaluate the performance of design chat assistance?\n", "answer": "The benchmark used to evaluate the performance of design chat assistance includes 88 questions.", "source": "ChipNemo.pdf", "id": "7313e64a59"}, {"context": "Figure 4: Taxonomy of RAG\u2019s Core Components\nextension of RETRO, increased the model\u2019s parameter scale.\nStudies have found consistent improvements in text genera-\ntion quality, factual accuracy, low toxicity, and downstream\ntask accuracy, particularly in knowledge-intensive tasks such\nas open-domain question answering. These research findings\nhighlight the promising direction of pretraining autoregres-\nsive language models in conjunction with retrieval for future\nfoundational models.\nIn summary, the advantages and limitations of augmented\npre-training are evident. On the positive side, this approach\noffers a more powerful foundational model, outperforming\nstandard GPT models in perplexity, text generation quality,\nand downstream task performance. Moreover, it achieves\nhigher efficiency by utilizing fewer parameters compared to\npurely pre-trained models. It particularly excels in handling\nknowledge-intensive tasks, allowing the creation of domain-", "question": "How does the approach of augmented pre-training perform in knowledge-intensive tasks compared to standard GPT models?\n", "answer": "The approach of augmented pre-training outperforms standard GPT models in handling knowledge-intensive tasks, particularly in open-domain question answering.", "source": "RAG.pdf", "id": "7411eec79c"}, {"context": "simultaneously saving all text annotations along with their respective bounding boxes. Ultimately,\nthese elements are converted into a unified question-answering format.\nWhile multi-task datasets provide an abundant source of data, they may not always be suitable\nfor complex real-world situations, such as engaging in multi-turn conversations. To address this\nchallenge, some research has explored the use of self-instruction by leveraging LLMs to gener-\nate text-based or multimodal instruction-following data from a limited number of hand-annotated\nsamples. SPHINX-X[14] assembles a rich multi-domain dataset with fine-grained correspondence\nbetween images and texts.It gathers images from diverse sources and then employs annotations to\napply various markers onto the original images. By prompting GPT-4V with these marked images\nand tailored domain-specific guidelines, the system generates captions that offer an image overview,", "question": "How does the SPHINX-X project create image captions with fine-grained correspondence to texts?\n", "answer": "The SPHINX-X project creates image captions with fine-grained correspondence to texts by using GPT-4V to generate captions from marked images and domain-specific guidelines.", "source": "multimodal.pdf", "id": "7d5705c52b"}, {"context": "augmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness\nThis capability measures the model\u2019s efficiency in han-\ndling noisy documents, which are those related to the\nquestion but do not contain useful information.\n2.Negative Rejection\nWhen documents retrieved by the model lack the knowl-\nedge required to answer a question, the model should\ncorrectly refuse to respond. In the test setting for neg-\native rejection, external documents contain only noise.\nIdeally, the LLM should issue a \u201dlack of information\u201d or\nsimilar refusal signal.\n3.Information Integration\nThis ability assesses whether the model can integrate\ninformation from multiple documents to answer more\ncomplex questions.4.Counterfactual Robustness\nThis test aims to evaluate whether the model can iden-\ntify and deal with known erroneous information in doc-\numents when receiving instructions about potential risks\nin retrieved information. Counterfactual robustness tests", "question": "What is one ability of the RGB model in augmented generation that deals with erroneous information?\n", "answer": "Counterfactual Robustness is the ability of the RGB model in augmented generation that deals with erroneous information by identifying and handling it when receiving instructions about potential risks in retrieved information.", "source": "RAG.pdf", "id": "070aa6c4f4"}, {"context": "specific cross-attention mechanisms. The training loss of the\nretriever is as follows:\n\u03b6=1\n|D|X\nx\u2208DKL(PR(d|x)||QLM(d|x, y)) (2)\nwhere Dis a set of input contexts, PRis the retrieval like-\nlihood, QLMis the LM likelihood of each document.\nUPRISE [Cheng et al. , 2023a ]also employs frozen large\nlanguage models to fine-tune the Prompt Retriever. But\nboth the language model and the retriever take Prompt-Input\nPairs as inputs, then uses the scores given by the large lan-\nguage model to supervise the training of the retriever, equiva-\nlent to using the large language model to label the dataset.\nAtlas [Izacard et al. , 2022 ]proposes four methods of fine-\ntuning supervised embedding models, among them, Attention\nDistillation distills using the cross-attention scores that the\nlanguage model generates during the output. EMDR2 em-\nploys the Expectation-Maximization algorithm to train with\nthe retrieved documents as latent variables. Perplexity Dis-", "question": "How does Atlas fine-tune supervised embedding models?\n", "answer": "Atlas proposes four methods of fine-tuning supervised embedding models, one of which is Attention Distillation that uses the cross-attention scores generated by the language model during the output.", "source": "RAG.pdf", "id": "ad8a664ee7"}, {"context": "Cobra [13] Mamba-2.8B 75.9 58.5 - 46.0 52.0 - - - - - - 88.0 - -\nMini-Gemini [26] Gemma-2B - - - 56.2 - 31.7/29.1 29.4 1341.0 312.0 59.8 - - - 31.1\nVary-toy [27] Qwen-1.8B - - - - - - - - - - - - - 29.0\nTinyGPT-V [28] Phi-2 (2.7B) - 33.6 - - 24.8 - - - - - - - - -\nSPHINX-Tiny [14] TinyLlama-1.1B - - - 57.8 - - 26.4 1261.2 242.1 56.6 17.1/- 82.2 52.3 23.8\nALLaV A-Longer [29] Phi-2 (2.7B) - 50.0 - 50.3 - 33.2/- - 1564.6\u202064.6 - - 71.7 35.5\nMM1-3B [30] MM1-3B 82.5 - 76.1 72.9 - 38.6/35.7 32.6 1469.4 303.1 70.8 63.9/69.4 87.6 76.8 42.2\nLLaV A-Gemma [31] Gemma-2b-it 71.4 58.7 - - - - - 1133.0 307.0 - - 85.3 - 19.1\nMipha-3B [32] Phi-2 (2.7B) 81.3\u221763.9\u221770.9 56.6 47.5 - - 1488.9 295.0 69.7 - 86.7 - 32.1\nVL-Mamba [18] Mamba-2.8B 76.6 56.2 65.4 48.9 - - - 1369.6 - 57.0 - 84.4 - 32.6\nMiniCPM-V 2.0[33] MiniCPM-2.4B - - - 74.1 - 38.2/- 38.7 1808.6\u202069.6 - - - -\nDeepSeek-VL [34] DeepSeek-LLM-1B - - - - - 32.2/- 31.1 - - 64.6 -/66.7 87.6 - 36.8", "question": "What is the perplexity score of the Mini-Gemini [26] Gemma-2B model?\n", "answer": "The perplexity score of the Mini-Gemini [26] Gemma-2B model is 56.2.", "source": "multimodal.pdf", "id": "0ad4077d27"}, {"context": "vant knowledge without altering the parameters of LLMs, en-\nabling the model to perform more sophisticated tasks. Addi-\ntionally, CREA-ICL [Liet al. , 2023b ]leverages synchronous\nretrieval of cross-lingual knowledge to assist in acquiring ad-\nditional information, while RECITE forms context by sam-\npling one or more paragraphs from LLMs.\nDuring the inference phase, optimizing the process of RAG\ncan benefit adaptation to more challenging tasks. For ex-ample, ITRG [Feng et al. , 2023a ]enhances adaptability for\ntasks requiring multiple-step reasoning by iteratively retriev-\ning and searching for the correct reasoning path. ITER-\nRETGEN [Shao et al. , 2023 ]employs an iterative approach\nto coalesce retrieval and generation, achieving an alternating\nprocess of \u201dretrieval-enhanced generation\u201d and \u201dgeneration-\nenhanced retrieval.\u201d\nOn the other hand, IRCOT [Trivedi et al. , 2022 ]merges the\nconcepts of RAG and CoT [Weiet al. , 2022 ], employing al-", "question": "How does ITRG enhance adaptability for tasks requiring multiple-step reasoning?\n", "answer": "ITRG enhances adaptability for tasks requiring multiple-step reasoning by iteratively retrieving and searching for the correct reasoning path.", "source": "RAG.pdf", "id": "1f6c13012c"}, {"context": "10 Bai, et al.\nTable 1. Summary of most relevant benchmarks and metrics of object hallucination in MLLMs. The order is\nbased on chronological order on arxiv. In the metric column, Acc/P/R/F1 denotes Accuracy/Precision/Recall/F1-\nScore.\nBenchmark VenueUnderlying\nData SourceSizeTask\nTypeMetricHallucination Type\nCategory Attribute Relation Others\nCHAIR [90] EMNLP\u201918 MSCOCO [70] 5,000 Gen CHAIR \u2713 \u2717 \u2717 \u2717\nPOPE [69] EMNLP\u201923 MSCOCO [70] 3,000 Dis Acc/P/R/F1 \u2713 \u2717 \u2717 \u2717\nMME [113] arXiv\u201923 Jun MSCOCO [70] 1457 Dis Acc/Score \u2713 \u2713 \u2717 \u2713\nCIEM [42] NeurIPS-W\u201923 MSCOCO [70] 78120 Dis Acc \u2713 \u2717 \u2717 \u2717\nM-HalDetect [32] arXiv\u201923 Aug. MSCOCO [70] 4,000 Dis Reward Model Score \u2713 \u2717 \u2717 \u2717\nMMHal-Bench [96] arXiv\u201923 Sep. Open-Images [61] 96 Gen LLM Assessment \u2713 \u2717 \u2717 \u2713\nGAVIE [73] ICLR\u201924 Visual-Genome [59] 1,000 Gen LLM Assessment Not Explicitly Stated\nNOPE [77] arXiv\u201923 Oct. Open-Images [61] 36,000 Dis Acc/METEOR [3] \u2713 \u2717 \u2717 \u2717\nHaELM [104] arXiv\u201923 Oct. MSCOCO [70] 5,000 Gen LLM Assessment Not Explicitly Stated", "question": "Which benchmark uses the Visual Genome dataset and assesses MLLMs using a method not explicitly stated?\n", "answer": "GAVIE [73] ICLR\u201924 Visual-Genome [59] 1,000 Gen LLM Assessment Not Explicitly Stated", "source": "hallucination.pdf", "id": "8705831e19"}, {"context": "\u2022 At present, efficient MLLMs face challenges in processing extended-context multimodal\ninformation, and they are typically limited to accepting single images. This constrains the\nadvancement of more sophisticated models capable of handling an increased number of\nmultimodal tokens. Such models would be beneficial for applications like comprehending\nlengthy videos and analyzing extensive documents that incorporate a mix of images and\ntext, creating more versatile and powerful systems.\n\u2022 The predominant efficient MLLMs mainly support dual input modalities - images and texts,\nand a singular output modality - text. However, the tangible world encompasses a more\nextensive array of modalities. By expanding the scope of efficient MLLMs to accommodate\n23", "question": "What is the typical output modality of efficient multimodal large language models?\n", "answer": "The typical output modality of efficient multimodal large language models is text.", "source": "multimodal.pdf", "id": "a1bd2d5193"}, {"context": "low-rank, fixed and learnable pattern strategies, and hardware-assisted attention.\nSharing-based Attention Sharing-based Attention aims to expedite attention computation dur-\ning inference by by sharing computation resources across multiple Key-Value heads. For exam-\nple, Llama-2 [91] incorporates a technique called grouped-query attention (GQA) [143] to opti-\nmize memory bandwidth during the autoregressive decoding. GQA is a Sharing-based Attention\ntechnique that seeks to achieve a balance between performance and efficiency, positioned between\nmulti-head attention and multi-query attention [144] mechanisms. In multi-head attention, each head\nutilizes a distinct set of linear transformation parameters for queries, keys, and values. Conversely,\nmulti-query attention shares a single set of key-value heads across all queries. GQA partitions all\nquery heads into several groups, with each group\u2019s query heads sharing a common key-value head,", "question": "How does Llama-2 optimize memory bandwidth during autoregressive decoding?\n", "answer": "Llama-2 optimizes memory bandwidth during autoregressive decoding by using a technique called grouped-query attention (GQA), which is a Sharing-based Attention mechanism that partitions all query heads into several groups, with each group\u2019s query heads sharing a common key-value head.", "source": "multimodal.pdf", "id": "7d67b0debb"}, {"context": "MM1-3B-MoE-Chat [30] CLIP DFN-ViT-H [83] 378 - - 3B\u2217C-Abstractor [19]\nLLaV A-Gemma [31] DinoV2 [76] - - Gemma-2b-it[78] 2B -\nMipha-3B [32] SigLIP [75] 384 - Phi-2[74] 2.7B -\nVL-Mamba [18] SigLIP-SO [75] 384 - Mamba-2.8B-Slimpj[77] 2.8B VSS-L2[18]\nMiniCPM-V 2.0[33] SigLIP [75] - 0.4B MiniCPM[70] 2.4B Perceiver Resampler [16]\nDeepSeek-VL [34] SigLIP-L [75] 384 0.4B DeepSeek-LLM[84] 1.3B MLP\nKarmaVLM[71] SigLIP-SO [75] 384 0.4B Qwen1.5[79] 0.5B -\nmoondream2[72] SigLIP[75] - - Phi-1.5[85] 1.3B -\nBunny-v1.1-4B[24] SigLIP[75] 1152\u2020- Phi-3-Mini-4K[86] 3.8B -\nTable 1: The summary of 17 mainstream efficient MMLMs.\u2217indicates activated parameters.\u2020:High\nresolution support is achieved with S2-Wrapper[40].\nIn line with mainstream MLLM practices, efficient MLLMs select pre-trained models that are se-\nmantically aligned with the text, represented by CLIP [73]. This approach facilitates better align-\nment between the feature spaces of visual and text inputs. Since the vision encoder constitutes a", "question": "What is the resolution supported by Bunny-v1.1-4B?\n", "answer": "The resolution supported by Bunny-v1.1-4B is high resolution, achieved with S2-Wrapper.", "source": "multimodal.pdf", "id": "e8fc8ad809"}, {"context": "analysis. Our future work will focus on further improving\nChipNeMo models and methods for production use.\n8", "question": "What will be the focus of future work on ChipNeMo?\n", "answer": "The focus of future work on ChipNeMo will be on further improving models and methods for production use.", "source": "ChipNemo.pdf", "id": "975cd8b0f3"}, {"context": "directly inputs image patches and employs a linear projection to transform the raw pixels of each\npatch into embeddings.\nThe abstracted pipeline is depicted in Fig. 2. MLLMs take input from both visual and textual\nmodalities, learning from multimodal instructions and responses, which leads to remarkable per-\nformance across various multimodal tasks. Regarding the training of MLLMs, we provide a concise\noverview of the training process for interface-based MLLMs. Given that end-to-end models are\nclosed-source, the training details are unknown. Typically, the training of interface-based MLLMs\nconsists of two stages: 1) pre-training, 2) instruction tuning.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How are multimodal large language models (MLLMs) typically trained?\n", "answer": "MLLMs are typically trained in two stages: pre-training and instruction tuning.", "source": "hallucination.pdf", "id": "a8f0bda3b0"}, {"context": "where Xinstruct contains an image Xvand a randomly sampled question Xqfrom a set of instruc-\ntions asking the assistant to briefly describe the image, and Xais the original image description.\nGiven such a conversation, the model is trained to autoregressively predict the image description.\nConsequently, we can compute the probability of predicting Xaconditioned by Xvand optimize it\n17", "question": "How is the model in Xinstruct trained to generate image descriptions?\n", "answer": "The model in Xinstruct is trained to generate image descriptions autoregressively and optimize the probability of predicting the description conditioned by the image.", "source": "multimodal.pdf", "id": "deb43b52c4"}, {"context": "large adapter exhibiting a slight improvement.\n3.4. Training Cost\nAll models have undergone training using 128 A100 GPUs.\nWe estimate the costs associated with domain adaptive pre-\ntraining for ChipNeMo as illustrated in Table 1. It is worth\nnoting that DAPT accounts for less than 1.5% of the overall\n5", "question": "What percentage of the overall cost does DAPT account for in ChipNeMo training?\n", "answer": "DAPT accounts for less than 1.5% of the overall cost in ChipNeMo training.", "source": "ChipNemo.pdf", "id": "d9ae12f819"}, {"context": "6 Bai, et al.\nPlease describe this image in detail.The image shows a group of people standing in a park. There is a large tree in the background with pink blossoms. The people are all wearing different clothing, including a man in a brown jacket and a woman in a white shirt and jeans. There are also some people in the background who are not in the foreground of the image. The people are standing in a grassy area and there are some benches and a fence in the background. The sky is clear and there are some clouds in the distance.\nWhat is the girl in the picture doing?The girl in the picture is standing with her arms outstretched, as if she is trying to catch something. It is not clear what she is trying to catch, but it could be a kite or a frisbee. The other people in the picture are standing around her, watching what she is doing. It appears that they are all having a good time together in the park.\nCategory HallucinationAttribute HallucinationRelation Hallucination", "question": "What is the girl in the image doing with her arms?\n", "answer": "The girl in the image is standing with her arms outstretched, as if she is trying to catch something.", "source": "hallucination.pdf", "id": "53acad2ed1"}, {"context": "play distinct roles. Parametric knowledge is acquired through\ntraining LLMs and stored in the neural network weights, rep-\nresenting the model\u2019s understanding and generalization of\nthe training data, forming the foundation for generated re-\nsponses. Non-parametric knowledge, on the other hand, re-\nsides in external knowledge sources such as vector databases,\nnot encoded directly into the model but treated as updatable\nsupplementary information. Non-parametric knowledge em-\npowers LLMs to access and leverage the latest or domain-\nspecific information, enhancing the accuracy and relevance\nof responses.\nPurely parameterized language models (LLMs) store their\nworld knowledge, which is acquired from vast corpora, in\nthe parameters of the model. Nevertheless, such models have\ntheir limitations. Firstly, it is difficult to retain all the knowl-\nedge from the training corpus, especially for less common\nand more specific knowledge. Secondly, since the model", "question": "How does a purely parameterized language model acquire and store world knowledge?\n", "answer": "A purely parameterized language model acquires world knowledge from vast corpora and stores it in the parameters of the model.", "source": "RAG.pdf", "id": "fc82ce8e28"}, {"context": "6.4 Establishing Standardized Benchmarks\nThe lack of standardized benchmarks and evaluation metrics poses significant challenges in as-\nsessing the degree of hallucination in MLLMs. In Table 1, it can be observed that there is a variety\nof evaluation benchmarks, but a lack of unified standards. Among them, one of the most popular\nbenchmarks might be POPE [ 69], which employs a \u2019Yes-or-No\u2019 evaluation protocol. However, this\nbinary-QA manner does not align with how humans use MLLMs. Accordingly, some benchmarks\nspecifically evaluate the hallucination of MLLMs in the (free-form) generative context. Yet, they\noften rely on external models, such as vision expert models or other LLMs, which limits their wide-\nspread application. Moving forward, future research can investigate standardized benchmarks that\nare theoretically sound and easy to use. Otherwise, research on methods to mitigate hallucinations\nmay be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature", "question": "What is one of the most popular evaluation benchmarks for assessing hallucination in machine-learned language models?\n", "answer": "POPE is one of the most popular evaluation benchmarks for assessing hallucination in machine-learned language models.", "source": "hallucination.pdf", "id": "312439a972"}, {"context": "may be built on an incorrect foundation.\n6.5 Reframing Hallucination as a Feature\nRecently, discussions on social media [ 56] have suggested that hallucination can be regarded as\nan inherent feature of LLMs and MLLMs. The models are like dream machines. Human users\ndirect their dreams with prompts. The prompts start the dream, and based on the model\u2019s hazy\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How can hallucination be viewed in the context of Language Learning Models (LLMs) and Multimodal Language Learning Models (MLLMs)?\n", "answer": "Hallucination can be considered as an inherent feature of LLMs and MLLMs, akin to a dream machine that generates dreams based on user prompts.", "source": "hallucination.pdf", "id": "f565d0de3b"}, {"context": "NOPE [ 77]This paper proposes to establish a distinction between object hallucination and\nincorrectness. a) Object hallucination refers to a phenomenon in VQA where a VL model\u2019s response\nincludes a non-existent object, despite the ground truth answer being a negative indefinite pronoun\n(e.g., \"none\", \"no one\", etc). This is denoted as NegP . b) Incorrectness occurs when a VL model\nfails to accurately respond to a question with a ground truth answer that is anything other than\nNegP , denoted as Others . This paper argues that the existing VQA datasets have a significantly\nimbalanced distribution, containing too little NegP data. Therefore, NOPE (Negative Object Presence\nEvaluation) is proposed in this paper to complement the absent NegP data. During evaluation,\ntraditional metrics, including Accuracy and METEOR, are employed.\nHaELM [ 104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4", "question": "What is the phenomenon in VQA referred to as object hallucination?\n", "answer": "Object hallucination in VQA is a phenomenon where a VL model's response includes a non-existent object, despite the ground truth answer being a negative indefinite pronoun such as \"none\", \"no one\", etc.", "source": "hallucination.pdf", "id": "47e4b169a1"}, {"context": "During this stage, language models engage in autoregressive prediction, wherein they predict the\nsubsequent token in a sequence. By undergoing self-supervised training on vast textual datasets,\nthese models develop an understanding of language syntax, gain access to world knowledge, and\nenhance their reasoning capabilities. This pre-training process establishes a solid groundwork for\nthe models to undertake subsequent fine-tuning tasks effectively.\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge\nand skills, it\u2019s important to acknowledge that its primary focus is on optimizing for completion.\nConsequently, pre-trained LLMs essentially function as completion machines, which may create\na misalignment between the objective of predicting the next word within LLMs and the user\u2019s\nobjective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-", "question": "How do pre-trained language models primarily function?\n", "answer": "Pre-trained language models primarily function as completion machines.", "source": "hallucination.pdf", "id": "f524021191"}, {"context": "trieval, it is measured within the semantic space. If the se-\nmantic expression is not accurate, then its effect on RAG is\nfatal, this section will introduce two methods to help us build\na accurate semantic space.\nChunk optimization\nWhen processing external documents, the first step is chunk-\ning to obtain fine-grained features. Then the chunks are Em-\nbedded. However, Embedding too large or too small text\nchunks may not achieve good results. Therefore, finding the\noptimal chunk size for the documents in the corpus is crucial\nto ensure the accuracy and relevance of the search results.\nWhen choosing a chunking strategy, important considera-\ntions include: the characteristics of the content being indexed,\nthe embedding model used and its optimal block size, the ex-\npected length and complexity of user queries, and how the\nretrieval results are used in a specific application. For exam-\nple, different chunking models should be selected for longer", "question": "What is the purpose of chunk optimization in the process of obtaining fine-grained features from external documents?\n", "answer": "The purpose of chunk optimization is to find the optimal chunk size for the documents in the corpus to ensure the accuracy and relevance of the search results.", "source": "RAG.pdf", "id": "986968af9f"}, {"context": "corpora are often brief and contain noise, which can be refined and filtered using automated meth-\nods, such as employing the CLIP [13] model to eliminate image-text pairs with low similarity scores.\nA summary of frequently used pre-training datasets can be found in Figure2.\nDataset Name X Modality #.X #.T #.X-T Representative Publications\nCC3M [162] Image 3.3M 3.3M 3.3M TinyGPT-V[28],MM1[30]\nCC12M [163] Image 12.4M 12.4M 12.4M MM1[30]\nSBU [164] Image 1M 1M 1M TinyGPT-V[28]\nLAION-5B [165] Image 5.9B 5.9B 5.9B TinyGPT-V[28]\nLAION-COCO[166] Image 600M 600M 600M Vary-toy [27]\nCOYO [167] Image 747M 747M 747M MM1[30]\nCOCO Caption[168] Image 164K 1M 1M Vary-toy [27]\nCC595k [7] Image 595K 595K 595KMobileVLM [20],LLaV A-Phi [21],\nLLaV A-Gemma [31],Mini-Gemini [26]\nRefCOCO[169] Image 20K 142K 142K Vary-toy [27]\nDocVQA[170] Image 12K 50K 50K Vary-toy [27]\nLLava-1.5-PT[54] Image 558K 558K 558KImp-v1 [22],MoE-LLaV A [25],\nVary-toy [27],Mipha [32],\nVL-Mamba [18],Tiny-LLaV A [23]", "question": "What is the number of image-text pairs in the LAION-5B dataset?\n", "answer": "The number of image-text pairs in the LAION-5B dataset is 5.9B.", "source": "multimodal.pdf", "id": "a75a61f59f"}, {"context": "different scenarios, including using query engines pro-\nvided by frameworks like LlamaIndex, employing tree\nqueries, utilizing vector queries, or employing the most\nbasic sequential querying of chunks.\u2022HyDE: This approach is grounded on the assumption\nthat the generated answers may be closer in the embed-\nding space than a direct query. Utilizing LLM, HyDE\ngenerates a hypothetical document (answer) in response\nto a query, embeds the document, and employs this em-\nbedding to retrieve real documents similar to the hypo-\nthetical one. In contrast to seeking embedding similarity\nbased on the query, this method emphasizes the embed-\nding similarity from answer to answer. However, it may\nnot consistently yield favorable results, particularly in\ninstances where the language model is unfamiliar with\nthe discussed topic, potentially leading to an increased\ngeneration of error-prone instances.\nModular RAG\nThe modular RAG structure breaks away from the traditional", "question": "How does HyDE retrieve real documents similar to the hypothetical one?\n", "answer": "HyDE retrieves real documents similar to the hypothetical one by generating a hypothetical document (answer) in response to a query, embedding the document, and employing this embedding to seek similar documents in the embedding space.", "source": "RAG.pdf", "id": "d96393bb4b"}, {"context": "cient MLLMs, research state of efficient structures and strategies, and the appli-\ncations. Finally, we discuss the limitations of current efficient MLLM research\nand promising future directions. Please refer to our GitHub repository for more\ndetails: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.\n1 Introduction\nLarge-scale pretraining, a leading approach in Artificial Intelligence(AI), has seen general-purpose\nmodels like large language and multimodal models outperform specialized deep learning models\nacross many tasks. The remarkable abilities of Large Language Models (LLM) have inspired efforts\nto merge them with other modality-based models to enhance multimodal competencies. This con-\ncept is further supported by the remarkable success of proprietary models like OpenAI\u2019s GPT-4V [1]\nand Google\u2019s Gemini[2]. As a result, Multimodal Large Language Models (MLLMs) have emerged,\nincluding the mPLUG-Owl series[3, 4], InternVL [5], EMU [6], LLaV A [7], InstructBLIP [8],", "question": "What is the name of the proprietary model that combines language and vision models, similar to Multimodal Large Language Models?\n", "answer": "OpenAI\u2019s GPT-4V and Google\u2019s Gemini are examples of proprietary models that combine language and vision models, similar to Multimodal Large Language Models (MLLMs).", "source": "multimodal.pdf", "id": "e021f7788d"}, {"context": "Reinforcement Learning from Human Feedback (RLHF). HalDetect [ 32] first introduces the M-\nHalDetect dataset for detecting hallucinations, which covers a wide range of hallucinatory content,\nincluding non-existent objects, unfaithful descriptions, and inaccurate relationships. It then proposes\na multimodal reward model to detect hallucinations generated by MLLMs. The reward model is\ntrained on the M-HalDetect dataset to identify hallucinations in the generated text. To utilize\nthe trained reward model to reduce hallucinations, the authors introduced Fine-grained Direct\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How was the M-HalDetect dataset first introduced and what does it cover?\n", "answer": "The M-HalDetect dataset was first introduced in the paper \"Reinforcement Learning from Human Feedback (RLHF). HalDetect [32]\" and it covers a wide range of hallucinatory content, including non-existent objects, unfaithful descriptions, and inaccurate relationships.", "source": "hallucination.pdf", "id": "7a0f374e2c"}, {"context": "and after revision to decide which one is better. Stages 2-4 are repeated iteratively. To provide\nbetter feedback and decision-making, the model is fine-tuned on a curated dataset. The dataset is\norganized using ChatGPT.\nLogicCheckGPT [ 108] is a more recent self-revising-based hallucination mitigation method.\nUnlike Volcano [ 63], which revises the generated response with the help of general feedback,\nLogicCheckGPT delves into the logical consistency of MLLMs\u2019 responses. Specifically, the approach\ncan be formulated into two stages: the first stage involves inquiring attributes of objects, followed by\ninquiring objects based on attributes. Whether their responses can form a logical closed loop serves\nan indicator of object hallucination. If the ratio of closed loops to the total number of questions\nexceeds a certain threshold, rectify the hallucinated objects by prompting the MLLM.\n6 CHALLENGES AND FUTURE DIRECTIONS", "question": "What is the hallucination mitigation method that uses a logical consistency check?\n", "answer": "LogicCheckGPT", "source": "hallucination.pdf", "id": "58d8a5c1a5"}, {"context": "3.3. Training Ablation Studies\nFor our ablation studies, we conducted multiple rounds of\ndomain adaptive pre-training. We provide brief summaries\nand refer to the Appendix A.6 for details.\nThe differences between training with the augmented tok-\nenizer and the original tokenizer appeared to be negligible.\nWe thus primarily attribute the accuracy degradation on\nopen-domain academic benchmarks to domain data. More-\nover, the removal of the public dataset only slightly re-\ngressed on most tasks including academic benchmarks.\nIn our exploration, we experimented with employing a larger\nlearning rate, as in CodeLLaMA (Rozi `ere et al., 2023). We\nobserved large spikes in training loss at the initial training\nsteps. Although this approach eventually led to improved\ntraining and validation loss, we noted substantial degrada-\ntions across all domain-specific and academic benchmarks,\nexcept on coding. We hypothesize that a smaller learning", "question": "How did using a larger learning rate affect the model's performance in the ablation studies?\n", "answer": "Using a larger learning rate led to substantial degradations across all domain-specific and academic benchmarks, except on coding.", "source": "ChipNemo.pdf", "id": "49056b4ebb"}, {"context": "except on coding. We hypothesize that a smaller learning\nrate played a dual role, facilitating the distillation of domain\nknowledge through DAPT while maintaining a balance that\ndid not veer too far from the base model, thus preserving\ngeneral natural language capabilities.\nWe also explored the application of Parameter Efficient\nFine-Tuning (PEFT) in the context of Domain-Adaptive\nPre-training (DAPT). In this pursuit, we conducted two ex-\nperiments involving the incorporation of LoRA adapters (Hu\net al., 2021), introducing additional parameters of 26.4 mil-\nlion (small) and 211.2 million (large) respectively. In both\ninstances, our findings revealed a significant accuracy gap\non in-domain tasks when compared to the full-parameter\nDAPT approach. Furthermore, when contrasting the out-\ncomes between small and large PEFT models, we observed\na marginal enhancement on in-domain task accuracy, with\nlarge adapter exhibiting a slight improvement.\n3.4. Training Cost", "question": "How many additional parameters were introduced with the small LoRA adapters in the PEFT experiments?\n", "answer": "26.4 million", "source": "ChipNemo.pdf", "id": "453536e071"}, {"context": "supervises MLLMs with mask prediction loss using a state-of-the-art expert vision model, SAM [ 57],\nguiding MLLMs to focus on highly-related image content. With the additional supervision from\nthe mask prediction loss, MLLMs are encouraged to extract features that can better represent these\ncrucial instances, thus generating more accurate responses and mitigating vision hallucination. The\nintuitive idea of supervising MLLMs with grounding shows promising performance in mitigating\nhallucination.\nAnother line of work analyzes the training loss from the perspective of embedding space distri-\nbution. As introduced earlier, popular MLLMs typically project the encoded vision features into the\ninput space of a specific LLM. A recent work, HACL [ 52], argues that an ideal projection should\nblend the distribution of visual and textual embeddings. However, despite visual projection, a sig-\nnificant modality gap exists between textual and visual tokens, suggesting that the current learned", "question": "How does the HACL model address the modality gap in popular MLLMs?\n", "answer": "A recent work, HACL, argues that an ideal projection should blend the distribution of visual and textual embeddings. It suggests that despite visual projection, a significant modality gap exists between textual and visual tokens in popular MLLMs.", "source": "hallucination.pdf", "id": "c505f06d1a"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 9\noverride the visual content. For example, given an image showing a red banana, which is\ncounter-intuitive in the real world, an MLLM may still respond with \"yellow banana\", as\n\"banana is yellow\" is a deep-rooted knowledge in the LLM. Such language/knowledge prior\nmakes the model overlook the visual content and response with hallucination.\n\u2022Weak alignment interface. The alignment interface plays an essential role in MLLMs, as\nit serves as the bridge between the two modalities. A weak alignment interface can easily\ncause hallucinations. One potential cause of a weak alignment interface is data, as discussed\nin earlier sections. Apart from that, the interface architecture itself and training loss design\nalso matter [ 52,77,123]. Recent work [ 52] argues that the LLaVA-like linear projection\ninterface preserves most of the information, but lacks supervision on the projected feature.", "question": "What is one potential cause of a weak alignment interface in multimodal large language models?\n", "answer": "Data is one potential cause of a weak alignment interface in multimodal large language models.", "source": "hallucination.pdf", "id": "a1c28916ce"}, {"context": "ple, different chunking models should be selected for longer\nor shorter content. Additionally, different embedding mod-\nels perform differently at different block sizes; for example,\nsentence-transformer is more suitable for single sentences,while text-embedding-ada-002 is better for blocks containing\n256 or 512 tokens. Furthermore, the length and complexity\nof the user\u2019s input question text, as well as the specific needs\nof your application such as semantic search or Q&A, will all\naffect the choice of chunking strategy. This might directly\ncorrelate with the token limits of your chosen LLM, and may\nrequire you to adjust the block size. In fact, accurate query\nresults are achieved by adaptively applying several chunking\nstrategies; there is no best, only most suitable.\nCurrent research in RAG employs diverse block optimiza-\ntion methods to improve retrieval efficiency and accuracy.\nTechniques such as sliding window technology implement", "question": "What is a more suitable embedding model for blocks containing 256 or 512 tokens?\n", "answer": "text-embedding-ada-002", "source": "RAG.pdf", "id": "20000f1ef4"}, {"context": "effect of noisy data.\n\u2022Lack of diversity. Recent works [ 73,117] reveal that the diversity of data also plays a crucial\nrole. For the data used in the two training stages, instruction tuning data are more likely to\nhave this issue since it is usually in a relatively small amount. One prominent property is that\nmost instruction following data samples are composed of conversations regarding the image\ncontent. We regard this type of data as positive instruction , as it always faithfully reflects the\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What is a common issue with instruction following data in the two training stages?\n", "answer": "Instruction following data is often limited in diversity and is likely to consist of conversations about image content.", "source": "hallucination.pdf", "id": "bc02f1300a"}, {"context": "to-end learned explainability-aware mask to measure each unit\u2019s contribution to predicting target\nclasses and adaptively searches layer-wise thresholds to preserve the most informative unit while\n11", "question": "How does the to-end learned explainability-aware mask measure a unit's contribution in a model?\n", "answer": "The to-end learned explainability-aware mask measures each unit's contribution to predicting target classes.", "source": "multimodal.pdf", "id": "1bc97054f4"}, {"context": "its selection is closely related to the lightweight nature of MLLM. In comparison to conventional\nMLLMs with parameter sizes ranging from 7 billion to tens of billions[87, 88], efficient MLLMs\ntypically employ language models with less than 3 billion parameters, such as phi2-2.7B[74] by\nMicrosoft and Gemma-2B[78] by Google. Phi-2 trained on special data recipes can match the per-\nformance of models 25 times larger trained on regular data. Phi-3-mini [86] can be easily deployed\nlocally on a modern phone and achieves a quality that seems on-par with models such as Mixtral\n8x7B [89] and GPT-3.5. In addition to utilizing pre-trained models, MobileVLM[20] downscales\nLLaMA[87] and trains from scratch using open-source datasets. The specific model scaling is illus-\ntrated in the Table.1 and Table.4.\n2.4 Vision Token Compression\nInitial research has underscored the potential of MLLMs across various tasks, including visual ques-", "question": "Which Microsoft language model, with less than 3 billion parameters, can match the performance of models 25 times larger?\n", "answer": "Phi-2, as trained on special data recipes, can match the performance of models 25 times larger trained on regular data.", "source": "multimodal.pdf", "id": "26327c579e"}, {"context": "We combined our domain alignment dataset, consisting\nof approximately 1.4k samples, with larger general chat\ndatasets. For SFT, we blended the domain instructional\ndata with 128k commercial-viable chat data and then per-\nformed fine-tuning for a single epoch after random shuffling.\nWe conducted experiments involving augmentation of the\ndomain-specific SFT dataset for more than one epoch. How-\never, it became apparent that the model rapidly exhibited\nsigns of overfitting when presented with in-domain ques-\ntions, often repeating irrelevant answers from the domain\nSFT dataset. For SteerLM, we closely followed the steps\noutlined in (Wang et al., 2023). We first trained an attribute\nmodel instantiated with LLaMA2-13B model on the Help-\nSteer and OASST datasets. We then used the attribute model\nto label all attributes for OASST data and our domain in-\nstructional data. Finally, we conducted attribute-conditioned\nfine-tuning and also masked the attribute labels and trained", "question": "How many samples are there in the domain alignment dataset?\n", "answer": "The domain alignment dataset consists of approximately 1.4k samples.", "source": "ChipNemo.pdf", "id": "a2a279215b"}, {"context": "Bit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain\naccuracy throughout quantization. PackQViT [129] mitigates outlier effects during quantization.\nBiViT [128] introduces Softmax-aware Binarization to adjust the binarization process, minimizing\nerrors in binarizing softmax attention values. Xiao et al. [142] integrated a gradient regularization\nscheme to curb weight oscillation during binarization training and introduced an activation shift\nmodule to reduce information distortion in activations. Additionally, BinaryViT [130] integrates\nessential architectural elements from CNNs into a pure ViT framework, enhancing its capabilities.\nHardware-Aware Quantization optimizes the quantization process of neural network models for\nspecific hardware platforms ( e.g., GPUs [131], FPGA [132]). It adjusts precision levels and quan-\ntization strategies to maximize performance and energy efficiency during inference. For example,", "question": "How does Hardware-Aware Quantization optimize the performance of neural network models on specific hardware platforms?\n", "answer": "Hardware-Aware Quantization optimizes the performance of neural network models on specific hardware platforms by adjusting precision levels and quantization strategies to maximize performance and energy efficiency during inference.", "source": "multimodal.pdf", "id": "31efe3044d"}, {"context": "customization for enhancing the effectiveness of\nlarge language models in specialized applications.\n1. Introduction\nOver the last few decades, Electronic Design Automation\n(EDA) algorithms and tools have provided huge gains in\nchip design productivity. Coupled with the exponential\nincreases in transistor densities provided by Moore\u2019s law,\nEDA has enabled the development of feature-rich complex\nSoC designs with billions of transistors. More recently, re-\n*Equal contribution1NVIDIA.searchers have been exploring ways to apply AI to EDA al-\ngorithms and the chip design process to further improve chip\ndesign productivity (Khailany et al., 2020; Ren & Fojtik,\n2021; Roy et al., 2021). However, many time-consuming\nchip design tasks that involve interfacing with natural lan-\nguages or programming languages still have not been auto-\nmated. The latest advancements in commercial (ChatGPT,\nBard, etc.) and open-source (Vicuna (Chiang et al., 2023),", "question": "Which large language models are mentioned as being explored for application in EDA algorithms and the chip design process?\n", "answer": "The large language models mentioned are ChatGPT, Bard, Vicuna, and those discussed in the papers by Khailany et al. (2020), Ren & Fojtik (2021), and Roy et al. (2021).", "source": "ChipNemo.pdf", "id": "0e1c7b711e"}, {"context": "UniNet [102] introduced context-aware down-sampling modules improving information accommo-\ndation by transformer and MLP operators.\nOptimization of Attention Mechanisms Methods focus on reducing computational complexity\nby introducing adaptive attention, learning sparse attention patterns, and dynamically adjusting at-\ntention mechanisms. Fayyaz et al. [135] implemented adaptive attention by scoring and adaptively\nsampling significant tokens. PatchMerger [103] extracted global information among regional to-\nkens and exchanged local self-attention with information among regional tokens via self-attention.\nDynamicViT [104] proposed an attention masking strategy to differentiably prune tokens by block-\ning interactions with other tokens. Additionally, Sepvit [105] conducted local-global information\ninteraction within and across windows using depthwise separable self-attention. These methods\ncollectively optimize attention mechanisms, enhancing computational efficiency and performance.", "question": "How did UniNet [102] improve information accommodation by transformer and MLP operators?\n", "answer": "UniNet [102] introduced context-aware down-sampling modules to improve information accommodation by transformer and MLP operators.", "source": "multimodal.pdf", "id": "e7939ae097"}, {"context": "referred to as \u201dopening a book to a poisonous mushroom\u201d.\nTherefore, enhancing the robustness of RAG has increasinglygained researchers\u2019 attention, as represented in studies such\nas[Yuet al. , 2023a, Glass et al. , 2021, Baek et al. , 2023 ].\nThirdly, the issue of RAG and Fine-tuning\u2019s synergy is\nalso a primary research point. Hybrid has gradually become\none of the mainstream methods in RAG, exemplified by RA-\nDIT [Linet al. , 2023 ]. How to coordinate the relationship\nbetween the two to simultaneously obtain the advantages of\nparameterization and non-parameterization is a problem that\nneeds addressing.\nLastly, the engineering practice of RAG is a significant\narea of interest. The ease of implementation and align-\nment with corporate engineering needs have contributed to\nRAG\u2019s rise. However, in engineering practice, questions\nlike how to improve retrieval efficiency and document re-\ncall rate in large-scale knowledge base scenarios, and how", "question": "What is one of the mainstream methods in RAG (Retrieval-Augmented Generation)?\n", "answer": "RADIT is one of the mainstream methods in RAG, as mentioned in the context.", "source": "RAG.pdf", "id": "d1bf418b08"}, {"context": "such as professional domain knowledge question-answering,\nRAG might offer lower training costs and better performance\nbenefits than fine-tuning.\nSimultaneously, improving the evaluation system of RAG\nfor assessing and optimizing its application in different down-\nstream tasks is crucial for the model\u2019s efficiency and bene-\nfits in specific tasks. This includes developing more accurate\nevaluation metrics and frameworks for different downstream\ntasks, such as context relevance, content creativity, and harm-\nlessness, among others.\nFurthermore, enhancing the interpretability of models\nthrough RAG, allowing users to better understand how and\nwhy the model makes specific responses, is also a meaning-\nful task.\nTechnical Stack\nIn the ecosystem of RAG, the development of the related\ntechnical stack has played a driving role. For instance,\nLangChain and LLamaIndex have become widely known\nquickly with the popularity of ChatGPT. They both offer a", "question": "What are two technical stacks that have gained popularity in the RAG ecosystem?\n", "answer": "LangChain and LLamaIndex", "source": "RAG.pdf", "id": "da182b99e8"}, {"context": "external knowledge, alleviates hallucination issues, identifies\ntimely information via retrieval technology, and enhances re-\nsponse accuracy. Additionally, by citing sources, RAG in-\ncreases transparency and user trust in model outputs. RAG\ncan also be customized based on specific domains by index-\ning relevant text corpora. RAG\u2019s development and charac-\nteristics are summarized into three paradigms: Naive RAG,\nAdvanced RAG, and Modular RAG, each with its models,\nmethods, and shortcomings. Naive RAG primarily involves\nthe \u2019retrieval-reading\u2019 process. Advanced RAG uses more\nrefined data processing, optimizes the knowledge base in-\ndexing, and introduces multiple or iterative retrievals. As\nexploration deepens, RAG integrates other techniques like\nfine-tuning, leading to the emergence of the Modular RAG\nparadigm, which enriches the RAG process with new mod-\nules and offers more flexibility.\nIn the subsequent chapters, we further analyze three key", "question": "What is the basic process involved in Naive RAG?\n", "answer": "The basic process involved in Naive RAG is the 'retrieval-reading' process.", "source": "RAG.pdf", "id": "123a2dcc44"}, {"context": "Homomorphic KDs can further classified into logit-level [114, 115], patch-level [117], module-\nlevel [116], and feature-level KDs [118]. For logit-level methods, in DeiT [114], a distillation token\nis incorporated into the self-attention module to emulate the class label inferred by the teacher model,\nfacilitating interaction between the student attention and layers, thus enabling the learning of hard\nlabels during back-propagation. TinyViT [115] applies distillation during pretraining, where logits\nfrom large teacher models are pre-stored in the hardware, enabling memory and computational ef-\nficiency when transferring knowledge to scaled-down student transformers. Patch-level techniques\nlike DeiT-Tiny [117] train a small student model to match a pre-trained teacher model on patch-level\nstructures, then optimize with a decomposed manifold matching loss for reduced computational\ncosts. Module-level methods involve segregating teacher modules from a pre-trained unified model,", "question": "How does TinyViT facilitate knowledge distillation during pretraining?\n", "answer": "TinyViT facilitates knowledge distillation during pretraining by pre-storing logits from large teacher models in the hardware, enabling memory and computational efficiency when transferring knowledge to scaled-down student transformers.", "source": "multimodal.pdf", "id": "534dcc9fda"}, {"context": "queries and documents. The specific optimization objectives\nare as follows:\nLDR=\u2212logesim(q,d+)\nef(q,d+)+P\nd\u2212\u2208D\u2212esim(q,d\u2212)(12)\nwhere q and d are the query and document encoded by the\nencoder. d\u2212,d+represent negative samples and positive sam-\nples respectively. In the initial training stage of the gener-\nator, we utilize contrastive learning to align structured data\nand the corresponding document description of unstructured\ndata. The optimization objective is as above.\nMoreover, in the later training stage of the gener-\nator, inspired by references [Sciavolino et al. , 2021,\nZhang et al. , 2019 ], we recognized the remarkable ef-\nfectiveness of entity semantics in learning textual data\nrepresentations in retrieval. Thus, we first perform entity\nidentification in the structured data, subsequently applying\na mask to the entities in the input section of the generator\u2019s\ntraining data, enabling the generator to predict these masks.\nThe optimization objective hereafter is:\nLMEP =kX", "question": "What is the optimization objective for the later training stage of the generator in the context?\n", "answer": "The optimization objective for the later training stage of the generator is LMEP, which is calculated as the expected value of the log probability of the masked entities in the input section of the generator's training data.", "source": "RAG.pdf", "id": "b496c53c02"}, {"context": "depending on the needs of different tasks. If there is historical\ndialogue information, it can also be merged into the prompt\nfor multi-round dialogues.\nDrawbacks in Naive RAG\nThe Naive RAG confronts principal challenges in three ar-\neas: retrieval quality, response generation quality, and the\naugmentation process.\nRegarding retrieval quality, the issues are multifaceted.\nThe primary concern is low precision, where not all blocks\nwithin the retrieval set correlate with the query, leading to\npotential hallucination and mid-air drop issues. A secondary\nissue is low recall, which arises when not all relevant blocks\nare retrieved, thereby preventing the LLM from obtaining suf-\nficient context to synthesize an answer. Additionally, out-\ndated information presents another challenge, where data re-\ndundancy or out-of-date data can result in inaccurate retrieval\noutcomes.\nIn terms of response generation quality, the issues are\nequally diverse. Hallucination is a prominent issue where the", "question": "What is a main concern with retrieval quality in the Naive RAG?\n", "answer": "A main concern with retrieval quality in the Naive RAG is low precision, where not all blocks within the retrieval set correlate with the query, leading to potential hallucination and mid-air drop issues.", "source": "RAG.pdf", "id": "b66fd4b3d0"}, {"context": "2.3 Hallucinations in Multimodal Large Language Models\nHallucination of MLLM generally refers to the phenomenon where the generated text response\ndoes not align with the corresponding visual content. State-of-the-art studies in this field primarily\nfocus on object hallucination, given that objects are central to research in computer vision and\nmultimodal contexts. Regarding inconsistency, two typical failure modes are: 1) missing objects,\nand 2) describing objects that are not present in the image or with incorrect statements. Empirically,\nthe second mode has been shown to be less preferable to humans. For example, the LSMDC\nchallenge [ 91] shows that correctness is more important to human judges than specificity. In\ncontrast, the coverage of objects is less perceptible to humans. Thus, object coverage is not a\nprimary focus in studies of object hallucination. Empirically, object hallucination can be categorized", "question": "What is the less preferable failure mode in multimodal large language models, according to human judges?\n", "answer": "Describing objects that are not present in the image or with incorrect statements.", "source": "hallucination.pdf", "id": "f58cf51d02"}, {"context": "independent hallucination categories. In this work, we classify them under the attribute category.\nThe definition of hallucination types aligns well with the domain of compositional generalization [ 79,\n121] of VLMs, which investigates visio-linguistic generalization and reasoning abilities.\n3 HALLUCINATION CAUSES\nHallucinations have multifaceted origins, spanning the entire spectrum of MLLMs\u2019 capability\nacquisition process. In this section, we delve into the root causes of hallucinations in MLLMs,\nprimarily categorized into four aspects: Data ,Model ,Training , and Inference .\n3.1 Data\nData stands as the bedrock for MLLMs, enabling them to gain cross-modal understanding and\ninstruction-following capabilities. However, it can inadvertently become the source of MLLM\nhallucinations. This mainly manifests in three aspects: quantity, quality, and statistical bias.\n3.1.1 Quantity. Deep learning models are data-hungry, especially large models like MLLMs. The", "question": "What are the three aspects of data that can cause hallucinations in multimodal language learning machines (MLLMs)?\n", "answer": "The three aspects of data that can cause hallucinations in MLLMs are quantity, quality, and statistical bias.", "source": "hallucination.pdf", "id": "c67d2cac99"}, {"context": "vance, and the LLaMA [Touvron et al. , 2023 ]index\nhas an automatic evaluation feature for different\nchunking methods. The method of querying across\nmultiple index paths is closely related to previous\nmetadata filtering and chunking methods, and may\ninvolve querying across different indexes simulta-\nneously. A standard index can be used to query spe-\ncific queries, or a standalone index can be used to\nsearch or filter based on metadata keywords, such\nas a specific \u201cdate\u201d index.\nIntroducing a graph structure involves transform-\ning entities into nodes and their relationships into\nrelations. This can improve accuracy by leverag-\ning the relationships between nodes, especially for\nmulti-hop questions. Using a graph data index can\nincrease the relevance of the retrieval.\n3.Adding Metadata Information: The focus here\nis to embed referenced metadata into chunks, such\nas dates and purposes used for filtering. Adding\nmetadata like chapters and subsections of refer-", "question": "How can adding metadata information improve the chunking methods in the Vance and LLaMA index?\n", "answer": "Adding metadata information like chapters, subsections, dates, and purposes can improve the filtering process in the chunking methods of the Vance and LLaMA index.", "source": "RAG.pdf", "id": "f15333b58e"}, {"context": "Modular RAG\nThe modular RAG structure breaks away from the traditional\nNaive RAG framework of indexing, retrieval, and genera-\ntion, offering greater diversity and flexibility in the over-\nall process. On one hand, it integrates various methods to\nexpand functional modules, such as incorporating a search\nmodule in similarity retrieval and applying a fine-tuning ap-\nproach in the retriever [Linet al. , 2023 ]. Additionally, spe-\ncific problems have led to the emergence of restructured\nRAG modules [Yuet al. , 2022 ]and iterative approaches like\n[Shao et al. , 2023 ]. The modular RAG paradigm is becom-\ning the mainstream in the RAG domain, allowing for ei-\nther a serialized pipeline or an end-to-end training approach\nacross multiple modules.The comparison between three RAG\nparadigms is illustrated in Fig 3.\nNew Modules\n\u2022Search Module: Diverging from the similarity re-\ntrieval between queries and corpora in Naive/Advanced\nRAG, the search module, tailored to specific sce-", "question": "What is a component of the modular RAG structure that diverges from the traditional similarity retrieval method?\n", "answer": "The Search Module is a component of the modular RAG structure that diverges from the traditional similarity retrieval method, as it is tailored to specific scenes.", "source": "RAG.pdf", "id": "1d479682a6"}, {"context": "Figure 8: Structure of MOE [89](left) and Mamba [77](right).\nlative decoding, the need for image tokens and their associated processing components is bypassed.\nFastV [46] finds that most image tokens receive inefficient attention after the second decoder layer\nand achieve computation reduction by eliminating redundant visual tokens during the inference stage\nwithout sacrificing performance. VTW [47] asserts that visual tokens are not essential in the deeper\nlayers of MLLM. It strategically removes all of them at a specific layer, allowing only text tokens to\nparticipate in the subsequent layers. This approach by VTW can reduce computational overhead by\nmore than 40% across a variety of multimodal tasks, without compromising performance.\n3 Efficient Vision\nVision Transformer (ViT) [94] architectures have gained significant popularity and are widely used\nin computer vision applications. However, as ViT models have grown in size, the number of train-", "question": "How can computational overhead be reduced in multimodal tasks without compromising performance?\n", "answer": "By removing visual tokens in the deeper layers of MLLM, as proposed by VTW, computational overhead can be reduced by more than 40%. This is achieved by allowing only text tokens to participate in the subsequent layers, and it does not affect the performance of the model on various multimodal tasks.", "source": "multimodal.pdf", "id": "dc27786588"}, {"context": "intensive tasks. By citing sources, users can verify\nthe accuracy of answers and increase trust in model\noutputs. It also facilitates knowledge updates\nand the introduction of domain-specific knowl-\nedge. RAG effectively combines the parameter-\nized knowledge of LLMs with non-parameterized\nexternal knowledge bases, making it one of the\nmost important methods for implementing large\nlanguage models. This paper outlines the develop-\nment paradigms of RAG in the era of LLMs, sum-\nmarizing three paradigms: Naive RAG, Advanced\nRAG, and Modular RAG. It then provides a sum-\nmary and organization of the three main compo-\nnents of RAG: retriever, generator, and augmenta-\ntion methods, along with key technologies in each\ncomponent. Furthermore, it discusses how to eval-\nuate the effectiveness of RAG models, introducing\ntwo evaluation methods for RAG, emphasizing key\nmetrics and abilities for evaluation, and presenting\nthe latest automatic evaluation framework. Finally,", "question": "What is one of the most important methods for implementing large language models?\n", "answer": "RAG (Retrieval-Augmented Generation) is one of the most important methods for implementing large language models, as it combines the parameterized knowledge of LLMs with non-parameterized external knowledge bases.", "source": "RAG.pdf", "id": "4fffd3dc2b"}, {"context": "phase to capture key semantic meanings. In the later\nstages of this process, larger blocks with more contex-\ntual information are provided to the language model\n(LM). This two-step retrieval method helps strike a bal-\nance between efficiency and contextually rich responses.\n\u2022StepBack-prompt: Integrated into the RAG process,\nthe StepBack-prompt approach [Zheng et al. , 2023 ]en-\ncourages LLM to step back from specific instances and\nengage in reasoning about the underlying general con-\ncepts or principles. Experimental findings indicate a sig-\nnificant performance improvement in various challeng-\ning, inference-intensive tasks with the incorporation of\nbackward prompts, showcasing its natural adaptability\nto RAG. The retrieval-enhancing steps can be applied in\nboth the generation of answers to backward prompts and\nthe final question-answering process.\n\u2022Subqueries: Various query strategies can be employed in\ndifferent scenarios, including using query engines pro-", "question": "What is the purpose of the StepBack-prompt approach in the RAG process?\n", "answer": "The StepBack-prompt approach encourages the language model to step back from specific instances and engage in reasoning about the underlying general concepts or principles.", "source": "RAG.pdf", "id": "ad03b3dcc5"}, {"context": "Recite-Read [Sunet al. , 2022 ]transforms external re-\ntrieval into retrieval from model weights, initially hav-\ning LLM memorize task-relevant information and gener-\nate output for handling knowledge-intensive natural lan-\nguage processing tasks.\n\u2022Adjusting the Flow between Modules In the realm of\nadjusting the flow between modules, there is an empha-\nsis on enhancing interaction between language models\nand retrieval models. DSP [Khattab et al. , 2022 ]intro-\nduces the Demonstrate-Search-predict framework, treat-\ning the context learning system as an explicit program\nrather than a terminal task prompt to address knowledge-\nintensive tasks. ITER-RETGEN [Shao et al. , 2023 ]\nutilizes generated content to guide retrieval, itera-\ntively performing \u201cretrieval-enhanced generation\u201d and\n\u201cgeneration-enhanced retrieval\u201d in a Retrieve-Read-\nRetrieve-Read flow. Self-RAG [Asai et al. , 2023b ]fol-\nlows the decide-retrieve-reflect-read process, introduc-", "question": "How does ITER-RETGEN use generated content in knowledge-intensive tasks?\n", "answer": "ITER-RETGEN uses generated content to guide retrieval, iteratively performing \"retrieval-enhanced generation\" and \"generation-enhanced retrieval\" in a Retrieve-Read-Retrieve-Read flow.", "source": "RAG.pdf", "id": "dfac20a7d8"}, {"context": "to provide fine-grained feedback at the sentence level. The collected human preference data is\nused to train a reward model. Additionally, it leverages advanced vision perception models to\nautomatically score the grounding and fidelity of the text generated by an MLLM. Both sources are\ncombined into a single reward score during the reinforcement learning procedure.\n5.3.3 Unlearning. Unlearning refers to a technique designed to induce a model to \u2019forget\u2019 specific\nbehaviors or data, primarily through the application of gradient ascent methods [ 9]. Recently,\nunlearning for LLMs has been receiving increasing attention [ 50], effectively eliminating privacy\nvulnerabilities in LLMs. In the context of MLLMs, a recent work [ 109] introduces the Efficient\nFine-grained Unlearning Framework (EFUF), applying an unlearning framework to address the\nhallucination problem. Specifically, it utilizes the CLIP model to construct a dataset comprised of", "question": "How does a recent work address the hallucination problem in Multi-Modal Language Learning Models (MLLMs)?\n", "answer": "A recent work addresses the hallucination problem in MLLMs by utilizing the Efficient Fine-grained Unlearning Framework (EFUF) and the CLIP model to construct a dataset.", "source": "hallucination.pdf", "id": "2dd3a385f4"}, {"context": "alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.\nCCS Concepts: \u2022Computing methodologies \u2192Computer vision ;Natural language processing ;Machine\nlearning .\nAdditional Key Words and Phrases: Hallucination, Multimodal, Large Language Models, Vision-Language\nModels.\nACM Reference Format:\nZechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou.\n2024. Hallucination of Multimodal Large Language Models: A Survey. Preprint 1, 1 (April 2024), 30 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n\u2217Corresponding Author\nAuthors\u2019 addresses: Zechen Bai, Show Lab, National University of Singapore, 4 Engineering Drive 3, Singapore, Singapore,\nzechenbai@u.nus.edu; Pichao Wang, Amazon Prime Video, Washington, USA, pichaowang@gmail.com; Tianjun Xiao,\nAWS Shanghai AI Lab, Shanghai, China, tianjux@amazon.com; Tong He, AWS Shanghai AI Lab, Shanghai, China, htong@", "question": "Who wrote the 2024 survey on hallucination of multimodal large language models?\n", "answer": "Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, and Zheng Zhang", "source": "hallucination.pdf", "id": "9bfe24c206"}, {"context": "8 question categories cover various types of hallucination, including object attributes, counting,\nspatial relations, etc. During the evaluation of MMHal-Bench, the GPT-4 model is employed to\nanalyze and rate the responses.\nGAVIE [ 73]GPT4-Assisted Visual Instruction Evaluation (GAVIE) is proposed to assess the LMM\noutput in two different aspects: Relevancy to evaluate the instruction-following performance and\nAccuracy to measure the visual hallucination in the LMM output. It comprises a benchmark with\n1,000 samples and an evaluation approach. GAVIE evaluates the output of MLLMs in an open-ended\nmanner and does not require human-annotated ground-truth answers. The core idea is to ask the\nadvanced GPT-4 to work as a smart teacher and score the answer by taking image content, human\ninstruction, and model response as input.\nNOPE [ 77]This paper proposes to establish a distinction between object hallucination and", "question": "How is the GPT-4 model used in the evaluation of visual hallucination in machine learning models?\n", "answer": "The GPT-4 model is employed as a smart teacher in the GAVIE evaluation approach to assess the accuracy of visual hallucination in the output of machine learning models by taking image content, human instruction, and model response as input.", "source": "hallucination.pdf", "id": "ed09817624"}, {"context": "in computer vision applications. However, as ViT models have grown in size, the number of train-\nable parameters and operations has also increased, impacting their deployment and performance.\nAdditionally, the computational and memory cost of self-attention grows quadratically with image\nresolution. Referring to the paper [95], this survey aims to explore the most efficient vision encoding\nmethodologies that may be used for efficient MLLMs.\nEfficient VisionCompact Architecture (\u00a73.1)Architecture Design MethodsReformer[96], EfficientFormer[97],\nEfficientFormerV2[98]\nArchitecture Search MethodsAutoformer [99], NASViT [100],\nTF-TAS [101], UniNet [102]\nOptimization of Attention\nMechanisms MethodsPatchMerger [103], DynamicViT [104],\nSepvit [105]\nPruning (\u00a73.2)Unstructured Pruning Cap [106], Cait [107]\nStructured PruningWDPruning [108], X-Pruner [109],\nVTP[110], PS-ViT[111]\nHybrid Pruning SPViT [112], ViT-Slim [113]", "question": "What are some methods for optimizing attention mechanisms in vision transformers?\n", "answer": "Some methods for optimizing attention mechanisms in vision transformers include PatchMerger, DynamicViT, and Sepvit.", "source": "multimodal.pdf", "id": "20b3b3179f"}, {"context": "uations, more than 70% correctness on the generation\nof simple EDA scripts, and expert evaluation ratings\nabove 5 on a 7 point scale for summarizations and\nassignment identification tasks.\n\u2022 Domain-adapted ChipNeMo models dramatically out-\nperforms all vanilla LLMs evaluated on both multiple-\nchoice domain-specific AutoEval benchmarks and hu-\nman evaluations for applications.\n\u2022Using the SteerLM alignment method (Dong et al.,\n2023) over traditional SFT improves human evaluation\nscores for the engineering assistant chatbot by 0.62\npoints on a 7 point Likert scale.\n\u2022SFT on an additional 1.4Kdomain-specific instruc-\ntions significantly improves the model\u2019s proficiency at\ngenerating correct EDA tool scripts by 18%.\n\u2022Domain-adaptive tokenization reduce domain data to-\nken count by up to 3.3%without hurting effectiveness\non applications.\n\u2022Fine-tuning our ChipNeMo retrieval model with\n2", "question": "How much does domain-adaptive tokenization reduce domain data token count by?\n", "answer": "Up to 3.3%", "source": "ChipNemo.pdf", "id": "c7d05c4b43"}, {"context": "Category HallucinationAttribute HallucinationRelation Hallucination\nFig. 3. Three types of typical hallucination.\nPre-training. Given that models from each modality are pre-trained on their respective data, the\nobjective of this pre-training phase is to achieve cross-modal feature alignment. During training,\nboth the pre-trained visual encoder and LLM remain frozen, with only the cross-modal interface\nbeing trained. Similar to traditional VLMs training, as exemplified by CLIP [ 88], web-scale image-\ntext pairs [ 92] are utilized for training. Given that the final output is at the LLM side, the most\nwidely used loss function in this stage is the text generation loss, typically cross-entropy loss, which\naligns with the pre-training of LLMs. Certain studies (e.g., [ 22,66]) explore the incorporation of\ncontrastive loss and image-text matching loss to further enhance alignment. After training, the\ninterface module maps the visual features into the input embedding space of the LLM.", "question": "What is the objective of the pre-training phase in cross-modal feature alignment?\n", "answer": "The objective of the pre-training phase is to achieve cross-modal feature alignment between models from each modality.", "source": "hallucination.pdf", "id": "0be0058571"}, {"context": "A summary of frequently used pre-training datasets can be found in Table.3. High-quality IT data\ncan be derived from task-specific datasets. For instance, consider a sample from VQA datasets where\nthe input includes an image and a natural language question, and the output is the text-based answer\nto the question based on the image. This could easily form the multimodal input and response\nof the instruction sample. The instructions, or task descriptions, can be obtained either through\nmanual creation or semi-automatic generation with the help of GPT. In addition to utilizing publicly\navailable task-specific datasets, SPHINX-X[14] assembles a dataset focused on OCR from a wide\nrange of PDF data sourced from the internet. Specifically, it begins by gathering a large-scale PDF\ndataset from the web. It then obtains the rendering results of each page in the PDF file, while\nsimultaneously saving all text annotations along with their respective bounding boxes. Ultimately,", "question": "What is one example of a task-specific dataset used to derive high-quality IT data?\n", "answer": "A sample from VQA (Visual Question Answering) datasets, where the input includes an image and a natural language question, and the output is the text-based answer to the question based on the image, is one example of a task-specific dataset used to derive high-quality IT data.", "source": "multimodal.pdf", "id": "db45826cee"}, {"context": "tiple downstream tasks, fine-tuning the retriever with two dif-\nferent supervised signals via hard labeling of the dataset and\nthe soft reward derived from LLM.\nThis somewhat improves the semantic representation\nthrough both domain knowledge injection and downstream\ntask fine-tuning. However, the retrievers trained by this ap-\nproach are not intuitively helpful for large language models,\nso some work has been done to supervise the fine-tuning of\nEmbedding models directly through feedback signals from\nthe LLM. (This section will be presented in 4.4)\n4.2 How to Match the Semantic Space of Queries\nand Documents\nIn the RAG application, some retrievers use the same embed-\nding model to encode the query and doc, while others use two\nmodels to separately encode the query and doc. Moreover, the\noriginal query of the user may have problems of poor expres-\nsion and lack of semantic information. Therefore, aligning\nthe semantic space of the user\u2019s query and documents is very", "question": "How do some retrievers in the RAG application encode the query and documents?\n", "answer": "Some retrievers in the RAG application use the same embedding model to encode the query and documents, while others use two separate models to encode the query and documents.", "source": "RAG.pdf", "id": "d5d9951817"}, {"context": "affected by the instruction designs and the length of generated captions. Therefore, it proposes a\nnew evaluation metric as well as a benchmark, called Pooling-based Object Probing Evaluation\n(POPE). The basic idea is to convert the evaluation of hallucination into a binary classification task\nby prompting MLLMs with simple Yes-or-Noshort questions about the probing objects ( e.g., Is there\nacarin the image?) Compared to CHAIR, POPE offers increased stability and flexibility. Based on\nthis metric design, it further proposed an evaluation benchmark, drawing 500 images from the\nMSCOCO dataset. The questions in the benchmark consist of both positive and negative questions.\nThe positive questions are formed based on the ground-truth objects, while the negative questions\nare built from sampling nonexistent objects. The benchmark is divided into three subsets according\nto different negative sampling strategy: random, popular, and adversarial. Popular and adversarial", "question": "What is the name of the new evaluation metric for hallucination in machine learning models?\n", "answer": "Pooling-based Object Probing Evaluation (POPE)", "source": "hallucination.pdf", "id": "d3d3d6a133"}, {"context": "including object recognition, instance counting, and identifying object-to-object relationships.\nMERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal\nhallucinations. Interestingly, when organizing the data, a set of edited images is intentionally added.\nBased on the original image, an inpainting strategy is employed to remove one object instance in\nthe image. With this original-edited image pair, one can compare the output of the target MLLM\nand identify the hallucinated objects that lack visual grounding.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How many image-question pairs does MERLIM contain and what is its focus?\n", "answer": "MERLIM contains over 279K image-question pairs and has a strong focus on detecting cross-modal hallucinations.", "source": "hallucination.pdf", "id": "198c99577c"}, {"context": "beneficial when learning larger and more diverse datasets, showing the choice of training recipe is\nclosely related to the quality of the data.\nMulti-stage pre-training To maximize compute efficiency, Idefics2 [48] decomposes the pre-\ntraining in two stages. In the first stage, it limits the max image resolution to 384 pixels and use\na large global batch size. In the second stage, PDF documents are introduced to increase image\nresolution to a maximum of 980 pixels for the text to be legible.\n5.2 Instruction-Tuning\nInstruction-tuning (IT) is a crucial aspect of efficient MLLMs, which aims to fine-tune the models on\nspecific tasks by leveraging task-specific instructions. This approach is built upon the concept that\nMLLMs can understand and follow instructions provided in natural language, thereby enhancing\ntheir performance on the target task. The benefits of IT in efficient MLLMs are manifold. Firstly,", "question": "What is one advantage of instruction-tuning in efficient MLLMs?\n", "answer": "Instruction-tuning helps fine-tune models on specific tasks by utilizing task-specific instructions, enhancing their performance on the target task.", "source": "multimodal.pdf", "id": "8bb6ba6dfa"}, {"context": "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndomain-specific data improves the retriever hit rate\nby 30% over a pre-trained state-of-the-art retriever, in\nturn improving overall quality of RAG responses.\nThe paper is organized as follows. Section 2 outlines do-\nmain adaptation and training methods used including the\nadapted tokenizer, DAPT, model alignment, and RAG. Sec-\ntion 3 describes the experimental results including human\nevaluations for each application. Section 4 describes rel-\nevant LLM methods and other work targeting LLMs for\nchip design. Finally, detailed results along with additional\nmodel training details and examples of text generated by the\napplication use-cases are illustrated in the Appendix.\n2. ChipNeMo Domain Adaptation Methods\nChipNeMo implements multiple domain adaptation tech-\nniques to adapt LLMs to the chip design domain. These\ntechniques include domain-adaptive tokenization for chip\ndesign data, domain adaptive pretraining with large corpus", "question": "How much does domain-specific data improve the retriever hit rate in the ChipNeMo system?\n", "answer": "The retriever hit rate is improved by 30% using domain-specific data in the ChipNeMo system.", "source": "ChipNemo.pdf", "id": "df0b9868f2"}, {"context": "task-specific performance, it comes with higher resource requirements in terms of computational\npower and memory consumption. In an effort to lessen the burden associated with training, numer-\nous studies have concentrated on enhancing memory efficiency during full-parameter fine-tuning.\nThis strategic approach has effectively diminished the obstacles that once hindered progress in this\nfield of research. LOMO [158] introduces a Low-Memory Optimization technique derived from\nStochastic Gradient Descent (SGD) to reduce memory consumption. Typically, the ADAM opti-\nmizer is employed; however, the optimizer states in this approach occupy a significant amount of\nmemory. By utilizing the modified SGD-based LOMO, memory usage can be reduced. While SGD\nitself faces three challenges, these issues tend to resolve themselves during model fine-tuning. The\nspecific modification involves updating the parameters within the gradient computation rather than", "question": "How does the LOMO technique reduce memory consumption during full-parameter fine-tuning?\n", "answer": "The LOMO technique, derived from Stochastic Gradient Descent (SGD), reduces memory consumption by updating the parameters within the gradient computation, rather than storing all the intermediate values needed for the Adam optimizer.", "source": "multimodal.pdf", "id": "11e7fa55e5"}, {"context": "knowledge-intensive tasks, allowing the creation of domain-\nspecific models through training on domain-specific corpora.\nHowever, there are drawbacks, including the requirement for\na substantial amount of pre-training data and larger training\nresources, as well as the issue of slower update speeds. Espe-\ncially as model size increases, the cost of retrieval-enhanced\ntraining becomes relatively higher. Despite these limitations,\nthis method demonstrates notable characteristics in terms of\nmodel robustness. Once trained, retrieval-enhanced models\nbased on pure pre-training eliminate the need for external li-brary dependencies, enhancing both generation speed and op-\nerational efficiency.\nFine-tuning Stage\nDuring the downstream fine-tuning phase, researchers have\nemployed various methods to fine-tune retrievers and gener-\nators for improved information retrieval, primarily in open-\ndomain question-answering tasks. Concerning retriever fine-", "question": "How does fine-tuning retrievers and generators in the downstream phase improve information retrieval?\n", "answer": "Fine-tuning retrievers and generators in the downstream phase primarily enhances information retrieval in open-domain question-answering tasks.", "source": "RAG.pdf", "id": "011ee221ab"}, {"context": "information retrieval process, providing more effective and\naccurate inputs for subsequent LLM processing.\n5.2 How to Optimize a Generator to Adapt Input\nData?\nIn the RAG model, the optimization of the generator is a cru-\ncial component of the architecture. The generator\u2019s task is\nto take the retrieved information and generate relevant text,\nthereby providing the final output of the model. The goal of\noptimizing the generator is to ensure that the generated text is\nboth natural and effectively utilizes the retrieved documents,\nin order to better satisfy the user\u2019s query needs.\nIn typical Large Language Model (LLM) generation tasks,\nthe input is usually a query. In RAG, the main difference\nlies in the fact that the input includes not only a query\nbut also various documents retrieved by the retriever (struc-\ntured/unstructured). The introduction of additional informa-\ntion may have a significant impact on the model\u2019s understand-", "question": "How does the input to the generator differ in a RAG model compared to typical Large Language Model (LLM) generation tasks?\n", "answer": "In a RAG (Retriever-Augmented Generator) model, the input to the generator includes not only a query but also various documents retrieved by the retriever, whereas in typical LLM generation tasks, the input is usually just a query.", "source": "RAG.pdf", "id": "7fabdba415"}, {"context": "put x, relevant documents z are retrieved (selecting Top-1\nin the paper), and after integrating (x, z), the model gener-\nates output y. The paper utilizes two common paradigms\nfor fine-tuning, namely Joint-Encoder [Arora et al. , 2023,\nWang et al. , 2022b, Lewis et al. , 2020 ]and Dual-Encoder\n[Xiaet al. , 2019, Cai et al. , 2021, Cheng et al. , 2022 ]. For\nJoint-Encoder, a standard model based on encoder-decoder\nis used, where the encoder initially encodes the input, and\nthe decoder, through attention mechanisms, combines the en-\ncoded results to generate tokens in an autoregressive manner:\nH=Encoder (x[SEP ]m) (5)\nhi=Decoder (CrossAttn (H), y < i ) (6)\nPG\u03be(.|x, y < i ) =Softmax (hi) (7)\nFor the Dual-Encoder, the system establishes two indepen-\ndent encoders, each responsible for encoding the input (query,\ncontext) and the document, respectively. The output is then\nsubject to bidirectional cross-attention processing by the de-", "question": "What is one difference between Joint-Encoder and Dual-Encoder in the context of fine-tuning?\n", "answer": "In the Joint-Encoder approach, the encoder and decoder are part of a single model, while in the Dual-Encoder approach, two independent encoders are used for encoding input and document.", "source": "RAG.pdf", "id": "fd4cf4a5ea"}, {"context": "tokens. The issue of \u2019losing attention\u2019 would also lead to the model\u2019s output response being\nirrelevant to the visual content.\n4 HALLUCINATION METRICS AND BENCHMARKS\nIn this section, we present a comprehensive overview of existing hallucination metrics and bench-\nmarks, which are designed to assess the extent of hallucinations generated by existing cutting-edge\nMLLMs. Currently, the primary focus of these benchmarks is on evaluating the object hallucination\nof MLLM-generated content. Tab. 1 illustrates a summary of related benchmarks.\nCHAIR [ 90]. As one of the early works, the metric of CHAIR was proposed to evaluate ob-\nject hallucination in the traditional image captioning task. This is achieved by computing what\nproportion of words generated are actually in the image according to the ground truth sentences\nand object segmentations. The computation of the CHAIR metric is straightforward and easy", "question": "What is the metric used to evaluate object hallucination in traditional image captioning?\n", "answer": "The metric used to evaluate object hallucination in traditional image captioning is CHAIR, which computes the proportion of generated words that are actually in the image according to the ground truth sentences and object segmentations.", "source": "hallucination.pdf", "id": "52c95dc6e8"}, {"context": "3.1 Compact Architecture\nCompact Architecture refers to the design of lightweight and efficient models while maintaining high\nperformance in downstream tasks. It encompasses various strategies and methodologies to reduce\nmodel size, computational complexity, and memory footprint without compromising performance.\nThese strategies can be broadly categorized into three categories, 1) Architecture Design Methods,\n2) Architecture Search Methods, and 3) Optimization of Attention Mechanisms Methods.\nArchitecture Design Methods involve creating new architectures [133] or adjusting existing\nones [134] to achieve compactness without sacrificing performance. For example, Reformer [96]\nintroduced locality-sensitive hashing in attention mechanisms to reduce complexity, while also\nemploying reversible residual layers to store activations more efficiently. Furthermore, Efficient-\nFormer [97] analyzed ViT-based model architectures and operators, introducing a dimension-", "question": "What is one example of an Architecture Design Method used to create compact models?\n", "answer": "One example of an Architecture Design Method is Reformer, which introduces locality-sensitive hashing in attention mechanisms to reduce complexity and uses reversible residual layers to store activations more efficiently.", "source": "multimodal.pdf", "id": "82a6543862"}, {"context": "Tuning (SFT) [ 125] has been introduced. SFT involves further training LLMs using a meticulously\nannotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability\nof LLMs.\nReinforcement Learning from Human Feedback. Although SFT has made strides in en-\nabling LLMs to adhere to user instructions, there remains a need for further alignment with\nhuman preferences. Among the various methods, Reinforcement Learning from Human Feedback\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How does Reinforcement Learning from Human Feedback contribute to improving large language models?\n", "answer": "Reinforcement Learning from Human Feedback is a method that helps further align large language models with human preferences by using a carefully annotated set of (instruction, response) pairs to enhance the models' capabilities and controllability.", "source": "hallucination.pdf", "id": "5d75ad8147"}, {"context": "8 Bai, et al.\nimage content. In contrast, negative instruction data [ 73] and reject answering responses [ 11]\nare rare in the datasets. Given such training data, one potential drawback observed by recent\nstudies [ 69,73] is that current models tend to answer \" Yes\" for any instructions presented\nto the model, even when a proper answer should be \" No\", leading to hallucination. This\nphenomenon indicates the effect of data diversity.\n\u2022Detailed descriptions (open question) The impact of the level of detail in textual de-\nscriptions on this matter remains an open question. As discussed in Sec. 2.2, the texts in\npre-training data, such as LAION [ 92], usually describe the salient objects\u2019 overall content.\nWhile the texts in the instructing tuning stage, such as LLaVA-150k [75], consist of more\ndetailed descriptions. This LLaVA-150k dataset is generated by GPT-4 based on objects rec-\nognized by vision models. One recent work [ 16] argues that within the training data, detailed", "question": "What generates the LLaVA-1", "answer": " Factoid question: What generates the LLaVA-1", "source": "hallucination.pdf", "id": "fdb2b3dc98"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 23\nrecollection of its training documents, most of the time the result goes someplace useful. It\u2019s only\nwhen the dreams enter deemed factually incorrect territory that we label them as \u2019hallucinations\u2019.\nFrom this perspective, leveraging hallucination capabilities as a feature in downstream applications\npresents exciting opportunities for enhancing user experiences and enabling new use cases. As\nhumans are the end-users of these models, the primary goal is to enrich human user experiences.\nFuture research may switch the optimization objective from specific cross-modal benchmarks to\nhuman experience. For example, Some content may cause hallucinations but will not affect the\nuser experience, while some content may. Alternatively, integrating hallucination to inspire more\ncreative ideas in real-world applications could also be intriguing.\n6.6 Enhancing Interpretability and Trust", "question": "How can hallucination capabilities of large language models be used to improve human user experiences?\n", "answer": "Hallucination capabilities can be leveraged to enhance user experiences and enable new use cases, such as integrating hallucination to inspire creative ideas in real-world applications or optimizing the models for human experiences rather than specific cross-modal benchmarks.", "source": "hallucination.pdf", "id": "dfb6343eae"}, {"context": "ture design engineers, which understands internal hardware\ndesigns and is capable of explaining complex design top-\nics;EDA scripts generation for two domain specific tools\nbased on Python and Tcl for VLSI timing analysis tasks\nspecified in English; bug summarization and analysis as\npart of an internal bug and issue tracking system.\nAlthough general-purpose LLMs trained on vast amounts of\ninternet data exhibit remarkable capabilities in generative AI\ntasks across diverse domains (as demonstrated in (Bubeck\net al., 2023)), recent work such as BloombergGPT (Wu et al.,\n2023) and BioMedLLM (Venigalla et al., 2022) demonstrate\nthat domain-specific LLM models can outperform a gen-\neral purpose model on domain-specific tasks. In the hard-\nware design domain, (Thakur et al., 2023; Liu et al., 2023)\nshowed that open-source LLMs (CodeGen (Nijkamp et al.,\n1arXiv:2311.00176v4  [cs.CL]  7 Mar 2024", "question": "Which large language models (LLMs) have been found to outperform general-purpose models on hardware design tasks?\n", "answer": "CodeGen, as mentioned in the context, has been shown to perform well in the hardware design domain. The work of Thakur et al. (2023) and Liu et al. (2023) highlights the effectiveness of open-source LLMs like CodeGen in this specific domain.", "source": "ChipNemo.pdf", "id": "f72f0559a7"}, {"context": "ChipNeMo: Domain-Adapted LLMs for Chip Design\nour application of a low learning rate.\nWe refer readers to Appendix for details on the training data\ncollection process A.2, training data blend A.3, and imple-\nmentation details and ablation studies on domain-adaptive\npretraining A.6.\n2.3. Model Alignment\nAfter DAPT, we perform model alignment. We specifically\nleverage two alignment techniques: supervised fine-tuning\n(SFT) and SteerLM (Dong et al., 2023). We adopt the iden-\ntical hyperparameter training configuration as DAPT for all\nmodels, with the exception of using a reduced global batch\nsize of 128. We employ an autoregressive optimization ob-\njective, implementing a strategy where losses associated\nwith tokens originating from the system and user prompts\nare masked (Touvron et al., 2023). This approach ensures\nthat during backpropagation, our focus is exclusively di-\nrected towards the optimization of answer tokens.\nWe combined our domain alignment dataset, consisting", "question": "What optimization objective is used during model alignment after domain-adaptive pretraining in ChipNeMo?\n", "answer": "An autoregressive optimization objective is used during model alignment after domain-adaptive pretraining in ChipNeMo.", "source": "ChipNemo.pdf", "id": "a5a7c4ceb0"}, {"context": "mizes the LLM to generate outputs that maximize rewards provided by the trained preference model,\noften utilizing reinforcement learning algorithms like Proximal Policy Optimization (PPO) [ 93]. This\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How are language models fine-tuned to generate outputs that maximize rewards from a trained preference model in the given context?\n", "answer": "Language models are fine-tuned using reinforcement learning algorithms, such as Proximal Policy Optimization (PPO), to generate outputs that maximize rewards provided by the trained preference model.", "source": "hallucination.pdf", "id": "cc95f8f80d"}, {"context": "most of the research on reinforcement during the inference\nstage emerged during the era of LLMs. This is primarily due\nto the high training costs associated with high-performance\nlarge models. Researchers have attempted to enhance model\ngeneration by incorporating external knowledge in a cost-\neffective manner through the inclusion of RAG modules dur-\ning the inference stage. Regarding the use of augmented\ndata, early RAG primarily focused on the application of un-\nstructured data, particularly in the context of open-domain\nquestion answering. Subsequently, the range of knowledge\nsources for retrieval expanded, with the use of high-quality\ndata as knowledge sources effectively addressing issues such\nas internalization of incorrect knowledge and hallucinations\nin large models. This includes structured knowledge, with\nknowledge graphs being a representative example. Recently,\nthere has been increased attention on self-retrieval, which in-", "question": "What is a recent focus in reinforcement research during the inference stage for large language models?\n", "answer": "A recent focus is self-retrieval, where models retrieve and use their own information during the inference stage.", "source": "RAG.pdf", "id": "326cdd7c26"}, {"context": "probabilities. This approach is designed to handle situations\nwhere LLMs might need additional knowledge.\nSelf-RAG [Asai et al. , 2023b ]introduces an important in-\nnovation called Reflection tokens. These special tokens are\ngenerated to review the output and come in two types: Re-\ntrieve and Critic. The model can autonomously decide when\nto retrieve paragraphs or use a set threshold to trigger re-\ntrieval. When retrieval is needed, the generator processes\nmultiple paragraphs simultaneously, performing fragment-\nlevel beam search to obtain the best sequence. The scores for\neach subdivision are updated using Critic scores, and these\nweights can be adjusted during the inference process to cus-\ntomize the model\u2019s behavior. The Self-RAG framework also\nallows the LLM to autonomously determine whether recall\nis necessary, avoiding training additional classifiers or rely-\ning on NLI models. This enhances the model\u2019s ability to au-\ntonomously judge inputs and generate accurate answers.", "question": "How do Reflection tokens in the Self-RAG framework determine when to retrieve additional information?\n", "answer": "Reflection tokens in the Self-RAG framework can autonomously decide when to retrieve paragraphs or use a set threshold to trigger retrieval.", "source": "RAG.pdf", "id": "2449b179e1"}, {"context": "igate alignment issues. PRCA [Yang et al. , 2023b ]lever-\naged reinforcement learning to train a context adapter\ndriven by LLM rewards, positioned between the re-\ntriever and generator. It optimizes the retrieved in-\nformation by maximizing rewards in the reinforcement\nlearning phase within the labeled autoregressive pol-\nicy. AAR [Yuet al. , 2023b ]proposed a universal plu-\ngin that learns LM preferences from known-source\nLLMs to assist unknown or non-co-finetuned LLMs.\nRRR [Maet al. , 2023a ]designed a module for rewriting\nqueries based on reinforcement learning to align queries\nwith documents in the corpus.\n\u2022Validation Module: In real-world scenarios, it is notalways guaranteed that the retrieved information is reli-\nable. Retrieving irrelevant data may lead to the occur-\nrence of illusions in LLM. Therefore, an additional val-\nidation module can be introduced after retrieving docu-\nments to assess the relevance between the retrieved doc-", "question": "How can the occurrence of illusions in a language model due to retrieval of irrelevant data be prevented?\n", "answer": "By introducing an additional validation module after document retrieval to assess the relevance between the retrieved documents.", "source": "RAG.pdf", "id": "8d0a82337c"}, {"context": "in retrieved information. Counterfactual robustness tests\ninclude questions that the LLM can answer directly, but\nthe related external documents contain factual errors.\n7.3 Evaluation Frameworks\nRecently, the LLM community has been exploring the use\nof \u201dLLMs as judge\u201d for automatic assessment, with many\nutilizing powerful LLMs (such as GPT-4) to evaluate their\nown LLM applications outputs. Practices by Databricks us-\ning GPT-3.5 and GPT-4 as LLM judges to assess their chatbot\napplications suggest that using LLMs as automatic evaluation\ntools is effective [Leng et al. , 2023 ]. They believe this method\ncan also efficiently and cost-effectively evaluate RAG-based\napplications.\nIn the field of RAG evaluation frameworks, RAGAS and\nARES are relatively new. The core focus of these evaluations\nis on three main metrics: Faithfulness of the answer, answer\nrelevance, and context relevance. Additionally, TruLens, an\nopen-source library proposed by the industry, also offers a", "question": "How are LLMs used in the evaluation of RAG-based applications?\n", "answer": "LLMs, such as GPT-3.5 and GPT-4, are used as automatic evaluation tools for RAG-based applications, assessing their faithfulness, answer relevance, and context relevance. This method has been found to be effective and efficient in evaluating these applications.", "source": "RAG.pdf", "id": "a05a21efce"}, {"context": "corporating a retrieval mechanism using the T5 architecture\n[Raffel et al. , 2020 ]in both the pre-training and fine-tuning\nstages. Prior to pre-training, it initializes the encoder-decoder\nLM backbone with a pre-trained T5, and initializes the dense\nretriever with a pre-trained Contriever. During the pre-\ntraining process, it refreshes the asynchronous index every\n1000 steps.\nCOG [Vaze et al. , 2021 ]is a text generation model that for-\nmalizes its generation process by gradually copying text frag-\nments (such as words or phrases) from an existing collection\nof text. Unlike traditional text generation models that select\nwords sequentially, COG utilizes efficient vector search tools\nto calculate meaningful context representations of text frag-\nments and index them. Consequently, the text generation task\nis decomposed into a series of copy and paste operations,\nwhere at each time step, relevant text fragments are sought\nfrom the text collection instead of selecting from an indepen-", "question": "How does the COG model select text fragments during the generation process?\n", "answer": "The COG model selects text fragments by utilizing efficient vector search tools to calculate meaningful context representations of text fragments and index them. At each time step, relevant text fragments are sought from the text collection instead of selecting from an independent set of options.", "source": "RAG.pdf", "id": "bff4917f9e"}, {"context": "VL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to\n448\u00d7448. InternVL [ 2] scales up the vision encoder to 6 billion parameters, enabling processing of\nhigh-resolution images. Regarding hallucination, HallE-Switch [ 123] has investigated the impact\nof vision encoder resolution on its proposed CCEval benchmark. Among the three studied vision\nencoders (CLIP-ViT-L-112, CLIP-ViT-L-224, CLIP-ViT-L-336), higher resolution generally results\nin lower degrees of hallucination. These works indicate that scaling up vision resolution is a\nstraightforward yet effective solution.\n5.2.2 Versatile Vision Encoders. Several studies [ 38,49,98] have investigated vision encoders for\nMLLMs. Typically, the CLIP ViT image encoder is used as the vision encoder in most MLLMs\nthanks to its remarkable ability to extract semantic-rich features. However, CLIP has been shown\nto lose some visual details compared to pure vision models like DINO ViT [ 10]. Therefore, recent", "question": "Which vision encoder, compared to pure vision models, tends to lose some visual details?\n", "answer": "CLIP has been shown to lose some visual details compared to pure vision models like DINO ViT.", "source": "hallucination.pdf", "id": "3f64cf9b55"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 5\nVision InputVision ModelLLMImageVideo\u2026CLIP DINO-v2Linear\u2026LLaMAVicunaChatGLMFuyuDecodingGreedyBeam SearchSamplingText InputInstruction\u2026TokenizerBPE SentencePiece\u2026\nFig. 2. Popular architecture of multimodal large language model.\nintegration of human feedback into the training loop has demonstrated effectiveness in enhancing\nthe alignment of LLMs.\n2.2 Multimodal Large Language Models\nMLLMs [ 22,75,111,138] typically refers to a series of models that enable LLMs to perceive and\ncomprehend data from various modalities. Among them, vision+LLM is particularly prominent,\nowing to the extensive research on vision-language models (VLMs) [ 51,88,116] prior to LLMs. As a\nresult, MLLMs are sometimes referred to as vision-LLMs (VLLMs) or large vision language models\n(LVLMs). The goal of MLLMs is to activate the visual capabilities of LLMs, enabling them to \"see\"", "question": "What are models that enable large language models to perceive and understand data from various modalities referred to as?\n", "answer": "Multimodal Large Language Models (MLLMs) or sometimes as vision-LLMs (VLLMs) or large vision language models (LVLMs).", "source": "hallucination.pdf", "id": "f49f3b54ce"}, {"context": "ChipNeMo: Domain-Adapted LLMs for Chip Design\nFigure 4: Domain-Adapted ChipNeMo Tokenizer Improvements.\n3.1. Domain-Adaptive Tokenization\nWe adapt the LLaMA2 tokenizer (containing 32K tokens) to\nchip design datasets using the previously outlined four-step\nprocess. Approximately 9K new tokens are added to the\nLLaMA2 tokenizer. The adapted tokenizers can improve\ntokenization efficiency by 1.6% to 3.3% across various chip\ndesign datasets as shown in Figure 4. We observe no obvious\nchanges to tokenizer efficiency on public data. Importantly,\nwe have not observed significant decline in the LLM\u2019s accu-\nracy on public benchmarks when using the domain-adapted\ntokenizers even prior to DAPT.\n3.2. Domain Adaptive Pretraining\nFigure 5: Chip Domain Benchmark Result for ChipNeMo.\nFigure 5 presents the outcomes for ChipNeMo models on\nthe AutoEval benchmark for chip design domain (detailed\nin Appendix A.5). Results on open domain academic bench-\nmark results are presented in Appendix A.6. Our research", "question": "How many new tokens were added to the LLaMA2 tokenizer for chip design datasets?\n", "answer": "Approximately 9K new tokens were added to the LLaMA2 tokenizer for chip design datasets.", "source": "ChipNemo.pdf", "id": "ac7c0c980b"}, {"context": "InfMLLM [135] 13B 195.00 145.00 170.00 195.00 - - - - - - - -\nLLaMA-Adapter V2 [26] 7B 185.00 133.33 56.67 118.33 - - - - - - - -\nMiniGPT-4 [138] 13B 68.33 55.00 43.33 75.00 78.86 72.21 71.37 - 69.3 76.7 48.2 53.0\nmPLUG-Owl2 [112] 7B 185.00 155.00 88.33 150.00 - - - - 78.5 84.0 - -\nLLaVA-1.5 [75] 7B - - - - - - - - 74.4 82.9 48.9 34.2\nCogVLM [106] 7B 195.00 165.00 103.33 160.00 - - - - 80 86.1 - -\n5.1 Data\nAs discussed in the section on hallucination causes 3, data is one of the primary factors inducing\nhallucination in MLLMs. For mitigating hallucination, recent works make attempts on data, includ-\ning introducing negative data [ 73], introducing counterfactual data [ 117], and reducing noise and\nerrors in existing dataset [105, 120].\nLRV-Instruction [ 73]LRV-Instruction is proposed to address the issue that existing instruction\ntuning data primarily focus on positive instruction samples, leading the model to consistently", "question": "What is one solution proposed to reduce hallucination in MLLMs related to data?\n", "answer": "Introducing negative data is one solution proposed to reduce hallucination in MLLMs related to data.", "source": "hallucination.pdf", "id": "a411f027c7"}, {"context": "ples include Hit Rate, MRR, NDCG, Precision, etc.\n2.Generation Module\nThe generation module here refers to the enhanced or\nsynthesized input formed by supplementing the retrieved\ndocuments into the query, distinct from the final an-\nswer/response generation, which is typically evaluated\nend-to-end. The evaluation metrics for the generation\nmodule mainly focus on context relevance, measuring\nthe relatedness of retrieved documents to the query ques-\ntion.\nEnd-to-End Evaluation\nEnd-to-end evaluation assesses the final response gener-\nated by the RAG model for a given input, involving the\nrelevance and alignment of the model-generated answers\nwith the input query. From the perspective of content\ngeneration goals, evaluation can be divided into unlabeled\nand labeled content. Unlabeled content evaluation met-\nrics include answer fidelity, answer relevance, harmless-\nness, etc., while labeled content evaluation metrics in-\nclude Accuracy and EM. Additionally, from the perspec-", "question": "What are some evaluation metrics for the final response generated by the RAG model?\n", "answer": "Accuracy, EM, answer fidelity, and answer relevance are some evaluation metrics for the final response generated by the RAG model.", "source": "RAG.pdf", "id": "b023f9e1c7"}, {"context": "frames for short video understanding.\nTo address the computational challenges associated with processing long videos due to the excessive\nnumber of visual tokens, several approaches have been developed. mPLUG-video [67] is designed\nfor video understanding tasks and begins with a TimeSformer-based video encoder to extract fea-\ntures from sparsely sampled video frames effectively, followed by a visual abstractor module to\nreduce sequence length. Video-LLaV A [44] excels in various video understanding tasks by unify-\ning visual representations of images and videos into a single language feature space before projec-\ntion. This approach enables effective learning of multi-modal interactions with LanguageBind [93].\nLLaMA-VID [69] addresses this issue by representing each frame with two distinct tokens, namely\ncontext token and content token. The context token encodes the overall image context based on user", "question": "How does LLaMA-VID represent each frame in a video?\n", "answer": "Each frame in a video is represented by two distinct tokens in LLaMA-VID: a context token that encodes the overall image context and a content token.", "source": "multimodal.pdf", "id": "c16b4c1887"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 15\nTable 3. Comparison of mainstream MLLMs on discriminative benchmarks. The numbers come from the\noriginal papers of these benchmarks.\nModelLLM\nSizeMME\nExistence\nScore\u2191MME\nCount\nScore\u2191MME\nPosition\nScore\u2191MME\nColor\nScore\u2191POPE\nRandom\nF1-Score\u2191POPE\nRandom\nF1-Score\u2191POPE\nAdversarial\nF1-Score\u2191RAH-Bench\nF1 Score\u2191AMBER\nDis.\nF1-Score\u2191AMBER\nScore\u2191Hal-Eval\nIn-domain\nEvent. F1\u2191Hal-Eval\nOut-of-domain\nEvent. F1\u2191\nmPLUG-Owl [111] 7B 120.00 50.00 50.00 55.00 68.06 66.79 66.82 69.3 31.2 54.1 47 46.6\nImageBind-LLM [34] 7B 128.33 60.00 46.67 73.33 - - - - - - - -\nInstructBLIP [22] (7B) 7B - - - - - - - 89.1 82.6 86.2 66.2 66.6\nInstructBLIP [22] (13B) 13B 185.00 143.33 66.67 153.33 89.29 83.45 78.45 84.7 - - - -\nVisualGLM-6B [25] 6B 85.00 50.00 48.33 55.00 - - - - - - - -\nMultimodal-GPT [28] 7B 61.67 55.00 58.33 68.33 66.68 66.67 66.67 - - - - -\nPandaGPT [95] 7B 70.00 50.00 50.00 50.00 - - - - - - - -", "question": "What is the in-domain event F1 score of InstructBLIP (13B) on the Hal-Eval benchmark?\n", "answer": "89.29", "source": "hallucination.pdf", "id": "90bbefc8ec"}, {"context": "hallucination problem. Specifically, it utilizes the CLIP model to construct a dataset comprised of\nboth positive samples and negative (hallucinated) samples. The training loss is applied separately\nfor positive and negative at the sub-sentence level. To the best of our knowledge, EFUF [ 109] is the\nfirst and only work that applies the unlearning framework to the task of hallucination mitigation,\nopening up a new path for future research.\n5.4 Inference\n5.4.1 Generation Intervention.\nContrastive Decoding. VCD (Visual Contrastive Decoding) [ 64] is designed to suppress the sta-\ntistical biases and language priors in MLLMs during the decoding phase. The main assumption\nof VCD is that a distorted visual input would lead to text responses with more biases and priors.\nThus, by contrasting output distributions derived from original and distorted visual inputs, VCD\naims to effectively reduce the over-reliance on statistical bias and language priors. Specifically, the", "question": "What is the assumption behind the Visual Contrastive Decoding (VCD) method?\n", "answer": "The assumption behind the Visual Contrastive Decoding (VCD) method is that a distorted visual input would lead to text responses with more biases and priors.", "source": "hallucination.pdf", "id": "2e0645de76"}, {"context": "et al., 2023) and SteerLM (Dong et al., 2023).\nResearchers have started to apply LLM to chip design prob-\nlems. Early works such as Dave (Pearce et al., 2020) first\nexplored the possibility of generating Verilog from En-\nglish with a language model (GPT-2). Following that work,\n(Thakur et al., 2023) showed that fine-tuned open-source\nLLMs (CodeGen) on Verilog datasets collected from GitHub\nand Verilog textbooks outperformed state-of-the-art OpenAI\nmodels such as code-davinci-002 on 17 Verilog questions.\n(Liu et al., 2023) proposed a benchmark with more than\n150 problems and demonstrated that the Verilog code gen-\neration capability of pretrained language models could be\nimproved with supervised fine-tuning by bootstrapping with\nLLM generated synthetic problem-code pairs. Chip-Chat\n(Blocklove et al., 2023) experimented with conversational\nflows to design and verify a 8-bit accumulator-based micro-\nprocessor with GPT-4 and GPT-3.5. Their findings showed", "question": "Which models outperformed OpenAI's code-davinci-002 on Verilog questions?\n", "answer": "Thakur et al., 2023 showed that fine-tuned open-source LLMs (CodeGen) on Verilog datasets outperformed state-of-the-art OpenAI models such as code-davinci-002 on 17 Verilog questions.", "source": "ChipNemo.pdf", "id": "cdf1ac39e3"}, {"context": "and analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence\nidentification, atomic fact generation, and fact verification. The evaluation metric involves fine-\ngrained object hallucination categories, including entity, count, color, relation, and other attributes.\nThe final computation of FaithScore is the ratio of hallucinated content.\nBingo [ 21]Bingo (Bias and Interference Challenges in Visual Language Models) is a benchmark\nspecifically designed for assessing and analyzing the limitations of current popular MLLMs, such as\nGPT-4V [ 83]. It comprises 190 failure instances, along with 131 success instances as a comparison.\nThis benchmark reveals that state-of-the-art MLLMs show the phenomenon of bias and interference.\nBias refers to the model\u2019s susceptibility to generating hallucinatory outputs on specific types of\nexamples, such as OCR bias, region bias, etc. Interference refers to scenarios in which the judgment", "question": "What is the name of the benchmark used to assess the limitations of machine language learning models?\n", "answer": "Bingo (Bias and Interference Challenges in Visual Language Models)", "source": "hallucination.pdf", "id": "db8870dfa6"}, {"context": "Vary-toy [27],Mipha [32],\nVL-Mamba [18],Tiny-LLaV A [23]\nShareGPT4V-PT [55] Image 1246K 1246K 1246K Tiny-LLava [23],MobileVLM V2 [17]\nShareGPT4 [55] Image 100K 100K 100K ALLaV A [29]\nBunny-pretrain-LAION-2M[24] Image 2M 2M 2M Bunny [24]\nALLaV A-Caption-4V [29] Image 715K 715K 715K Mini-Gemini [26], ALLaV A[29]\nMMC4 (Interleaved) [171] Image 571M 43B 101.2M (Instances) DeepSeek-VL [34]\nObelics (Interleaved)[172] Image 353M 115M 141M (Instances) MM1[30]\nTable 2: The statistics for common MLLM PT datasets.#.X represents the quantity of X, #.T repre-\nsents the quantity of Text, and #.X-T represents the quantity of X-Text pairs, where X can be Image,\nVideo, or Audio.\nA growing number of studies have investigated the production of high-quality fine-grained pre-\ntrained data by leveraging powerful MLLMs like GPT-4V . These datasets typically offer more de-\ntailed and accurate image descriptions compared to their coarse-grained counterparts, enabling a", "question": "What is the quantity of text in the ALLaV A dataset?\n", "answer": "The quantity of text in the ALLaV A dataset is 29.\n\n(This answer is derived from the context by looking at Table 2, where it indicates that for the ALLaV A dataset, #.T represents the quantity of Text, and the value is 29.)", "source": "multimodal.pdf", "id": "25b0c0255a"}, {"context": "Data quality relevant to hallucinations can be further categorized into the following three facets.\n\u2022Noisy data. As mentioned in the definition section, training MLLMs involves two stages. The\npre-training stage employs image-text pairs crawled from the web, which contain inaccurate,\nmisaligned, or corrupted data samples. The noisy data would limit the cross-modal feature\nalignment [ 117,120], which serves as the foundation of MLLMs. As for the instruction tuning\ndata, prevalent methods, such as LLaVA [ 75], utilize the advanced GPT-4 [ 82] model to\ngenerate instructions. However, ChatGPT is a language model that cannot interpret visual\ncontent, leading to the risk of noisy data. Moreover, language models themselves suffer\nfrom the issue of hallucination [ 44], further increasing the risk. LLaVA-1.5 [ 74] adds human\nannotated QA data into instruction following and shows improved results, revealing the\neffect of noisy data.", "question": "What is one negative impact of noisy data on multimodal large language models?\n", "answer": "Noisy data can limit cross-modal feature alignment, which is the foundation of multimodal large language models.", "source": "hallucination.pdf", "id": "dcdb797076"}, {"context": "DeepSeek-VL [34] DeepSeek-LLM-1B - - - - - 32.2/- 31.1 - - 64.6 -/66.7 87.6 - 36.8\nKarmaVLM[71] Qwen1.5-0.5B - - 53.86 45.25 - - - - - 55.8 - - 47.5 -\nmoondream2[72] Phi-1.5(1.3B) 77.7 61.7 - 49.7 - - - - - - - - - -\nBunny-v1.1-4B[24] Phi-3-Mini-4K 81.7 63.4 76.3 - - 40.2/38.8 - 1503.9 362.9 74.1 64.6/71.7 87.0 - -\nTable 4: Comparison of mainstream MLLMs and efficient MLLMs on 14 VL benchmarks.\nVQAv2[57]; VQAT: TextVQA [58]; GQA [59]; SQAI: ScienceQA-IMG [188]; VizWiz [189];\nMMMU [190]; MathV: MathVista [191]; MMEP/C: the Perception/Cognition split of MME [60];\nMMB: MMBench [61]; SEED: SEED-Bench [192]; POPE [62]; LLaV AW: LLaV A-Bench (In-the-\nWild) [7]; MM-Vet [193]. The two numbers reported in MMMU denote the performance on the\nval and test split, respectively. The two numbers reported in SEED denote the performance on the\nwhole SEED-Bench and the image part, respectively.\u2020denotes the combined points of two splits.", "question": "What is the performance of Phi-3-Mini-4K on the VizWiz benchmark?\n", "answer": "1503.9", "source": "multimodal.pdf", "id": "db4bd7ca64"}, {"context": "reduce hallucination. Visual context refers to the visual tokens that can be grounded from the\ngenerated text response. An oracle study showed that decoding from the provided optimal visual\ncontexts eliminates over 84.5% of hallucinations. Based on the insight and observation, the authors\ndesigned mechanisms to locate the fine-grained visual information to correct each generated\ntoken that might be hallucinating. This is essentially a visual content-guided decoding strategy.\nIn addition to token-level correction, HALC also incorporates a matching-based beam search that\nutilizes a visual matching score to steer the generation of the final outputs, balancing both object\nhallucination mitigation and text generation quality.\nOthers. The work of OPEAR [ 45] makes an interesting observation that most hallucinations\nare closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,", "question": "How does providing optimal visual contexts reduce hallucinations in text generation?\n", "answer": "Providing optimal visual contexts eliminates over 84.5% of hallucinations in text generation, as shown in an oracle study.", "source": "hallucination.pdf", "id": "31eefbd9eb"}, {"context": "tions and responses following [7], where the instruction Xt\ninstruct at the t-th turn as:\nXt\ninstruct =\u001aRandomly choose [X1\nq, Xv]or[Xv, X1\nq], the first turn t= 1\nXt\nq. the remaining turns t >1(5)\n18", "question": "What is the action for the first turn in the given context?\n", "answer": "The action for the first turn is to randomly choose either X1 followed by Xq or Xq followed by X1.", "source": "multimodal.pdf", "id": "c83420cefd"}, {"context": "models: LLaMA2 7B/13B/70B. Each DAPT model is ini-\ntialized using the weights of their corresponding pretrained\nfoundational base models. We name our domain-adapted\nmodels ChipNeMo . We employ tokenizer augmentation\nas depicted in Section 2.1 and initialize embedding weight\naccordingly (Koto et al., 2021). We conduct further pre-\ntraining on domain-specific data by employing the standard\nautoregressive language modeling objective. All model\ntraining procedures are conducted using the NVIDIA NeMo\nframework (Kuchaiev et al., 2019), incorporating techniques\nsuch as tensor parallelism (Shoeybi et al., 2019) and flash\nattention (Dao et al., 2022) for enhanced efficiency.\nOur models undergo a consistent training regimen with\nsimilar configurations. A small learning rate of 5\u00b710\u22126\nis employed, and training is facilitated using the Adam\noptimizer, without the use of learning rate schedulers. The\nglobal batch size is set at 256, and a context window of 4096", "question": "What is the learning rate used for training the ChipNeMo models?\n", "answer": "The learning rate used for training the ChipNeMo models is 5\u00b710\u22126.", "source": "ChipNemo.pdf", "id": "7eb44773ae"}, {"context": "enhance diversity. Additionally, weight distillation over self-attention is utilized to transfer knowl-\nedge from large-scale ViT models to compact models with multiplexed weights.\nHeteromorphic KDs involves transferring knowledge between models with differing architec-\ntures. For example, DearKD [119] proposes a novel two-stage framework, DearKD, departing from\ntraditional methods for ViT architectures. In the first stage, they use a vanilla KD strategy to transfer\nCNN features to the ViT student model, representing a heteromorphic transfer. In the subsequent\nphase, if real samples are limited, they introduce a boundary-preserving intra-divergence loss to en-\nhance the process. Similarly, CiT [120] proposes a heteromorphic KD strategy, where knowledge\nis transferred from diverse models, such as a CNN and an involution neural network, resulting in\nimproved performance for the ViT student model.\n3.4 Quantization", "question": "How does DearKD transfer knowledge heteromorphically to a ViT student model?\n", "answer": "DearKD transfers knowledge heteromorphically to a ViT student model by using a vanilla KD strategy to transfer CNN features in the first stage.", "source": "multimodal.pdf", "id": "c69dafbf7a"}, {"context": "3.1.1 Quantity. Deep learning models are data-hungry, especially large models like MLLMs. The\namount of data plays an important role in building robust and reliable MLLMs. Currently, image-text\npair datasets [ 92] and visual QA [ 48,80] data are used for training MLLMs. Although these datasets\nare usually larger than typical datasets in computer vision, they are still far less abundant than the\ntext-only data used for training LLMs in terms of quantity. Insufficient data could potentially lead\nto problematic cross-modal alignment, resulting in hallucinations [96, 103].\n3.1.2 Quality. Given the increasing demand for large-scale training data, heuristic data collection\nmethods are employed to efficiently gather vast volumes of data. While these methods provide\nextensive data, they offer no guarantee of quality, thereby increasing the risk of hallucinations.\nData quality relevant to hallucinations can be further categorized into the following three facets.", "question": "What is a potential consequence of insufficient data quality in building multimodal large language models (MLLMs)?\n", "answer": "Insufficient data quality could potentially lead to problematic cross-modal alignment, resulting in hallucinations.", "source": "hallucination.pdf", "id": "77ce09f375"}, {"context": "call rate in large-scale knowledge base scenarios, and how\nto ensure enterprise data security, such as preventing LLMs\nfrom being induced to disclose the source, metadata, or\nother information of documents, are crucial issues that need\nresolution [Alon et al. , 2022 ].\nHorizontal expansion of RAG\nResearch on RAG has rapidly expanded in the horizontal\nfield. Starting from the initial text question answering do-\nmain, RAG\u2019s ideas have gradually been applied to more\nmodal data, such as images, code, structured knowledge, au-\ndio and video, and so on. There are already many works in\nthis regard.\nIn the image field, the propozhiyosal of BLIP-\n2[Liet al. , 2023a ], which uses frozen image encoders\nand large-scale language models for visual language\npre-training, has lowered the cost of model training. Addi-\ntionally, the model can generate image-to-text conversions\nfrom zero samples. In the field of text generation, the\nVBR [Zhuet al. , 2022 ]method is used to generate images to", "question": "What is a method that uses frozen image encoders and large-scale language models for visual language pre-training?\n", "answer": "BLIP-2 (Liu et al., 2023a)", "source": "RAG.pdf", "id": "5eafc6eb9a"}, {"context": "to quantized values, the quantization error is significantly reduced under certain conditions. This\ntechnique successfully modifies heavy-tailed activation distributions to fit a given quantizer.\nQuantization-Aware Training (QAT) integrates quantization into the training cycle. This in-\ntegration is particularly advantageous when scaling down to ultra-low bit precision, such as 4 bits\nor lower, where PTQ struggles with significant performance loss. For example, Quantformer [124]\nleverages entropy information to maintain consistency in self-attention ranks and introduces a dif-\nferentiable search mechanism to optimally group patch feature dimensions, reducing rounding and\nclipping inaccuracies. Q-ViT [126] incorporates a distillation token and Information Rectification\nModule (IRM) to counteract altered distributions in quantized attention modules. TerViT [127] and\nBit-shrinking [125] progressively reduce model bit-width while regulating sharpness to maintain", "question": "How does Quantformer reduce quantization error during training?\n", "answer": "Quantformer reduces quantization error during training by leveraging entropy information to maintain consistency in self-attention ranks and introducing a differentiable search mechanism to optimally group patch feature dimensions, reducing rounding and clipping inaccuracies.", "source": "multimodal.pdf", "id": "900b3dde3f"}, {"context": "6 CHALLENGES AND FUTURE DIRECTIONS\nThe research of hallucination in MLLMs is still at early stage, remaining a variety of research\nproblems to be explored. In this section, we delve into the challenges and future directions of this\npivotal domain.\n6.1 Data-centric Challenges and Innovations\nThe reliance of MLLMs on large volumes of data presents significant challenges in terms of data\nquality, diversity, and bias. In Sec. 3.1, previous works have identified several core issues that may\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What are the significant challenges in MLLMs regarding data quality, diversity, and bias?\n", "answer": "The significant challenges in MLLMs regarding data quality, diversity, and bias are related to the reliance of MLLMs on large volumes of data.", "source": "hallucination.pdf", "id": "31219ed1ce"}, {"context": "the prompt together with the question, which grounds the\nLLM to produce more accurate answers. We find that using\na domain adapted language model for RAG significantly\nimproves answer quality on our domain specific questions.\nAlso, we find that fine-tuning an off-the-shelf unsupervised\npre-trained dense retrieval model with a modest amount\nof domain specific training data significantly improves re-\ntrieval accuracy. Our domain-adapted RAG implementation\ndiagram is illustrated on Figure 3.\nFigure 3: RAG Implementation Variations\nWe created our domain adapted retrieval model by fine-\ntuning the e5small unsupervised model (Wang et al., 2022)\nwith 3000 domain specific auto-generated samples using the\nTevatron framework (Gao et al., 2022). We refer readers to\nthe details on the sample generation and training process in\nAppendix A.8.\nEven with the significant gains that come with fine-tuning a\nretrieval model, the fact remains that retrieval still struggles", "question": "What model was used as the base for the domain adapted retrieval model?\n", "answer": "The e5small unsupervised model (Wang et al., 2022) was used as the base for the domain adapted retrieval model.", "source": "ChipNemo.pdf", "id": "ad55562468"}, {"context": "Former [97] analyzed ViT-based model architectures and operators, introducing a dimension-\nconsistent pure transformer paradigm and employing latency-driven slimming to produce optimized\nmodels. Additionally, EfficientFormerV2 [98] proposed a supernet with low latency and high pa-\nrameter efficiency.\nArchitecture Search Methods involve employing neural architecture search algorithms [113]\nto explore and discover compact architectures tailored to specific tasks or constraints. For in-\nstance, Autoformer [99] intertwined weights within layers, enabling thorough training of thousands\nof subnets. NASViT [100] introduced a gradient projection algorithm, switchable layer scaling,\nand streamlined data augmentation, enhancing convergence and performance. Additionally, TF-\nTAS [101] investigated training-free architecture search methods, proposing an efficient scheme.\nUniNet [102] introduced context-aware down-sampling modules improving information accommo-", "question": "What is a feature of the Autoformer architecture search method?\n", "answer": "The Autoformer architecture search method intertwines weights within layers, enabling thorough training of thousands of subnets.", "source": "multimodal.pdf", "id": "6ed104ce6b"}, {"context": "The results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We\nobserve that the MLLMs\u2019 performance is not always consistent across different benchmarks. It\nindicates that different benchmarks have different evaluation dimensions and emphases.\nTable 2. Comparison of mainstream MLLMs on generative benchmarks. The numbers come from the original\npapers of these benchmarks.\nModel LLM SizeCHAIR\n(On AMBER)\u2193AMBER\nScore\u2191HallusionBench\nAll-Acc\u2191FaithScore\n(LLaVA-1k)\u2191FaithScore\n(COCO-Cap)\u2191Hal-Eval\nIn-domain\nGen. Acc\u2191Hal-Eval\nOut-of-domain\nGen. Acc\u2191\nmPLUG-Owl [111] 7B 23.1 54.1 43.93 0.7167 0.8546 27.3 29.5\nMultimodal-GPT [28] 7B - - - 0.5335 0.5440 - -\nInstructBLIP [22] 7B 10.3 86.2 45.26 0.8091 0.9392 35.5 41.3\nGPT-4V [83] - 4.3 92.7 65.28 - - - -\nLLaVA (7B) [75] 7B 13.5 69.3 - - - 23.3 26.3\nLLaVA (13B) [75] 13B - - - 0.8360 0.8729 - -\nMiniGPT-4 (7B) [138] 7B - - 35.78 0.5713 0.6359 61.4 50.1\nMiniGPT-4 (13B) [138] 13B 15.9 76.7 - - - - -", "question": "Which model has the highest FaithScore on LLaVA-1k among all 7B models?\n", "answer": "InstructBLIP has the highest FaithScore on LLaVA-1k among all 7B models, with a score of 0.8091.", "source": "hallucination.pdf", "id": "3939d93618"}, {"context": "from the text collection instead of selecting from an indepen-\ndent vocabulary. COG demonstrates superior performance\nto RETRO in various aspects, including question-answering,\ndomain adaptation, and expanded phrase indexing.\nOn the other hand, following the discovery of the scal-\ning law, there has been a rapid increase in model parameters,\nmaking autoregressive models the mainstream. Researchers\nare also exploring whether larger models can be pretrained\nusing the RAG approach. RETRO++ [Wang et al. , 2023a ], an", "question": "In what area does COG demonstrate superior performance compared to RETRO?\n", "answer": "COG demonstrates superior performance to RETRO in question-answering, domain adaptation, and expanded phrase indexing.", "source": "RAG.pdf", "id": "c2dce2386a"}, {"context": "cally for the fine-tuning process of embedding models, signif-\nicantly streamlining this procedure. By preparing a corpus of\ndomain knowledge and utilizing the methods it provides, we\ncan easily obtain the specialized embedding model tailored to\nour desired domain.\nFine-tuning of downstream tasks It is equally im-\nportant to adapt Embedding models to downstream tasks.\nWhen using RAG in downstream tasks, some works have\nfine-tuned Embedding models by using the capabilities\nof LLMs.PROMPTAGATOR [Daiet al. , 2022 ]utilizes the\nLarge Language Model (LLM) as a few-shot query gener-\nator and creates task-specific retrievers based on the gen-\nerated data, and alleviates the problem of supervised fine-\ntuning, which is difficult in some domains due to data\nscarcity.LLM-Embedder [Zhang et al. , 2023a ]uses the Large\nLanguage Model to output reward values for data from mul-\ntiple downstream tasks, fine-tuning the retriever with two dif-", "question": "How does PromptTagator utilize Large Language Models (LLMs) for fine-tuning embedding models in downstream tasks?\n", "answer": "PromptTagator uses LLMs as a few-shot query generator to create task-specific retrievers based on the generated data, alleviating the problem of supervised fine-tuning which is difficult in some domains due to data scarcity.", "source": "RAG.pdf", "id": "0886fad261"}, {"context": "Thoughts (PoT) learning and Visual Token Merging strategy while excelling in faster inference\nspeed at the same time. TextHawk [36] explores efficient fine-grained perception by designing four\ndedicated components to address challenges posed by document-oriented tasks. HRVDA [66] and\nMonkey [65] are also large multimodal models designed to address the challenges posed by high-\nresolution requirements in visual document understanding tasks.\n7.3 Video Comprehension\nVideos provide an impressively accurate representation of how humans continuously perceive the\nvisual world. Intelligent video understanding is vital for a variety of real-world applications, in-\ncluding video category classification, video captioning, and video-text retrieval. Several works like\nvideoChat [197] and Video-LLaMA [198] are LLM-based large multimodal models for end-to-end\nchat-centric video comprehension. However, these methods can only take in a limited number of\nframes for short video understanding.", "question": "What is a limitation of videoChat and Video-LLaMA models in video comprehension?\n", "answer": "These models can only understand a limited number of frames for short video durations.", "source": "multimodal.pdf", "id": "73fba2ab9b"}, {"context": "22 Bai, et al.\ncause hallucination. In order to improve the accuracy and reliability of hallucinated content, it is\ncrucial to ensure that MLLMs have access to high-quality and diverse training data. Future research\nshould focus on developing techniques for data collection, augmentation, and calibration. Firstly,\ncollecting enough data at the initial stage is crucial to address the data scarcity issue and increase\ndata diversity. Secondly, data augmentation is an effective solution to further expand the size of data.\nFinally, exploring methods for re-calibrating existing datasets is crucial. This includes eliminating\nbiases, promoting diversity and inclusivity, and mitigating other potential issues that may induce\nhallucinations.\n6.2 Cross-modal Alignment and Consistency\nThe key challenge of multimodal hallucination is the cross-modal consistency issue. Ensuring that\ngenerated content remains consistent and contextually relevant to the input modality requires", "question": "How can the issue of multimodal hallucination be addressed in terms of cross-modal consistency?\n", "answer": "The issue of multimodal hallucination can be addressed in terms of cross-modal consistency by ensuring that generated content remains consistent and contextually relevant to the input modality.", "source": "hallucination.pdf", "id": "7ed2952b17"}, {"context": "PandaGPT [95] 7B 70.00 50.00 50.00 50.00 - - - - - - - -\nLaVIN [78] 13B 185.00 88.33 63.33 75.00 - - - - - - - -\nCheetor [67] 7B 180.00 96.67 80.00 116.67 - - - - - - - -\nGPT-4V [83] - 190.00 160.00 95.00 150.00 - - - - 89.6 92.7 - -\nLLaVA [75] (7B) 7B - - - - - - - 73.3 32.0 69.3 35.1 14.0\nLLaVA [75] (13B) 13B 185.00 155.00 133.33 170.00 68.65 67.72 66.98 71.8 - - - -\nLRV-Instruction [73] 7B 165.00 111.67 86.67 165.00 - - - - - - - -\nLynx [122] 7B 195.00 151.67 90.00 170.00 - - - - - - - -\nMMICL [130] 11B 170.00 160.00 81.67 156.67 - - - - - - - -\nMuffin [118] 13B 195.00 163.33 66.67 165.00 - - - - - - - -\nOtter [65] 7B 195.00 88.33 86.67 113.33 - - - - - - - -\nQwen-VL-Chat [2] 7B 158.33 150.00 128.33 170.00 - - - - - - - -\nSPHINX [71] 13B 195.00 160.00 153.33 160.00 - - - - - - - -\nVPGTrans [124] 7B 70.00 85.00 63.33 73.33 - - - - - - - -\nBLIVA [43] 11B 180.00 138.33 81.67 180.00 - - - - - - - -\nInfMLLM [135] 13B 195.00 145.00 170.00 195.00 - - - - - - - -", "question": "What is the parameter size of GPT-4V?\n", "answer": "The parameter size of GPT-4V is not specified in the context. The dashes (-) in the parameter size column indicate missing data.", "source": "hallucination.pdf", "id": "51df4d7076"}, {"context": "Model LLM Backbone VQAv2GQA SQAIVQATVizWiz MMMU MathV MMEPMMECMMB SEED POPE LLA V AWMM-Vet\nFlamingo [16] Chinchilla-7B - - - - 28.8 - - - - - - - - -\nBLIP-2 [15] Flan-T5XXL(13B) 65.0 44.7 61.0 42.5 19.6 - - 1293.8 290.0 - -/46.4 85.3 38.1 22.4\nLLaV A [7] Vicuna-13B - 41.3 - 38.9 - - - - - - - - - -\nMiniGPT-4 [10] Vicuna-13B - 30.8 - 19.4 - - - - - - - - - -\nInstructBLIP [8] Vicuna-13B - 49.5 63.1 50.7 33.4 - - 1212.8 291.8 - - 78.9 58.2 25.6\nQwen-VL-Chat [187] Qwen-7B 78.2\u221757.5\u221768.2 61.5 38.9 35.9/32.9 - 1487.5 360.7 60.6 -/58.2 - - -\nLLaV A-1.5 [54] Vicuna-1.5-13B 80.0\u221763.3\u221771.6 61.3 53.6 - - 1531.3 295.4 67.7 -/68.2 85.9 70.7 35.4\nMiniGPT-v2-Chat [9] LLaMA-2-Chat-7B - 58.8 - 52.3 42.4 - - - - - - - - -\nInternVL-Chat [5] Vicuna-13B 81.2\u221766.6\u2217- 61.5 58.5 - - 1586.4 - - - 87.6 - -\nEmu2-Chat [6] LLaMA-33B 84.9\u221765.1\u2217- 66.6\u221754.9 -/34.1 - - - - 62.8 - - 48.5\nGemini Pro [2] - 71.2 - - 74.6 - 47.9/\u2013 45.2 - 436.79 73.6 \u2013/70.7 - - 64.3", "question": "What is the score of LLaMA-2-Chat-7B in the InstructBLIP benchmark?\n", "answer": "The score of LLaMA-2-Chat-7B in the InstructBLIP benchmark is 58.8.", "source": "multimodal.pdf", "id": "88e0397250"}, {"context": "Preprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "When was the first issue of Preprint published?\n", "answer": "The first issue of Preprint was published in April 2024.", "source": "hallucination.pdf", "id": "6158839d4c"}, {"context": "expressiveness and generalization capabilities. Adapter-based tuning introduces lightweight adapter\nmodules into the pre-trained model\u2019s architecture. These adapter modules, typically composed of\nfeed-forward neural networks with a small number of parameters, are inserted between the layers\nof the original model. During fine-tuning, only the adapter parameters are updated, while the pre-\ntrained model\u2019s parameters remain fixed. This method significantly reduces the number of trainable\nparameters, leading to faster training and inference times without compromising the model\u2019s per-\nformance. LLM-Adapters [154] presents a framework for integrating various adapters into large\nlanguage models, enabling parameter-efficient fine-tuning for diverse tasks. This framework en-\n16", "question": "How are the parameters of a pre-trained model updated during adapter-based tuning?\n", "answer": "During adapter-based tuning, only the adapter parameters are updated, while the pre-trained model\u2019s parameters remain fixed.", "source": "multimodal.pdf", "id": "004ffc5dd9"}, {"context": "hallucinated content in practical applications.\n6.7 Navigating the Ethical Landscape\nAs MLLMs become increasingly proficient at generating realistic text, ethical considerations sur-\nrounding the use of generated content become paramount. Especially in the context of hallucination,\nthe generated response may contain severely concerning ethical content, amplifying the importance\nof the problem. Addressing ethical concerns related to misinformation, bias, privacy, and societal\nimpact is crucial for promoting responsible AI practices in the development and deployment of\nMLLMs. In addition to addressing typical object hallucination, future research on MLLM hallucina-\ntions should prioritize ethical considerations throughout the entire lifecycle of MLLM development,\nfrom data collection and model training to deployment and evaluation.\n7 CONCLUSION\nBased on powerful large language models, multimodal large language models demonstrate remark-", "question": "What are some ethical considerations when using hallucinated content in multimodal large language models?\n", "answer": "Ethical considerations include misinformation, bias, privacy, and societal impact. It's important to address these concerns throughout the entire lifecycle of MLLM development.", "source": "hallucination.pdf", "id": "70b1de1850"}, {"context": "7.2 Document Understanding\nDocuments or charts serve as a crucial source of information, offering an intuitive visualization\nof data in various forms. They have become an indispensable part of information dissemination,\nbusiness decision-making, and academic research. However, current chart understanding models\nstill face two primary limitations: (1) The considerable number of parameters makes training and\ndeployment challenging. For instance, ChartLlama [196], a 13-billion-parameter model, is difficult\nto deploy on a single consumer-grade GPU. (2) These models struggle with efficiently encoding\nhigh-resolution images, as vision transformers tend to produce lengthy feature sequences.\nTo address the challenges of fine-grained visual perception and visual information compression for\ndocument-oriented MLLMs. TinyChart [37] outperforms several 13B MLLMs with Program-of-\nThoughts (PoT) learning and Visual Token Merging strategy while excelling in faster inference", "question": "Which model outperforms several 13 billion parameter MLLMs in document understanding?\n", "answer": "TinyChart [37]", "source": "multimodal.pdf", "id": "6b47636d3a"}, {"context": "SPHINX-X are activated. Cobra[13] also argues that the initial phase of pre-alignment may not\nbe requisite, with the model remaining underfitted even post-finetuning. Consequently, it discards\nthe pre-alignment stage, opting instead to directly finetune the entire SLM backbone along with the\nprojector. TinyGPT-V[28] training process consists of four stages: an initial pre-training stage for\nvision-language understanding, a second stage for refining image modality processing, a third stage\nfor human-like learning through fine-tuning, and a fourth stage for multi-task learning to enhance\nits conversational abilities as a chatbot.\n5.4 Parameter Efficient Transfer Learning\nSeveral studies adopt Parameter-Efficient Fine-Tuning (PEFT) techniques for transfer learning, like\nLoRA [161], to safeguard against the loss of pre-trained knowledge. Efficient Attention Skipping\n(EAS) module[52] proposes a novel parameter and computation-efficient tuning method for MLLMs", "question": "What is one technique used for parameter-efficient fine-tuning in transfer learning?\n", "answer": "LoRA (Layer-wise Relevance Analysis) [161] is one technique used for parameter-efficient fine-tuning in transfer learning.", "source": "multimodal.pdf", "id": "374490e448"}, {"context": "into three types: object category ,object attribute , and object relation . An example of the three types\nof hallucination is shown in Fig. 3.\n\u2022Category. MLLMs identify nonexistent object categories or incorrect categories in the given\nimage. For example, in Fig. 3, \"some benches and a fence\", \"some clouds\", described in the\ntext response do not exist in the given image.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What are the three types of hallucination identified in the context?\n", "answer": "The three types of hallucination identified in the context are object category, object attribute, and object relation.", "source": "hallucination.pdf", "id": "ac4a6e18c3"}, {"context": "Figure 5: MobileVLM v2 [17] and Honeybee [19] efficient vision-language projector.\n2.3 Small Language Model\nThe pre-trained small language model(SLM) serves as the core component of MLLMs, endowing\nit with many outstanding capabilities, such as zero-shot generalization, instruction following, and\nin-context learning. The SLM accepts input sequences containing multiple modalities and outputs\ncorresponding text sequences. A text tokenizer is typically bundled with the SLM, mapping text\nprompts Xqto the text tokens Hq. The text tokens Hqand the visual tokens Hvare concatenated as\nthe input of the language model, which outputs the final response sequence Yain an autoregressive\nmanner:\np(Ya|Hv, Hq) =LY\ni=1p(yi|Hv, Hq, y<i), (3)\nwhere Ldenotes the length of Ya. As the SLM contributes the vast majority of MLLM parameters,\nits selection is closely related to the lightweight nature of MLLM. In comparison to conventional", "question": "What is the core component of MLLMs that allows for zero-shot generalization, instruction following, and in-context learning?\n", "answer": "The pre-trained small language model (SLM) is the core component of MLLMs that allows for zero-shot generalization, instruction following, and in-context learning.", "source": "multimodal.pdf", "id": "00e8c4ea32"}, {"context": "truth captions. However, it may risk expressing details that it cannot discern from the image, and\ntherefore exhibit hallucinations. Thus, the authors explored approaches to enhance the model\u2019s\nend-of-sequence (EOS) decision-making process, ensuring timely termination when it reaches the\nperception limit. Regarding data, this work proposes a data filtering strategy to eliminate harmful\ntraining data that could impair the model\u2019s ability to end sequences.\n5.2 Model\n5.2.1 Scale-up Resolution. Enhancing the perception ability of MLLMs has been shown to improve\ntheir overall performance and reduce hallucination [ 14,74,75,123]. One important update when\nupgrading from LLaVA [ 75] to LLaVA-1.5 [ 74] is to scale up the CLIP ViT vision encoder from\nCLIP-ViT-L-224 to CLIP-ViT-L-336, resulting in considerable performance improvement. Qwen-\nVL [ 2] has shown the effectiveness of gradually enlarging image resolution from 224\u00d7224to", "question": "How does scaling up the resolution of the CLIP ViT vision encoder improve the performance of MLLMs?\n", "answer": "Scaling up the CLIP ViT vision encoder from CLIP-ViT-L-224 to CLIP-ViT-L-336 enhances the perception ability of MLLMs, leading to improved overall performance and reduced hallucination.", "source": "hallucination.pdf", "id": "7d6e083404"}, {"context": "Token Processing Techniques designed to process lengthy visual token squence are critical in ef-\nficient MLLMs as they address the dual challenges of preserving fine-grained details and reducing\ncomputational complexity. LLaV A-UHD [35] presents a novel approach to manage the computa-\ntional burden associated with high-resolution images. It puts forward two key components: (1) a\ncompression module that further condenses image tokens from visual encoders, significantly re-\nducing the computational load, and (2) a spatial schema to organize slice tokens for LLMs. No-\ntably, LLaV A-UHD demonstrates its efficiency by supporting 6 times larger resolution images using\nonly 94% of the inference computation compared to previous models. Furthermore, the model\ncan be efficiently trained in academic settings, completing the process within 23 hours on 8 A100\nGPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-", "question": "How much computation does LLaV A-UHD use compared to previous models when supporting 6 times larger resolution images?\n", "answer": "LLaV A-UHD uses 94% of the inference computation compared to previous models when supporting 6 times larger resolution images.", "source": "multimodal.pdf", "id": "986687f08e"}, {"context": "the latest automatic evaluation framework. Finally,\npotential future research directions are introduced\nfrom three aspects: vertical optimization, horizon-\ntal scalability, and the technical stack and ecosys-\ntem of RAG.1\n1 Introduction\nThe large language models (LLMs) are more pow-\nerful than anything we have seen in Natural Lan-\nguage Processing (NLP) before. The GPT series\n\u2217Corresponding Author\n1Resources are available at: https://github.com/Tongji-KGLLM/\nRAG-Surveymodels [Brown et al. , 2020, OpenAI, 2023 ], the LLama series\nmodels [Touvron et al. , 2023 ], Gemini [Google, 2023 ], and\nother large language models demonstrate impressive lan-\nguage and knowledge mastery, surpassing human benchmark\nlevels in multiple evaluation benchmarks [Wang et al. , 2019,\nHendrycks et al. , 2020, Srivastava et al. , 2022 ].\nHowever, large language models also exhibit\nnumerous shortcomings. They often fabricate\nfacts [Zhang et al. , 2023b ]and lack knowledge when", "question": "Which models demonstrate impressive language and knowledge mastery in the latest automatic evaluation framework?\n", "answer": "The GPT series models, the LLama series models, and Gemini demonstrate impressive language and knowledge mastery in the latest automatic evaluation framework.", "source": "RAG.pdf", "id": "7a7ee00ab2"}, {"context": "HaELM [ 104]Most LLM-based evaluation benchmarks employ advanced ChatGPT or GPT-4\nmodels to assess the quality of the MLLM response. In contrast, the work of Hallucination Evaluation\nbased on Large Language Models (HaELM) proposes to train a specialized LLM for hallucination\ndetection. It collects a set of hallucination data generated by a wide range of MLLMs, simulates data\nusing ChatGPT, and trains an LLM based on LLaMA [ 99]. After that, the HaELM model becomes\nproficient in hallucination evaluation, leveraging reference descriptions of images as the basis of\nassessment.\nFaithScore [ 55]Considering the natural forms of interaction between humans and MLLMs,\nFaithScore aims to evaluate free-form responses to open-ended questions. Different from LLM-based\noverall assessment, FaithScore designs an automatic pipeline to decompose the response, evaluate,\nand analyze the elements in detail. Specifically, it includes three steps: descriptive sub-sentence", "question": "How does FaithScore evaluate free-form responses from MLLMs?\n", "answer": "FaithScore evaluates free-form responses from MLLMs by decomposing the response into elements, evaluating, and analyzing them in detail.", "source": "hallucination.pdf", "id": "23d981a684"}, {"context": "whole SEED-Bench and the image part, respectively.\u2020denotes the combined points of two splits.\n\u2217indicates that training images of the datasets are observed during training.The reddenotes the\nhighest result of efficient MLLMs, and the blue denotes that of large-scale MLLMs.\n7 Applications\nFrom the preceding analysis, it\u2019s clear that many efficient MLLM approaches evaluate their perfor-\nmances across a range of scenarios, like VQA, visual grounding, image segmentation, etc. However,\nit\u2019s also crucial to explore these efficient architectures in well-established tasks to achieve their ul-\ntimate performance. Therefore, we have chosen to introduce several downstream tasks, such as\nmedical analysis, document understanding, and video comprehension.\n7.1 Biomedical Analysis\nDue to the high cost of annotating biomedical data, foundation models are poised to become a new\nparadigm in biomedicine, achieving state-of-the-art results on many applications, including medical", "question": "In what field are foundation models achieving state-of-the-art results due to the high cost of annotating data?\n", "answer": "Biomedicine", "source": "multimodal.pdf", "id": "cd7157ebfa"}, {"context": "els. In comparison with them, this paper aims to systemati-\ncally outline the entire process of Retrieval-Augmented Gen-\neration (RAG) and focuses specifically on research related to\naugmenting the generation of large language models through\nknowledge retrieval.\nThe development of RAG algorithms and models is il-\nlustrated in Fig 1. On a timeline, most of the research re-\nlated to RAG emerged after 2020, with a significant turn-\ning point in December 2022 when ChatGPT was released.\nSince the release of ChatGPT, research in the field of natu-\nral language processing has entered the era of large models.\nNaive RAG techniques quickly gained prominence, leading\nto a rapid increase in the number of related studies.In terms\nof enhancement strategies, research on reinforcement during\nthe pre-training and supervised fine-tuning stages has been\nongoing since the concept of RAG was introduced. However,\nmost of the research on reinforcement during the inference", "question": "When did most of the research related to Retrieval-Augmented Generation (RAG) emerge?\n", "answer": "Most of the research related to Rrieval-Augmented Generation (RAG) emerged after 2020.", "source": "RAG.pdf", "id": "483a7b216e"}, {"context": "that require further investigation.\nFirstly, the issue of long context in RAG is a significant\nchallenge. As mentioned in the literature [Xuet al. , 2023c ],\nRAG\u2019s generation phase is constrained by the context win-\ndow of LLMs. If the window is too short, it may not contain\nenough relevant information; if it\u2019s too long, it might lead to\ninformation loss. Currently, expanding the context window\nof LLMs, even to the extent of limitless context, is a critical\ndirection in LLM development. However, once the context\nwindow constraint is removed, how RAG should adapt re-\nmains a noteworthy question.\nSecondly, the robustness of RAG is another important re-\nsearch focus. If irrelevant noise appears during retrieval, or\nif the retrieved content contradicts facts, it can significantly\nimpact RAG\u2019s effectiveness. This situation is figuratively\nreferred to as \u201dopening a book to a poisonous mushroom\u201d.", "question": "How does the length of the context window in RAG affect its performance?\n", "answer": "The performance of RAG's generation phase is constrained by the context window of LLMs. If the window is too short, it may not contain enough relevant information, and if it's too long, it might lead to information loss.", "source": "RAG.pdf", "id": "6080afb1ff"}, {"context": "event, etc., as independent hallucination categories; however, in this work, we include them into\nattribute category.\nAs numerous studies exist on the underlying causes of hallucinations in LLMs, the unique chal-\nlenges posed by cutting-edge MLLMs warrant an in-depth investigation. Our analysis specifically\ntargets the unique origins of hallucinations in MLLMs, spanning a spectrum of contributing factors\nfrom data, model, training, to the inference stage. In addition, we provide a comprehensive overview\nof benchmarks and metrics designed specifically for evaluating hallucinations in MLLMs. Then,\nwe review and discuss recent works tailored to mitigate the problem of hallucination from the\nviewpoints of the identified causes.\nThrough our comprehensive survey, we aim to contribute to advancing the field of MLLMs and\noffer valuable insights that deepen understanding of the opportunities and challenges associated", "question": "What is the focus of the analysis in the given context?\n", "answer": "The focus of the analysis in the given context is the unique origins of hallucinations in modern large-scale language models (MLLMs), spanning a spectrum of contributing factors from data, model, training, to the inference stage.", "source": "hallucination.pdf", "id": "6da15b5bb7"}, {"context": "ideas and foster the field\u2019s development.\nIn the realm of computer vision, object recognition is the core task, including sub-tasks such as\nobject classification [ 60], detection [ 27], and segmentation [ 37], etc. Similarly, studies on halluci-\nnation in MLLMs primarily focus on object hallucination. In pre-MLLM era, there is a pioneering\nwork on object hallucination in image captioning [ 90], evaluating object existence by comparing\ncaptions and image content. In MLLMs, object hallucination has been empirically categorized into\nthree categories: 1) category , which identifies nonexistent or incorrect object categories in the given\nimage; 2) attribute , which emphasizes descriptions of the objects\u2019 attributes, such as color, shape,\nmaterial, etc; and 3) relation , which assesses the relationships among objects, such as human-object\ninteractions or relative positions. Note that some literature may consider objects counting, objects", "question": "What are the three categories of object hallucination in MLLMs?\n", "answer": "The three categories of object hallucination in MLLMs are category, attribute, and relation.", "source": "hallucination.pdf", "id": "595dbaf855"}, {"context": "dle\u201d phenomenon [Liuet al. , 2023 ]. This redundant informa-\ntion can obscure key information or contain information con-\ntrary to the real answer, negatively impacting the generation\neffect [Yoran et al. , 2023 ]. Additionally, the information ob-\ntained from a single retrieval is limited in problems requiring\nmulti-step reasoning.\nCurrent methods to optimize the retrieval process mainly\ninclude iterative retrieval and adaptive retrieval. These allow\nthe model to iterate multiple times during the retrieval process\nor adaptively adjust the retrieval process to better accommo-\ndate different tasks and scenarios.\nIterative Retrieval\nRegularly collecting documents based on the original query\nand generated text can provide additional materials for\nLLMs [Borgeaud et al. , 2022, Arora et al. , 2023 ]. Providing\nadditional references in multiple iterative retrievals has im-\nproved the robustness of subsequent answer generation.\nHowever, this method may be semantically discontinuous and", "question": "How does iterative retrieval improve the robustness of answer generation in large language models?\n", "answer": "Iterative retrieval improves robustness by regularly collecting documents based on the original query and generated text, providing additional materials for large language models. The robustness is further enhanced by providing additional references in multiple iterative retrievals.", "source": "RAG.pdf", "id": "f24827ee1d"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 19\nPreference Optimization (FDPO). FDPO uses fine-grained preferences from individual examples to\ndirectly reduce hallucinations in generated text by enhancing the model\u2019s ability to distinguish\nbetween accurate and inaccurate descriptions.\nLLaVA-RLHF [ 96] also try to involve human feedback to mitigate hallucination. It extends the\nRLHF paradigm from the text domain to the task of vision-language alignment, where human\nannotators were asked to compare two responses and pinpoint the hallucinated one. The MLLM is\ntrained to maximize the human reward simulated by an reward model. To address the potential\nissue of reward hacking ,i.e.,achieving high scores from the reward model does not necessarily lead\nto improvement in human judgements, it proposes an algorithm named Factually Augmented RLHF.\nThis algorithm calibrates the reward signals by augmenting them with additional information such\nas image captions.", "question": "How does Factually Augmented RLHF address the issue of reward hacking in LLaVA-RLHF?\n", "answer": "Factually Augmented RLHF addresses the issue of reward hacking in LLaVA-RLHF by calibrating the reward signals with additional information such as image captions.", "source": "hallucination.pdf", "id": "92e73c053a"}, {"context": "improved performance for the ViT student model.\n3.4 Quantization\nViT quantization is the process of reducing the precision of numerical representations in ViT models,\ntypically transitioning from floating-point to fixed-point arithmetic [140]. This reduction in preci-\nsion aims to decrease memory usage, computational complexity, and energy consumption while\npreserving model accuracy to an acceptable level. Current research can be mainly categorized into\npost-training quantization, quantization-aware training, and hardware-aware quantization.\nPost-Training Quantization (PTQ) compresses trained ViT models by converting their param-\neters from high-precision floating-point numbers to lower-precision fixed-point numbers, such as\n8-bit integers. For example, Liu et al. [141] introduced a ranking loss method to identify opti-\nmal low-bit quantization intervals for weights and inputs, ensuring the functionality of the attention", "question": "How does post-training quantization reduce the precision of ViT models?\n", "answer": "Post-training quantization compresses trained ViT models by converting their parameters from high-precision floating-point numbers to lower-precision fixed-point numbers, such as 8-bit integers.", "source": "multimodal.pdf", "id": "354a427ccf"}, {"context": "As a knowledge-intensive task, RAG employs different tech-\nnical approaches during the language model training\u2019s pre-\ntraining, fine-tuning, and inference stages.\nPre-training Stage\nSince the emergence of pre-trained models, researchers have\ndelved into enhancing the performance of Pre-trained Lan-\nguage Models (PTMs) in open-domain Question Answering\n(QA) through retrieval methods at the pre-training stage. Rec-\nognizing and expanding implicit knowledge in pre-trained\nmodels can be challenging. REALM [Arora et al. , 2023 ]in-\ntroduces a more modular and interpretable knowledge em-\nbedding approach. Following the Masked Language Model\n(MLM) paradigm, REALM models both pre-training and\nfine-tuning as a retrieve-then-predict process, where the lan-\nguage model pre-trains by predicting masked tokens ybased\non masked sentences x, modeling P(x|y).\nRETRO [Borgeaud et al. , 2022 ]leverages retrieval aug-\nmentation for pre-training a self-regressive language model,", "question": "How does REALM model the pre-training process?\n", "answer": "REALM models the pre-training process as a retrieve-then-predict process, where the language model pre-trains by predicting masked tokens based on masked sentences, modeling P(x|y).", "source": "RAG.pdf", "id": "6240233238"}, {"context": "swers given a retrieval-enhanced directive. It updates the gen-\nerator and retriever to minimize the semantic similarity be-\ntween documents and queries, effectively leveraging relevant\nbackground knowledge.\nAdditionally, SUGRE [Kang et al. , 2023 ]introduces the\nconcept of contrastive learning. It conducts end-to-end fine-\ntuning of both retriever and generator, ensuring highly de-\ntailed text generation and retrieved subgraphs. Using a\ncontext-aware subgraph retriever based on Graph Neural Net-\nworks (GNN), SURGE extracts relevant knowledge from a\nknowledge graph corresponding to an ongoing conversation.\nThis ensures the generated responses faithfully reflect the re-\ntrieved knowledge. SURGE employs an invariant yet efficient\ngraph encoder and a graph-text contrastive learning objective\nfor this purpose.\nIn summary, the enhancement methods during the fine-\ntuning phase exhibit several characteristics. Firstly, fine-\ntuning both LLM and retriever allows better adaptation", "question": "How does SURGE minimize the semantic similarity between documents and queries?\n", "answer": "SURGE minimizes the semantic similarity between documents and queries by updating the generator and retriever during the fine-tuning phase, which leverages relevant background knowledge.", "source": "RAG.pdf", "id": "977e0e1405"}, {"context": "GPUs. LLaV A-PruMerge[41] and MADTP [42] propose an adaptive visual token reduction ap-\nproach that significantly decreases the number of visual tokens while preserving comparable model\nperformance. TinyChart [37] and TextHawk [36] focus on document-oriented tasks, with the former\nadopting the Vision Token Merging module and the latter introducing the ReSampling and ReAr-\nrangement module. These modules can enhance fine-grained visual perception and information\ncompression capabilities.\nMulti-Scale Information Fusion Utilizing multi-scale image information is indeed crucial for\nvisual feature extraction. This approach allows the model to capture both the fine-grained details\npresent in smaller scales and the broader context available in larger scales. Mini-Gemini [26] com-\nprises twin encoders, one for high-resolution images and the other for low-resolution visual em-\nbedding. It proposes Patch Info Mining, which uses low-resolution visual embeddings as queries", "question": "What is the name of the approach that uses low-resolution visual embeddings as queries in Mini-Gemini?\n", "answer": "Patch Info Mining", "source": "multimodal.pdf", "id": "8e97c297be"}, {"context": "ModelVision Encoder LLMVision-LLM ProjectorVariants Resolution Parameter Size Variants Parameter Size\nMobileVLM [20] CLIP ViT-L/14 [73] 336 0.3B MobileLLaMA[20] 2.7B LDP[20]\nLLaV A-Phi [21] CLIP ViT-L/14 [73] 336 0.3B Phi-2[74] 2.7B MLP\nImp-v1 [22] SigLIP [75] 384 0.4B Phi-2[74] 2.7B -\nTinyLLaV A [23] SigLIP-SO [75] 384 0.4B Phi-2[74] 2.7B MLP\nBunny [24] SigLIP-SO [75] 384 0.4B Phi-2[74] 2.7B MLP\nMobileVLM-v2-3B [17] CLIP ViT-L/14 [73] 336 0.3B MobileLLaMA[17] 2.7B LDPv2[17]\nMoE-LLaV A-3.6B [25] CLIP-Large [73] 384 - Phi-2[74] 2.7B MLP\nCobra [13]DINOv2 [76]\nSigLIP-SO [75]384 0.3B+0.4B Mamba-2.8b-Zephyr[77] 2.8B MLP\nMini-Gemini [26] CLIP-Large [73] 336 - Gemma[78] 2B MLP\nVary-toy [27] CLIP [73] 224 - Qwen[79] 1.8B -\nTinyGPT-V [28] EV A [80] 224/448 - Phi-2[74] 2.7B Q-Former [15]\nSPHINX-Tiny [14]DINOv2 [76]\nCLIP-ConvNeXt [81]448 - TinyLlama[82] 1.1B -\nALLaV A-Longer [29] CLIP-ViT-L/14 [73] 336 0.3B Phi-2[74] 2.7B -\nMM1-3B-MoE-Chat [30] CLIP DFN-ViT-H [83] 378 - - 3B\u2217C-Abstractor [19]", "question": "What is the resolution parameter for the Phi-2 variant in the Imp-v1 model?\n", "answer": "The resolution parameter for the Phi-2 variant in the Imp-v1 model is 384.", "source": "multimodal.pdf", "id": "14f018b2c6"}, {"context": "employs an LLM to generate sentences based on the extracted keywords. Ultimately, the framework\nproduces a set of high-quality image-caption pairs. Experiment results show that the model trained\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What is the model trained on in the described framework?\n", "answer": "The model is trained on extracted keywords to generate sentences and produce high-quality image-caption pairs.", "source": "hallucination.pdf", "id": "f5c7517032"}, {"context": "The Visual Dependent questions are defined as questions that do not have an affirmative answer\nwithout the visual context. This setting aims to evaluate visual commonsense knowledge and visual\nreasoning skills. The Visual Supplement questions can be answered without the visual input; the\nvisual component merely provides supplemental information or corrections. This setting is designed\nto evaluate visual reasoning ability and the balance between parametric memory (language prior)\nand image context. This division provides a new perspective for understanding and diagnosing\nMLLMs.\nCCEval [ 123]CCEval focuses on the hallucination evaluation of detailed captions. Traditional\ncaption-based evaluation benchmarks and metrics, like CHAIR, are known to favor short captions.\nHowever, short captions often lack detail and contain less information. To address this issue, CCEval\nrandomly samples 100 images from Visual Genome to form a benchmark. In evaluation, GPT-4", "question": "What is the purpose of CCEval in evaluating detailed captions?\n", "answer": "CCEval focuses on the hallucination evaluation of detailed captions, addressing the issue that traditional caption-based evaluation benchmarks and metrics favor short captions which often lack detail and contain less information.", "source": "hallucination.pdf", "id": "4e7d38fc3d"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 11\nthere are four popular object related subtasks in its perception evaluation, including object existence,\ncount, position, color. Similar to POPE, these tasks are formulated as Yes-or-Notasks.\nCIEM [ 42]CIEM is a benchmark to evaluate hallucination of MLLMs. Unlike previous works\nutilize human annotated objects, CIEM is generated using an automatic pipeline. The pipeline takes\nthe text description of a specific image as input and utilize advanced LLMs to generate QA pairs.\nAlthough the LLM-based data generation pipeline is not completely reliable, empirical result shows\nthat the generated data has low error rate, around 5%.\nMMHal-Bench [ 96]Comprising 96 image-question pairs, ranging in 8 question categories \u00d712\nobject topics, MMHal-Bench is a dedicated benchmark for evaluating hallucination in MLLMs. The\n8 question categories cover various types of hallucination, including object attributes, counting,", "question": "What is the error rate of the LLM-based data generation pipeline used in CIEM?\n", "answer": "The error rate of the LLM-based data generation pipeline used in CIEM is around 5%.", "source": "hallucination.pdf", "id": "004e988006"}, {"context": "interface preserves most of the information, but lacks supervision on the projected feature.\nVisualization in [ 52] reveals that the features after the projection layer remain distinct from\nthe language embeddings. The distribution gap causes trouble in cross-modal interaction,\nleading to hallucination. On the other hand, Q-former-like [ 66] architecture has diverse\nsupervision on the extracted visual feature, aligning it to the language embedding space.\nHowever, the use of learnable queries inevitably results in the loss of fine-grained visual\ninformation.\n3.3 Training\nThe training objective of MLLMs is basically the same as LLMs, i.e,auto-regressive next token\nprediction loss. This loss is straightforward yet effective and easy to scale up, showing promising\nperformance in language modeling. However, some studies in the field of MLLMs have suggested\nthat the next-token prediction loss might not be suitable for learning visual content due to its", "question": "What is the training objective of MLLMs?\n", "answer": "The training objective of MLLMs is auto-regressive next token prediction loss.", "source": "hallucination.pdf", "id": "0342fa09e6"}, {"context": "mechanism. They also conducted an analysis to understand the relationship between quantization\nloss in different layers and feature diversity, exploring a mixed-precision quantization approach\nleveraging the nuclear norm of each attention map and output feature. Additionally, PTQ4ViT [121]\nintroduced the twin uniform quantization method to minimize quantization error on activation val-\nues following softmax and GELU functions, incorporating a Hessian-guided metric to enhance cal-\nibration accuracy. APQ-ViT [122] proposed a unified Bottom-elimination Blockwise Calibration\nscheme to optimize the calibration metric, prioritizing crucial quantization errors and designing a\nMatthew-effect Preserving Quantization for Softmax to maintain the power-law character and at-\ntention mechanism functionality. NoisyQuant [123] proposes to add a fixed Uniform noisy bias\nto quantized values, the quantization error is significantly reduced under certain conditions. This", "question": "How does NoisyQuant reduce quantization error?\n", "answer": "NoisyQuant reduces quantization error by adding a fixed Uniform noisy bias to quantized values.", "source": "multimodal.pdf", "id": "b9d5e110ca"}, {"context": "MiniGPT-4 (13B) [138] 13B 15.9 76.7 - - - - -\nmPLUG-Owl2 [112] 7B 10.6 84.0 47.30 - - - -\nLLaVA-1.5 (7B) [74] 7B 8.6 82.9 - - - 44.6 46.4\nLLaVA-1.5 (13B) [74] 13B - - 46.94 0.8566 0.9425 - -\nCogVLM [106] 7B 7.9 86.1 - - - - -\nQwen-VL-Chat [2] 7B - - 39.15 - - - -\nOpen-Flamingo [1] 9B - - 38.44 - - - -\nLRV-Instruction [73] - - - 42.78 - - - -\n5 HALLUCINATION MITIGATION\nIn this section, we present a comprehensive review of contemporary methods aimed at mitigating\nhallucinations in MLLMs. Based on the properties and perspectives of these methods, we sys-\ntematically categorize them into four groups. Specifically, we investigate approaches addressing\nhallucination from Data, Model, Training, and Inference.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What is a comprehensive review of contemporary methods to mitigate hallucinations in MLLMs published in April 2", "answer": " Factoid question: What is a comprehensive review of contemporary methods to mitigate hallucinations in MLLMs published in April 2", "source": "hallucination.pdf", "id": "e515c37930"}, {"context": "12 Bai, et al.\nof the model can be disrupted, making it more susceptible to hallucination. Due to the small amount\nof data in this benchmark, the assessment and analysis are mostly conducted by humans.\nAMBER [ 103]Upon the application and evaluation of MLLMs, the tasks can be roughly divided\ninto generative tasks and discriminative tasks. For generative tasks, this paper argues that most\nexisting works rely on additional LLMs, suffering from computational cost. As for discriminative\ntasks, the most popular evaluation suite is POPE [ 69]. However, POPE lacks fine-grained hallucina-\ntion types such as attributes and relations. AMBER (An LLM-free Multi-dimensional Benchmark)\nis proposed to support the evaluation of generative tasks and discriminative tasks, including object\nexistence hallucination, attribute hallucination, and relation hallucination. It further combines the\nCHAIR [90] metric in generative tasks and F1in discriminative tasks to form the AMBER Score as\nfollows:", "question": "What is the name of the proposed benchmark for evaluating generative and discriminative tasks of MLLMs?\n", "answer": "AMBER", "source": "hallucination.pdf", "id": "5d9a4f66e0"}, {"context": "We use largely publicly available general-purpose chat in-\nstruction datasets for multi-turn chat together with a small\namount of domain-specific instruction datasets to perform\nalignment on the ChipNeMo foundation model, which pro-\nduces the ChipNeMo chat model. We observe that align-\nment with a general purpose chat instruction dataset is\nadequate to align the ChipNeMo foundation models with\nqueries in the chip design domain. We also added a small\namount of task-specific instruction data, which further im-\nproves the alignment. We trained multiple ChipNeMo foun-\ndation and chat models based on variants of LLaMA2 mod-\nels used as the base foundation model.\nTo improve performance on the engineering assistant chat-\nbot application, we also leverage Retrieval Augmented Gen-\neration (RAG). RAG is an open-book approach for giving\nLLMs precise context for user queries. It retrieves rele-\nvant in-domain knowledge from its data store to augment", "question": "How is the ChipNeMo chat model aligned with queries in the chip design domain?\n", "answer": "The ChipNeMo chat model is aligned with queries in the chip design domain by using a general-purpose chat instruction dataset and a small amount of domain-specific instruction datasets. This alignment is further improved by adding a small amount of task-specific instruction data.", "source": "ChipNemo.pdf", "id": "411c489c58"}, {"context": "offer valuable insights that deepen understanding of the opportunities and challenges associated\nwith hallucinations in MLLMs. This exploration not only enhances our understanding of the limita-\ntions of current MLLMs but also offers essential guidance for future research and the development\nof more robust and trustworthy MLLMs.\nComparison with existing surveys. In pursuit of reliable generative AI, hallucination stands\nout as a major challenge, leading to a series of survey papers on its recent advancements. For pure\nLLMs, there are several surveys [ 44,129], describing the landscape of hallucination in LLMs. In\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What is a major challenge in the development of reliable generative AI?\n", "answer": "Hallucination is a major challenge in the development of reliable generative AI.", "source": "hallucination.pdf", "id": "76835931c1"}, {"context": "numerous shortcomings. They often fabricate\nfacts [Zhang et al. , 2023b ]and lack knowledge when\ndealing with specific domains or highly specialized\nqueries [Kandpal et al. , 2023 ]. For instance, when the infor-\nmation sought extends beyond the model\u2019s training data or\nrequires the latest data, LLM may fail to provide accurate\nanswers. This limitation poses challenges when deploying\ngenerative artificial intelligence in real-world production\nenvironments, as blindly using a black-box LLM may not\nsuffice.\nTraditionally, neural networks adapt to specific domains\nor proprietary information by fine-tuning models to param-\neterize knowledge. While this technique yields significant\nresults, it demands substantial computational resources, in-\ncurs high costs, and requires specialized technical expertise,\nmaking it less adaptable to the evolving information land-\nscape. Parametric knowledge and non-parametric knowledge\nplay distinct roles. Parametric knowledge is acquired through", "question": "How do large language models (LLMs) fail when dealing with real-world production environments?\n", "answer": "LLMs may fail to provide accurate answers when the information sought extends beyond the model\u2019s training data or requires the latest data. This limitation poses challenges when deploying generative artificial intelligence in real-world production environments.", "source": "RAG.pdf", "id": "b40c0db2f1"}, {"context": "clude Accuracy and EM. Additionally, from the perspec-\ntive of evaluation methods, end-to-end evaluation can be di-\nvided into manual evaluation and automated evaluation us-\ning LLMs. The above summarizes the general case of end-\nto-end evaluation for RAG. Furthermore, specific evalua-\ntion metrics are adopted based on the application of RAG\nin particular domains, such as EM for question-answering\ntasks [Borgeaud et al. , 2022, Izacard et al. , 2022 ], UniEval\nand E-F1 for summarization tasks [Jiang et al. , 2023b ], and\nBLEU for machine translation [Zhong et al. , 2022 ]. These\nmetrics help in understanding the performance of RAG in var-\nious specific application scenarios.\n7.2 Key Metrics and Abilities\nExisting research often lacks rigorous evaluation of the im-\npact of retrieval-augmented generation on different LLMs.\nIn most cases, the evaluaion of RAG\u2019s application in vari-\nous downstream tasks and with different retrievers may yield", "question": "What metrics are used to evaluate the performance of RAG in summarization tasks?\n", "answer": "UniEval and E-F1 are used to evaluate the performance of RAG in summarization tasks.", "source": "RAG.pdf", "id": "b65d7790f9"}, {"context": "is also summarized as a \u201cRetrieve\u201d-\u201cRead\u201d framework\n[Maet al. , 2023a ].\nIndexing\nThe pipeline for obtaining data from the source and building\nan index for it generally occurs in an offline state. Specifi-\ncally, the construction of a data index involves the following\nsteps:1.Data Indexing: This involves cleaning and extracting the\noriginal data, converting different file formats such as PDF,\nHTML, Word, Markdown, etc., into plain text.\n2.Chunking: This involves dividing the loaded text into\nsmaller chunks. This is necessary because language mod-\nels typically have a limit on the amount of context they can\nhandle, so it is necessary to create as small text chunks as\npossible.\n3. Embedding and Creating Index: This is the process of\nencoding text into vectors through a language model. The re-\nsulting vectors will be used in the subsequent retrieval process\nto calculate the similarity between the vector and the problem\nvector.The embedding models require a high inference speed.", "question": "What is the process of creating an index for a language model?\n", "answer": "The process involves data cleaning and extraction, converting file formats into plain text, dividing the text into smaller chunks, encoding the text into vectors through a language model, and requiring high inference speed for the embedding models.", "source": "RAG.pdf", "id": "58f1dd1f89"}, {"context": "niques: domain-adaptive tokenization, domain-\nadaptive continued pretraining, model alignment\nwith domain-specific instructions, and domain-\nadapted retrieval models. We evaluate these\nmethods on three selected LLM applications for\nchip design: an engineering assistant chatbot,\nEDA script generation, and bug summarization\nand analysis. Our evaluations demonstrate that\ndomain-adaptive pretraining of language models,\ncan lead to superior performance in domain re-\nlated downstream tasks compared to their base\nLLaMA2 counterparts, without degradations in\ngeneric capabilities. In particular, our largest\nmodel, ChipNeMo-70B, outperforms the highly\ncapable GPT-4 on two of our use cases, namely en-\ngineering assistant chatbot and EDA scripts gener-\nation, while exhibiting competitive performance\non bug summarization and analysis. These re-\nsults underscore the potential of domain-specific\ncustomization for enhancing the effectiveness of\nlarge language models in specialized applications.", "question": "How does the ChipNeMo-70B model perform compared to GPT-4 in engineering assistant chatbot and EDA scripts generation?\n", "answer": "The ChipNeMo-70B model outperforms the GPT-4 on two use cases, namely engineering assistant chatbot and EDA scripts generation.", "source": "ChipNemo.pdf", "id": "a6c3d05123"}, {"context": "quently, it utilizes this retrieved information to generate re-\nsponses or text, thereby enhancing the quality of predictions.\nThe RAG method allows developers to avoid the need for\nretraining the entire large model for each specific task. In-\nstead, they can attach a knowledge base, providing additional\ninformation input to the model and improving the accuracy\nof its responses. RAG methods are particularly well-suited\nfor knowledge-intensive tasks. In summary, the RAG system\nconsists of two key stages:1. Utilizing encoding models to retrieve relevant docu-\nments based on questions, such as BM25, DPR, Col-\nBERT, and similar approaches [Robertson et al. , 2009,\nKarpukhin et al. , 2020, Khattab and Zaharia, 2020 ].\n2. Generation Phase: Using the retrieved context as a con-\ndition, the system generates text.\n2.2 RAG vs Fine-tuning\nIn the optimization of Large Language Models (LLMs), in\naddition to RAG, another important optimization technique\nis fine-tuning.", "question": "What is one optimization technique for Large Language Models besides RAG?\n", "answer": "Fine-tuning is an optimization technique for Large Language Models besides RAG.", "source": "RAG.pdf", "id": "80558327ad"}, {"context": "Retrieve-Read process, utilizing LLM performance as a\nreward in reinforcement learning for a rewritter module.\nThis allows the rewritter to adjust retrieval queries, im-\nproving the downstream task performance of the reader.\nSimilarly, modules can be selectively replaced in ap-\nproaches like Generate-Read [Yuet al. , 2022 ], where the\nLLM generation module replaces the retrieval module.", "question": "How is LLM performance used in reinforcement learning for a rewriter module?\n", "answer": "LLM performance is used as a reward in reinforcement learning for a rewriter module, allowing the rewriter to adjust retrieval queries and improve the downstream task performance of the reader.", "source": "RAG.pdf", "id": "79cd640612"}, {"context": "module is trained using contrastive training data from both contextual (visual content-related) and\nparametric datasets. During inference, addressing hallucination can be attempted by tuning the\ncontrol parameter \ud835\udf16.\n5.3 Training\n5.3.1 Auxiliary supervision. The primary supervision signal of training MLLMs is language model-\ning loss (implemented as CrossEntropyLoss ) in both pre-training and finetuning stage. However,\nsuch supervision may not be sufficient to process the rich information encoded in the visual content.\nAccordingly, the work of [ 16] constructs a fine-grained vision instruction dataset based on\nPanoptic Scene Graph (PSG), called Relation-Associated Instruction (RAI-30k). In addition to\nstandard dialogues, each instruction in RAI-30k is associated with a relation annotation in PSG,\nwhich includes mask annotations for related instances. With these additional annotations, it further", "question": "How is a module trained to prevent hallucination in processing visual content?\n", "answer": "The module is trained using contrastive training data from both contextual (visual content-related) and parametric datasets. During inference, hallucination can be addressed by tuning the control parameter \ud835\udf16.", "source": "hallucination.pdf", "id": "b83b30e99c"}, {"context": "ITER-RETGEN [Shao et al. , 2023 ]collaboratively utilizes\n\u201dretrieval-enhanced generation\u201d and \u201dgeneration-enhanced\nretrieval\u201d for tasks requiring reproduction of information.\nThat is, the model uses the content needed to complete the\ntask to respond to the input task, and these target contents\nserve as the information context for retrieving more relevant\nknowledge. This helps to generate better responses in another\niteration.\nIRCoT [Trivedi et al. , 2022 ]also explores retrieving docu-\nments for each generated sentence, introducing retrieval at\nevery step of the thought chain. It uses CoT to guide the re-\ntrieval and uses the retrieval results to improve CoT, ensuring\nsemantic completeness.\nAdaptive Retrieval\nIndeed, the RAG methods described in the previous two\nsections follow a passive approach where retrieval is prior-", "question": "How does IRCoT use retrieval in its thought chain?\n", "answer": "IRCoT retrieves documents for each generated sentence, introducing retrieval at every step of the thought chain.", "source": "RAG.pdf", "id": "89c7fd1852"}, {"context": "utilizes the content generated by LLM itself for retrieval, aim-\ning to enhance performance in downstream tasks. The follow-\ning outlines notable studies within this category:\nSKR [Wang et al. , 2023d ]employs a labeled training set,\ncategorizing questions that the model can directly answer\nas known and those requiring retrieval enhancement as un-\nknown. The model is trained to discern whether a question is\nknown, applying retrieval enhancement only to inputs identi-\nfied as unknown, while directly answering the rest.\nGenRead [Yuet al. , 2022 ]substitutes the LLM generator\nfor the retriever. Experimental results indicate that situations\nwhere the generated context document contains correct an-\nswers are more prevalent than those retrieved by Naive RAG.\nThe generated answers also demonstrate superior quality. The\nauthors attribute this to the alignment between the task of gen-\nerating document-level context and the pre-training objective", "question": "Which model uses a labeled training set to categorize questions as known or unknown for retrieval enhancement?\n", "answer": "SKR (Wang et al., 2023d)", "source": "RAG.pdf", "id": "fbf6f2136a"}, {"context": "their performance on the target task. The benefits of IT in efficient MLLMs are manifold. Firstly,\nit enables the model to adapt to a wide range of tasks with minimal changes to its architecture\nor training data. This makes it a flexible and efficient approach for fine-tuning on diverse tasks.\nSecondly, IT allows for better generalization, as the model learns to follow instructions and apply\nits knowledge to new and unseen tasks.\nThe IT stage is typically conducted within the paradigm of Supervised Fine-Tuning (SFT). SFT\ndatasets are often derived from a portion of the pre-training data, which is transformed into an\ninstruction-based format, presented in the form of single-turn or multi-turn dialogue structures.\nGiven an image Xvand its caption, a conversation data (X1\nq, X1\na, . . . , XT\nq, XT\na)can be generated,\nwhere T is the total number of turns. Typically, we can organize the data into a sequence of instruc-\ntions and responses following [7], where the instruction Xt", "question": "How is the IT stage typically conducted in the context of Multi-Task Language Model Learning?\n", "answer": "The IT stage is typically conducted within the paradigm of Supervised Fine-Tuning (SFT).", "source": "multimodal.pdf", "id": "0806fe2e1a"}, {"context": "follows:\nAMBER Score =\ud835\udc34\ud835\udc63\ud835\udc54(1\u2212CHAIR,F1). (1)\nRAH-Bench [ 16]Relation-Associated Hallucination Benchmark (RAH-Bench) can be regarded\nas an upgraded version of POPE, containing 3,000 yes-or-no questions with their corresponding\nimages. Different from POPE, RAH-Bench further divides the negative questions into three subsets.\nEach subset contains 500 questions with misleading statements in the different aspects, including:\n1) categorical hallucination, 2) attribute hallucination, 3) relation hallucination.\nHallusionBench [ 72]To diagnose and analyze the potential failure modes of MLLMs, Hallu-\nsionBench evaluates hallucination from a different perspective. It consists of 455 visual-question\ncontrol pairs, with 346 different figures and a total of 1129 questions covering diverse topics and\nformats. The questions are divided into two categories: Visual Dependent andVisual Supplement .\nThe Visual Dependent questions are defined as questions that do not have an affirmative answer", "question": "What is the number of different figures in HallusionBench?\n", "answer": "The number of different figures in HallusionBench is 346.", "source": "hallucination.pdf", "id": "4461b8f912"}, {"context": "Figure 13: The elements(left) block(middle) and architecture(right) in RWKV [151].\nThis approach parallelizes computations during training and maintains constant computational and\nmemory complexity during inference.\nState Space Models (SSMs) [152] can be formulated as a type of RNN for efficient autoregressive\ninference and have emerged as a promising alternative to attention mechanisms, offering near-linear\ncomputational complexity compared to the quadratic complexity of attention. SSMs are formulated\nas x\u2019(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), mapping a single-dimension input signal u(t) to an N-\ndimension latent state x(t) before projecting it to a single-dimension output signal y(t), with A, B, C,\nand D being parameters learned by gradient descent [152]. Several techniques have been proposed\nto enhance SSMs, such as the Structured State Space sequence model (S4) [152], which refines\nSSMs by conditioning matrix A with a low-rank correction, and the Diagonal State Space (DSS)", "question": "What is the computational complexity of State Space Models (SSMs) during inference?\n", "answer": "State Space Models (SSMs) offer near-linear computational complexity during inference.", "source": "multimodal.pdf", "id": "bb2e9ee3f0"}, {"context": "retrieval. It includes about 1.8K documents, which were\nsegmented into 67K passages, each about 512 characters.\nFirst, we compare our domain adapted retrieval model with\nSentence Transformer (Reimers & Gurevych, 2019) and\ne5small unsupervised (Wang et al., 2022) on each category.\nEach model fetches its top 8 passages from the data store.\nAs shown in Figure 6, our domain-adapted model performed\n2x better than the original e5small unsupervised model and\n30% better than sentence transformer.\nFigure 6: Retrieval Model Accuracy Comparison\nThe queries in the Specs category are derived directly from\npassages in the documents, so their answers are often nicely\ncontained in a concise passage and clearly address the query.\nOn the other hand, the queries of the Testbench and Build\ncategories are not directly derived from passages, so their\nanswers were often not as apparent in the fetched passages\nand required more context (see Appendix A.8 for detailed", "question": "How did the domain-adapted retrieval model perform compared to Sentence Transformer and e5small unsupervised in the Specs category?\n", "answer": "The domain-adapted model performed 2x better than the original e5small unsupervised model and 30% better than the sentence transformer in the Specs category.", "source": "ChipNemo.pdf", "id": "79a9ff88c8"}, {"context": "compasses state-of-the-art openly accessible large language models and a wide range of widely-used\nadapters.(IA)3[155] introduces a novel Parameter-Efficient Fine-Tuning method, Infused Adapters\nby Inhibiting and Amplifying Inner Activations, which learns vectors to weight model parame-\nters through multiplication with activations, enabling robust few-shot performance and task mixing\nwithin batches during inference without manual model structure adjustments. Low-rank adapta-\ntion [161] employs matrix factorization techniques to reduce the number of parameters in the model.\nBy decomposing the original weight matrices into lower-rank matrices, low-rank adaptation captures\nthe most significant components of the model\u2019s representations while discarding less important in-\nformation. This results in a more compact model with a reduced number of parameters, which can\nbe fine-tuned more efficiently.In LoRA-FA [156], a variant of LoRA, the first low-rank matrix is", "question": "What technique does LoRA-FA use to reduce the number of parameters in the model?\n", "answer": "LoRA-FA uses matrix factorization techniques to decompose the original weight matrices into lower-rank matrices, reducing the number of parameters in the model.", "source": "multimodal.pdf", "id": "2fa5fcadef"}, {"context": "is costly for LLMs, and too much irrelevant information\ncan reduce the efficiency of LLMs in utilizing context.\nThe OpenAI report also mentioned \u201dContext Recall\u201d as\na supplementary metric, measuring the model\u2019s abil-\nity to retrieve all relevant information needed to an-\nswer a question. This metric reflects the search opti-\nmization level of the RAG retrieval module. A low re-\ncall rate indicates a potential need for optimization of\nthe search functionality, such as introducing re-ranking\nmechanisms or fine-tuning embeddings, to ensure more\nrelevant content retrieval.\nKey abilities\nThe work of RGB [Chen et al. , 2023b ]analyzed the perfor-\nmance of different large language models in terms of four\nbasic abilities required for RAG, including Noise Robust-\nness, Negative Rejection, Information Integration, and Coun-\nterfactual Robustness, establishing a benchmark for retrieval-\naugmented generation.RGB focuses on the following four\nabilities:\n1.Noise Robustness", "question": "What is a key ability that the RGB model analyzes in large language models?\n", "answer": "Noise Robustness\n\nExplanation: The context mentions that the RGB model analyzes the performance of different large language models in terms of four basic abilities required for RAG, including Noise Robustness.", "source": "RAG.pdf", "id": "6291d3f5de"}, {"context": "generated output after code execution. The second set of\ntasks \u201cHard\u201d come from real use case scenarios that our\nengineers chose. These tasks are much harder requiring\nmultiple API calls and understanding relationship between\n6", "question": "What is one characteristic of the \"Hard\" tasks in the given context?\n", "answer": "The \"Hard\" tasks come from real use case scenarios and require multiple API calls and understanding of relationships.", "source": "ChipNemo.pdf", "id": "ec6c007f5e"}, {"context": "HaELM [104] arXiv\u201923 Oct. MSCOCO [70] 5,000 Gen LLM Assessment Not Explicitly Stated\nFaithScore [55] arXiv\u201923 Nov. MSCOCO [70] 2,000 Gen FaithScore \u2713 \u2713 \u2713 Obj. Counting\nBingo [21] arXiv\u201923 Nov. Unknown 370 Gen Human Assessment \u2717 \u2717 \u2717 Model Bias\nAMBER [103] arXiv\u201923 Nov. Web 15,202 Dis & Gen AMBER Score \u2713 \u2713 \u2713 \u2717\nRAH-Bench [16] arXiv\u201923 Nov. MSCOCO [70] 3,000 Dis False Positive Rate \u2713 \u2713 \u2713 \u2717\nHallusionBench [72] CVPR\u201924 Unknown 1,129 Gen LLM Assessment \u2717 \u2717 \u2717 Model Diagnose\nCCEval [123] arXiv\u201923 Dec. Visual-Genome [59] 100 Gen LLM-based CHAIR \u2713 \u2717 \u2717 \u2717\nMERLIM [100] arXiv\u201923 Dec. MSCOCO [70] 31,373 Dis Accuracy \u2713 \u2717 \u2713 Obj. Counting\nFGHE [105] arXiv\u201923 Dec. MSCOCO [70] 200 Dis Acc/P/R/F \u2713 \u2713 \u2713 Obj. Behavior\nMOCHa [5] arXiv\u201923 Dec. Synthetic 2,000 Gen OpenCHAIR [5] \u2713 \u2713 \u2717 \u2717\nCorrelationQA [35] arXiv\u201924 Feb. Synthetic 7,308 Dis Acc/AccDrop \u2717 \u2717 \u2717 Model Bias\nVQAv2-IDK [11] arXiv\u201924 Feb. VQAv2 [30] 6,624 Dis Acc \u2717 \u2717 \u2717 IK [11]\nMHaluBench [13] arXiv\u201924 Feb. MSCOCO [70] 1,860 Gen Acc/P/R/F \u2713 \u2713 \u2717 T2I", "question": "What is the name of the benchmark that uses the Visual-Genome dataset and reports LLM-based CHAIR metric?\n", "answer": "CCEval [123] arXiv\u201923 Dec. Visual-Genome [59] 100 Gen LLM-based CHAIR \u2713 \u2717 \u2717 \u2717", "source": "hallucination.pdf", "id": "3869137eeb"}, {"context": "Based on powerful large language models, multimodal large language models demonstrate remark-\nable performance across various multimodal tasks. However, the phenomenon of hallucination\npresents a significant challenge to the practical applications of MLLMs, giving rise to undeniable\nconcerns about safety, reliability, and trustworthiness. In this comprehensive survey, we conducted\na thorough examination of hallucinations within multimodal large language models, focusing\non their underlying causes, evaluation metrics, benchmarks, and mitigation methods. Despite\nconsiderable progress, hallucination remains a complex and persistent concern that warrants ongo-\ning investigation. The challenge of hallucination in multimodal large language models remains\ncompelling, requiring continuous scrutiny and innovation. In light of these challenges, we have\noutlined several promising future directions in this burgeoning domain. Through navigating the", "question": "What is a significant challenge in the practical application of multimodal large language models?\n", "answer": "The phenomenon of hallucination is a significant challenge in the practical application of multimodal large language models.", "source": "hallucination.pdf", "id": "114f3dada8"}, {"context": "itized. This method, which involves querying related doc-\numents and inputting into a LLM based on context, may\nlead to efficiency issues. Adaptive retrieval methods such\nas those introduced by Flare [Jiang et al. , 2023b ]and Self-\nRAG [Asai et al. , 2023b ], optimize the RAG retrieval process,\nenabling the LLM to actively judge the timing and content of\nretrieval. This helps to improve the efficiency and relevance\nof the information retrieved.\nIn fact, the way in which LLM actively uses tools and\nmakes judgments is not originated from RAG but has been\nwidely used in the agents of large models [Yang et al. , 2023c,\nSchick et al. , 2023, Zhang, 2023 ]. The retrieval steps\nof Graph-Toolformer [Zhang, 2023 ]are roughly divided\ninto: LLMs actively use the retriever, Self-Ask and\nDSP[Khattab et al. , 2022 ]try to use few-shot prompts to trig-\nger LLM search queries. When LLMs think it is necessary,\nthey can decide to search for a relevant query to collect the", "question": "How do large language models (LLMs) decide when to search for relevant queries?\n", "answer": "LLMs decide to search for a relevant query when they think it is necessary.", "source": "RAG.pdf", "id": "8d605d7952"}, {"context": "Figure 3: The architectures of efficient MLLMs.\nquestion-answering, this section highlights the potential of efficient MLLMs to broaden\ntheir application scope and contribute to real-world problem-solving.\nIn summary, this survey delves into these research endeavors, exploring various strategies for making\nMLLMs more resource-efficient. We review the development history of efficient MLLMs, provide\na taxonomy of the strategies for efficient MLLMs, and comprehensively compare the performance\nof existing efficient MLLMs.Through this exploration, we aspire to provide a comprehensive under-\nstanding of the current state-of-the-art, thereby illuminating the intricate nuances of this emerging\nfield. Furthermore, this survey serves as a roadmap, highlighting potential avenues for future re-\nsearch, and fostering a deeper comprehension of the challenges and opportunities that lie ahead in\nthe domain of efficient MLLMs. In addition to the survey, we have established a GitHub repository", "question": "What is the purpose of the survey on efficient MLLMs?\n", "answer": "The purpose of the survey is to provide a comprehensive understanding of the current state-of-the-art in efficient MLLMs, highlight potential avenues for future research, and foster a deeper comprehension of the challenges and opportunities in this field.", "source": "multimodal.pdf", "id": "d5970c4389"}, {"context": "Bard, etc.) and open-source (Vicuna (Chiang et al., 2023),\nLLaMA2 (Touvron et al., 2023), etc.) large language mod-\nels (LLM) provide an unprecedented opportunity to help\nautomate these language-related chip design tasks. Indeed,\nearly academic research (Thakur et al., 2023; Blocklove\net al., 2023; He et al., 2023) has explored applications of\nLLMs for generating Register Transfer Level (RTL) code\nthat can perform simple tasks in small design modules as\nwell as generating scripts for EDA tools.\nWe believe that LLMs have the potential to help chip de-\nsign productivity by using generative AI to automate many\nlanguage-related chip design tasks such as code generation,\nresponses to engineering questions via a natural language\ninterface, analysis and report generation, and bug triage. In\nthis study, we focus on three specific LLM applications: an\nengineering assistant chatbot for GPU ASIC and Architec-\nture design engineers, which understands internal hardware", "question": "What is one potential application of large language models in GPU ASIC and Architecture design?\n", "answer": "One potential application of large language models in GPU ASIC and Architecture design is as an engineering assistant chatbot that understands internal hardware terminology and can assist engineers with their tasks.", "source": "ChipNemo.pdf", "id": "f23b3625e0"}, {"context": "inaccuracies when dealing with dynamic data, lacking\ntransparency and credibility.\n3 RAG Framework\nThe research paradigm of RAG is constantly evolving. This\nchapter primarily introduces the evolution of the RAG re-\nsearch paradigm. We categorize it into three types: Naive\nRAG, Advanced RAG, and Modular RAG. Although the\nearly RAG was cost-effective and performed better than the\nnative LLM, it still faced many shortcomings. The emergence", "question": "What were the shortcomings of the early RAG research paradigm?\n", "answer": "The early RAG research paradigm, also known as Naive RAG, faced many shortcomings including inaccuracies when dealing with dynamic data and lacking transparency and credibility.", "source": "RAG.pdf", "id": "e482f0535a"}, {"context": "VHTest [ 46]VHTest categorizes visual properties of objects in an image into 1) individual\nproperties, such as existence, shape, color, orientation, and OCR; and 2) group properties, which\nemerge from comparisons across multiple objects, such as relative size, relative position, and\ncounting. Based on such categorization, the authors further defined 8 visual hallucination modes,\nproviding a very detailed evaluation of hallucination in MLLMs. Furthermore, the collected 1,200\nevaluation instances are divided into two versions: \"open-ended question\" (OEQ) and \"yes/no\nquestion\" (YNQ). Such design enables this benchmark to evaluate both generative and discriminative\ntasks.\nComparison of mainstream models We compare the mainstream MLLMs on some represen-\ntative benchmarks, providing a holistic overview of their performance from different dimensions.\nThe results are shown in Table 2 for generative tasks and Table 3 for discriminative tasks. We", "question": "What are the two types of evaluation instances in VHTest?\n", "answer": "The two types of evaluation instances in VHTest are \"open-ended question\" (OEQ) and \"yes/no question\" (YNQ).", "source": "hallucination.pdf", "id": "8ef8344de6"}, {"context": "erating document-level context and the pre-training objective\nof causal language modeling, allowing for better utilization\nof world knowledge stored in the model parameters.\nSelfmem [Cheng et al. , 2023b ]iteratively uses a retrieval-\nenhanced generator to create an unbounded memory pool. A\nmemory selector is employed to choose an output as the mem-\nory for subsequent generations. This output serves as the dual\nproblem to the original question. By combining the originaland dual problems, a retrieval-enhanced generative model can\nleverage its own output to enhance itself.\nThese diverse approaches showcase innovative strategies in\nRAG retrieval enhancement, aiming to elevate model perfor-\nmance and effectiveness.\n6.3 Augmentation Process\nMost RAG research typically only performs a single retrieval\nand generation process. However, single retrievals may con-\ntain redundant information, leading to a \u201dlost in the mid-\ndle\u201d phenomenon [Liuet al. , 2023 ]. This redundant informa-", "question": "What phenomenon can occur when only a single retrieval is performed in RAG research?\n", "answer": "The \"lost in the middle\" phenomenon can occur when only a single retrieval is performed in RAG research.", "source": "RAG.pdf", "id": "6958fee9ba"}, {"context": "while GPT-4 is considered to be the state-of-the-art propri-\netary chat model.\n4", "question": "What is the name of the state-of-the-art proprietary chat model?\n", "answer": "GPT-4", "source": "ChipNemo.pdf", "id": "8f6b70d3f1"}, {"context": "are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix,\ni.e.,MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all\nthe previous tokens. Such a partial over-trust inclination results in neglecting image tokens and\ndescribing the image content with hallucination. Based on this observation, a decoding method for\nMLLMs grounded in an Over-trust Penalty and a Retrospection- Allocation strategy is proposed.\nFirst, a penalty term on the model logits is introduced during the MLLM beam-search decoding\nprocess to mitigate the over-trust issue. Additionally, to handle the hard cases that cannot be\naddressed by the penalty term, a more aggressive strategy called the rollback strategy is proposed\nto retrospect the presence of summary tokens in the previously generated tokens and reallocate\nthe token selection if necessary.\nAnother interesting study observes that the hallucination of MLLMs seems to be easily triggered", "question": "How does the proposed decoding method for MLLMs address the issue of over-trust?\n", "answer": "The proposed decoding method for MLLMs addresses the issue of over-trust by introducing a penalty term on the model logits during the beam-search decoding process to mitigate the over-trust issue.", "source": "hallucination.pdf", "id": "4f752eeea2"}, {"context": "results on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks\nwhere we check for fully accurate code. For \u201cHard\u201d bench-\nmarks in Figure 9 we check for partial correctness of the\ncode, which is evaluated by a human user on a 0-10 scale.\nChipNeMo-70B-Steer performs significantly better than off-\nthe-shelf GPT-4 and LLaMA2-70B-Chat model.\nFigure 8: EDA Script Generation Evaluation Results, Pass@5\nAs seen in Figure 8, models like GPT-4 and LLaMA2-70B-\nChat have close to zero accuracy for the Python tool where\nthe domain knowledge related to APIs of the tool are neces-\nsary. This shows the importance of DAPT. Without DAPT,\nthe model had little to no understanding of the underlying\nAPIs and performed poorly on both automatic and human\nevaluated benchmarks. Our aligned model further improved\nthe results of DAPT because our domain instructional data\nhelps guide the model to present the final script in the most\nuseful manner. An ablation study on inclusion of domain", "question": "How does ChipNeMo-70B-Steer perform compared to GPT-4 and LLaMA2-70B-Chat on \"Hard\" benchmarks?\n", "answer": "ChipNeMo-70B-Steer performs significantly better than off-the-shelf GPT-4 and LLaMA2-70B-Chat on \"Hard\" benchmarks, which are evaluated by a human user on a 0-10 scale.", "source": "ChipNemo.pdf", "id": "cf9d13203d"}, {"context": "addition to RAG, another important optimization technique\nis fine-tuning.\nRAG is akin to providing a textbook to the model, allow-\ning it to retrieve information based on specific queries. This\napproach is suitable for scenarios where the model needs to\nanswer specific inquiries or address particular information re-\ntrieval tasks. However, RAG is not suitable for teaching the\nmodel to understand broad domains or learn new languages,\nformats, or styles.\nFine-tuning is similar to enabling students to internal-\nize knowledge through extensive learning. This approach\nis useful when the model needs to replicate specific struc-\ntures, styles, or formats. Fine-tuning can enhance the perfor-\nmance of non-fine-tuned models and make interactions more\nefficient. It is particularly suitable for emphasizing exist-\ning knowledge in the base model, modifying or customizing\nthe model\u2019s output, and providing complex directives to the\nmodel. However, fine-tuning is not suitable for incorporating", "question": "How can the performance of a non-fine-tuned model be enhanced?\n", "answer": "The performance of a non-fine-tuned model can be enhanced through fine-tuning, which can make interactions more efficient and emphasize existing knowledge in the base model.", "source": "RAG.pdf", "id": "58ee2a38bb"}, {"context": "and efficiently handling intricate weight correlations during pruning, alongside an effective fine-\ntuning procedure for post-compression recovery. Cait [107] introduced asymmetric token merging\nto integrate neighboring tokens efficiently while preserving the spatial structure, paired with consis-\ntent dynamic channel pruning for uniform pruning of unimportant channels in Vision Transformers,\nenhancing model compression.\nStructured Pruning aims to remove structural components, such as attention heads or layers\nbased on predefined criteria. For example, WDPruning [108] employed a binary mask to discern\ninsignificant parameters based on their magnitudes. Additionally, Yu et al. [136] presented a unified\nframework integrating pruning to generate compact transformers. X-Pruner [109] utilizes an end-\nto-end learned explainability-aware mask to measure each unit\u2019s contribution to predicting target", "question": "How does Cait enhance model compression in Vision Transformers?\n", "answer": "Cait enhances model compression in Vision Transformers by introducing asymmetric token merging to integrate neighboring tokens efficiently and preserving the spatial structure, along with consistent dynamic channel pruning for uniform pruning of unimportant channels.", "source": "multimodal.pdf", "id": "28380a85e1"}, {"context": "useful manner. An ablation study on inclusion of domain\ninstructional data for model alignment and the application\nof retrieval is provided in Appendix A.9.\nFigure 9: EDA Script Generation Evaluation Results, Single Gen-\neration (temperature=0), Human Evaluated 0-10 Point Scale.\nOur non-domain models performed better on our Tcl tool\nthan the Python tool, but the trend for our domain trained\nmodel was the opposite. We suspect this was due to the\nproprietary nature of our Python tool. It was difficult for\ngeneral LLMs to perform well on our Python tool bench-\nmark without knowledge of the APIs. Since ChipNeMo is\ntrained with domain data, the inherent python coding ability\nof the base model allows ChipNeMo-70B-Steer to perform\nbetter. This again highlights the importance of DAPT for\nlow-volume or proprietary programming languages.\nFigure 10: Bug Summarization and Analysis Evaluation Results, 7\npoint Likert scale.\n3.7. Bug Summarization and Analysis", "question": "Why did ChipNeMo perform better on the Python tool compared to other general language models?\n", "answer": "ChipNeMo performed better on the Python tool because it is trained with domain data, which provides it with inherent python coding ability that allows it to understand the APIs used in the Python tool, unlike other general language models.", "source": "ChipNemo.pdf", "id": "f1e5db7ca9"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 7\n\u2022Attribute. The object categories identified by MLLMs are accurate, while the descriptions of\nthese objects\u2019 attributes (such as color, shape, material, content, counting, action, etc.) are\nwrong. In Fig. 3, \"pink blossoms\" is hallucinated by the MLLM as the color is inaccurate.\n\u2022Relation. All objects and their attributes are described correctly, but the relationships among\nthem (such as human-object interactions or relative positions) do not align with the actual\nimage content. In Fig. 3, \"...standing around her, watching...\" is a typical example of relation\nhallucination, as the objects are presented in the image but the relation is inaccurate.\nIt\u2019s worth noting that some literature may categorize objects counting, objects event, etc., as\nindependent hallucination categories. In this work, we classify them under the attribute category.", "question": "What type of information is often inaccurately described by multimodal large language models (MLLMs) in images?\n", "answer": "MLLMs often inaccurately describe the attributes of objects in images, such as color, shape, material, content, counting, action, etc.", "source": "hallucination.pdf", "id": "f2b3e09bb2"}, {"context": "and preserving user privacy.\nIn light of these challenges, there has been growing attention on the study of efficient MLLMs.\nThe primary objective of these endeavors is to decrease the resource consumption of MLLMs\nand broaden their applicability while minimizing performance degradation. Research on efficient\nMLLMs began with replacing large language models with lightweight counterparts and performing\ntypical visual instruction tuning. Subsequent studies further enhanced capabilities and expanded\nuse cases in the following ways: (1) lighter architectures were introduced with an emphasis on ef-\nficiency, aiming to reduce the number of parameters or computational complexity[25, 13, 18]; (2)\nmore specialized components were developed, focusing on efficiency optimizations tailored to ad-\nvanced architectures or imbuing specific properties, such as locality[19, 17, 12]; and (3) support\nfor resource-sensitive tasks was provided, with some works employing visual token compression", "question": "How have more specialized components been developed in the study of efficient MLLMs?\n", "answer": "More specialized components have been developed in the study of efficient MLLMs by focusing on efficiency optimizations tailored to advanced architectures or imbuing specific properties, such as locality.", "source": "multimodal.pdf", "id": "04b6ebc53f"}, {"context": "quickly with the popularity of ChatGPT. They both offer a\nrich set of RAG-related APIs, gradually becoming one of\nthe indispensable technologies in the era of large models.\nMeanwhile, new types of technical stacks are constantly be-\ning developed. Although they do not offer as many features\nas LangChain and LLamaIndex, they focus more on their\nunique characteristics. For example, Flowise AI6emphasizes\nlow-code, allowing users to implement various AI applica-\ntions represented by RAG without writing code, simply by\ndragging and dropping. Other emerging technologies include\nHayStack, Meltno, and Cohere Coral.\nIn addition to AI-native frameworks, traditional software\nor cloud service providers have also expanded their service\nrange. For instance, Verba7, provided by the vector database\ncompany Weaviate, focuses on personal assistants. Amazon\noffers its users the intelligent enterprise search service tool\nKendra, based on RAG thinking. Users can search in different", "question": "Which company offers a low-code AI solution for implementing various RAG applications?\n", "answer": "Flowise AI", "source": "RAG.pdf", "id": "9ff21c1039"}, {"context": "(EAS) module[52] proposes a novel parameter and computation-efficient tuning method for MLLMs\nto retain the high performance and reduce both parameter and computation expenditures on down-\nstream tasks. MemVP [53] argues that this transfer learning paradigm still exhibits inefficiency\nsince it significantly increases the input length of the language models. Visual prompts in MemVP\nare concatenated with the weights of Feed Forward Networks for visual knowledge injection to re-\nduce the training time and inference latency of the finetuned MLLMs and surpass the performance\nof previous PEFT methods.\n6 Data and Benchmarks\nIn this section, we provide an overview of the data and benchmarks used for training and evaluating\nefficient MLLMs. We discuss the significance of pre-training data, instruction-tuning data, and the\n19", "question": "Which method is used in MemVP to reduce the training time and inference latency of finetuned MLLMs?\n", "answer": "In MemVP, visual prompts are concatenated with the weights of Feed Forward Networks for visual knowledge injection to reduce the training time and inference latency of the finetuned MLLMs.", "source": "multimodal.pdf", "id": "100e4a1fcc"}, {"context": "evaluation metrics. Additionally, the latest evalu-\nation frameworks like RAGAS [Eset al. , 2023 ]and\nARES [Saad-Falcon et al. , 2023 ]also involve RAG eval-\nuation metrics. Summarizing these works, three core metrics\nare primarily focused on: Faithfulness of the answer, Answer\nRelevance, and Context Relevance.\n1.Faithfulness\nThis metric emphasizes that the answers generated by\nthe model must remain true to the given context, ensur-\ning that the answers are consistent with the context infor-\nmation and do not deviate or contradict it. This aspect of\nevaluation is vital for addressing illusions in large mod-\nels.\n2.Answer Relevance\nThis metric stresses that the generated answers need to\nbe directly related to the posed question.\n3.Context Relevance\nThis metric demands that the retrieved contextual infor-\nmation be as accurate and targeted as possible, avoid-\ning irrelevant content. After all, processing long texts\nis costly for LLMs, and too much irrelevant information", "question": "What are the three core metrics primarily focused on in the latest evaluation frameworks like RAGAS and ARES?\n", "answer": "Faithfulness of the answer, Answer Relevance, and Context Relevance.", "source": "RAG.pdf", "id": "57b75e5528"}, {"context": "Figure 4: BRA VE [12] concatenates features from K different Vision Encoders in a sequence-wise\nmanner. These concatenated features are then reduced by the MEQ-Former.\navoids the high cost of training an end-to-end multimodal model from scratch and effectively lever-\nages the capabilities of pre-trained language and vision models.\nMLP-based As outlined in [7, 54], the vision-language projector is typically realized using a\nstraightforward, learnable Linear Projector or a Multi-Layer Perceptron (MLP), i.e., several linear\nprojectors interleaved with non-linear activation functions, as illustrated in Table.1.\nAttention-based BLIP2 [15] introduces Q-Former, a lightweight transformer, which employs a\nset of learnable query vectors to extract visual features from a frozen vision model. Perceiver\nResampler, proposed by Flamingo[16], contemplates the use of learnable latent queries as Q in\ncross-attention, while image features are unfolded and concatenated with Q to serve as K and V in", "question": "How does BLIP2 [15] extract visual features from a frozen vision model?\n", "answer": "BLIP2 [15] extracts visual features from a frozen vision model using a lightweight transformer called Q-Former, which employs a set of learnable query vectors.", "source": "multimodal.pdf", "id": "1fea51e26c"}, {"context": "cross-attention, while image features are unfolded and concatenated with Q to serve as K and V in\ncross-attention. By this means, the transformer output at the corresponding positions of the learn-\nable latent queries is taken as the aggregated representation of visual features, thereby standardizing\nvariable-length video frame features into fixed-size features. MEQ-Former in BRA VE [12] designs\na multi-encoder querying transformer to amalgamate features from multiple frozen vision encoders\ninto a versatile representation that can be directly inputted into a frozen language model.\nCNN-based MobileVLMv2[17] proposes LDPv2, a new projector consisting of three parts: fea-\nture transformation, token reduction, and positional information enhancement. By using point-wise\nconvolution layers, average pooling, and a PEG module with a skip connection, LDPv2 achieves\nbetter efficiency, a 99.8% reduction in parameters, and slightly faster processing compared to the\noriginal LDP[20].", "question": "What is the reduction in parameters achieved by LDPv2 compared to the original LDP in CNN-based MobileVLMv2?\n", "answer": "LDPv2 achieves a 99.8% reduction in parameters compared to the original LDP in CNN-based MobileVLMv2.", "source": "multimodal.pdf", "id": "f4853839e9"}, {"context": "tion may have a significant impact on the model\u2019s understand-\ning, especially for smaller models. In such scenarios, fine-\ntuning the model to adapt to the input of query + retrieved\ndocuments becomes particularly important. Specifically, be-\nfore providing the input to the fine-tuned model, there is usu-\nally post-retrieval processing of the documents retrieved by\nthe retriever. It is essential to note that the method of fine-\ntuning the generator in RAG is essentially similar to the gen-\neral fine-tuning approach for LLMs. Here, we will brieflyintroduce some representative works, including data (format-\nted/unformatted) and optimization functions.\nGeneral Optimization Process\nRefers to the training data containing pairs of (input, output),\naiming to train the model\u2019s ability to generate output y given\ninput x. In the work of Self-mem [Cheng et al. , 2023b ], a\nrelatively classical training process is employed. Given in-\nput x, relevant documents z are retrieved (selecting Top-1", "question": "How does the work of Self-mem retrieve relevant documents in their training process?\n", "answer": "The work of Self-mem retrieves relevant documents by selecting the Top-1 documents based on the input.", "source": "RAG.pdf", "id": "5e50b58781"}, {"context": "inate ambiguity in entities and terms, along with\neliminating duplicate or redundant information to\nsimplify the retriever\u2019s focus. Ensuring factual ac-\ncuracy is crucial, and whenever possible, the accu-\nracy of each piece of data should be verified. Con-\ntext retention, to adapt to the system\u2019s interaction\ncontext in the real world, can be achieved by adding\nanother layer of context with domain-specific anno-\ntations, coupled with continuous updates through\nuser feedback loops. Time sensitivity is essential\ncontextual information, and mechanisms should be\ndesigned to refresh outdated documents. In sum-\nmary, the focus of optimizing indexed data should\nbe on clarity, context, and correctness to make the\nsystem efficient and reliable. The following intro-\nduces best practices.\n2.Optimizing Index Structures: This can be\nachieved by adjusting the size of the chunks, alter-\ning the index paths, and incorporating graph struc-\nture information. The method of adjusting chunks", "question": "How can optimizing index structures be achieved?\n", "answer": "Optimizing index structures can be achieved by adjusting the size of the chunks, altering the index paths, and incorporating graph structure information.", "source": "RAG.pdf", "id": "d2f0150bc9"}, {"context": "to different negative sampling strategy: random, popular, and adversarial. Popular and adversarial\nsampling are specifically designed to assess frequently appeared objects and object co-occurrence.\nAs an early representative work, POPE serves as a foundation of object hallucination evaluation.\nMME [ 113]. MME is a comprehensive evaluation benchmark for MLLMs. It covers the examina-\ntion of perception and cognition abilities, encompassing 14 subtasks. Regarding object hallucination,\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What is an early representative work for evaluating object hallucination?\n", "answer": "POPE is an early representative work for evaluating object hallucination.", "source": "hallucination.pdf", "id": "f8c4ba40df"}, {"context": "maintains the timeliness and accuracy of responses.\n\u2022 Transparency is an advantage of RAG. By citing\nsources, users can verify the accuracy of the answers,\nincreasing trust in the model\u2019s output.\n\u2022 RAG has customization capabilities. Models can be tai-\nlored to different domains by indexing relevant textual\ncorpora, providing knowledge support for specific fields.\n\u2022 In terms of security and privacy management, RAG,\nwith its built-in roles and security controls in the\ndatabase, can better control data usage. In contrast, fine-\ntuned models may lack clear management of who can\naccess which data.\n\u2022 RAG is more scalable. It can handle large-scale datasets\nwithout the need to update all parameters and create\ntraining sets, making it more economically efficient.\n\u2022 Lastly, results produced by RAG are more trustworthy.\nRAG selects deterministic results from the latest data,\nwhile fine-tuned models may exhibit hallucinations and\ninaccuracies when dealing with dynamic data, lacking", "question": "How does RAG increase trust in its output?\n", "answer": "By citing sources, RAG allows users to verify the accuracy of the answers, which increases trust in its output.", "source": "RAG.pdf", "id": "62ff7bd487"}, {"context": "prehension of visual data. Cobra[13] integrates DINOv2[76] and SigLIP[75] as its vision backbone,\nwith the rationale that merging the low-level spatial features from DINOv2 and the semantic at-\ntributes offered by SigLIP will enhance performance on subsequent tasks. SPHINX-X[14] employs\ntwo vision encoders \u2013 DINOv2 and CLIP-ConvNeXt. Given that these models have been pre-trained\nvia distinct learning methodologies (self-supervised versus weakly supervised) and network archi-\ntectures (ViT versus CNN), they are naturally capable of offering the most complementary and\nsophisticated visual knowledge.\nLightweight Vision Encoder Vision Transformer architectures in real-world applications pose\nchallenges due to hardware and environmental limitations, including processing power and compu-\ntational capabilities. ViTamin [11] represents a lightweight vision model, specifically tailored for\nvision and language models. It commences with a convolutional stem, succeeded by Mobile Con-", "question": "What is a lightweight vision model designed for vision and language tasks?\n", "answer": "ViTamin [11]", "source": "multimodal.pdf", "id": "18b9cdbf0e"}, {"context": "vector.The embedding models require a high inference speed.\nSince it is necessary to encode a large amount of corpus and\nencode the problem in real time when the user asks a question,", "question": "Which characteristic of embedding models require a high inference speed?\n", "answer": "The embedding models require a high inference speed because it is necessary to encode a large amount of corpus and encode the problem in real time when the user asks a question.", "source": "RAG.pdf", "id": "ddf28ff0f7"}, {"context": "vant in-domain knowledge from its data store to augment\nthe response generation given a user query. This method\nshows significant improvement in grounding the model to\nthe context of a particular question. Crucially we observed\nsignificant improvements in retrieval hit rate when finetun-\ning a pretrained retrieval model with domain data. This led\nto even further improvements in model quality.\nOur results show that domain-adaptive pretraining was the\nprimary technique driving enhanced performance in domain-\nspecific tasks. We highlight the following contributions and\nfindings for adapting LLMs to the chip design domain:\n\u2022We demonstrate domain-adapted LLM effectiveness on\nthree use-cases: an engineering assistant chatbot, EDA\ntool script generation, and bug summarization and anal-\nysis. We achieve a score of 6.0 on a 7 point Likert scale\nfor engineering assistant chatbot based on expert eval-\nuations, more than 70% correctness on the generation", "question": "How well did the domain-adapted LLM perform for the engineering assistant chatbot based on expert evaluations?\n", "answer": "The domain-adapted LLM achieved a score of 6.0 on a 7-point Likert scale for the engineering assistant chatbot based on expert evaluations.", "source": "ChipNemo.pdf", "id": "28f0897bcb"}, {"context": "the raw dataset, then continued-pretrain a foundation model\nwith the domain-specific data. We call the resulting model a\nChipNeMo foundation model. DAPT is done on a fraction\nof the tokens used in pre-training, and is much cheaper, only\nrequiring roughly 1.5% of the pretraining compute.\nLLM tokenizers convert text into sequences of tokens for\ntraining and inference. A domain-adapted tokenizer im-\nproves the tokenization efficiency by tailoring rules and\npatterns for domain-specific terms such as keywords com-\nmonly found in RTL. For DAPT, we cannot retrain a new\ndomain-specific tokenizer from scratch, since it would make\nthe foundation model invalid. Instead of restricting Chip-\nNeMo to the pre-trained general-purpose tokenizer used\nby the foundation model, we instead adapt the pre-trained\ntokenizer to our chip design dataset, only adding new tokens\nfor domain-specific terms.\nChipNeMo foundation models are completion models whichrequire model alignment to adapt to tasks such as chat.", "question": "How is the general-purpose tokenizer adapted to the chip design dataset in ChipNeMo foundation models?\n", "answer": "The general-purpose tokenizer is adapted to the chip design dataset by only adding new tokens for domain-specific terms, without retraining the tokenizer from scratch to maintain model validity.", "source": "ChipNemo.pdf", "id": "273b593026"}, {"context": "and required more context (see Appendix A.8 for detailed\nexamples). This significantly contributes to the differencein retrieval quality between the categories.\nFigure 7: Human Evaluation of Different Models. Model Only\nrepresents results without RAG. RAG (hit)/(miss) only include\nquestions whose retrieved passages hit/miss their ideal context,\nRAG (avg) includes all questions. 7 point Likert scale.\nWe conducted evaluation of multiple ChipNeMo models\nand LLaMA2 models with and without RAG. The results\nwere then scored by human evaluators on a 7 point Likert\nscale and shown in Figure 7. We highlight the following:\n\u2022ChipNeMo-70B-Steer outperforms GPT-4 in all cate-\ngories, including both RAG misses and hits.\n\u2022ChipNeMo-70B-Steer outperforms similar sized\nLLaMA2-70b-Chat in model-only and RAG evalua-\ntions by 3.31 and 1.81, respectively.\nOur results indicate that RAG significantly boosts human\nscores. RAG improves ChipNeMo-70B-Steer, GPT-4, and", "question": "Which model outperforms GPT-4 in all categories, including both RAG misses and hits, according to the human evaluation?\n", "answer": "ChipNeMo-70B-Steer outperforms GPT-4 in all categories, including both RAG misses and hits, according to the human evaluation.", "source": "ChipNemo.pdf", "id": "1ed1c2ae54"}, {"context": "16 Bai, et al.\non the rewritten caption dataset has higher accuracy on certain benchmarks, such as the POPE\nbenchmark [ 69]. Despite the performance improvement, the question of why rewritten captions\ncan reduce hallucination remains an open problem.\nEOS Decision [ 120]Previous work [ 137] provides an observation that hallucination tends to\noccur with objects positioned later in the generated descriptions. Intuitively, an ideal scenario is\nthat the MLLM can terminate the generation process in a timely manner. This idea is thoroughly\nexplored in the work of [ 120] from the perspective of end-of-sequence (EOS) decision. The key\ninsight is that the training data may exceed the perception limit of the MLLM. When trained\nwith such data, the model may attempt to fit the detail level and length distribution of ground\ntruth captions. However, it may risk expressing details that it cannot discern from the image, and", "question": "What is an open problem related to rewritten captions in machine learning language models?\n", "answer": "The question of why rewritten captions can reduce hallucination remains an open problem.", "source": "hallucination.pdf", "id": "65b7afe29f"}, {"context": "formation of the preceding blocks (C1, . . . , C i\u22121)and the\nretrieval information of N(Ci\u22121)through cross-attention to\nguide the generation of the next block Ci. To maintain causal-\nity, the autoregressive generation of the i-th block Cican only\nuse the nearest neighbor of the previous block N(Ci\u22121)and\nnotN(Ci).\nAugmented with Structured Data\nStructured data sources like Knowledge Graphs (KG) are\ngradually integrated into the paradigm of RAG. Verified KGs\ncan offer higher-quality context, reducing the likelihood of\nmodel hallucinations.\nRET-LLM [Modarressi et al. , 2023 ]constructs a per-\nsonalized knowledge graph memory by extracting\nrelation triples from past dialogues for future use.\nSUGRE [Kang et al. , 2023 ]embeds relevant subgraphs\nretrieved from the knowledge graph using Graph Neural\nNetworks (GNN) to prevent the model from generating\ncontextually irrelevant replies. SUGRE [Kang et al. , 2023 ]\nemploys a graph encoding method that reflects the graph", "question": "How does SUGRE embed relevant information from a knowledge graph?\n", "answer": "SUGRE embeds relevant subgraphs retrieved from the knowledge graph using Graph Neural Networks (GNN).", "source": "RAG.pdf", "id": "812e372c75"}, {"context": "Multi-view Input Directly employing high-resolution vision encoders for fine-grained percep-\ntion is prohibitively costly and does not align with practical usage requirements. Therefore, to\nutilize low-resolution vision encoders while enabling MLLM to perceive detailed information, a\ncommon approach is to input multi-view HR images, i.e., a global view: low-resolution images\nobtained through resizing, and a local view: image patches derived from splitting. For example,\n7", "question": "How can multi-view low-resolution vision encoders capture detailed information for MLLM?\n", "answer": "By inputting multi-view high-resolution images, specifically a global view (low-resolution images from resizing) and a local view (image patches from splitting).", "source": "multimodal.pdf", "id": "f8392fc0db"}, {"context": "ChipNeMo: Domain-Adapted LLMs for Chip Design\ndifferent VLSI objects. Because these are hard to evaluate\nin an automated way (with current model performance), we\nhad human engineers judge the correctness between 0-10.\nWe evaluate the model on two tools, one is a fully in-house\nPython based tool and the other is a Tcl based EDA tool\nwith limited public data. The size of these benchmarks are\ndescribed in Table 2. Work is ongoing to both increase the\nsize and scope for these benchmarks to allow us to further\nassess and improve these models.\nEvaluation Benchmark Name Size\nPython Tool - Automatic (Easy) 146\nPython Tool - Automatic (Medium) 28\nPython Tool - Human (Hard) 25\nTcl Tool - Automatic (Easy) 708\nTcl Tool - Automatic (Medium) 27\nTcl Tool - Human (Hard) 25\nTable 2: EDA Script Generation Evaluation Benchmarks\nThe comparative performance of our models on these eval-\nuations are shown in Figures 8 and 9. Figure 8 shows the\nresults on automated \u201ceasy\u201d and \u201cmedium\u201d benchmarks", "question": "What is the size of the Tcl tool - automatic (easy) benchmark?\n", "answer": "The size of the Tcl tool - automatic (easy) benchmark is 708.", "source": "ChipNemo.pdf", "id": "76983d04e5"}, {"context": "Figure 15: Comparison of different multimodal adaptation schemes for LLMs in LaVIN [50].\nWith this multimodal instruction-following sequence, IT can be performed by using the same auto-\nregressive training objective as that of the pre-training stage. A prevalent strategy involves main-\ntaining the visual encoder weights in a fixed state while continuing to update the pre-trained weights\nof both the projector and the SLM during the IT process.\nEfficient IT Current IT solutions are prohibitively expensive, requiring optimization of a large\nnumber of parameters and additional large-scale training. LaVIN [50] introduces an innovative\nand cost-effective solution for efficient instruction tuning of MLLMs. The Mixture-of-Modality\nAdaptation (MMA) in LaVIN uses lightweight modules to bridge the gap between LLMs and VL\ntasks. This also facilitates the joint optimization of vision and language models. The actual cost of", "question": "What is the innovative solution introduced by LaVIN for efficient instruction tuning of MLLMs?\n", "answer": "LaVIN introduces the Mixture-of-Modality Adaptation (MMA), which uses lightweight modules to bridge the gap between LLMs and VL tasks, enabling joint optimization of vision and language models.", "source": "multimodal.pdf", "id": "cde7cc5eff"}, {"context": "Taking the input image Xvas input, the vision encoder compresses the original image into more\ncompact patch features Zv, as represented by the following formula:\nZv=g(Xv). (1)\n4", "question": "How does the vision encoder modify the input image in a mathematical formula?\n", "answer": "The vision encoder compresses the original image (X) into more compact patch features (Zv) using the formula Zv=g(Xv).", "source": "multimodal.pdf", "id": "11ffce909a"}, {"context": "transforming MT data into ST data. UEOP [Chan et al. , 2023 ]\nintroduces a new breakthrough in end-to-end automatic\nspeech recognition by introducing external offline strate-\ngies for voice-to-text mapping. Audio embeddings and\nsemantic text embeddings generated by text-to-speech\nmethods can bias ASR through KNN-based attention fu-\nsion, effectively shortening domain adaptation time. The\nVid2Seq [Yang et al. , 2023a ]architecture enhances the lan-\nguage model by introducing special time markings, enabling\nit to seamlessly predict event boundaries and text descriptions", "question": "How does Vid2Seq improve language model prediction?\n", "answer": "Vid2Seq improves language model prediction by introducing special time markings, enabling it to predict event boundaries and text descriptions seamlessly.", "source": "RAG.pdf", "id": "535efdce62"}, {"context": "Inference (\u00a73.4)Lose Visual Attention e.g.OPERA [45], HaELM [104]\nHallucination\nMetrics and\nBenchmarks(\u00a74)Hallucination MetricsCHAIR CHAIR [90]\nPOPE POPE [69]\nLLM-based e.g.GAVIE [73], HaELM [104], HallusionBench [72]\nOthers e.g.Faith-Score [55], AMBER [103]\nHallucination BenchmarksDiscriminative Task e.g.POPE [69], RAH-Bench [16], FGHE [105]\nGenerative Task e.g.GAVIE [73], Faith-Score [55]\nHallucination\nMitigation (\u00a75)Mitigating Data-related\nHallucinations (\u00a75.1)Introducing\nNegative Datae.g.LRV-Instruction [73]\nIntroducing\nCounterfactual Datae.g.HalluciDoctor [117]\nMitigating Noises\nand Errorse.g.ReCaption [105], EOS [120]\nMitigating Model-related\nHallucinations (\u00a75.2)Scale-up Resolution e.g.LLaVA-1.5 [74], InternVL [14], HallE-Switch [123]\nVersatile\nVision Encoderse.g.VCoder [49], IVE [38]\nDedicated Module e.g.HallE-Switch [123]\nMitigating Training-related\nHallucinations (\u00a75.3)Auxiliary SupervisionVisual Supervision e.g.Chen et al. [16]\nContrastive Loss e.g.HACL [52]", "question": "What is an example of a method for mitigating data-related hallucinations in NLP?\n", "answer": "Introducing negative data, such as LRV-Instruction, is an example of a method for mitigating data-related hallucinations in NLP.", "source": "hallucination.pdf", "id": "19a4c2c778"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 21\n5.4.2 Post-hoc Correction. Post-hoc correction refers to first allowing the MLLM to generate a text\nresponse and then identifying and eliminating hallucinating content, resulting in less hallucinated\noutput. This is usually achieved by grounding on visual content [ 114], pre-trained revisior [ 137],\nand self-revision [63].\nWoodpecker [ 114] is an early attempt on hallucination detection and correction. Similar to how\na woodpecker heals trees, Woodpecker picks out and corrects hallucinations from the generated\ntext. The key idea of Woodpecker is to extract key concepts from the generated text and validate\nthem using visual content. Subsequently, the hallucinated concepts can be detected and corrected\naccordingly. Specifically, it consists of five stages: 1) Key concept extraction identifies the main objects\nmentioned in the generated sentences; 2) Question formulation asks questions around the extracted", "question": "How does Woodpecker, an early attempt on hallucination detection and correction, identify and correct hallucinations?\n", "answer": "Woodpecker identifies and corrects hallucinations by extracting key concepts from the generated text and validating them using visual content. It then detects and corrects any hallucinated concepts by asking questions around the extracted concepts.", "source": "hallucination.pdf", "id": "b4dda01e19"}, {"context": "Figure 10: Efficient vision transformer techniques in [138]. The dashed orange block highlights the\ncomponent on which each optimization technique mainly focuses.\ndetermining the pruning rate. Additionally, VTP [110] reduces embedding dimensions through the\nintegration of control coefficients, concurrently removing neurons with negligible coefficients. Tang\net al. [111] eliminate redundant patches by first identifying effective patches in the last layer and\nthen leveraging them to guide the selection process of previous layers, where patches with minimal\nimpact on the final output feature are subsequently discarded.\nHybrid Pruning , such as [137], investigates both unstructured and structured sparsity, intro-\nducing a first-order importance approximation approach for attention head removal. SPViT [112]\ndevelops a dynamic attention-based multi-head token selector for adaptive instance-wise token se-", "question": "Which technique is used for reducing embedding dimensions in VTP?\n", "answer": "VTP reduces embedding dimensions through the integration of control coefficients and concurrent removal of neurons with negligible coefficients.", "source": "multimodal.pdf", "id": "ad28faccc0"}, {"context": "be fine-tuned more efficiently.In LoRA-FA [156], a variant of LoRA, the first low-rank matrix is\nfrozen after initialization and used as a random projection, while the other is trained. This leads to\na reduction in the number of parameters by half, while maintaining a performance comparable to\nthe conventional LoRA technique.DyLoRa [157] introduces a dynamic low-rank adaptation tech-\nnique that enables the training of LoRA blocks for a range of ranks instead of a single rank, which\nis achieved by sorting the representations learned by the adapter modules during training across\ndifferent ranks.\nFull-Parameter fine-tuning Full-parameter fine-tuning is an approach in which all the parame-\nters of a pre-trained model are updated during the fine-tuning process. This method aims to achieve\noptimal performance on a specific downstream task by leveraging the entire capacity of the pre-\ntrained model. While full-parameter fine-tuning often leads to state-of-the-art results and improved", "question": "How does LoRA-FA reduce the number of parameters in LoRA?\n", "answer": "LoRA-FA reduces the number of parameters in LoRA by freezing the first low-rank matrix after initialization and using it as a random projection, while only training the other low-rank matrix. This results in a reduction in the number of parameters by half.", "source": "multimodal.pdf", "id": "d7e84c92f4"}, {"context": "processor with GPT-4 and GPT-3.5. Their findings showed\nthat although GPT-4 produced relatively high-quality codes,\nit still does not perform well enough at understanding and\nfixing the errors. ChipEDA (He et al., 2023) proposed to use\nLLMs to generate EDA tools scripts. It also demonstrated\nthat fine-tuned LLaMA2 70B model outperforms GPT-4\nmodel on this task.\n5. Conclusions\nWe explored domain-adapted approaches to improve LLM\nperformance for industrial chip design tasks. Our results\nshow that domain-adaptive pretrained models, such as the\n7B, 13B, and 70B variants of ChipNeMo, achieve simi-\nlar or better results than their base LLaMA2 models with\nonly 1.5% additional pretraining compute cost. Our largest\ntrained model, ChipNeMo-70B, also surpasses the much\nmore powerful GPT-4 on two of our use cases, engineering\nassistant chatbot and EDA scripts generation, while show-\ning competitive performance on bug summarization and\nanalysis. Our future work will focus on further improving", "question": "Which model outperforms GPT-4 in generating EDA tools scripts and as an engineering assistant chatbot?\n", "answer": "The fine-tuned LLaMA2 70B model, as demonstrated by ChipEDA (He et al., 2023), outperforms the GPT-4 model on these tasks.", "source": "ChipNemo.pdf", "id": "e6b9ba907a"}, {"context": "4.Initialize model embeddings of the new tokens by uti-\nlizing the general-purpose tokenizer.\nSpecifically for Step 4, when a new token is encountered,\nit is first re-tokenized using the original pretrained general-\npurpose tokenizer. The LLM\u2019s token embedding for the new\ntoken is determined by averaging the embeddings of the\ntokens generated by the general-purpose tokenizer (Koto\net al., 2021). The LLM\u2019s final output layer weights for the\nnew tokens are initialized to zero.\nStep 2 helps maintain the performance of the pre-trained\nLLM on general datasets by selectively introducing newtokens that are infrequently encountered in general-purpose\ndatasets. Step 4 reduces the effort required for retraining or\nfinetuning the LLM via initialization of the embeddings of\nnew tokens guided by the general-purpose tokenizer.\n2.2. Domain Adaptive Pretraining\nIn our study, we apply DAPT on pretrained foundation base\nmodels: LLaMA2 7B/13B/70B. Each DAPT model is ini-", "question": "How are the embeddings of new tokens initialized in the LLM?\n", "answer": "The embeddings of new tokens in the LLM are initialized by averaging the embeddings of the tokens generated by the general-purpose tokenizer after re-tokenizing the new token. The LLM's final output layer weights for the new tokens are initialized to zero.", "source": "ChipNemo.pdf", "id": "85cb6bbe71"}, {"context": "Vision Expert Agents Most MLLMs, due to their non-lossless image tokenization, struggle to\nfully capture the intricate details of text and objects. Leveraging vision expert agents is a solution\nto the problem of a single vision encoder\u2019s limited generalization ability on detail-abundant content.\nP2G [38] employs expert agents for real-time grounding, enabling efficient and purposeful reasoning\n8", "question": "How can detail-abundant content be better processed by a machine learning model?\n", "answer": "By leveraging vision expert agents for real-time grounding, detailed content can be processed more efficiently and purposefully.", "source": "multimodal.pdf", "id": "f09b6750be"}, {"context": "overall smaller parameter size, both the retriever and gener-\nator often undergo synchronized end-to-end training or fine-\ntuning [Izacard et al. , 2022 ].\nAfter the emergence of LLM like ChatGPT, generative lan-\nguage models became predominant, showcasing impressive\nperformance across various language tasks [Baiet al. , 2022,\nOpenAI, 2023, Touvron et al. , 2023, Google, 2023 ]. How-\never, LLMs still face challenges such as hallucina-\ntions [Yaoet al. , 2023, Bang et al. , 2023 ], knowledge up-\ndates, and data-related issues. This affects the relia-\nbility of LLMs, making them struggle in certain seri-\nous task scenarios, especially in knowledge-intensive tasks\nrequiring access to a vast amount of knowledge, such\nas open-domain question answering [Chen and Yih, 2020,\nReddy et al. , 2019, Kwiatkowski et al. , 2019 ]and common-\nsense reasoning [Clark et al. , 2019, Bisk et al. , 2020 ]. Im-\nplicit knowledge within parameters may be incomplete and\ninsufficient.", "question": "What issue do large language models (LLMs) face that affects their reliability in knowledge-intensive tasks?\n", "answer": "LLMs like ChatGPT often struggle with hallucinations, knowledge updates, and data-related issues, which can affect their performance in tasks requiring access to a vast amount of knowledge, such as open-domain question answering and common-sense reasoning.", "source": "RAG.pdf", "id": "ee184b2a82"}, {"context": "point Likert scale.\n3.7. Bug Summarization and Analysis\nTo evaluate our models on bug summarization and analysis\nwe have a hold out set of 30 bugs which are ideal candidates\nfor summarization. This includes having a long comment\nhistory or other data which makes the bugs hard for a human\nto quickly summarize. As described in Appendix A.10.3\nthe long length of each individual bug requires the LLM to\nperform hierarchical summarization.\nWe study three separate sub-tasks: summarization focused\non technical details, summarization focused on manage-\nrial details, and a post-summarization recommendation of\n7", "question": "What is being evaluated in the hold out set of 30 bugs?\n", "answer": "The models on bug summarization and analysis.", "source": "ChipNemo.pdf", "id": "72d5a6c1dd"}, {"context": "Efficient Multimodal Large Language Models:\nA Survey\nYizhang Jin1,2,*, Jian Li1,*, Yexin Liu3, Tianjun Gu4, Kai Wu1, Zhengkai Jiang1,\nMuyang He3, Bo Zhao3, Xin Tan4, Zhenye Gan1, Yabiao Wang1, Chengjie Wang1,\nLizhuang Ma2\n1Youtu Lab, Tencent,2SJTU,3BAAI,4ECNU\nAbstract\nIn the past year, Multimodal Large Language Models (MLLMs) have demon-\nstrated remarkable performance in tasks such as visual question answering, vi-\nsual understanding and reasoning. However, the extensive model size and high\ntraining and inference costs have hindered the widespread application of MLLMs\nin academia and industry. Thus, studying efficient and lightweight MLLMs\nhas enormous potential, especially in edge computing scenarios. In this survey,\nwe provide a comprehensive and systematic review of the current state of effi-\ncient MLLMs. Specifically, we summarize the timeline of representative effi-\ncient MLLMs, research state of efficient structures and strategies, and the appli-", "question": "Which area of application has potential for efficient Multimodal Large Language Models?\n", "answer": "Efficient Multimodal Large Language Models have potential for widespread application in academia and industry, especially in edge computing scenarios.", "source": "multimodal.pdf", "id": "ac70fcc9f2"}, {"context": "scores. RAG improves ChipNeMo-70B-Steer, GPT-4, and\nLLaMA2-70b-Chat by 0.56, 1.68, and 2.05, respectively.\nEven when RAG misses, scores are generally higher than\nwithout using retrieval. The inclusion of relevant in-domain\ncontext still led to improved performance, as retrieval is not\na strictly binary outcome. Furthermore, while ChipNeMo-\n70B-SFT outperforms GPT4 by a large margin through\ntraditional supervised fine-tuning, applying SteerLM meth-\nods (Wang et al., 2023) leads to further elevated chatbot\nratings. We refer readers to the complete evaluation results\nin Appendix A.9.\n3.6. EDA Script Generation\nIn order to evaluate our model on the EDA script generation\ntask, we created two different types of benchmarks. The first\nis a set of \u201cEasy\u201d and \u201cMedium\u201d difficulty tasks (1-4 line\nsolutions) that can be evaluated without human intervention\nby comparing with a golden response or comparing the\ngenerated output after code execution. The second set of", "question": "How much does RAG improve the scores of ChipNeMo-70B-Steer, GPT-4, and LLaMA2-70b-Chat?\n", "answer": "RAG improves ChipNeMo-70B-Steer by 0.56, GPT-4 by 1.68, and LLaMA2-70b-Chat by 2.05.", "source": "ChipNemo.pdf", "id": "af6e8c3fb2"}, {"context": "tion, we need to construct domain-specific datasets to fine-\ntune the Embedding model. However fine-tuning an Em-\nbedding model is different from an ordinary language model,\nmainly in that the datasets used are different. In the current\nmain method of fine-tuning Embedding models, the dataset\nused consists of three parts, including Queries, Corpus and\nRelevant Docs. The Embedding model looks up relevant doc-\numents in Corpus based on the Query, and then whether the\nRelevant Docs of the query hit or not is used as a metric for\nthe model.\nIn the construction of datasets, fine-tuning models, and\nevaluation, numerous challenges may arise in each of these\nthree components. In the LlamaIndex [Liu, 2023 ], a series\nof key classes and functions have been introduced specifi-", "question": "What are the three parts of the dataset used in fine-tuning Embedding models?\n", "answer": "The three parts of the dataset used in fine-tuning Embedding models are Queries, Corpus, and Relevant Docs.", "source": "RAG.pdf", "id": "c33ce6cbee"}, {"context": "they can decide to search for a relevant query to collect the\nnecessary materials, similar to the tool call of the agent.\nWebGPT [Nakano et al. , 2021 ]employs a reinforcement\nlearning framework to automatically train the GPT-3 model\nto use a search engine for text generation. It uses special to-\nkens to perform actions, including querying on a search en-\ngine, scrolling rankings, and citing references. This allows\nGPT-3 to leverage a search engine for text generation.\nFlare [Jiang et al. , 2023b ], on the other hand, automates the\ntiming of retrieval and addresses the cost of periodic docu-\nment retrieval based on the probability of the generated text.\nIt uses probability as an indicator of LLMs\u2019 confidence during\nthe generation process. When the probability of a term falls\nbelow a predefined threshold, the information retrieval sys-\ntem would retrieve references and removes terms with lower\nprobabilities. This approach is designed to handle situations", "question": "How does Flare decide when to retrieve references during text generation?\n", "answer": "Flare decides to retrieve references based on the probability of the generated text. When the probability of a term falls below a predefined threshold, Flare's information retrieval system retrieves references and removes terms with lower probabilities.", "source": "RAG.pdf", "id": "b844a74991"}, {"context": "BioMedLLM(Venigalla et al., 2022) for biomed, and Galac-\ntica(Taylor et al., 2022) for science. These models were\nusually trained on more than 100B tokens of raw domain\ndata. The second approach is domain-adaptive pretraining\n(DAPT) (Gururangan et al., 2020) which continues to train\na pretrained foundation model on additional raw domain\ndata. It shows slight performance boost on domain-specific\ntasks in domains such as biomedical, computer science pub-\nlications, news, and reviews. In one example, (Lewkowycz\net al., 2022) continued-pretrained a foundation model on\ntechnical content datasets and achieved state-of-the-art per-\nformance on many quantitative reasoning tasks.\nRetrieval Augmented Generation (RAG) helps ground the\nLLM to generate accurate information and to extract up-to-\ndate information to improve knowledge-intensive NLP tasks\n(Lewis et al., 2021a). It is observed that smaller models with\nRAG can outperform larger models without RAG (Borgeaud", "question": "How can smaller language models perform better than larger ones in knowledge-intensive NLP tasks?\n", "answer": "By using Retrieval Augmented Generation (RAG), smaller language models can be enhanced to outperform larger models in generating accurate information and extracting up-to-date data for knowledge-intensive NLP tasks.", "source": "ChipNemo.pdf", "id": "a92ee29506"}, {"context": "ules and offers more flexibility.\nIn the subsequent chapters, we further analyze three key\nparts of RAG in detail. Chapter 4 introduces the retriever of\nRAG, how to process corpora to obtain better semantic repre-\nsentations, how to mitigate the semantic gap between Query\nand documents, and how to adjust the retriever to fit the gen-\nerator. Chapter 5 explains how the generator obtains better\ngeneration results by post-processing retrieved documents,\navoiding the \u201dLost in the middle\u201d issue, as well as methods to\nadjust the generator to fit the retriever. Subsequently, in Chap-\nter 6, we review the current retrieval enhancement methods\nfrom the aspects of the retrieval stage, retrieval data sources,\nand retrieval process.\nChapter 7 explains how to evaluate current RAG methods,\nincluding evaluation, key indicators, and current evaluation\nframeworks Finally, we provided an outlook on the poten-\ntial future research directions for RAG. As a method that", "question": "What is being analyzed in Chapter 4 of the text?\n", "answer": "In Chapter 4 of the text, the retriever of RAG, processing corpora for better semantic representations, mitigating the semantic gap between Query and documents, and adjusting the retriever to fit the generator are being analyzed.", "source": "RAG.pdf", "id": "8cdb0d5ff2"}, {"context": "model. During the inference phase, given an input, all experts are ranked, and the most relevant\nones are selected for computation. This approach considerably reduces the amount of computa-\ntion, as only a subset of experts is involved in the calculation.By distributing computational tasks\namong different experts, MoE achieves more efficient utilization of computational resources during\nboth training and inference phases. In MoE, each expert has its own set of parameters; however,\nthese parameters are shared during the training process. This parameter-sharing strategy reduces\nthe overall number of parameters in the model, consequently lowering storage and computational\ncosts. GShard [149] is a module composed of a set of lightweight annotation APIs and XLA com-\npiler extensions, which offers an elegant way to express various parallel computation patterns while\nmaking minimal changes to existing model code. It enables us to scale multi-lingual neural machine", "question": "What is the role of GShard in MoE?\n", "answer": "GShard is a module composed of a set of lightweight annotation APIs and XLA compiler extensions that offers an elegant way to express various parallel computation patterns while making minimal changes to existing model code, enabling scaling of multi-lingual neural machine translation in MoE.", "source": "multimodal.pdf", "id": "a7d187c571"}, {"context": "proaches such as Selective Context [Litman et al. , 2020 ]\nand LLMLingua [Anderson et al. , 2022 ]utilize small\n3https://huggingface.co/BAAI/bge-large-en\n4https://platform.openai.com/docs/guides/embeddings\n5https://huggingface.co/BAAI/bge-reranker-large", "question": "Which models are used in Selective Context and LLMLingua approaches?\n", "answer": "The Selective Context approach uses the BAAI/bge-large-en model, and the LLMLingua approach uses the BAAI/bge-reranker-large model. These models can be found on the Hugging Face model hub.", "source": "RAG.pdf", "id": "9281897d96"}, {"context": "The problem of hallucination originates from LLMs themselves. In the NLP community, the\nhallucination problem is empirically categorized into two types [ 44]: 1) factuality hallucination\nemphasizes the discrepancy between generated content and verifiable real-world facts, typically\nmanifesting as factual inconsistency or fabrication; 2) faithfulness hallucination refers to the di-\nvergence of generated content from user instructions or the context provided by the input, as\nwell as self-consistency within generated content. In contrast to pure LLMs, research efforts of\nhallucination in MLLMs mainly focus on the discrepancy between generated text response and\nprovided visual content [69,76,137],i.e., cross-modal inconsistency. This difference suggests that\nstudies in LLMs cannot be seemingly transferred to MLLMs. Therefore, there is a growing need to\ncomprehensively survey recent advancements in MLLMs\u2019 hallucination phenomena to inspire new\nideas and foster the field\u2019s development.", "question": "What is the main difference in hallucination research between LLMs and MLLMs?\n", "answer": "The main difference is that hallucination research in LLMs typically focuses on discrepancies between generated content and real-world facts or user instructions, while research in MLLMs mainly focuses on cross-modal inconsistency between generated text response and provided visual content.", "source": "hallucination.pdf", "id": "ce4c90f626"}, {"context": "further categorized into 1) learnable query-based and 2) projection layer based. Learnable query-\nbased methods, exemplified by Q-Former [ 66], as used in MiniGPT-4 [ 138] and Instruct-BLIP [ 22],\nutilize a set of learnable query tokens to capture visual signals via cross-attention. Projection layer-\nbased methods, as widely applied in LLaVA [ 75], Shikra [ 12], etc., involve training a linear projection\nlayer or a Multi-Layer Perceptron (MLP) module to transform extracted visual features. Both types\nof interfaces aim to transform pre-trained visual features into the input space of pre-trained LLMs.\nAnother line of work is represented by Fuyu-8B [ 4] and Gemini [ 97]. Unlike previous methods\nthat leverage pre-trained uni-modal models, these works employ end-to-end training from scratch.\nTaking Fuyu-8B as an example, it does not employ any pre-trained vision encoder. Instead, it\ndirectly inputs image patches and employs a linear projection to transform the raw pixels of each", "question": "Which method of vision-language alignment uses end-to-end training from scratch without any pre-trained vision encoder?\n", "answer": "Fuyu-8B and Gemini are methods that use end-to-end training from scratch for vision-language alignment, without relying on any pre-trained vision encoder. They directly input image patches and employ a linear projection to transform the raw pixels of each patch.", "source": "hallucination.pdf", "id": "b3ef5fcc7d"}, {"context": "context token and content token. The context token encodes the overall image context based on user\ninput, whereas the content token encapsulates visual cues in each frame. This dual-token strategy\nsignificantly reduces the overload of long videos while preserving critical information. Instead of\ntrying to process more frames simultaneously like most existing work, MA-LMM [68] proposes to\nprocess videos in an online manner and store past video information in a memory bank to reference\nhistorical video content for long-term analysis without exceeding LLMs\u2019 context length constraints\nor GPU memory limits.\n8 Discussion and Conclusion\n8.1 Limitations and Future work\nThe development of efficient MLLMs is still in its nascent stage, and there is ample room for im-\nprovement. We summarize the current state of affairs as follows:\n\u2022 At present, efficient MLLMs face challenges in processing extended-context multimodal", "question": "How does MA-LMM process long videos to analyze historical video content?\n", "answer": "MA-LMM processes videos in an online manner, storing past video information in a memory bank to reference historical video content for long-term analysis without exceeding LLMs\u2019 context length constraints or GPU memory limits.", "source": "multimodal.pdf", "id": "886d74aab5"}, {"context": "Kendra, based on RAG thinking. Users can search in different\ncontent repositories through built-in connectors.\nThe development of the technical stack and RAG are mu-\ntually reinforcing. New technologies pose higher demands\n6https://flowiseai.com\n7https://github.com/weaviate/Verbaon the existing technical stack, while the optimization of the\ntechnical stack\u2019s functions further promotes the development\nof RAG technology. Overall, the technical stack of RAG\u2019s\ntoolchain has initially formed, and many enterprise-level ap-\nplications have gradually emerged, but an all-in-one platform\nstill needs to be refined.\n9 Conclusion\nThis paper thoroughly explores Retrieval-Augmented Gener-\nation (RAG), a technique that uses an external knowledge\nbase to supplement the context of Large Language Models\n(LLMs) and generate responses. Notably, RAG combines pa-\nrameterized knowledge from LLMs and non-parameterized\nexternal knowledge, alleviates hallucination issues, identifies", "question": "What is Retrieval-Augmented Generation (RAG) used for?\n", "answer": "RAG is a technique that uses an external knowledge base to supplement the context of Large Language Models (LLMs) and generate responses. It combines parameterized knowledge from LLMs and non-parameterized external knowledge, and alleviates hallucination issues.", "source": "RAG.pdf", "id": "f3d56bbc09"}, {"context": "tial future research directions for RAG. As a method that\ncombines retrieval and generation, RAG has numerous po-\ntential development directions in future research. By contin-\nuously improving the technology and expanding its applica-\ntions, the performance and practicality of RAG can be further\nenhanced.\nReferences\n[Alon et al. , 2022 ]Uri Alon, Frank Xu, Junxian He, Sudipta\nSengupta, Dan Roth, and Graham Neubig. Neuro-\nsymbolic language modeling with automaton-augmented\nretrieval. In International Conference on Machine Learn-\ning, pages 468\u2013485. PMLR, 2022.", "question": "How can the performance and practicality of RAG be enhanced according to Alon et al., 2022?\n", "answer": "The performance and practicality of RAG can be enhanced by continuously improving the technology and expanding its applications.", "source": "RAG.pdf", "id": "08c361d4ad"}, {"context": "Hallucination of Multimodal Large Language Models: A\nSurvey\nZECHEN BAI, Show Lab, National University of Singapore, Singapore\nPICHAO WANG, Amazon Prime Video, USA\nTIANJUN XIAO, AWS Shanghai AI Lab, China\nTONG HE, AWS Shanghai AI Lab, China\nZONGBO HAN, Show Lab, National University of Singapore, Singapore\nZHENG ZHANG, AWS Shanghai AI Lab, China\nMIKE ZHENG SHOU\u2217,Show Lab, National University of Singapore, Singapore\nThis survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large\nlanguage models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated\nsignificant advancements and remarkable abilities in multimodal tasks. Despite these promising developments,\nMLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination,\nwhich poses substantial obstacles to their practical deployment and raises concerns regarding their reliability", "question": "What is the challenge faced by multimodal large language models in their practical deployment?\n", "answer": "The challenge is that these models often generate outputs that are inconsistent with the visual content, a phenomenon known as hallucination.", "source": "hallucination.pdf", "id": "72dc971633"}, {"context": "sion tokens, implementing efficient structures, and utilizing compact language models, among other\nstrategies. A diagram of the architecture is illustrated in Figure. 3. Table. 1 surveys a summary of\nthe efficient MLLMs, which outlines the base LLM, the vision encoder, image resolution, and the\nprojector used to connect vision and language. These efficient MLLMs include: MobileVLM [20],\nLLaV A-Phi [21], Imp-v1 [22], TinyLLaV A [23], Bunny [24], Gemini Nano-2 [2], MobileVLM-\nv2 [17], MoE-LLaV A-3.6B [25], Cobra [13], Mini-Gemini [26], Vary-toy [27], TinyGPT-V [28],\nSPHINX-Tiny [14], ALLaV A [29], MM1-3B [30], LLaV A-Gemma [31], Mipha-3B [32], VL-\nMamba[18], MiniCPM-V2.0 [70], DeepSeek-VL [34], KarmaVLM [71], moondream2 [72]. In\nthis section, we sequentially present a comprehensive overview of these three modules, along with\nother efficient components.\n2.1 Vision Encoder\nTaking the input image Xvas input, the vision encoder compresses the original image into more", "question": "What are some of the models used in efficient Multimodal Language Learning Machines (MLLMs)?\n", "answer": "MobileVLM, LLaV A-Phi, Imp-v1, TinyLLaV A, Bunny, Gemini Nano-2, MobileVLM-v2, MoE-LLaV A-3.6B, Cobra, Mini-Gemini, Vary-toy, TinyGPT-V, SPHINX-Tiny, ALLaV A, MM1-3B, LLaV A-Gemma, Mipha-3B, VL-Mamba, MiniCPM-V2.0, DeepSeek-VL, KarmaVLM, moondream2 are some of the models used in efficient MLLMs.", "source": "multimodal.pdf", "id": "b24e6a172f"}, {"context": "2 Bai, et al.\n1 INTRODUCTION\nRecently, the emergence of large language models (LLMs) [ 29,81,85,99,132] has dominated a wide\nrange of tasks in natural language processing (NLP), achieving unprecedented progress in language\nunderstanding [ 39,47], generation [ 128,140] and reasoning [ 20,58,87,107,115]. Leveraging\nthe capabilities of robust LLMs, multimodal large language models (MLLMs) [ 22,75,111,138],\nsometimes referred to as large vision-language models (LVLMs), are attracting increasing attention.\nMLLMs show promising ability in multimodal tasks, such as image captioning [ 66], visual question\nanswering [ 22,75], etc. However, there is a concerning trend associated with the rapid advancement\nin MLLMs. These models exhibit an inclination to generate hallucinations [ 69,76,137], resulting in\nseemingly plausible yet factually spurious content.\nThe problem of hallucination originates from LLMs themselves. In the NLP community, the", "question": "What is the issue associated with the rapid advancement of multimodal large language models?\n", "answer": "These models, also known as large vision-language models, have a tendency to generate hallucinations, producing seemingly plausible but factually inaccurate content.", "source": "hallucination.pdf", "id": "da0a465b6c"}, {"context": "the semantic space of the user\u2019s query and documents is very\nnecessary. This section introduces two key technologies to\nachieve this goal.\nQuery Rewrite\nThe most intuitive way to align the semantics of\nquery and document is to rewrite the query. As\nmentioned in Query2Doc [Wang et al. , 2023b ]and ITER-\nRETGEN [Shao et al. , 2023 ], the inherent capabilities of\nlarge language models are utilized to generate a pseudo-\ndocument by guiding it, and then the original query is\nmerged with this pseudo-document to form a new query.\nIn HyDE [Gao et al. , 2022 ], query vectors are established\nthrough the use of text indicators, using these indicators to\ngenerate a \u2019hypothetical\u2019 document that is relevant, yet may\nnot truly exist, it only needs to capture the relevant pattern.\nRRR [Maet al. , 2023a ]introduced a new framework that in-\nverts the order of retrieval and reading, focusing on query\nrewriting. This method generates a query using a large lan-", "question": "How does HyDE generate a hypothetical document relevant to the query?\n", "answer": "HyDE generates a hypothetical document relevant to the query by establishing query vectors through the use of text indicators and using these indicators to generate a document that captures the relevant pattern, even if it may not truly exist.", "source": "RAG.pdf", "id": "71a4057422"}, {"context": "paradigm in biomedicine, achieving state-of-the-art results on many applications, including medical\nquestion answering [194] and medical image classification [195]. Recently, multimodal generative\nAI has emerged as an exciting frontier in the biomedical domain, expanding the application scope\nfrom single-modality to multi-modality, such as VQA and radiology report generation.\nThe mixture of Expert Tuning has effectively enhanced the performance of general MLLMs with\nfewer parameters, yet its application in resource-limited medical settings has not been fully explored.\nMoE-TinyMed [64] is a model tailored for medical applications that significantly lower parameter\ndemands. LLaV A-Rad [63] is a state-of-the-art tool that demonstrates rapid performance on a sin-\ngle V100 GPU in private settings, making it highly applicable for real-world clinical scenarios. It\nemploys a modular approach, integrating unimodal pre-trained models and emphasizing the training", "question": "What is a model that tailors for medical applications and significantly lowers parameter demands?\n", "answer": "MoE-TinyMed [64]", "source": "multimodal.pdf", "id": "51c7c3d212"}, {"context": "and object segmentations. The computation of the CHAIR metric is straightforward and easy\nto understand. The metric has two variants: per-instance (denoted as CHAIR \ud835\udc56) and per-sentence\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "What is the name of the metric with two variants, per-instance and per-sentence, for evaluating object segmentations, as described in a preprint from April 2024?\n", "answer": "CHAIR (without quotation marks)", "source": "hallucination.pdf", "id": "7168b77a46"}, {"context": "dataset has strong effects on the behavior of the model. Frequently appeared objects and object\nco-occurrence are two prominent types of statistical bias, as discussed in [ 69,90,137]. For example,\n\u2018person \u2019 might be one of the most frequently appearing objects in the training data. During inference,\neven if the given image does not contain a person, the model still tends to predict the presence\nof a person. On the other hand, object co-occurrence refers to the phenomenon that the model\nwill remember which two objects usually \u2018go together\u2019 [ 90]. For instance, given an image of a\nkitchen with a refrigerator, MLLMs are prone to answer \u2018 Yes\u2019 when asked about a microwave, as\nrefrigerators and microwaves frequently appear together in kitchen scenes. Bias exists in most\ndatasets. Increasing the scale of data may alleviate the effect, but cannot fully resolve it, given the\nlong-tail distribution of the real world.\n3.2 Model", "question": "How can increasing the scale of data affect bias in a model?\n", "answer": "Increasing the scale of data can alleviate the effect of bias in a model, but it cannot fully resolve it due to the long-tail distribution of the real world.", "source": "hallucination.pdf", "id": "44cf8ffcb0"}, {"context": "there has been increased attention on self-retrieval, which in-\nvolves mining the knowledge of LLMs themselves to enhance\ntheir performance.\nThe subsequent chapters of this paper are structured as fol-\nlows: Chapter 2 provides an introduction to the background\nof RAG.Chapter 3 introduces the mainstream paradigms of\nRAG.Chapter 4 analyzes the retriever in RAG.Chapter 5 fo-", "question": "What is the focus of the subsequent chapters of the paper?\n", "answer": "The subsequent chapters of the paper provide an introduction to the background of RAG, introduce the mainstream paradigms of RAG, analyze the retriever in RAG, and focus on self-retrieval which involves mining the knowledge of Large Language Models (LLMs) themselves to enhance their performance.", "source": "RAG.pdf", "id": "c3380c77fb"}, {"context": "tion answering and image captioning. However, MLLMs face considerable challenges in tasks ne-\ncessitating intricate recognition, including crowd counting and OCR of small characters. A direct\napproach to address these challenges involves increasing the image resolution, practically, the num-\nber of visual tokens. This strategy, nonetheless, imposes a substantial computational burden on\nMLLMs, primarily due to the quadratic scaling of computational costs with the number of input to-\nkens in the Transformer architecture. Motivated by this challenge, vision token compression, aimed\nto reduce the prohibitive computation budget caused by numerous tokens, has become an essential\naspect of efficient MLLMs. We will explore this topic through several key techniques, including\nmulti-view input, token processing, multi-scale information fusion, vision expert agents and video-\nspecific methods.\nMulti-view Input Directly employing high-resolution vision encoders for fine-grained percep-", "question": "What is the challenge faced by MLLMs in tasks requiring intricate recognition?\n", "answer": "MLLMs face challenges in tasks like crowd counting and OCR of small characters, which require detailed recognition.", "source": "multimodal.pdf", "id": "8beea9b82e"}, {"context": "into a high-dimensional space, where task-related information can be more readily captured. In\nthis new space, each word in the text sequence is represented as a high-dimensional vector, and the\ndistances between these vectors serve to measure their similarities. Low-Rank [147] aims to decom-\npose a high-dimensional matrix into the product of two lower-dimensional matrices. Consequently,\nby calculating the inverses of these two lower-dimensional matrices, an approximate inverse of the\nattention matrix can be obtained, thereby significantly reducing computational complexity.\n4.2 Framework\nMixture of Experts The core idea behind MoE [89] is to decompose a large-scale model into sev-\neral smaller models, each of which focuses on learning a specific part of the input data. During the\ntraining process, each expert is assigned a weight that determines its importance within the overall\nmodel. During the inference phase, given an input, all experts are ranked, and the most relevant", "question": "How does Mixture of Experts (MoE) decompose a large-scale model?\n", "answer": "MoE decomposes a large-scale model into several smaller models, each focusing on learning a specific part of the input data.", "source": "multimodal.pdf", "id": "811840a2cd"}, {"context": "RAG can outperform larger models without RAG (Borgeaud\net al., 2022). Retrieval methods include sparse retrieval\nmethods such as TF-IDF or BM25(Robertson & Zaragoza,\n2009), which analyze word statistic information and find\nmatching documents with a high dimensional sparse vec-\ntor. Dense retrieval methods such as (Karpukhin et al.,\n2020; Izacard et al., 2022a) find matching documents on\nan embedding space generated by a retrieval model pre-\ntrained on a large corpus with or without fine-tuning on a\nretrieval dataset. The retrieval model can be trained stan-\ndalone (Karpukhin et al., 2020; Izacard et al., 2022a; Shi\net al., 2023) or jointly with language models (Izacard et al.,\n2022b; Borgeaud et al., 2022). In addition, it has been shown\nthat off-the-shelf general purpose retrievers can improve a\nbaseline language model significantly without further fine-\ntuning (Ram et al., 2023). RAG is also proposed to perform\ncode generation tasks (Zhou et al., 2023) by retrieving from", "question": "How can a baseline language model be improved without further fine-tuning?\n", "answer": "By using off-the-shelf general purpose retrievers.", "source": "ChipNemo.pdf", "id": "392133bc25"}, {"context": "samples) and hallucinatory descriptions (negative samples). HA-DPO then trains the model using\nthese sample pairs, enabling it to distinguish between accurate and hallucinatory descriptions. This\ngoal is achieved through direction preference optimization (DPO), which optimizes a specific loss\nfunction designed to maximize the model\u2019s preference for positive samples while minimizing its\npreference for negative samples.\nA concurrent work, Silkie [ 68], introduces a similar approach of utilizing preference-based\nreinforcement learning to enhance the faithfulness of MLLMs. Specifically, it emphasizes the\nconcept of reinforcement learning from AI feedback (RLAIF) by distilling preferences from a more\nrobust MLLM, i.e., GPT-4V [ 83]. Responses are first generated by models from 12 MLLMs, and then\nassessed by GPT-4V. The constructed dataset, termed as VLFeedback, contains preferences distilled\nfrom GPT-4V and is utilized to train other MLLMs through direct preference optimization.", "question": "What is the name of the concurrent work that also uses preference-based reinforcement learning to improve the faithfulness of MLLMs?\n", "answer": "Silkie [68]", "source": "hallucination.pdf", "id": "0b66cff7c9"}, {"context": "ture information. The method of adjusting chunks\n(Small to Big) involves collecting as much relevant\ncontext as possible and minimizing noise. When\nconstructing a RAG system, the chunk size is a key\nparameter. There are different evaluation frame-\nworks comparing the size of individual chunks.\nLlamaIndex2uses GPT4 to assess fidelity and rele-\n2https://www.llamaindex.ai", "question": "How does LlamaIndex2 evaluate the fidelity and relevance of individual chunks?\n", "answer": "LlamaIndex2 uses GPT-4 to assess the fidelity and relevance of individual chunks.", "source": "RAG.pdf", "id": "e21b322899"}, {"context": "MHaluBench [13] arXiv\u201924 Feb. MSCOCO [70] 1,860 Gen Acc/P/R/F \u2713 \u2713 \u2717 T2I\nVHTest [46] arXiv\u201924 Feb. MSCOCO [70] 1,200 Dis & Gen Acc \u2713 \u2713 \u2717 \u2713\nHal-Eavl [53] arXiv\u201924 Feb.MSCOCO [70] &\nLAION [92]10,000 Dis & GenAcc/P/R/F &\nLLM Assessment\u2713 \u2713 \u2713 Obj. Event\n(denoted as CHAIR \ud835\udc60):\nCHAIR \ud835\udc56=|{hallucinated objects }|\n|{all objects mentioned }|,\nCHAIR \ud835\udc60=|{sentences with hallucinated object }|\n|{all sentences}|.\nIn the paper of CHAIR [ 90], the range of objects is restricted to the 80 MSCOCO objects. Sentence\ntokenization and synonyms mapping are applied to determine whether a generated sentence\ncontains hallucinated objects. Ground-truth caption and object segmentations both serve as ground-\ntruth objects in the computation. In the MLLM era, this metric is still widely used for assessing the\nresponse of MLLMs.\nPOPE [ 69]. When used in MLLMs, the work of [ 69] argues that the CHAIR metric can be\naffected by the instruction designs and the length of generated captions. Therefore, it proposes a", "question": "How can the CHAIR metric be affected in the context of MLLMs?\n", "answer": "The CHAIR metric can be affected by the instruction designs and the length of generated captions in the context of MLLMs.", "source": "hallucination.pdf", "id": "84a3c00c17"}, {"context": "target corpus using a language model to create positive\nand negative samples.\n2. Preparing LLM Judges: Next, ARES fine-tunes\nlightweight language models using the synthetic dataset\nto train them to evaluate Context Relevance, Answer\nFaithfulness, and Answer Relevance.\n3. Ranking RAG Systems Using Confidence Intervals: Fi-\nnally, ARES applies these judge models to score RAG\nsystems and combines them with a manually annotated\nvalidation set using the PPI method to generate confi-\ndence intervals, reliably estimating the performance of\nRAG systems.\n8 Future Prospects\nIn this chapter, we delve into three future prospects for\nRAG, namely vertical optimization, horizontal expansion and\necosystem of RAG.\n8.1 Vertical Optimization of RAG\nDespite the rapid advancements in RAG technology over the\npast year, there are still several areas in its vertical domain\nthat require further investigation.\nFirstly, the issue of long context in RAG is a significant", "question": "What is one area of RAG technology that requires further investigation?\n", "answer": "The issue of long context in RAG is a significant area that requires further investigation.", "source": "RAG.pdf", "id": "07fe001fee"}, {"context": "and more specific knowledge. Secondly, since the model\nparameters cannot be updated dynamically, the parametric\nknowledge is susceptible to becoming outdated over time.\nLastly, an expansion in parameters leads to increased com-arXiv:2312.10997v1  [cs.CL]  18 Dec 2023", "question": "Why is parametric knowledge not updated dynamically in this model?\n", "answer": "The model parameters cannot be updated dynamically, making the parametric knowledge susceptible to becoming outdated over time.", "source": "RAG.pdf", "id": "9ebdac13a1"}, {"context": "employs a modular approach, integrating unimodal pre-trained models and emphasizing the training\nof lightweight adapters. As a result, LLaV A-Rad outperforms larger models such as GPT-4V and\nMed-PaLM in terms of standard metrics, showcasing its superior efficiency and effectiveness.\n22", "question": "Which model outperforms both GPT-4V and Med-PaLM in terms of efficiency and effectiveness?\n", "answer": "LLaV A-Rad", "source": "multimodal.pdf", "id": "0da5fa4a36"}, {"context": "fine-tuning and also masked the attribute labels and trained\non ChipNeMo models for 2 epochs. We refer readers to\nAppendix A.4 for details on the alignment datasets and A.7\non implementations details.\nWe also experimented with DAPT directly on a chat aligned\nmodel, such as the LLaMA2-Chat model. We found that\nDAPT significantly degraded the model\u2019s alignment, mak-\ning the resulting model useless for downstream tasks.\n2.4. Domain-Adapted Retrieval Model\nIt is well known that LLMs can generate inaccurate text,\nso-called hallucination (Ji et al., 2023). Although the phe-\nnomenon is not completely understood, we still must miti-\ngatehallucinations since they are particularly problematic\nin an engineering assistant chatbot context, where accu-\nracy is critical. Our proposal is to leverage the retrieval\naugmented generation (RAG) method. RAG tries to re-trieve relevant passages from a database to be included in\nthe prompt together with the question, which grounds the", "question": "How does the paper suggest mitigating hallucinations in a chatbot context?\n", "answer": "The paper suggests mitigating hallucinations in a chatbot context by leveraging the retrieval augmented generation (RAG) method, which involves retrieving relevant passages from a database to be included in the prompt together with the question, thereby grounding the response.", "source": "ChipNemo.pdf", "id": "aec87069e2"}, {"context": "Hallucination of Multimodal Large Language Models: A Survey 3\ncontrast, there are very few surveys on hallucination in the field of MLLMs. To the best of our\nknowledge, there is only one concurrent work [ 76], a short survey on the hallucination problem of\nLVLMs. However, our survey distinguishes itself in terms of both taxonomy and scope. We present a\nlayered and granular classification of hallucinations, as shown in Fig. 1, drawing a clearer landscape\nof this field. Additionally, our approach does not limit itself to specific model architectures as\nprescribed in the work of [ 76], but rather dissects the causes of hallucinations by tracing back to\nvarious affecting factors. We cover a larger range of literature both in terms of paper number and\ntaxonomy structure. Furthermore, our mitigation strategies are intricately linked to the underlying\ncauses, ensuring a cohesive and targeted approach.\nOrganization of this survey. In this paper, we present a comprehensive survey of the latest", "question": "What is the only concurrent survey on the hallucination problem in Multimodal Large Language Models (MLLMs)?\n", "answer": "The only concurrent survey on the hallucination problem in MLLMs is a short survey on the hallucination problem of LVLMs, as mentioned in the context.", "source": "hallucination.pdf", "id": "33d47ad8cc"}, {"context": "MLLMs. Based on the detection result, the hallucinated content can be eliminated. Secondly, this\nwork observes that long-tail distribution and object co-occurrence in the training data are two\nprimary factors of hallucination. Thus, a counterfactual visual instruction generation strategy is\nproposed to expand the dataset. Using the proposed methods, the instruction tuning data can be\nbalanced and experience reduced hallucination. MLLMs trained on the calibrated dataset are shown\nto be less prone to hallucination.\nReCaption [ 105]This work proposes a framework called ReCaption to rewrite the text captions\nof existing image-text pairs in datasets. The framework comprises two steps: 1) keyword extraction,\nwhich extracts verbs, nouns, and adjectives from the caption; and 2) caption generation, which\nemploys an LLM to generate sentences based on the extracted keywords. Ultimately, the framework", "question": "How does the ReCaption framework address the issue of hallucination in MLLMs?\n", "answer": "The ReCaption framework addresses the issue of hallucination in MLLMs by proposing a counterfactual visual instruction generation strategy to expand the dataset and balance the instruction tuning data, resulting in reduced hallucination for MLLMs trained on the calibrated dataset.", "source": "hallucination.pdf", "id": "294848c460"}, {"context": "Figure 6: Comparision of Phi[86] (from left to right: phi-1.5, phi-2, phi-3-mini, phi-3-small) versus\nLlama-2 [91] family of models(7B, 13B, 34B, 70B) that were trained on the same fixed data.\nLLaV A-UHD [35] proposes an image modularization strategy that divides native-resolution im-\nages into smaller variable-sized slices for efficient and extensible encoding. Inaddition, InternLM-\nXComposer2-4KHD [90] introduces a strategy that dynamically adjusts resolution with an automatic\nlayout arrangement, which not only maintains the original aspect ratios of images but also adaptively\nalters patch layouts and counts, thereby enhancing the efficiency of image information extraction.\nBy implementing an adaptive input strategy for images of varying resolutions, a balance between\nperceptual capability and efficiency can be achieved.\nToken Processing Techniques designed to process lengthy visual token squence are critical in ef-", "question": "What is a technique for efficiently encoding images of varying resolutions?\n", "answer": "LLaVA A-UHD [35] proposes an image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding.", "source": "multimodal.pdf", "id": "c0bdc4830f"}, {"context": "open-source library proposed by the industry, also offers a\nsimilar evaluation mode. These frameworks all use LLMs as\njudges for evaluation. As TruLens is similar to RAGAS, this\nchapter will specifically introduce RAGAS and ARES.\nRAGAS\nThis framework considers the retrieval system\u2019s ability to\nidentify relevant and key context paragraphs, the LLM\u2019s abil-\nity to use these paragraphs faithfully, and the quality of\nthe generation itself. RAGAS is an evaluation framework\nbased on simple handwritten prompts, using these prompts\nto measure the three aspects of quality - answer faithfulness,\nanswer relevance, and context relevance - in a fully auto-\nmated manner. In the implementation and experimentation\nof this framework, all prompts are evaluated using the gpt-\n3.5-turbo-16k model, which is available through the OpenAI\nAPI[Eset al. , 2023 ].\nAlgorithm Principles\n1. Assessing Answer Faithfulness: Decompose the answer\ninto individual statements using an LLM and verify", "question": "What model is used to evaluate all prompts in the RAGAS framework?\n", "answer": "The gpt-3.5-turbo-16k model, which is available through the OpenAI API, is used to evaluate all prompts in the RAGAS framework.", "source": "RAG.pdf", "id": "ffd5c8b41e"}, {"context": "decoding probability distribution is calibrated using the reference (distorted) distribution.\nFollowing the same idea of contrastive decoding, IBD [ 139] proposes an image-biased decoding\nstrategy. Specifically, IBD involves computing a more reliable next-token probability distribution\nby contrasting the predictions of the original model with those of an image-biased model, which\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How does IBD compute a more reliable next-token probability distribution?\n", "answer": "IBD calculates a more reliable next-token probability distribution by contrasting the predictions of the original model with those of an image-biased model.", "source": "hallucination.pdf", "id": "9a2cc490f3"}, {"context": "a richer diversity of input modalities, and augmenting their generative capacities, we can\nsignificantly bolster their multifunctionality and widen their applicability.\n\u2022 There are two principal pathways to fortify efficient MLLM models. Firstly, the incorpora-\ntion of a more varied set of lightweight LLMs can render the design of MLLMs more adapt-\nable, facilitating their customization to cater to a broad spectrum of requirements. Sec-\nondly, leveraging high-quality instruction tuning datasets can empower efficient MLLMs\nto better comprehend and implement a vast array of instructions, thereby amplifying their\nzero-shot learning capabilities.\n\u2022 The development of embodied agents capable of deployment on edge devices represents a\ncrucial application prospect for efficient MLLMs. An agent possessing specialized knowl-\nedge and the capability to interact with the real world has far-reaching implications, poten-", "question": "How can the adaptability of MLLMs be improved according to the context?\n", "answer": "The adaptability of MLLMs can be improved by incorporating a more varied set of lightweight LLMs, which can be customized to cater to a broad spectrum of requirements.", "source": "multimodal.pdf", "id": "f10976c224"}, {"context": "rely on well-recognized large language models like GPT-\n4[OpenAI, 2023 ]to leverage their robust internal knowl-\nedge for the comprehensive retrieval of document knowledge.\nHowever, inherent issues of these large models, such as con-\ntext length restrictions and vulnerability to redundant infor-\nmation, persist. To mitigate these issues, some research has\nmade efforts in post-retrieval processing. Post-retrieval pro-\ncessing refers to the process of further treating, filtering, or\noptimizing the relevant information retrieved by the retriever\nfrom a large document database. Its primary purpose is to en-\nhance the quality of retrieval results to better meet user needs\nor for subsequent tasks. It can be understood as a process of\nreprocessing the documents obtained in the retrieval phase.\nThe operations of post-retrieval processing usually involve in-\nformation compression and result rerank.\nInformation Compression\nEven though the retriever can fetch relevant information from", "question": "What is one of the processes involved in post-retrieval processing to enhance the quality of information retrieval?\n", "answer": "Information compression is one of the processes involved in post-retrieval processing. It is used to optimize the relevant information retrieved by the retriever from a large document database.", "source": "RAG.pdf", "id": "faf8e03358"}, {"context": "making minimal changes to existing model code. It enables us to scale multi-lingual neural machine\ntranslation Transformer models with sparse gated mixtures of experts to over 600 billion parameters\nusing automatic sharding. Switch Transformer [150] replaces the feedforward network (FFN) layer\nin the standard Transformer with a MoE routing layer, where each expert operates independently on\nthe tokens in the sequence. Its training speed is four times faster than Google\u2019s previously developed\nlargest model, T5-XXL, under the same computational resources. The proposed training techniques\nhave eliminated instability during the training process, demonstrating that large sparse models can\nalso be trained in a low-precision format, such as bfloat16.\nTransformer-Alternative Structures Although the Transformer is the dominant architecture in\ncurrent large-scale language models, models like RWKV [151] and Mamba [77] have emerged as", "question": "What is the speed difference in training between Switch Transformer and Google's T5-XXL?\n", "answer": "The Switch Transformer trains four times faster than T5-XXL under the same computational resources.", "source": "multimodal.pdf", "id": "45effa0e86"}, {"context": "Gemini Pro [2] - 71.2 - - 74.6 - 47.9/\u2013 45.2 - 436.79 73.6 \u2013/70.7 - - 64.3\nGemini Ultra [2] - 77.8 - - 82.3 - 59.4/\u2013 53.0 - - - - - - -\nGPT4V [1] - 77.2 - - 78.0 - 56.8/55.7 49.9 - 517.14 75.8 67.3/69.1 - - 67.6\nMobileVLM [20] MobileLLaMA (2.7B) - 59.0\u221761.0 47.5 - - - 1288.9 - 59.6 - 84.9 - -\nLLaV A-Phi [21] Phi-2 (2.7B) 71.4\u2217- 68.4 48.6 35.9 - - 1335.1 - 59.8 - 85.0 - 28.9\nImp-v1 [22] Phi-2 (2.7B) 79.5 - 70.0 59.4 - - - 1434.0 - 66.5 - 88.0 - 33.1\nTinyLLaV A [23] Phi-2 (2.7B) 79.9\u221762.0\u221769.1 59.1 - - - 1464.9 - 66.9 - 86.4 75.8 32.0\nBunny [24] Phi-2 (2.7B) 79.8 62.5 70.9 - - 38.2/33.0 - 1488.8 289.3 68.6 62.5/- 86.8 - -\nGemini Nano-2 [2] - 67.5 - - 65.9 - 32.6/- 30.6 - - - - - - -\nMobileVLM-v2 [17] MobileLLaMA(2.7B) - 61.1 70.0 57.5 - - - 1440.5 - - - 84.7 - -\nMoE-LLaV A [25] Phi-2 (2.7B) 79.9\u221762.6\u221770.3 57.0 43.7 - - 1431.3 - 68.0 - 85.7 - 35.9\nCobra [13] Mamba-2.8B 75.9 58.5 - 46.0 52.0 - - - - - - 88.0 - -", "question": "What is the highest score for the first metric in the table?\n", "answer": "The highest score for the first metric is 82.3, for the Gemini Ultra model.", "source": "multimodal.pdf", "id": "b8cafcd1b9"}, {"context": "to the data filtering strategy. This is achieved by simply modifying the Maximum Likelihood\nEstimation (MLE), enabling the model to mitigate hallucination through learning from regular\ninstruction data.\n5.3.2 Reinforcement Learning. Reinforcement learning (RL) is introduced to train MLLMs for\nmitigating hallucinations by conducting the following perspectives: 1) Automatic Metric-based\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How is reinforcement learning used to mitigate hallucinations in MLLMs?\n", "answer": "Reinforcement learning is introduced to train MLLMs for mitigating hallucinations by using automatic metric-based pre-print data.", "source": "hallucination.pdf", "id": "11d0900242"}, {"context": "the development of novel technologies.\n\u2022 Efficient Vision explores optimizing efficient visual fracture extraction strategies, empha-\nsizing methods that boost efficiency while maintaining accuracy. It addresses integrating\nhigh-quality visual data for effective cross-modal understanding.\n\u2022 Efficient LLMs explores these strategies of improving the computational efficiency and\nscalability of language models. It examines the trade-offs between model complexity and\nperformance while suggesting promising avenues for balancing these competing factors.\n2", "question": "How does Efficient Vision aim to optimize visual fracture extraction strategies?\n", "answer": "Efficient Vision explores optimizing visual fracture extraction strategies by emphasizing methods that enhance efficiency without compromising accuracy. It also focuses on integrating high-quality visual data for effective cross-modal understanding.", "source": "multimodal.pdf", "id": "f53fc9e54d"}, {"context": "desired answer is \u2019I don\u2019t know\u2019. The concept is defined as \u2019I Know (IK)\u2019 hallucination in the work\nof [11]. Accordingly, a new benchmark, VQAv2-IDK, is proposed to specifically evaluate this type of\nhallucination. VQAv2-IDK is a subset of VQAv2 comprising unanswerable image-question pairs as\ndetermined by human annotators. In this benchmark, \u2019I Know (IK)\u2019 hallucination has been further\ncategorized into four types:\n\u2022Unanswerable: no one can know.\n\u2022Don\u2019t know: human may not know, but robot might.\n\u2022False questions: refers non-existing.\n\u2022Not sure: ambiguous to answer.\nThis benchmark opens a new track for the study of hallucination in MLLMs.\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.", "question": "How does the VQAv2-IDK benchmark categorize \"I Know (IK)\" hallucination in machine-learned language models?\n", "answer": "The VQAv", "source": "hallucination.pdf", "id": "d18c108916"}, {"context": "where xiis the training data, piis the positive sample, and\nnjis the negative sample,sim(x,y) is to calculate the simi-\nlarity between x and y. Another study has chosen to further\nstreamline the quantity of documents, aiming to enhance the\nmodel\u2019s answer accuracy by reducing the number of retrieved\ndocuments. [Maet al. , 2023b ]proposed the \u201cFilter-Ranker\u201d\nparadigm, which integrates the strengths of Large Language\nModels (LLMs) and Small Language Models (SLMs). In this\nparadigm, SLMs serve as filters, while LLMs function as re-\nordering agents. By prompting LLMs to rearrange portions\nof difficult samples identified by SLMs, the research results\nindicate significant improvements across various Information\nExtraction (IE) tasks.\nRerank\nThe pivotal role of the reordering model lies in optimizing\nthe set of documents retrieved from retriever. LLMs ex-\nperience performance degradation with retrospective perfor-\nmance when additional context is added, and reordering pro-", "question": "How do Large Language Models (LLMs) help improve information extraction tasks in the \"Filter-Ranker\" paradigm?\n", "answer": "LLMs function as reordering agents in the \"Filter-Ranker\" paradigm, optimizing the set of documents retrieved from the retriever. They help improve information extraction tasks by rearranging portions of difficult samples identified by Small Language Models (SLMs).", "source": "RAG.pdf", "id": "04132c5b1f"}, {"context": "Techniques such as sliding window technology implement\nlayered retrieval by aggregating globally related information\nthrough multiple retrievals. The Small2big technique uti-\nlizes small text blocks during the search process and provides\nlarger affiliated text blocks to the language model for pro-\ncessing. The Abstract embedding technique performs Top K\nretrieval on document abstracts, offering full document con-\ntext. The Metadata Filtering technique leverages document\nmetadata for filtering. The Graph Indexing technique con-\nverts entities and relationships into nodes and connections,\nsignificantly enhancing relevance in the context of multi-hop\nissues. The amalgamation of these methods has resulted in\nimproved retrieval outcomes and enhanced performance for\nRAG.\nFine-tuning Embedding Models\nAfter getting the proper size of Chunks, we need to Em-\nbedding the chunks and query in the semantic space by an\nEmbedding model, so it is crucial whether Embedding can", "question": "How does the Small2big technique handle text blocks during search?\n", "answer": "The Small2big technique handles text blocks by using small blocks during the search process and providing larger affiliated text blocks to the language model for processing.", "source": "RAG.pdf", "id": "0959faf412"}, {"context": "the retrieved documents as latent variables. Perplexity Dis-\ntillation directly trains using the perplexity of the model-\ngenerated tokens as an indicator.LOOP introduces a new loss\nfunction based on the effect of document deletion on LM\nprediction, providing an effective training strategy for better\nadapting the model to specific tasks.\nPlug in an adapter However, fine-tuning an embed-\nding model can be challenging due to factors such as\nutilizing an API to implement embedding functionality\nor insufficient local computational resources. There-\nfore, some works choose to externally attach an adapter\nfor alignment.PRCA [Yang et al. , 2023b ]trains the Adapter\nthrough the Contextual Extraction Stage and the Reward-\nDriven Stage, and optimizes the output of the re-\ntriever based on a token-based autoregressive strategy.\nTokenFiltering [Berchansky et al. , 2023 ]method calculates\ncross-attention scores, selecting the highest scoring input to-", "question": "How does PRCA train the adapter?\n", "answer": "PRCA trains the adapter through the Contextual Extraction Stage and the Reward-Driven Stage.", "source": "RAG.pdf", "id": "af13cfcd4c"}, {"context": "VTP[110], PS-ViT[111]\nHybrid Pruning SPViT [112], ViT-Slim [113]\nKnowledge Distillation (\u00a73.3)Homomorphic KDDeiT [114], TinyViT [115], m2mKD [116],\nDeiT-Tiny [117], MiniViT [118]\nHeteromorphic KD DearKD [119], CiT [120]\nQuantization (\u00a73.4)Post-Training QuantizationPTQ4ViT [121], APQ-ViT [122],\nNoisyQuant [123]\nQuantization-Aware TrainingQuantformer [124] Bit-shrinking [125],\nQ-ViT [126], TerViT [127], BiViT [128],\nPackQViT [129], BinaryViT [130]\nHardware-Aware Quantization GPUSQ-ViT[131], Auto-ViT-Acc [132]\nFigure 9: Organization of efficient vision advancements.\n10", "question": "What is a hardware-aware quantization method for ViTs?\n", "answer": "GPUSQ-ViT", "source": "multimodal.pdf", "id": "8a087225e4"}, {"context": "20 Bai, et al.\nfocuses more on the image information. The image-based model is created by modifying the\nattention weight matrix structure within the original model, without altering its parameters. This\napproach emphasizes the knowledge of the image-biased model and diminishes that of the original\nmodel, which may be text-biased. Thus, it encourages the extraction of correct content while\nsuppressing hallucinations resulting from textual over-reliance.\nGuided Decoding. MARINE [ 131] proposes a training-free approach. It employs an additional\nvision encoder for object grounding and utilizes the grounded objects to guide the decoding process.\nSpecifically, it innovatively adapts the classifier-free guidance [ 40] technique to implement guided\ndecoding, showing promising performance in emphasizing the detected objects while reducing\nhallucination in the text response.\nSimilarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that", "question": "How does MARINE implement guided decoding?\n", "answer": "MARINE implements guided decoding by employing an additional vision encoder for object grounding and utilizing the grounded objects to guide the decoding process, using the classifier-free guidance technique.", "source": "hallucination.pdf", "id": "9e707211bd"}, {"context": "Figure 14: Training stages of efficient MLLMs.\nusing a standard cross-entropy loss function:\nmax\n\u03b8LX\ni=1logp\u03b8(xi|Xv, Xinstruct , Xa,<i), (4)\nwhere Lis the length of Xaand\u03b8denotes the trainable parameters. In order to better align different\nmodalities of knowledge and avoid catastrophic forgetting during the pre-training stage, \u03b8typically\nincludes only a learnable modality interface, i.e., a vision-language projector.\nWhich part to unfreeze? Considering that only training the connector may not well align the\nvision and text information when using SLMs, TinyLlava[23] also opt to partially freeze pre-\ntrained modules (i.e. vision encoder and SLM) to activate more parameters for learning alignment.\nVILA[49] reveals that updating the base LLM throughout the pre-training stage is essential to in-\nheriting some of the appealing LLM properties like in-context learning. ShareGPT4V[55] found\nthat unfreezing more parameters, particularly in the latter half of the vision encoder\u2019s layers, proves", "question": "How does TinyLlava adjust pre-trained modules during pre-training?\n", "answer": "TinyLlava partially freezes pre-trained vision encoder and symmetric language model (SLM) modules to activate more parameters for learning alignment during the pre-training stage.", "source": "multimodal.pdf", "id": "1bd741e7c9"}, {"context": "Organization of this survey. In this paper, we present a comprehensive survey of the latest\ndevelopments regarding hallucinations in MLLMs. The survey is organized as follows: We begin by\nproviding sufficient context and defining concepts related to LLMs, MLLMs, hallucination, etc. Next,\nwe delve into an in-depth analysis of the factors contributing to hallucinations in MLLMs. Following\nthis, we present a set of metrics and benchmarks employed for evaluating hallucinations in MLLMs.\nWe then elaborate on a range of approaches designed to mitigate hallucinations in MLLMs. Finally,\nwe delve into the challenges and open questions that frame the current limitations and future\nprospects of this field, offering insights and delineating potential pathways for forthcoming research.\n2 DEFINITIONS\n2.1 Large Language Models\nBefore moving to multimodal large language models, it is essential to introduce the concept of large", "question": "What is the focus of the survey presented in the paper?\n", "answer": "The focus of the survey presented in the paper is the latest developments regarding hallucinations in multimodal large language models (MLLMs).", "source": "hallucination.pdf", "id": "e1b4ac9436"}, {"context": "employs a graph encoding method that reflects the graph\nstructure into PTMs\u2019 representation space and utilizes a\nmulti-modal contrastive learning objective between graph-\ntext modes to ensure consistency between retrieved facts\nand generated text. KnowledgeGPT [Wang et al. , 2023c ]\ngenerates search queries for Knowledge Bases (KB) in code\nformat and includes predefined KB operation functions.\nApart from retrieval, KnowledgeGPT also offers the ca-\npability to store knowledge in a personalized knowledge\nbase to meet individual user needs. These structured data\nsources provide RAG with richer knowledge and context,\ncontributing to improved model performance.\nLLM Generated Content RAG\nObserving that the auxiliary information recalled by RAG\nis not always effective and may even have negative effects,\nsome studies have expanded the paradigm of RAG by delving\ndeeper into the internal knowledge of LLM. This approach\nutilizes the content generated by LLM itself for retrieval, aim-", "question": "How does KnowledgeGPT utilize the content generated by LLM for retrieval?\n", "answer": "Some studies have expanded the paradigm of RAG by having KnowledgeGPT utilize the content generated by LLM itself for retrieval.", "source": "RAG.pdf", "id": "4c3ac6cb2e"}, {"context": "models and retrieval-Augmented generation. They become\nfamiliar with the evolutionary path and key technologies of\nknowledge retrieval augment, enabling them to discern the\nadvantages and disadvantages of different techniques, iden-\ntify applicable scenarios, and explore current typical applica-\ntion cases in practice.It is noteworthy that in previous work,\nFeng el al. [2023b ]systematically reviewed the methods, ap-\nplications, and future trends of combining large models with\nknowledge, with a primary focus on knowledge editing and\nretrieval augmentation methods. Zhu et al. [2023 ]introduced\nthe latest advancements in augmenting retrieval systems for\nLarge Language Models, with a specific focus on the retrieval\nsystem. Meanwhile, Asai et al. [2023a ]focusing on ques-\ntions such as \u201cWhat\u201d, \u201cWhen\u201d, \u201cHow\u201d, analyzed and eluci-\ndated the key processes in Retrieval-based Language Mod-\nels. In comparison with them, this paper aims to systemati-", "question": "Who introduced the latest advancements in augmenting retrieval systems for Large Language Models?\n", "answer": "Zhu et al. [2023]", "source": "RAG.pdf", "id": "fe157c6a57"}, {"context": "customize model behavior or writing style.Allows adjustments of LLM behavior, writ-\ning style, or specific domain knowledge\nbased on specific tones or terms.\nInterpretabilityAnswers can be traced back to specific data\nsources, providing higher interpretability and\ntraceability.Like a black box, not always clear why the\nmodel reacts a certain way, with relatively\nlower interpretability.\nComputational ResourcesRequires computational resources to support\nretrieval strategies and technologies related\nto databases. External data source integration\nand updates need to be maintained.Preparation and curation of high-quality\ntraining datasets, definition of fine-tuning\nobjectives, and provision of corresponding\ncomputational resources are necessary.\nLatency RequirementsInvolves data retrieval, potentially leading to\nhigher latency.LLM after fine-tuning can respond without\nretrieval, resulting in lower latency.\nReducing HallucinationsInherently less prone to hallucinations as", "question": "Why are fine-tuned language models less prone to hallucinations?\n", "answer": "Fine-tuned language models are less prone to hallucinations because they are adjusted with specific domain knowledge and based on specific data sources, which makes their answers more interpretable and traceable.", "source": "RAG.pdf", "id": "5f646e4ddc"}, {"context": "benchmarks employed to assess the performance of these models. The discussion highlights the\nimportance of diverse and high-quality datasets in achieving robust and accurate MLLMs, as well\nas the various strategies employed to generate and refine these datasets. Furthermore, we present a\ncomprehensive comparison of MLLM performance across established benchmarks, emphasizing the\nneed for a thorough evaluation to ensure the effectiveness of these models in real-world applications.\n6.1 Pre-Training Data\nPre-training data primarily serve two critical objectives: (1) promoting the integration of various\nmodalities and (2) conveying comprehensive knowledge. Large-scale image-caption pair datasets\nnaturally fulfill these requirements. Firstly, they predominantly originate from the internet, provid-\ning an extensive data volume with a broad knowledge coverage. Secondly, the direct alignment\nbetween the two modalities is beneficial for training modality projectors. However, captions in such", "question": "What is one primary use of pre-training data in Multimodal Language Learning Models?\n", "answer": "Pre-training data primarily serve to promote the integration of various modalities in Multimodal Language Learning Models.", "source": "multimodal.pdf", "id": "8f79f68811"}, {"context": "costs. Module-level methods involve segregating teacher modules from a pre-trained unified model,\nand student modules from a modular model. In m2mKD [116], these modules are combined with\na shared meta-model, allowing the student module to emulate the behavior of the teacher module.\nFeature-level KD methods, as demonstrated by MiniViT [118], combine the weights of consecutive\ntransformer blocks. This entails sharing weights across layers while introducing transformations to\n12", "question": "How does the MiniViT method combine the weights in feature-level knowledge distillation?\n", "answer": "The MiniViT method combines the weights of consecutive transformer blocks by sharing weights across layers and introducing transformations.", "source": "multimodal.pdf", "id": "1795a81d37"}, {"context": "additional LResNet blocks, which facilitate the abstraction of visual features to any squared num-\nber of visual tokens. Conversely, D-Abstractor, or Deformable attention-based Abstractor utilizes\ndeformable attention, which maintains the local context through a 2-D coordinate-based sampling\nprocess, using reference points and sampling offsets.\n6", "question": "How does D-Abstractor maintain the local context in visual feature abstraction?\n", "answer": "D-Abstractor, or Deformable attention-based Abstractor, maintains the local context through a 2-D coordinate-based sampling process, using reference points and sampling offsets.", "source": "multimodal.pdf", "id": "3a3d9edb48"}, {"context": "ChipNeMo: Domain-Adapted LLMs for Chip Design\ntask assignnment. Participants are tasked with rating the\nmodel\u2019s performance on a 7-point Likert scale for each of\nthese three assignments. The results can be found in Fig-\nure 10. Although the GPT-4 model excels in all three tasks,\noutperforming both our ChipNeMo-70B-Steer model and\nthe LLaMA2-70B-Chat model, ChipNeMo-70B-Steer does\nexhibit enhancements compared to the off-the-shelf LLaMA\nmodel of equivalent size. We attribute the comparatively\nlower improvements in summarization tasks resulting from\nour domain-adaptation to the limited necessity for domain-\nspecific knowledge in summarization compared to other\nuse-cases.\n4. Related Works\nMany domains have a significant amount of proprietary data\nwhich can be used to train a domain-specific LLM. One ap-\nproach is to train a domain specific foundation model from\nscratch, e.g., BloombergGPT(Wu et al., 2023) for finance,\nBioMedLLM(Venigalla et al., 2022) for biomed, and Galac-", "question": "How does the ChipNeMo-70B-Steer model compare to the off-the-shelf LLaMA model in chip design tasks?\n", "answer": "The ChipNeMo-70B-Steer model exhibits enhancements compared to the off-the-shelf LLaMA model of equivalent size in chip design tasks.", "source": "ChipNemo.pdf", "id": "74fe22ec46"}, {"context": "Figure 1: A timeline of existing RAG research. The timeline was established mainly according to the release date.\ncuses on introducing the generator in RAG.Chapter 6 em-\nphasizes the introduction of the augmentation methods in\nRAG.Chapter 7 introduces the evaluation system of RAG.\nChapter 8 provides an outlook on the future development\ntrends of RAG. Finally, in Chapter 9, we summarize the main\ncontents of the survey.\n2 Background\nIn this chapter, we will introduce the definition of RAG, as\nwell as the comparison between RAG and other model opti-\nmization techniques, such as fine-tuning.\n2.1 Definition\nThe meaning of RAG has expanded in tandem with techno-\nlogical developments. In the era of Large Language Mod-\nels, the specific definition of RAG refers to the model, when\nanswering questions or generating text, first retrieving rele-\nvant information from a vast corpus of documents. Subse-\nquently, it utilizes this retrieved information to generate re-", "question": "What is the specific definition of RAG in the era of Large Language Models?\n", "answer": "In the era of Large Language Models, RAG (Retrieval-Augmented Generation) refers to a model that, when answering questions or generating text, first retrieves relevant information from a vast corpus of documents and then utilizes this retrieved information to generate a response.", "source": "RAG.pdf", "id": "7470fe30c9"}, {"context": "iments show that it not only achieves competitive performance with state-of-the-art efficient meth-\nods but also boasts faster speeds due to its linear sequential modeling.It also excels in overcom-\ning visual illusions and spatial relationship judgments in closed-set challenging prediction bench-\nmarks and achieves performance comparable to LLaV A while using only 43% of the parameters.\nVL-Mamba[18] substitutes the Transformer-based backbone language model with the pre-trained\nMamba language model. It explores how to effectively implement the 2D vision selective scan\nmechanism for multimodal learning and the combinations of different vision encoders and pre-\ntrained Mamba language model variants.\nInference Acceleration SPD[45] proposes the speculative decoding with a language-only model\nto improve inference efficiency. By employing a language-only model as a draft model for specu-\n9", "question": "Which model is used as a draft model for speculative decoding to improve inference efficiency?\n", "answer": "A language-only model is used as a draft model for speculative decoding to improve inference efficiency.", "source": "multimodal.pdf", "id": "6bebc6e320"}, {"context": "mising the inference speed. MoE-LLaV A[25] presents an MoE-based sparse MLLM framework\nthat effectively increases the number of parameters without compromising computational efficiency.\nFurthermore, it introduces MoE-Tuning, a three-stage training strategy designed to adapt MoE [89]\nto MLLMs and prevent model degradation caused by sparsity. MM1[30] designs two variants of\nMoE models. The first is a 3B-MoE model that employs 64 experts and substitutes a dense layer\nwith a sparse one every two layers. The second is a 7B-MoE model that utilizes 32 experts and\nsubstitutes a dense layer with a sparse one every four layers.\nMamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modal-\nity and explores different modal fusion schemes to develop an effective multi-modal Mamba. Exper-\niments show that it not only achieves competitive performance with state-of-the-art efficient meth-", "question": "How does Mamba Cobra [13] incorporate the efficient Mamba [77] language model into the vision modality?\n", "answer": "Mamba Cobra [13] incorporates the efficient Mamba [77] language model into the vision modality by exploring different modal fusion schemes to develop an effective multi-modal Mamba.", "source": "multimodal.pdf", "id": "5510d4cc4e"}, {"context": "as image captions.\nSimilarly, RLHF-V [ 119] also employs the RLHF paradigm to enhance the pre-trained MLLM.\nSpecifically, this work emphasizes two improvements: 1) at the data level, it proposes to collect\nhuman feedback in the form of fine-grained segment-level corrections, providing a clear, dense,\nand fine-grained human preference. 2) at the method level, it proposes dense direct preference\noptimization (DDPO) that directly optimizes the policy model against dense and fine-grained\nsegment-level preference.\nAnother similar work, ViGoR [ 110], also designs a fine-grained reward model to update pre-\ntrained MLLMs, aiming to improve visual grounding and reduce hallucination. The reward modeling\nin this work encompasses both human preferences and automatic metrics. Specifically, it collects\nhuman judgment and preferences for the responses generated by MLLMs by asking crowd-workers\nto provide fine-grained feedback at the sentence level. The collected human preference data is", "question": "How does ViGoR collect human feedback for its reward model?\n", "answer": "ViGoR collects human feedback for its reward model by asking crowd-workers to provide fine-grained feedback at the sentence level.", "source": "hallucination.pdf", "id": "9da785fedf"}, {"context": "kens (e.g., kNN-LM [Khandelwal et al. , 2019 ]), phrases (e.g.,\nNPM [Leeet al. , 2020 ], COG [Vaze et al. , 2021 ]), and docu-\nment paragraphs. Finer-grained retrieval units can often bet-\nter handle rare patterns and out-of-domain scenarios but come\nwith an increase in retrieval costs.\nAt the word level, FLARE employs an active retrieval strat-\negy, conducting retrieval only when the LM generates low-\nprobability words. The method involves generating a tempo-\nrary next sentence for retrieval of relevant documents, then\nre-generating the next sentence under the condition of the re-\ntrieved documents to predict subsequent sentences.\nAt the chunk level, RETRO uses the previous chunk to re-\ntrieve the nearest neighboring chunk and integrates this infor-\nmation with the contextual information of the previous chunk\nto guide the generation of the next chunk. RETRO achieves\nthis by retrieving the nearest neighboring block N(Ci\u22121)\nfrom the retrieval database, then fusing the contextual in-", "question": "How does the RETRO system retrieve and integrate information for chunk-level language generation?\n", "answer": "RETRO retrieves the nearest neighboring chunk (N(Ci\u22121)) from the retrieval database and integrates this information with the contextual information of the previous chunk to guide the generation of the next chunk.", "source": "RAG.pdf", "id": "92f5901d31"}, {"context": "generated content remains consistent and contextually relevant to the input modality requires\nsophisticated techniques for capturing and modeling cross-modal relationships. The direction of\ncross-modal alignment encompasses both MLLMs training and hallucination evaluation. Regarding\ntraining, future research should explore methods for aligning representations between different\nmodalities. Achieving this goal may involve designing more advanced architectures, introducing\nadditional learning objectives [ 52], or incorporating diverse supervision signals [ 16]. Regarding\nevaluation, cross-modal consistency checking has been a long-standing topic, ranging from multi-\nmodal understanding [ 66,88] to text-to-image generation [ 13,17]. Drawing on proven experiences\nfrom these domains to improve the assessment of MLLM hallucination, or unifying them into an\noverall framework, may be promising research directions.\n6.3 Advancements in Model Architecture", "question": "How can cross-modal alignment be improved in MLLMs training?\n", "answer": "Cross-modal alignment in MLLMs training can be improved by designing more advanced architectures, introducing additional learning objectives, or incorporating diverse supervision signals.", "source": "hallucination.pdf", "id": "83c3718d9d"}, {"context": "global batch size is set at 256, and a context window of 4096\ntokens is applied, resulting in an effective batch size of 1M\ntokens. The total number of training steps is set to 23,200,\nequating to roughly 1 epoch of the data blend.\nFigure 2: Smoothed Training Loss for ChipNeMo with Tokenizer\nAugmentation.\nFigure 2 illustrates the training loss of ChipNeMo under\nthe specified hyperparameters. We do observe spikes in the\ntraining loss. In contrast to the hypothesis in (Chowdhery\net al., 2022), we postulate that in our scenario, these spikes\ncan be attributed to \u201cbad data\u201d since these irregularities\nseem to consistently occur in similar training steps for the\nsame model, even across different model sizes. We chose\nnot to address this issue, as these anomalies did not appear\nto significantly impede subsequent training steps (with no\nnoticeable degradation in validation loss), possibly due to\n3", "question": "What is the total number of training steps in the scenario described?\n", "answer": "The total number of training steps is 23,200.", "source": "ChipNemo.pdf", "id": "1162f7259e"}, {"context": "Figure 2: RAG compared with other model optimization methods\nnew knowledge into the model or for situations that demand\nquick iteration for new use cases.\nFine-tuning is similar to having students internalize knowl-\nedge through prolonged learning. This method is applicable\nwhen the model needs to replicate specific structures, styles,\nor formats. Fine-tuning can achieve performance superior to\nnon-fine-tuned models, and interactions are more efficient.\nFine-tuning is particularly suitable for emphasizing existing\nknowledge in the base model, modifying or customizing the\nmodel\u2019s output, and instructing the model with complex di-\nrectives. However, fine-tuning is not suitable for adding new\nknowledge to the model or for scenarios that require rapid it-\neration for new use cases. The specific comparison between\nRAG and Fine-tuning (FT) can be elucidated in Table 1.\nRAG and fine-tuning are not mutually exclusive but can\ncomplement each other, enhancing the model\u2019s capabilities at", "question": "When is fine-tuning not suitable according to the context?\n", "answer": "Fine-tuning is not suitable for adding new knowledge to the model or for scenarios that require rapid iteration for new use cases.", "source": "RAG.pdf", "id": "8ce5ce445b"}, {"context": "current large-scale language models, models like RWKV [151] and Mamba [77] have emerged as\npopular solutions for achieving heightened efficiency and processing lengthy texts. These innovative\nmodels have demonstrated attributes similar to transformers, including the ability to handle long-\nrange dependencies and parallel processing.RWKV model leverages a linear attention mechanism,\nenabling us to formulate the model as either a Transformer or a Recurrent Neural Network (RNN).\n15", "question": "Which attention mechanism does the RWKV model use to handle long-range dependencies?\n", "answer": "The RWKV model uses a linear attention mechanism to handle long-range dependencies.", "source": "multimodal.pdf", "id": "bdf1fcbf41"}, {"context": "Similarly, GCD [ 24] devises a CLIP-Guided Decoding (GCD) approach. It first verifies that\nCLIPScore [ 88] can effectively distinguish between hallucinated and non-hallucinated sentences\nthrough a series of studies across different models and datasets. Based on this conclusion, it further\nrecalibrates the decoding process of MLLMs, including two steps: 1) reliability scoring, which\ndesigns a (CLIP-based) scoring function aiming to assign higher scores to candidate responses that\nare less likely to be hallucinated, and 2) guided sentence generation, which generates responses\nbased on this scoring. This is implemented in a similar way to beam search but at the sentence\nlevel.\nHALC [ 15] provides a key insight that when decoding a specific token in the MLLM, identifying a\ntoken-wise optimal visual context to provide the most informative visual grounding can effectively\nreduce hallucination. Visual context refers to the visual tokens that can be grounded from the", "question": "How can a token-wise optimal visual context reduce hallucination in MLLMs?\n", "answer": "By providing the most informative visual grounding when decoding a specific token in the MLLM, it can effectively reduce hallucination.\n\nHere, MLLMs refer to Multimodal Language Learning Models. The factoid answer is derived from the context, specifically the key insight provided by HALC [15].", "source": "hallucination.pdf", "id": "17a462daf3"}, {"context": "prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a92024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 0000-0000/2024/4-ART\nhttps://doi.org/XXXXXXX.XXXXXXX\nPreprint, Vol. 1, No. 1, Article . Publication date: April 2024.arXiv:2404.18930v1  [cs.CV]  29 Apr 2024", "question": "In what month and year was the preprint with arXiv ID 2404.1893v1 published in the field of computer vision?\n", "answer": "The preprint was published in the field of computer vision in April 2024.", "source": "hallucination.pdf", "id": "35a7709274"}, {"context": "language models. Typically, LLMs encompass a range of transformer-based models that are ex-\ntensively trained on vast textual datasets. Prominent examples include GPT-3 [ 8], PaLM [ 18],\nLLaMA [ 99], and GPT-4 [ 82]. Through scaling both data volume and model capacity, LLMs\ndemonstrate notable emergent capabilities, including In-Context Learning[ 8], Chain-of-Thought\nprompting[107] and instruction following[86], among others.\nThe characteristics and behaviors of LLMs are intricately linked to their training processes. LLMs\ntypically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and\nReinforcement Learning from Human Feedback (RLHF). Below, we provide a concise overview of\neach stage to facilitate comprehension.\nPre-trianing. Pre-training serves as a fundamental phase in the learning process of LLMs [ 134].\nDuring this stage, language models engage in autoregressive prediction, wherein they predict the", "question": "What is the first stage of training for large language models?\n", "answer": "Pre-training is the first stage of training for large language models.", "source": "hallucination.pdf", "id": "0d2806c1d5"}, {"context": "input document. The objective of the training process is to\nminimize the discrepancy between Cextracted and the actual\ncontext Ctruth as much as possible. The loss function they\nadopted is as follows:\nminL (\u03b8) =\u22121\nNNX\ni=1C(i)\ntruthlog(f.(S(i)\ninput;\u03b8)) (3)\nwhere f.is the information extractor and \u03b8is the parameter\nof the extractor. RECOMP [Xuet al. , 2023a ]similarly trains\nan information condenser by leveraging contrastive learning.\nFor each training data point, there exists one positive sample\nand five negative samples. The encoder is trained using con-\ntrastive loss [Karpukhin et al. , 2020 ]during this process.The\nspecific optimization goals are as follows:\n\u2212logesim(xi,pi)\nsim(xi, pi) +P\nnj\u2208Niesim(xi,pi)(4)", "question": "What is the optimization goal of the information condenser training process in RECOMP [Xuet al., 2023a]?\n", "answer": "The optimization goal is to minimize the contrastive loss, which involves maximizing the similarity between a data point and its positive sample while minimizing the similarity between the data point and negative samples.", "source": "RAG.pdf", "id": "ba4f3a6fe9"}, {"context": "to attention mechanisms by providing near-linear computational complexity and effectively captur-\ning long-range dependencies. With continuous advancements and refinements, SSMs are poised to\nbecome an influential approach in the field of deep learning and sequence processing.\n4.3 Fine-Tuning\nFine-tuning, as the primary stage for adapting LLMs to downstream tasks and training MLLLMs to\nfollow visual instructions, plays a crucial role in enhancing the efficiency of LLMs.\nParameter-Efficient Fine-Tuning Parameter-Efficient Fine-Tuning (PEFT) is an approach that\naims to achieve high performance with fewer parameters in Large Language Models (LLMs). Tech-\nniques such as adapter-based tuning and low-rank adaptation provide effective solutions to mitigate\nthe computational and memory challenges associated with fine-tuning LLMs while maintaining their\nexpressiveness and generalization capabilities. Adapter-based tuning introduces lightweight adapter", "question": "How does adapter-based tuning help with fine-tuning large language models?\n", "answer": "Adapter-based tuning introduces lightweight adapter modules to the model, allowing for fine-tuning with fewer parameters. This approach helps mitigate computational and memory challenges while maintaining the expressiveness and generalization capabilities of the models.", "source": "multimodal.pdf", "id": "0e48bbbaa3"}, {"context": "code generation tasks (Zhou et al., 2023) by retrieving from\ncoding documents.Foundation models are completion models, which have lim-\nited chat and instruction following capabilities. Therefore, a\nmodel alignment process is applied to the foundation models\nto train a corresponding chat model. Instruction fine-tuning\n(Wei et al., 2022) and reinforcement learning from human\nfeedback (RLHF) (Ouyang et al., 2022) are two common\nmodel alignment techniques. Instruction fine-tuning further\ntrains a foundation model using instructions datasets. RLHF\nleverages human feedback to label a dataset to train a re-\nward model and applies reinforcement learning to further\nimprove models given the trained reward model. RLHF is\nusually more complex and resource hungry than instruction\nfine-tuning. Therefore, recent studies also propose to reduce\nthis overhead with simpler methods such as DPO (Rafailov\net al., 2023) and SteerLM (Dong et al., 2023).", "question": "What is one method to reduce the complexity and resource intensity of RLHF in model alignment?\n", "answer": "One method is DPO, proposed by Rafailov et al. in 2023.", "source": "ChipNemo.pdf", "id": "dd25ad58ad"}]