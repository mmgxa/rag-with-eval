<div align="center">

# RAG Frontend with Monitoring

</div>

## Project Overview

This repository is RAG application where a knowledge base stores data and the user can send a query via a frontend and an LLM is used to generate the answer. The database provided contains the text chunks and embeddings for a few sample research papers.

## Tools Used

- **Amazon Bedrock:** For access to the Llama 3.1 Models
- **LanceDB:** As the database/knowledge base
- **Docker (Compose):** To containerize the different components
- **Grafana:** To display the metrics logged in the PostgreSQL database
- **Streamlit:** The frontend UI
- **Kestra:** Data ingestion workflow
- **PostgreSQL:** The Relational database used to store metrics such as tokens, feedback, model usage etc.

## Launching the App

First, build the frontend in the `frontend` folder using

```
docker build -t frontend .
```

Then, start the stack using

```
docker compose up
```

**Iff** you need to add more data, you can run the `ingest.ipynb` notebook as a quick hack.  
**Be careful** since the volume directory already has the dataset ingested beforehand!

Notes:

- First, minio (an S3 compatible storage) was used for storing LanceDB database. Unfortunately, it doesn't support text-search as of now. So LanceDB is stored on disk directly.
- The app is designed to be used with Amazon Bedrock. You need to provide credentials in the `.env` file to run the app. If using any other provider/API, feel free to modify the `llm.py` file.

## Grafana Dashboard

Grafana is used to deploy a few metrics e.g.

- number of conversations
- user feedback count (positive vs negative)
- token count
- model used
- response time

Using volume mounts, the dashboard and datasources are automatically set up along the docker image.

![](./img/grafana_dashboard.png)

## Folder Structure

```sh
â””â”€â”€ rag-with-eval
    â””â”€â”€ ğŸ“data                              # Folder that contains pdf files as a source
    â””â”€â”€ ğŸ“docker                            # Holds config files to be mounted for docker containers
        â””â”€â”€ ğŸ“grafana                       # pre-populated grafana dashboard and datasource
    â””â”€â”€ ğŸ“evaluation                        # Top-level directory for our evaluation
        â””â”€â”€ ğŸ“db                            # directory where database will be stored (chunks, embeddings)
        â””â”€â”€ ğŸ“img                           # images of sample outputs as well as results
        â””â”€â”€ ğŸ“results                       # results from each step in our evaluation
        â””â”€â”€ ğŸ“script
            â””â”€â”€ evaluate.py                 # top-level functions to evaluate the dataset
            â””â”€â”€ ingest.py                   # Inserts data  into the vector database
            â””â”€â”€ parse.py                    # used to create splits of our documents
            â””â”€â”€ prompts.py                  # Defines our prompts for LLM-as-a-Judge and RAG
            â””â”€â”€ query.py                    # Defines our helper functions to invoke LLMsquery the database
            â””â”€â”€ score.py                    # Defines our helper functions to evaluate retireval
        â””â”€â”€ ğŸ“utils
            â””â”€â”€ config.py                   # function to get project root
            â””â”€â”€ embeddings.py               # Defines our helper functions to create embeddings
            â””â”€â”€ id_gen.py                   # A simple function to create hashes out of text strings
            â””â”€â”€ models.py                   # Defines our helper functions to invoke LLMs
            â””â”€â”€ vecdb.py                    # contains functions to create/load LanceDB table
        â””â”€â”€ .env                            # Loads environment variable for invoking AWS/Together
        â””â”€â”€ evaluate.ipynb                  # Notebook to run the evaluation steps
        â””â”€â”€ README.md                       # README for the evaluation
    â””â”€â”€ ğŸ“frontend                          # Top level directory for the frontend docker image
        â””â”€â”€ app.py                          # Entry-point for the Streamlit UI
        â””â”€â”€ db.py                           # Contains python functions to interact with the PostgreSQL database
        â””â”€â”€ Dockerfile                      # Dockerfile for the frontend container
        â””â”€â”€ llm.py                          # Contains Python functions to invoke the LLM as well as the prompt templates
        â””â”€â”€ requirements.txt                # Lists of Python dependencies for the project
        â””â”€â”€ sqlqueries.py                   # Contains SQL queries needed to ingest metrics into the PostgreSQL database
        â””â”€â”€ util.py                         # Contains Python functions for rerankers, embedding models, searching vector database
    â””â”€â”€ ğŸ“img                               # Images displayed on Github README
        â””â”€â”€ grafana_dashboard.png           # A sample image for the deployed Grafana dashboard
    â””â”€â”€ ğŸ“volumes                           # Volume mounts for the different containers
    â””â”€â”€ .env                                # Loads environment variable for invoking AWS/Together
    â””â”€â”€ docker-compose.yml                  # Docker compose for launching the different containers.
    â””â”€â”€ ingest.ipynb                        # A quick-hack setup for ingesting (more) data into the vector database
    â””â”€â”€ README.md                           # You are here!
```


## Evaluation

Both retrieval and generation evaluation has been done. For the former, hit-rate and MRR were used as the metrics and for the latter, we use [Prometheus 2 (7B)](https://github.com/prometheus-eval/prometheus-eval) as the LLM-as-a-judge evaluator.

For details, see [here](./evaluation/README.md)
